<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
							<email>aviralk@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrial Engineering and Computer Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
							<email>abhigupta@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrial Engineering and Computer Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<email>svlevine@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrial Engineering and Computer Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Blog post presenting a summary of this work is available at: https://bair.berkeley.edu/blog/2020/03/16/discor/.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides "hard negatives" that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon "corrective feedback." We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function. In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. def _ i n i t _ c r i t i c _ u p d a t e _ w i t h _ d i s t ( self ) : 2 """ Update critic with distribution weighting , 3 and update \ delta_ \ phi using recursive update . """ 4 next_actions = self . _policy . actions ([ self . _n ext_ obser vati ons_p h ]) 5 6 # # Compute errors at next state , and an action from the policy 7 qf_pred_errs = self . _error_fns ([ self . _next_observations_ph , next_actions ]) 8 9 # # error_model_tau_ph : moving mean of the error values over batches 10 err_logits = -tf . stop_gradient ( 11 self . _discount * qf_pred_errs / self . _error_model_tau_ph ) 12 13 Q_target = tf . stop_gradient ( self . _get_Q_target () ) 14 Q_values = self . _Q ([ self . _observations_ph , self . _actions_ph ]) 15 16 # # Compute importance sampled loss , also perform self -normalized sampling 17 loss , weights = im po r ta n ce _s a mp le d _l o ss ( 18 labels = Q_target , predictions = Q_values , 19 weights = err_logits , weight_options = ' self_normalized ') 20 21</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning (RL) algorithms, when combined with high-capacity deep neural net function approximators, have shown promise in domains ranging from robotic manipulation <ref type="bibr" target="#b0">[1]</ref> to recommender systems <ref type="bibr" target="#b1">[2]</ref>. However, current deep RL methods can be difficult to use, due to sensitivity with respect to hyperparameters and inconsistent and unstable convergence. While a number of hypotheses have been proposed to understand these issues <ref type="bibr">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and gradual improvements have led to more powerful algorithms in recent years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, an effective solution has proven elusive. We hypothesize that a major source of instability in reinforcement learning with function approximation and value function estimation, such as Q-learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> and actor-critic algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, is a pathological interaction between the data distribution induced by the latest policy, and the errors induced in the learned approximate value function as a consequence of training on this distribution, which then exacerbates the issues for the data distribution at the next iteration.</p><p>While a number of prior works have provided a theoretical examination of various approximation dynamic programming (ADP) methods -which encompasses Q-learning and actor-critic algorithms -prior work has not extensively studied the relationship between the data distribution induced by the latest value function, and the errors in the future value functions obtained by training on this data.</p><p>When using supervised learning to train a model, as in the case of contextual bandits or model-based RL, using the resulting model to select the most optimal actions results in a kind of "hard negative" mining: the model collects precisely those transitions that lead to good outcomes according to the model (potentially erroneously). This results in collecting precisely the data needed to fix those errors and improve. We refer to this as "corrective feedback." We argue that ADP algorithms (e.g., Q-learning and actor-critic), which use bootstrapped targets rather than ground-truth labels, often do not enjoy this sort of corrective feedback. Since they regress onto bootstrapped estimates of the current value function, rather than the true optimal value function (which is unknown), simply visiting states with high error and updating the value function at those states does not necessarily correct those errors, since errors in the target value might be due to upstream errors in other states that are visited less often. This absence of corrective feedback can result in severe detrimental consequences on performance.</p><p>We demonstrate that naïvely training a value function on transitions collected either by the latest policy or a mixture of recent policies (i.e., with replay buffers) may not result in corrective feedback. In fact, in some cases, it can actually lead to increasing accumulation of errors, which can lead to poor performance even on simple tabular MDPs. We then show how to address this issue by re-weighting the data buffer using a distribution that explicitly optimizes for corrective feedback, which gives rise to our proposed algorithm, DisCor.</p><p>The two main contributions of our work consist of an analysis showing that ADP methods may not benefit from corrective feedback, even with online data collection, as well as a proposed solution to this problem based on estimating target value error and resampling the replay buffer to mitigate error accumulation. Our method, DisCor, is general and can be used in conjunction with most modern ADP-based deep RL algorithms, such as DQN <ref type="bibr" target="#b10">[11]</ref> and SAC <ref type="bibr" target="#b6">[7]</ref>. Our experiments show that DisCor substantially improves performance of standard RL methods, especially in challenging settings, such as multi-task RL and learning from noisy rewards. We evaluate our approach on both continuous control tasks and discrete-action Atari games. On the multi-task MT10 benchmark <ref type="bibr" target="#b13">[14]</ref> and several robotic manipulation tasks, our method learns policies with a final success rate that is 50% higher than that of SAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>The goal in reinforcement learning is to learn a policy that maximizes the expected cumulative discounted reward in a Markov decision process (MDP), which is defined by a tuple (S, A, T, R, γ). S, A represent state and action spaces, T (s |s, a) and r(s, a) represent the dynamics and reward function, and γ ∈ (0, 1) represents the discount factor. ρ 0 (s) is the initial state distribution. The infinite-horizon, discounted marginal state distribution of the policy π(a|s) is denoted as d π (s) and the corresponding state-action marginal is d π (s, a) = d π (s)π(a|s).</p><p>Approximate dynamic programming (ADP) algorithms, such as Q-learning and actor-critic methods, aim to acquire the optimal policy by modeling the optimal state (V * (s)) and state-action (Q * (s, a)) value functions. These algorithms are based on recursively iterating the Bellman optimality operator, B * , defined as </p><p>We refer to B * Q as the target value for the projection step. Q-function fitting is usually interleaved with additional data collection, which typically uses a policy derived from the latest value function, augmented with either -greedy <ref type="bibr" target="#b10">[11]</ref> or Boltzmann-style <ref type="bibr" target="#b6">[7]</ref> exploration. To simplify analysis, we mainly consider an underlying RL algorithm (Appendix C, Algorithm 2) that alternates between fitting the action-value function, Q(s, a), fully with the current data by minimizing Bellman error, and then collecting data with the policy derived from this value function. This corresponds to fitted Q-iteration. For commonly used ADP methods, µ simply corresponds to the on-policy state-action marginal, µ k = d π k (at iteration k) or else a "replay buffer" <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref> formed as a mixture distribution over all past policies, such that µ k = k i=1 d πi . However, as we will show in this paper, the choice of the sampling distribution µ is of crucial importance for the stability and efficiency of ADP algorithms, and that many commonly-used choices of this distribution can lead to convergence to sub-optimal solutions, even with online data collection. We analyze this issue in Section 3, and then discuss a potential solution to this problem in Section 5.</p><p>Experiment setup for analysis. For the purposes of the analysis of corrective feedback in Section 3, we use tabular MDPs from <ref type="bibr" target="#b5">[6]</ref>, which provide the ability to measure oracle quantities, such as the error against Q * , the ground truth optimal Q-function. We remove other sources of error, such as sampling error, by providing all transitions in the MDP to the ADP algorithm. We simulate different data distributions by re-weighting these transitions. We use a two hidden layer feed-forward network to represent the Q-function. More details on this setup are provided in Appendix F.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Corrective Feedback in Q-Learning</head><p>In this paper, we study how the policies induced by value functions learned via ADP result in data distributions that can fail to correct systematic errors in those value functions, in contrast to non-bootstrapped methods (e.g., supervised learning of models). That is, ADP methods lack "corrective feedback." To define corrective feedback intuitively and formally, we start with a contextual bandit example, where the goal is to learn the optimal stateaction value function Q * (s, a) which, for a bandit, is equal to the reward r(s, a) for performing action a in state s. At iteration k, the algorithm minimizes the estimation error of the Q-function: E s∼β(s),a∼π k (a|s) [|Q k (s, a) − Q * (s, a)|]. Using a greedy or Boltzmann policy π k (a|s) to collect data for training Q k+1 leads the agent to choose actions a with over-optimistic Q k (s, a) values at a state s, and observe the corresponding true Q * (s, a) values as a result. Minimizing this estimation error then leads to the errors being corrected, as Q k (s, a) is pushed closer to match the true Q * (s, a) for actions a with incorrectly high Q-values. This constructive interaction between online data collection and error correction is what we refer to as corrective feedback.</p><p>In contrast, as we will observe shortly, ADP methods that employ bootstrapped target values do not necessarily benefit from such corrective feedback, and therefore can converge to suboptimal solutions. Because value functions are trained with target values computed by applying the Bellman backup on the previous learned Q-function, rather than the true optimal Q * , errors in the previous Q-function at the states that serve as backup targets can result in incorrect Q-value targets at the current state. In this case, no matter how often the current transition is observed, the error at this transition is not corrected. Since ADP algorithms typically use data collected using past Q-functions for training, thus coupling the data distribution to the learned value function, this issue can make them unable to correct target value errors.</p><p>As shown in <ref type="figure">Figure 1</ref> (experiment setup described at the end of Section 2), state visitation frequencies of the latest policy in an ADP algorithm can often correlate positively with increasing error, suggesting that visiting a state can actually increase error at that state, in contrast to supervised learning in bandit problems, where this correlation is either negative (i.e., the error decreases) or 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visitation and Error Correction</head><p>Bandit ADP <ref type="figure">Figure 1</ref>: Left: Return attained by with direct supervised learning of values in a bandit problem ("Bandit") and with bootstrapped targets ("ADP") on a simple tabular MDP. Right: Cosine similarity between per-iteration error increase E k+1 − E k and the policy's state visitation frequency d π k for the two training runs. The ADP run performs poorly, and exhibits a positive cosine similarity for prolonged periods during training, suggesting that the policy visitation at least at some states correlates with an increase in error. With ground truth targets ("Bandit"), this quantity is negative until convergence (around iteration 25), and then fluctuates around 0, suggesting that this approach does benefit from corrective feedback.</p><p>Thus, naïvely coupling the choice of data distribution µ and the Q function being optimized, by sampling uniformly from a replay buffer collected by policies corresponding to prior Q-functions <ref type="bibr" target="#b10">[11]</ref>, or even just sampling from data collected by the latest policy, can cause an absence of corrective feedback. As we will show in Section 3.2, this can lead to several detrimental effects such as sub-optimal convergence, instability, and slow learning progress. To theoretically analyze the lack of corrective feedback, we first define error against the optimal value function as follows:</p><formula xml:id="formula_1">Definition 3.1</formula><p>The value error is defined as the expected absolute error to the optimal Q-function Q * weighted under the corresponding on-policy (π k ) state-action marginal, d π k :</p><formula xml:id="formula_2">E k = E d π k [|Q k − Q * |].</formula><p>A smooth decrease in the value error E k indicates that the algorithm is effectively correcting errors in the Q-function. If E k fluctuates or increases, the algorithm may be making poor learning progress. When the value error E k is roughly stagnant at a non-zero value, this may indicate premature, sub-optimal convergence, provided that the function approximator class is able to support lower-error solutions (which is typically the case for large deep networks). Thus, having the learning process monotonically and quickly bring the value error E k down to 0 is desirable for effective learning. By means of a simple didactic example, we now discuss some reasons why corrective feedback may be absent in ADP methods. We will then describe several examples that illustrate the negative consequences of absent corrective feedback on learning progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Didactic Example and Theoretical Analysis of an Absence of Corrective Feedback</head><p>In this section, we first present a diadctic example which provides intuition for why corrective feedback may be absent in RL, and then we generalize this intuition to a more formal result in Section 3.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Didactic Example</head><p>We first describe our didactic example which is a tree-structured deterministic MDP (Figures 2 and 3) with 7 states and 2 actions, a 1 and a 2 , at each state. The MDP has deterministic transitions, where action a 1 transitions to a node's left child, and a 2 transitions to the right child. At each leaf node, the episode terminates and the agent receives a reward. We illustrate the learning progress of Q-learning (Algorithm 2) on this tree-MDP in <ref type="figure">Figure 2</ref> and the learning progress of a method with an optimal distribution in <ref type="figure">Figure 3</ref>. In the on-policy setting, Q-values at states are updated according to the visitation frequency of these states under the current policy. Since the leaf nodes are the least likely in this distribution, the Bellman backup is slow to correct errors at the leaves. Using these incorrect leaf Q-values as target values for nodes higher in the tree then gives rise to incorrect values, even if Bellman error is fully minimized for the sampled transitions. Thus, most of the Bellman updates do not actually bring the updated Q-values closer to Q * . If we carefully order the states, such that the lowest level nodes are updated first before proceeding upwards in the tree, as shown in <ref type="figure">Figure 3</ref>, the errors are much lower. We will show in Section 5 that our proposed approach in this paper aims at performing updates in the manner shown in <ref type="figure">Figure 3</ref> by re-weighting transitions collected by on-policy rollouts via importance sampling.</p><p>Reasons for the absence of corrective feedback. The above example provides an intuitive explanation for how on-policy or replay buffer distributions may not lead to error correction. Updates on states from such distributions may fail to correct Q-values at states that are the causes of the errors. In general, Bellman backups rely on the correctness of the values at the states that are used as targets, which may mean relying on the correctness of states that occur least frequently in any distribution generated by online data collection. While this is sufficient to guarantee convergence in tabular settings, with function approximation this can lead to an absence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>States being updated</head><p>Intermediate values of error (high (L) to low (R) error) <ref type="figure">Figure 3</ref>: Iterations of Q-learning on a tree-structured MDP with an optimal training distribution, where states are sampled starting from the leaf nodes, progressing upwards towards the root node in the tree. Note that this method backs up very few incorrect values in any iterations, and takes only a few iterations to converge. Our aim will be to approximate such an optimal training distribution.</p><p>of corrective feedback, as states with erroneous targets are visited more and more often, while the visitation frequency of the states that cause these errors does not increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Theoretical Analysis</head><p>We next present a theoretical result that generalizes the didactic example in a formal result. Proof for this result can be found in Appendix C. Our result is a lower-bound on the iteration complexity of on-policy Q-learning. We show that, under the on-policy distribution, Q-learning may require exponentially many iterations to learn the optimal Q-function. This will also provide an explanation for the slow and unstable learning as described in <ref type="figure">Figure 2</ref> and in Section 3.2, <ref type="figure" target="#fig_5">Figure 4c</ref>. We state the result next. An extension to the replay buffer setting is given in Appendix C. Thus, on-policy or replay buffer distributions can induce extremely slow learning in certain environments, even when all transitions are available for training (i.e., without any exploration issues), requiring exponentially many Bellman backups. When function approximation is employed, the smaller the frequency of a transition, the less likely Q-learning is to correct the Q-value of the state-action pair corresponding to this transition. In contrast, we show in Appendix C.2.2 that the method we propose in this paper requires only poly(H) iterations in this scenario, and it behaves similarly to a method that learns Q-values from the leaf nodes, gradually moving upwards toward the root of the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Consequences of Absent Corrective Feedback</head><p>In this section, we investigate the phenomenon of an absence of corrective feedback in a number of RL tasks. We first plot the value error E k for the Q-function by assuming access to Q * for analysis, but this is not available while learning from scratch. We first show that an absence of corrective feedback happens in practice. In Figures 4a, 4b and 4c, we plot E k for on-policy and replay buffer sampling. Observe that ADP methods can suffer from prolonged periods where E k is increasing or fluctuating, and returns degrade or stagnate ( <ref type="figure" target="#fig_5">Figure 4b</ref>). Next, we empirically analyze a number of pathological outcomes that can occur when corrective feedback is absent.</p><p>1. Convergence to suboptimal Q-functions. We find that on-policy sampling can cause ADP to converge to a suboptimal solution, even in the absence of sampling error. This is not an issue with the capacity of the function approximator -even when the optimal Q-function Q * can be represented in the function class <ref type="bibr" target="#b5">[6]</ref>, learning converges to a suboptimal fixed point far from Q * . <ref type="figure" target="#fig_5">Figure 4a</ref> shows that the value error E k rapidly decreases initially, and eventually converges to a value significantly greater than 0, from which the learning process never recovers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suboptimal Convergence</head><p>(a) Sub-optimal convergence for onpolicy distributions: return (dashed) and value error (solid). Note that value error decreases rapidly at the start and finally converges to a nonzero value, leading to sub-optimal return.  2. Instability in the learning process. Q-learning with replay buffers may not converge to sub-optimal solutions as often as on-policy sampling ( <ref type="figure" target="#fig_20">Figure 13</ref>). However, we observe that even then ( <ref type="figure" target="#fig_5">Figure 4b</ref>), ADP with replay buffers can be unstable. For instance, the algorithm is prone to degradation even if the latest policy obtains returns that are very close to optimal returns ( <ref type="figure" target="#fig_5">Figure 4b</ref>). This instability is often correlated with a lack of corrective feedback, and exists even with all transitions present in the buffer controlling for sampling error. 3. Inability to learn with low signal-to-noise ratio. Lack of corrective feedback can also prevent ADP algorithms from learning quickly in scenarios with low signal-to-noise ratio, such as tasks with sparse or noisy rewards ( <ref type="figure" target="#fig_5">Figure 4c</ref>). For efficient learning, Q-learning needs to effectively "exploit" the reward signal even in the presence of noise and delay, and we find that the learning becomes significantly worse in the presence of these factors. This is not an exploration issue, since all transitions in the MDP are provided to the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimal Distributions for Optimizing Corrective Feedback</head><p>In the previous section, we observed that an absence of corrective feedback can occur when ADP algorithms naïvely use the on-policy or replay buffer distributions for training Q-functions. However, if we can compute an "optimal" data distribution that provides maximal corrective feedback, and train Q-functions using this distribution, then we can ensure that the ADP algorithm always enjoys corrective feedback, and hence makes steady learning progress. In this section, we aim to derive a functional form for this optimal data distribution. We first present an optimization problem for this optimal data distribution, which we refer to as p k (different from µ, which refers to the data distribution in the replay buffer, or the on-policy distribution), for any iteration k of the ADP algorithm, and then present a solution to this optimization problem. Proofs from this section can be found in Appendix A. We will show in this section that the resulting optimization when approximated practically, yields a very simple and intuitive algorithm. A more intuitive description of the resulting algorithm can be found in <ref type="bibr">Section 5</ref> Our goal is to minimize the value error E k at every iteration k, with respect to the the distribution p k used for Bellman error minimization at iteration k. The resulting optimization problem is:</p><formula xml:id="formula_3">min p k E d π k [|Q k − Q * |] s.t. Q k = arg min Q E p k (Q − B * Q k−1 ) 2 .</formula><p>(2) Theorem 4.1 An optimal solution p k to optimization problem 2 satisfies the following relationship:</p><formula xml:id="formula_4">p k (s, a) ∝ exp (−|Q k − Q * |(s, a)) |Q k − B * Q k−1 |(s, a) λ *<label>(3)</label></formula><p>where λ * ∈ R + , is an optimal Lagrange dual variable that ensures p k is a valid distribution in Problem 2.</p><p>Proof (Sketch) A complete proof is provided in Appendix A. Here we provide a rough sketch of the steps involved for completeness. We first use the Fenchel-Young inequality <ref type="bibr" target="#b15">[16]</ref> to obtain an upper bound on the true objective, in terms of the "soft-min" of the errors |Q k − Q * | and then minimize this upper bound. Formally, this is given by the RHS of the equation below, H denotes Shannon entropy.</p><formula xml:id="formula_5">E d π k [|Q k −Q * |] ≤ H (d π k )−log s,a exp(−|Q k −Q * |) .</formula><p>Then, we solve for p k by setting the gradient of the Lagrangian for Problem 2 to 0, which requires an addition of constraints s,a p k (s, a) = 1 and p k (s, a) &gt; 0 to ensure that p k is a valid distribution. We then use the implicit function theorem (IFT) <ref type="bibr" target="#b16">[17]</ref> to compute implicit gradients of Q k with respect to p k . IFT is required for this step since Q k is an output of a minimization procedure that uses p k as an input. Rearranging the expression gives the above result.</p><p>Intuitively, p k , shown in Equation 3, assigns higher probability to state-action tuples with high Bellman error |Q k − B * Q k−1 |, but only when the resulting Q-value Q k is close to Q * . However, the expression for p k contains terms that depend on Q * . Since both Q k and Q * are observed only after p k is chosen, we need to estimate these quantities using surrogate quantities which we discuss in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tractable Approximation to Optimal Distribution</head><p>The expression for p k in Equation 3 contains terms dependent on Q * and Q k , namely |Q k − Q * | and |Q k − B * Q k−1 |. As described previously, since both Q k and Q * are observed only after p k is chosen, in this section, we develop surrogates to estimate these quantities. For error against Q * , we show that the cumulative sum of discounted Bellman errors over the past iterations of training, denoted as ∆ k , shown in Equation 4, is a valid proxy (equivalent to an upper bound) for |Q k − Q * |. In fact, Theorem 4.2 shows that ∆ k , offset by a state-action independent constant, is a tractable upper bound on |Q k − Q * | constructed only from prior Q-function iterates, Q 0 , · · · , Q k−1 .</p><formula xml:id="formula_6">∆ k = k i=1 γ k−i   k−1 j=i P πj   |Q i − B * Q i−1 | =⇒ ∆ k =|Q k − B * Q k−1 | + γP π k−1 ∆ k−1 .<label>(4)</label></formula><p>The following theorem formally states this result. Theorem 4.2 There exists a k 0 ∈ N, such that ∀ k ≥ k 0 and ∆ k from Equation 4, ∆ k satisfies the following inequality, pointwise, for each s, a:</p><formula xml:id="formula_7">∆ k + k i=1 γ k−i α i ≥ |Q k − Q * |, α i = 2R max 1 − γ D TV (π i , π * ) Proof (Sketch) A full proof is provided in Appendix B.</formula><p>The key insight in this argument is to use a recursive inequality, presented in Lemma B.0.1, App. B, to decompose |Q k − Q * |, which allows us to show that ∆ k + i γ k−i α i is a solution to the corresponding recursive equality, and hence, an upper bound on |Q k − Q * |. Note that, the initialization |Q 0 − Q * | matters only infinitesimally once, k ≥ k 0 , with k 0 being such that γ k0 |Q k − Q * | &lt; 1, therefore, agnostic to the initialization of ∆, ∆ 0 , we note that the statement in the theorem holds true for large-enough k.</p><p>Estimating |Q k − B * Q k−1 |. The expression for p k in Equation 3 also includes an unobserved Bellman error multiplier term, |Q k − B * Q k−1 | as well. With no available information about Q k -which will only be observed after Bellman error minimization under p k -a viable approximation is to bound this term |Q k − B * Q k−1 | between the minimum and maximum Bellman errors obtained at the previous iteration, c 1 = min s,a |Q k−1 − B * Q k−2 | and c 2 = max s,a |Q k−1 − B * Q k−2 |. We can simply restrict the support of state-action pairs (s, a) used to compute c 1 and c 2 to come from transitions observed in the replay buffer used for the Q-function update, to ensure that both c 1 and c 2 are finite.</p><p>Re-weighting the replay buffer µ. Since it is challenging to directly obtain samples from p k via online interaction, a practically viable alternative is to instead perform weighted Bellman updates by re-weighting transitions drawn from the a regular replay buffer µ using importance weights given by w k = p k (s,a) µ(s,a) . However, naïve importance sampling often suffers from high-variance of these importance weights, leading to instabilities in learning. To prevent such issues, instead of directly re-weighting to p k , we re-weight samples from µ to a projection of p k , denoted as q k , that is still close to µ under the KL-divergence metric:</p><formula xml:id="formula_8">q k = arg min q D KL (q||p k ) + (τ − 1)D KL (q||µ),</formula><p>where τ is a scalar, τ &gt; 1. The equation for weights w k , in this case, is thus given by:</p><formula xml:id="formula_9">w k (s, a) ∝ exp −|Q k − Q * |(s, a) τ |Q k − B * Q k−1 | λ *<label>(5)</label></formula><p>A derivation is provided in Appendix B. Having described all approximations, we now discuss how to obtain a tractable and practically usable data distribution for training the Q-function that maximally mitigates error accumulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Putting it All Together</head><p>We have noted all practical approximations to the expression for optimal p k (Equation 3), including estimating surrogates for Q k and Q * , and the usage of importance weights to develop a method that can achieve the benefits of the optimal distribution, simply by re-weighting transitions in the replay buffer, rather than altering the exploration strategy. We also discussed a technique to reduce the variance of weights used for this reweighting. We now put these techniques together to obtain the final, practically tractable expression for the weights used for our practical approach. We note that the term |Q k − Q * |, appearing inside the exponent in the expression for w k in Equation 5 can be approximated by the tractable upper bound ∆ k . However, computing ∆ k requires the quantity |Q k − B * Q k−1 | which also is unknown when w k is being chosen. Combining the upper bound on |Q k − B * Q k−1 | ≤ c 2 , Theorem 4.2 and Equation 4, we obtain the following bound:</p><formula xml:id="formula_10">|Q k − Q * | ≤ γP π k−1 ∆ k−1 + c 2 + i γ i α i<label>(6)</label></formula><p>Using this bound in the expression for w k , along with the lower bound, |Q k − B * Q k−1 | ≥ c 1 , we obtain the following lower bound on weights w k :</p><formula xml:id="formula_11">w k ∝ exp −c 2 − γ [P π k−1 ∆ k−1 ] (s, a) τ c 1 λ *<label>(7)</label></formula><p>Finally, we note that using a worst-case lower bound for w k (Equation 7) will down-weight some additional transitions which in reality lead to low error accumulation, but this scheme will never up-weight a transition with high error, thus providing for a "conservative" distribution. A less conservative expression for getting these weights is a subject of future work. Simplifying the constants c 1 , c 2 and λ * , the final expression for the practical choice of w k is:</p><formula xml:id="formula_12">w k (s, a) ∝ exp − γ [P π k−1 ∆ k−1 ] (s, a) τ .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Distribution Correction (DisCor) Algorithm</head><p>In this section, we present the resulting algorithm, that uses weights w k from Equation 8 to re-weight the Bellman backup in order to induce corrective feedback. We first present an intuitive explanation of our algorithm, and then describe the implementation details.</p><p>Intuitive Explanation. Using weights w k in Equation 8 for weighting Bellman backups possess a very clear and intuitive explanation. P π k−1 ∆ k−1 corresponds to the estimated upper bound on the error of the target values for the current transition, due to the backup operator P π k−1 . Intuitively, this implies that weights w k downweight those transitions for which the bootstrapped target Q-value estimate has a high estimated error to Q * , effectively focusing the learning on samples where the supervision (target value) is accurate, which are precisely the samples that we expect maximally improve the accuracy of the Q function. This prevents error accumulation, and hence provides correct feedback. Such a scheme also resembles prior methods for learning with noisy labels by "abstention" from training on labels that are likely to be inaccurate <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 DisCor (Distribution Correction)</head><p>1: Initialize Q-values Q θ (s, a), initial distribution p0(s, a), a replay buffer µ, and an error model ∆ φ (s, a). <ref type="bibr">2:</ref> for step k in {1, . . . , N} do 3:</p><p>Collect M samples using π k , add them to replay buffer µ, sample</p><formula xml:id="formula_13">{(si, ai)} N i=1 ∼ µ 4:</formula><p>Evaluate Q θ (s, a) and ∆ φ (s, a) on samples (si, ai).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute target values for Q and ∆ on samples:</p><formula xml:id="formula_14">yi = ri + γ max a Q k−1 (s i , a ) ai = arg maxa Q k−1 (s i , a) ∆i = |Q θ (s, a) − yi| + γ∆ k−1 (s i ,âi) 6:</formula><p>Compute w k using Equation 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Minimize Bellman error for training Q θ weighted by w k .</p><formula xml:id="formula_15">θ k+1 ← argmin θ 1 N N i=1 w k (si, ai) · (Q θ (si, ai) − yi) 2 8: Minimize ADP error for training φ. φ k+1 ← argmin φ 1 N N i=1 (∆ θ (si, ai) −∆i) 2 9: end for</formula><p>Details. Pseudocode for our approach, which we call DisCor (Distribution Correction), is presented in Algorithm 1, with the main differences from standard ADP methods highlighted in red. In addition to a standard Q-function, DisCor trains another parametric model, ∆ φ , to estimate ∆ k (s, a) at each state-action pair. The recursion in Equation 4 is used to obtain a simple approximate dynamic programming update rule for the parameters φ (Line 8). The second change is the introduction of a weighted Q-function backup with weights w k (s, a), as shown in Equation 8, on Line 7. We also present a practical implementation of the DisCor algorithm, built on top of standard DQN/SAC algorithm pseudocodes in Algorithm 3, Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Prior work has pointed out a number of issues arising when dynamic programming is used with function approximation. Some work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> focused on analysing error induced from the typically used projected Bellman operator under the assumption of an abstract error model. Further, fully gradient-based objectives for Q-learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> gave rise to convergent ADP algorithms with function approximation. In contrast to these works, which mostly focus on ensuring convergence of the Bellman backup, we focus on the interaction between the ADP update and the data distribution µ. On the other hand, prior work on fully online Q-learning from a stochastic approximation viewpoint analyzes time-varying µ, but in the absence of function approximation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, or at the granularity of a single time-step in the environment <ref type="bibr" target="#b26">[27]</ref>, unlike our setting. Our setting involves both a time-varying µ depending on the prior and latest Q-functions as well as function approximation.</p><p>A number of prior works have focused on studying non-convergence and generalization effects in ADP methods with deep net function approximators, both theoretically <ref type="bibr" target="#b27">[28]</ref> and empirically <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. In this paper, we study a different issue that is distinct from non-convergence and generalization issues due to the specific choice of deep net function approximators: the interaction between the data distribution and the fitting error in the value function.</p><p>In the fully offline setting, prior works have noted that sampling distributions can affect performance of ADP methods <ref type="bibr" target="#b5">[6]</ref>. This has been generally aimed at resolving what is typically known as the "deadly triad" <ref type="bibr" target="#b30">[31]</ref>: divergence caused by an interaction between function approximation, bootstrapped updates and off-policy sampling, resulting in the development of batch RL algorithms that choose sampling schemes <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> for guaranteed convergence. However, we show that sampling distributions can have a drastic impact on the learning process even where the algorithm performs online (on-policy) interaction to collects its own data. <ref type="bibr" target="#b33">[34]</ref> studies the interaction between data collection and training for multi-objective policy gradient methods and note the bias towards optimizing only a few components of the objective that arises.</p><p>Our proposed algorithm weights the transition in the buffer based on an estimate of their error to the true optimal value function. Related to our approach, prioritized sampling has been used previously in ADP methods, such as PER <ref type="bibr" target="#b34">[35]</ref>, to prioritize transitions with higher Bellman error. We show in Section 7, that this choice may fail to perform in many cases. Recent work <ref type="bibr" target="#b35">[36]</ref> has attempted to use a distribution-checking oracle to control the amount of exploration performed. Our work aims at ensuring corrective feedback, solely by re-weighting the data distribution for Q-learning with bootstrapped backups.</p><p>We further discuss the relationship with other prior works in an extended related work section in Appendix D.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Evaluation of DisCor</head><p>The goal of our empirical evaluation is to study the following questions:</p><p>1. Can DisCor ensure continuous corrective feedback in RL tasks, mitigating the issues raised in Section 3.2? 2. How does DisCor compare to prior methods, including those that also reweight the data in various ways? 3. Can DisCor attain good performance in challenging settings, such as multi-task RL or noisy reward signals? 4. How do approximations from Section 4 affect the efficacy of DisCor in mitigating error accumulation? We start by presenting a detailed analysis on tabular MDPs with function approximation, studying each component of DisCor in isolation, and then study six challenging robotic manipulation tasks, the multi-task MT10 benchmark from MetaWorld <ref type="bibr" target="#b13">[14]</ref>, and three Atari games <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Analysis of DisCor on Tabular Environments</head><p>We first use the tabular domains from Section 3, described in detail in Appendix F.1, to analyze the corrective feedback induced by DisCor and evaluate the effect of the approximations used in our method, such as the upper bound estimator ∆ k . We first study the setting without sampling error, where all transitions in the MDP are added to the training buffer, and then consider the setting with sampling error, where the algorithm also needs to explore the environment and collect transitions.</p><p>Results. In both settings, shown in <ref type="figure" target="#fig_7">Figure 5</ref>, DisCor consistently provides correct feedback ( <ref type="figure" target="#fig_7">Figure 5(a)</ref>). An oracle version of the algorithm (labeled DisCor (oracle); Equation 5), which uses the true error |Q k − Q * | in place of ∆ k , is somewhat better than DisCor ( <ref type="figure" target="#fig_7">Figure 5</ref>(b) histograms, red vs blue), but DisCor still outperforms on-policy and replay buffer sampling schemes, which often suffer from an absence of corrective feedback as shown in Section 3. Prioritizing based on high Bellman error struggles on domains with sparse rewards (App. F.1, <ref type="figure" target="#fig_18">Fig. 12</ref>).</p><p>Analysis. While DisCor (oracle) consistently performs better than DisCor, as we would expect, the approximate DisCor method still attains better performance than naïve uniform weighting and prioritization similar to PER. This shows that the principle behind DisCor is effective when applied exactly, and that even the approximation that we utilize to make the method practical still improves performance considerably.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Continuous Control Experiments</head><p>We next perform a comparative evaluation of DisCor on several continuous control tasks, using six robotic manipulation tasks from the Meta-World suite (pull stick, hammer, insert peg side, push stick, push with wall and turn dial). A pictorial description of these tasks is provided in <ref type="figure" target="#fig_8">Figure 6</ref>. These domains were chosen because they are challenging for state-of-the-art RL methods, such as soft actor-critic (SAC).</p><p>Meta-World. We applied DisCor to these tasks by modifying the weighting of samples in soft actor-critic (SAC) <ref type="bibr" target="#b6">[7]</ref>. DisCor does not alter any hyperparameter from SAC, and requires minimal tuning. There is only one additional temperature hyperparameter, which is also automatically chosen across all domains. More details are presented in Appendix E.2.</p><p>We compare DisCor to standard SAC without weighting, as well as prioritized experience replay (PER) <ref type="bibr" target="#b34">[35]</ref>, which uses weights based on the last Bellman error. On these tasks, DisCor often attains better performance, as shown in <ref type="figure" target="#fig_8">Figures 7 and 16</ref>, achieving better success rates than standard SAC on several tasks. DisCor is also more efficient, achieving good performance earlier than the other methods on these tasks. PER, on the other hand, performs poorly, as prioritizing high Bellman error states may lead to higher error accumulation if the target values are incorrect.</p><p>Gym. We also performed comparisons on the conventional OpenAI gym benchmarks, where we see a small but consistent benefit from DisCor reweighting. Since prior methods, such as SAC already solves these tasks easily, and have been tuned well for them, the room for improvement is very small. We include these results in Appendix F.3 for completeness. We also modified the gym environments to have stochastic rewards, thus lowering the signal-to-noise ratio, and compare different algorithms on these domains. The results, along with a description of the noise added, are shown in Appendix F.3. In these experiments, DisCor generally exhibits better sample efficiency as compared to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Multi-Task Reinforcement Learning</head><p>Another challenging setting for current RL methods is the multi-task setting. This is known to be difficult, to the point that oftentimes learning completely separate policies for each of the tasks is actually faster, and resulting in better performance, than learning the tasks together <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>.  MT10 multi-task benchmark. We evaluate on the MT10 MetaWorld benchmark <ref type="bibr" target="#b13">[14]</ref>, which consists of ten robotic manipulation tasks to be learned jointly. We follow the protocol from <ref type="bibr" target="#b13">[14]</ref>, and append task ID to the state. As shown in <ref type="figure" target="#fig_11">Figure 8</ref>(a), DisCor applied on top of SAC outperforms standard unweighted SAC by a large margin, achieving 50% higher success rates compared to SAC, and a high overall return <ref type="figure">(Fig 19)</ref>. <ref type="figure" target="#fig_11">Figure 8</ref>(b) shows that DisCor makes progress on 7/10 tasks, as compared to 3/10 for SAC.</p><p>MT50 multi-task benchmark. We further compare the performance of DisCor and SAC on the more challenging MT50 benchmark <ref type="bibr" target="#b13">[14]</ref>, which is shown in <ref type="figure" target="#fig_25">Figure 20</ref>, and observe a similar benefit as compared to MT10, where the standard unweighted SAC algorithm tends to saturate at a suboptimal success rate for about 4M environment steps, whereas DisCor continuously keeps learning, and achieves asymptotic performance faster than SAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Arcade Learning Environment</head><p>Our final experiments were aimed at testing the efficacy of DisCor on stochastic, discrete-action environments.</p><p>To this end, we evaluated DisCor on three commonly reported tasks from the Atari suite -Pong, Breakout and Asterix. We compare to the baseline DQN <ref type="bibr" target="#b10">[11]</ref>, all our implementations are built off of Dopamine <ref type="bibr" target="#b37">[38]</ref> and use the evaluation protocol with sticky actions <ref type="bibr" target="#b38">[39]</ref>. We build DisCor on top of DQN by simply replacing the standard replay buffer sampling scheme in DQN with the DisCor weighted update. We show in <ref type="figure">Figure 9</ref> that DisCor usually outperforms unweighted DQN in learning speed and performance.  <ref type="figure">Figure 9</ref>: DQN vs DisCor on Atari. Note that DisCor generally improves learning speed and asymptotic performance on all three tasks that we evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion, Future Work and Open Problems</head><p>In this work, we show that deep RL algorithms suffer from the absence of a corrective feedback mechanism in scenarios with naïve online data collection. This results in a number of problems during learning, including slow convergence and oscillation. We propose a method to compute the optimal data distribution in order to achieve corrective feedback, and design a practical algorithm, DisCor, that applies this distribution correction by re-weighting the transitions in the replay buffer based on an estimate of the accuracy of their target values. DisCor yields improvements across a wide range of RL problems, including challenging robotic manipulation tasks, multi-task reinforcement learning and Atari games and can be easily combined with a variety of ADP algorithms. As shown through our analysis and experiments, studying the connection between data distributions, function approximation, and approximate dynamic programming can allow us to devise stable and efficient reinforcement learning algorithms. This suggests several exciting directions for future work. First, a characterization of the learning dynamics and their interaction with corrective feedback and data distributions in ADP algorithms may lead to even better and more stable algorithms, by better identifying the accuracy of target values. Second, our approach is limited by the transitions that are actually collected by the behavior policy, since it only reweights the replay buffer. An exciting direction of future investigation would involve studying how we might directly modify the exploration policy to change which transitions are collected, so as to more directly alter the training distribution and produce large gains in sample efficiency and asymptotic performance. If we can devise RL methods that are guaranteed to enjoy corrective feedback and thus are stable, robust, and effective, then RL algorithms can be reliably scaled to large open-world settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices Appendix A. Detailed Proof of Theorem 4.1 (Section 4)</head><p>In this appendix, we present a detailed proofs for the theoretical derivation of DisCor outlined in Section 4. To get started, we mention the optimization problem being solved for convenience.</p><formula xml:id="formula_16">min p k E d π k [|Q k − Q * |] s.t. Q k = arg min Q E p k (Q − B * Q k−1 ) 2 .<label>(9)</label></formula><p>We break down this derivation in steps marked as relevant paragraphs. The first step is to decompose the objective into a more tractable one via an application of the Fenchel-Young inequality <ref type="bibr" target="#b15">[16]</ref>.</p><p>Step 1: Fenchel-Young Inequality. The optimization objective in Problem 9 is the inner product of d π k−1 and |Q k − Q * |. We can decompose this objective by applying the Fenchel-Young inequality <ref type="bibr" target="#b15">[16]</ref>. For any two vectors, x, y ∈ R d , and any convex function f and its Fenchel conjugate f * , we have that, x T y ≤ f (x) + f * (y). We therefore have:</p><formula xml:id="formula_17">E d π k [|Q k − Q * |] ≤ f (|Q k − Q * |) + f * (d π k ) .<label>(10)</label></formula><p>Since minimizing an upper bound leads to minimization of the original objective, we can replace the objective in Problem 9 with the upper bound in Equation <ref type="bibr" target="#b9">10</ref>. As we will see below, a convenient choice of f is the soft-min function:</p><formula xml:id="formula_18">f (x) = − log i e −xi , f * (y) = H(y).<label>(11)</label></formula><p>f * in this case is given by the Shannon entropy, which is defined as H(y) = − j y j log y j . Plugging this back in problem 9, we obtain an objective that dictates minimization of the marginal state-action entropy of the policy π.</p><p>In order to make this objective even more convenient and tractable, we upper bound the Shannon entropy, H(y) by the entropy of the uniform distribution over states and actions, H(U). This step ensures that the entropy of the state-action marginal d π is not reduced drastically due to the choice of p. We can now minimize this upper bound, since minimizing an upper bound, leads to a minimization of the original problem, and therefore, we obtain the following new optimization problem shown in Equation 12 is:</p><formula xml:id="formula_19">min p k − log s,a exp(−|Q k − Q * |(s, a)) s.t. Q k = arg min Q E p k (Q − B * Q k−1 ) 2 .<label>(12)</label></formula><p>Another way to interpret this step is to modify the objective in Problem 9 to maximize entropy-augmented V CF : V CF (k) + H(d π k ) as is common in a number of prior works, albeit with entropy over different distributions such as <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b6">7]</ref>. This also increases the smoothness of the loss landscape, which is crucial for performance of RL algorithms <ref type="bibr" target="#b40">[41]</ref>.</p><p>Step 2: Computing the Lagrangian. In order to solve optimization problem Problem 12, we follow standard procedures for finding solutions to constrained optimization problems. We first write the Lagrangian for this problem, which includes additional constraints to ensure that p k is a valid distribution:</p><formula xml:id="formula_20">L(p k ; λ, µ) = − log s,a exp(−|Q k − Q * |(s, a)) + λ s,a p k (s, a) − 1 − µ T p k .<label>(13)</label></formula><p>with constraints s,a p k (s, a) = 1 and p k (s, a) ≥ 0 (∀s, a) and their corresponding Lagrange multipliers, λ and µ, respectively, that ensure p k is a valid distribution. An optimal p k is obtained by setting the gradient of the Lagrangian with respect to p k to 0. This requires computing the gradient of Q k , resulting from Bellman error minimization, i.e. computing the derivative through the solution of another optimization problem, with respect to the distribution p k . We use the implicit function theorem (IFT) <ref type="bibr" target="#b16">[17]</ref> to compute this gradient. We next present an application of IFT in our scenario.</p><p>Step 3: IFT gradient used in the Lagrangian. We derive an expression for ∂Q k ∂p k which will be used while computing the gradient of the Lagrangian Equation <ref type="bibr" target="#b12">13</ref> which involves an application of the implicit function theorem. The IFT gradient is given by:</p><formula xml:id="formula_21">∂Q k ∂p k Q k ,p k = − [Diag(p k )] −1 [Diag(Q k − B * Q k−1 )] .<label>(14)</label></formula><p>To get started towards showing Equation <ref type="bibr" target="#b13">14</ref>, we consider a non-parametric representation for Q k (a <ref type="table">table)</ref>, so that we can compute a tractable term without going onto the specific calculations for Jacobian or inverse-Hessian vector products for different parametric models. In this case, the Hessians in the expression for IFT and hence, the implicit gradient are given by:</p><formula xml:id="formula_22">H Q = 2 Diag(p k ) H Q,p k = 2 Diag(Q k − B * Q k−1 ) ∂Q k ∂p k = − [H Q ] −1 H Q,p k = −Diag Q k − B * Q k−1 p k .<label>(15)</label></formula><p>provided p k ≥ 0 (which is true, since we operate in a full coverage regime, as there is no exploration bottleneck when all transitions are provided). This quantity is 0 only if the Bellman residuals Q k − B * Q k−1 are 0, however, that is rarely the case, hence this gradient is non-zero, and intuitively quantifies the right relationship: Bellman residual errors Q k − B * Q k−1 should be higher at state-action pairs with low density p k , indicating a roughly inverse relationship between the two terms -which is captured by the IFT term.</p><p>Step 4: Computing optimal p k . Now that we have the equation for IFT (Equation <ref type="formula" target="#formula_0">14</ref>) and an expression for the Lagrangian <ref type="figure" target="#fig_20">(Equation 13</ref>), we are ready to compute the optimal p k via an application of the KKT conditions. At an optimal p k , we have,</p><formula xml:id="formula_23">∂L(p k ; λ, µ) ∂p k = 0 =⇒ sgn(Q k − Q * ) exp(−|Q k − Q * |(s, a)) s ,a exp(−|Q k − Q * |(s , a )) · ∂Q k ∂p k + λ − µ s,a = 0.<label>(16)</label></formula><p>Now, re-arranging Equation <ref type="bibr" target="#b15">16</ref> and plugging in the expression for ∂Q k ∂p k from Equation 14 in this Equation to obtain an expression for p k (s, a), we get:</p><formula xml:id="formula_24">p k (s, a) ∝ exp (−|Q k − Q * |(s, a)) |Q k − B * Q k−1 |(s, a) λ * .<label>(17)</label></formula><p>Provided, each component of p is positive, i.e. p k (s, a) ≥ 0 for all s, a, the optimal dual variable µ * s,a = 0, satisfies µ * (s, a)p k (s, a) = 0 by KKT-conditions, and µ * ≥ 0 (since it is a Lagrange dual variable), thus implying that µ * = 0.</p><p>Intuitively, the expression in Equation 17 assigns higher probability to state-action tuples with high Bellman error |Q k − B * Q k−1 |, but only when the post-update Q-value Q k is close to Q * . Hence we obtain the required theorem.</p><p>Summary of the derivation. To summarize, our derivation for the optimal p k consists of the following key steps:</p><p>• Use the Fenchel-Young inequality to get a convenient form for the objective.</p><p>• Compute the Lagrangian, and use the implicit function theorem to compute gradients of the Q-function Q k with respect to the distribution p k . • Compute the expression for optimal p k by setting the Lagrangian gradient to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Proofs for Tractable Approximations in Section 4.1</head><p>Here we present the proofs for the arguments behind each of the approximations described in Section 4.1.</p><p>Computing weights w k for re-weighting the buffer distribution, µ. Since sampling directly from p k may not be easy, we instead choose to re-weight samples transitions drawn from a replay buffer µ, using weights w k to make it as close to p k . How do we obtain the exact expression for w k (s, a)? One option is to apply importance sampling: choose w k as the importance ratio, w k (s, a) = p k (s,a) µ(s,a) , however this suffers from two problems - <ref type="formula" target="#formula_0">(1)  18</ref> importance weights tend to exhibit high variance, which can be detrimental for stochastic gradient methods; and (2) densities µ(s, a), needed to compute w k are unknown.</p><p>In order to circumvent these problems, we solve a different optimization problem, shown in Problem 18 to find the optimal surrogate projection distribution q k , which is closest to p k , and at the same time closest to µ as well, trading off these quantities by a factor τ − 1.</p><formula xml:id="formula_25">q * k = arg min q k D KL (q k ||p k ) + (τ − 1)D KL (q k ||µ)<label>(18)</label></formula><p>where λ is a temperature hyperparameter that trades of bias and variance. The solution to the above optimization is shown in <ref type="bibr">Equation 19</ref>, where the second statement follows by using a tractable approximation of setting µ 1− 1 τ to be equal to µ, which can be ignored if τ is large, hence 1 − 1 τ ≈ 1. The third statement follows by an application of Equation 17 and the fourth statement denotes the importance ratio, q k (s,a) µ k (s,a) , as the weights w k .</p><formula xml:id="formula_26">q * k (s, a) ∝ (µ k ) 1− 1 τ · exp log p k (s, a) τ ∴ q * k (s, a) ∝ (µ k ) · exp log p k (s, a) τ ∴ q * k µ k ∝ exp −|Q k − Q * |(s, a) τ |Q k − B * Q k−1 |(s, a) λ * ∴ w k ∝ exp −|Q k − Q * |(s, a) τ |Q k − B * Q k−1 |(s, a) λ * .<label>(19)</label></formula><p>Our next proof justifies the usage of the estimate ∆ k , which is a worst-case upper bound on |Q k − Q * | in Equation <ref type="bibr" target="#b18">19</ref>. Lemma B.0.1 For any k ∈ N, |Q k − Q * | satisfies the following recursive inequality, pointwise for each s, a:</p><formula xml:id="formula_27">|Q k − Q * | ≤ |Q k − B * Q k−1 | + γP π k−1 |Q k−1 − Q * | + 2R max 1 − γ max s D TV (π k−1 , π * ).</formula><p>Proof Our proof relies on a worst-case expansion of the quantity |Q k − Q * |. The proof follows the following steps. The first few steps follow common expansions/inequalities operated upon in the work on error propagation in Q-learning <ref type="bibr" target="#b18">[19]</ref>.</p><formula xml:id="formula_28">|Q k − Q * | (a) = |Q k − B * Q k−1 + B * Q k−1 − Q * | (b) ≤|Q k − B * Q k−1 | + |B * Q k−1 − B * Q * | (c) = |Q k − B * Q k−1 | + |R + γP π k−1 Q k−1 − R − γP π * Q * | (d) = |Q k − B * Q k−1 | + γ|P π k−1 Q k−1 − P π k−1 Q * + P π k−1 Q * − P π * Q * | (e) ≤ |Q k − B * Q k−1 | + γP π k−1 |Q k−1 − Q * | + γ|P π k−1 − P π * ||Q * | (f ) ≤ |Q k − B * Q k−1 | + γP π k−1 |Q k−1 − Q * | + 2R max 1 − γ max s D TV (π k−1 , π * )</formula><p>where (a) follows from adding and subtracting B * Q k−1 , (b) follows from an application of triangle inequality, (c) follows from the definition of B * applied to two different Q-functions, (d) follows from algebraic manipulation, (e) follows from an application of the triangle inequality, and (f) follows from bounding the maximum difference in transition matrices |P π k−1 − P * | by maximum total variation divergence between policy π k−1 and π * , and bounding the maximum possible value of Q * by Rmax 1−γ .</p><p>We next show that an estimator that satisfies the recursive equality corresponding to Lemma B.0.1 is a pointwise upper bound on |Q k − Q * |. Lemma B.0.2 For any k ∈ N, an vector ∆ k satisfying</p><formula xml:id="formula_29">∆ k := |Q k − B * Q k−1 | + γP π k−1 ∆ k−1 .<label>(20)</label></formula><p>with α k = 2Rmax 1−γ max s D TV (π k , π * ), and an initialization ∆ 0 := |Q 0 − Q * |, pointwise upper bounds |Q k − Q * | with an offset depending on α i , i.e. ∆ k + i α i γ k−i ≥ |Q k − Q * |.</p><p>Proof Let ∆ k be an estimator satisfying Equation <ref type="bibr" target="#b19">20</ref>. In order to show that ∆ k + i γ k−i α i ≥ |Q k − Q * |, we use the principle of mathematical induction. The base case, k = 0 is satisfied, since ∆ 0 + α 0 ≥ |Q 0 − Q * |. Now, let us assume that for a given k = m, <ref type="figure">for each (s, a)</ref>. Now, we need to show that a similar relation holds for k = m + 1, and then we can appeal to the principle of mathematical induction to complete the argument. In order to show this, we note that,</p><formula xml:id="formula_30">∆ m + i γ m−i α i ≥ |Q m − Q * | pointwise</formula><formula xml:id="formula_31">∆ m+1 =|Q m+1 − B * Q m | + γP πm ∆ m + m+1 i γ m+1−i α i (21) =|Q m+1 − B * Q m | + γP πm (∆ m + m i=0 γ m−i α i ) + α m+1 (22) ≥|Q m+1 − B * Q m | + γP πm |Q m − Q * | + α m (23) ≥|Q m+1 − Q * |<label>(24)</label></formula><p>where <ref type="formula" target="#formula_0">(21)</ref>  log γ , the error estimator ∆ k pointwise satisfies:</p><formula xml:id="formula_32">∆ k + k i=0 γ i α k−i ≥ |Q k − Q * |</formula><p>where α i 's are scalar constants independent of any state-action pair. (Note that Theorem 4.2 has a typo γ i instead of γ k−i , this theorem presents the correct inequality.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>Main Idea/ Sketch: As shown in Algorithm 1, the estimator ∆ k is initialized randomly, without taking into account |Q 0 − Q * |. Therefore, in this theorem, we want to show that irrespective of the initialization of Q 0 , a randomly initialized ∆ k eventually satisfies the inequality shown in Theorem 4.2. Now, we present the formal proof.</p><p>Consider k 0 to be the smallest k, such that the following inequality is satisfied:</p><formula xml:id="formula_33">γ k max Q0,Q * |Q 0 − Q * | ≤ 1<label>(25)</label></formula><p>Thus, k 0 ≥ log(1−γ) log γ , assuming R max = 1 without loss of generality. For a different reward scaling, the bound can be scaled appropriately. To see this, we substitute |Q 0 − Q * | as an upper-bound R max /(1 − γ), and bound R max by 1.</p><p>Let ∆ k correspond to the upper-bound estimator as derived in Lemma B.0.2. For each k ≥ k 0 , the contribution of the initial error |Q 0 − Q * | in |Q k − Q * | is upper-bounded by 1, and gradually decreases with a rate γ as more backups are performed, i.e., as k increases. Thus we can construct another sequence ∆ 1 , ·, ∆ k , · · · which chooses to ignore |Q 0 − Q * |, and initializes ∆ 0 = 0 (or randomly) and the sequences ∆ and ∆ k satisfy:</p><formula xml:id="formula_34">|∆ k − ∆ k | &lt; 1, ∀k ≥ k 0 (26)</formula><p>Furthermore, the difference |∆ k − ∆ k | steadily shrinks to 0, with a linear rate γ, so the overall contribution of the initialization sub-optimality |Q 0 − Q * | drops linearly with a rate of γ. Hence, ∆ and ∆ converge to the same sequence beyond a fixed k = k 0 . Since ∆ k is computed using the RHS of Lemma B.0.1, it is guaranteed to be an upper bound on |Q k − Q * |:</p><formula xml:id="formula_35">∆ k + k i=1 γ k−i α i − ∆ k + i=1 γ k−i α i ≤ 1.<label>(27)</label></formula><formula xml:id="formula_36">Since, ∆ k + i γ k−i α i ≥ |Q k − Q * |, we get ∀ k ≥ k 0 , using 27, that ∆ k + k i=1 γ k−i α i ≥ |Q k − Q * | − γ k−k0 .<label>(28)</label></formula><formula xml:id="formula_37">Hence, ∆ k + k i=1 γ k−i α i ≥ |Q k − Q * | for large k.</formula><p>A note on the value of k 0 . For a discounting of γ = 0.95, we get that k 0 ≈ 59 and for γ = 0.99, k 0 ≈ 460. In practical instances, an RL algorithm takes a minimum of about ≥ 1M gradient steps, so this value of k 0 is easy achieved. Even in the gridworld experiments presented in Section 7.1, γ = 0.95, hence, the effects of initialization stayed significant only untill about 59 iterations during training, out of a total of 300 or 500 performed, which is a small enough percentage.</p><p>Summary of Proof for Theorem 4.2. ∆ k in DisCor is given by the quantity ∆ k = |Q k − B * Q k−1 | + γP π k−1 ∆ k−1 , is an upper bound for the error |Q k − Q * |, and we can safely initialize the parametric function ∆ φ using standard neural network initialization, since the value of initial error will matter only infinitesimally after a large enough k.</p><p>As k → ∞, the following is true:</p><formula xml:id="formula_38">lim k→∞ |∆ k − |Q k − Q * | ≤ lim k→∞ k i=1 γ k−i α i (29) = lim k→∞ k i=1 γ k−i D TV (π i , π * )<label>(30)</label></formula><p>Also, note that if π k is improving, i.e. π k → π * , then, we have that D TV (π k , π * ) → 0, and since limit of a sum is equal to the sum of the limit, and γ &lt; 1, therefore, the final inequality in Equation 30 tends to 0 as k → ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Missing Proofs From Section 3</head><p>In this section, we provide omitted proofs from Section 3 of this paper. Before going into the proofs, we first describe notation and prove some lemmas that will be useful later in the proofs. We also describe the underlying ADP algorithm we use as an ideal algorithm for the proofs below. Collect trajectories using πt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Choose distribution Dt for projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Qt+1 <ref type="figure">Qt−1(s, a)</ref>) 2 ] 6: end for Assumptions. The assumptions used in the proofs are as follows:</p><formula xml:id="formula_39">← D t B * Qt D t B * = arg minQ ED t [(Q(s, a) − B *</formula><p>• Q-function is linearly represented, i.e. given a set of features, φ(s, a) ∈ R d for each state and action, concisely represented as the matrix Φ ∈ R |S||A|×d , Q-learning aims to learn a d-dimensional feature vector w, such that Q(s, a) = w T φ(s, a). This is not a limiting factor as we will prove in Assumption C.1, for problems with sufficiently large |S| and |A|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Suboptimal Convergence of On-policy Q-learning</head><p>We first present a result that describes how Q-learning can converge sub-optimally when performed with on-policy distributions, thus justifying our empirical observation of suboptimal convergence with on-policy distributions.</p><p>Theorem C.1 <ref type="bibr">([42]</ref>) Projected Bellman optimality operator under the on-policy distribution H = Dπ B * with a Boltzmann policy, π ∝ exp(Q/τ ), where 0 &lt; τ always has one or more fixed points.</p><p>Proof This statement was proven to be true in <ref type="bibr" target="#b41">[42]</ref>, where it was shown the projection operator H has the same fixed points as another operator, F α given by:</p><formula xml:id="formula_40">F α (x) := x + αΦ T D π (B * Φx − Φx)<label>(31)</label></formula><p>where α ∈ (0, 1) is a constant. They showed that the operator F α is a contraction for small-enough α and used a compact set argument to generalize it to other positive values of α. We refer the reader to <ref type="bibr" target="#b41">[42]</ref> for further reference. They then showed a 2-state MDP example (Example 6.1, <ref type="bibr" target="#b41">[42]</ref>) such that the Bellman operator H has 2 fixed points, thereby showing the existence of one or more fixed points for the on-policy Bellman backup operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Theorem 3.1</head><p>We now provide an existence proof which highlights the difference in the speeds of learning accurate Q-values from online or on-policy and replay buffer distributions versus a scheme like DisCor. We first state an assumption (Assumption C.1) on the linear features parameterization used for the Q-function. This assumption ensures that the optimal Q-function exists in the function class (i.e. linear function class) used to model the Q-function. This assumption has also been used in a number of recent works including <ref type="bibr" target="#b42">[43]</ref>. Analogous to <ref type="bibr" target="#b42">[43]</ref>, in our proof, we show that this assumption is indeed satisfied for features lying in a space that is logarithmic in the size of the state-space. For this theorem, we present an episodic example, and operate in a finite horizon setting with discounting γ and H denotes the horizon length. An episode terminates deterministically as soon as the run reaches a terminal node -in our case a leaf node of the tree MDP, i.e. a node at level H − 1 -as we will see next.</p><p>Assumption C.1 There exists δ ≥ 0, and w ∈ R d , such that for any (s, a) ∈ S × A, the optimal Q-function satisfies: |Q * (s, a) − w T φ(s, a)| ≤ δ.</p><p>We first prove an intermediate property of Φ satisfying the above assumption that will be crucial for the lower bound argument for on-policy distributions.</p><p>Corollary C.1.1 There exists a set of features Φ ∈ R 2 H ×O(H 2 /ε 2 ) satisfying assumption C.1, such that the following holds:</p><formula xml:id="formula_41">||I 2 H − ΦΦ T || ∞ ≤ .</formula><p>Proof This proof builds on the existence argument presented in <ref type="bibr" target="#b42">[43]</ref>. Using the ε-rank property of the identity matrix, one can show that there exists a feature set Φ ∈ R 2 H ×O(H/ε 2 ) such that ||I 2 H − ΦΦ T || ∞ ≤ . Thus, we can choose any such Φ, for a sufficiently low threshold ε. In order to assign features Φ to a state, we can simply perform an enumeration of nodes in the tree via a standard graph search procedure such as depth first search and assign a node (s, a) a feature vector φ(s, a). To begin with, let's show how we can satisfy assumption C.1 by choosing a different weight vector w h for each level h, such that we obtain |Q h (s, a) − w T h φ(s, a)| ≤ . Since for each level h exactly one state satisfies Q * (s j , a j ) = γ H−j+1 , so we can just let w j = γ H−j+1 φ(s j , a j ) and thus we are able to satisfy Assumption C.1. This is the extent of the argument used in <ref type="bibr" target="#b42">[43]</ref>. Now we generalize this argument to find a single w ∈ R d , unlike different weights w h for different levels h. In order to do this, we create a new Φ , of size Φ ∈ R 2 H ×O(H 2 / 2 ) (note H 2 versus H dimensions for Φ and Φ) given , 0, ..., 0</p><formula xml:id="formula_42">  <label>(32)</label></formula><p>Essentially, we pad Φ with zeros, such that for (s, a) belonging to a level h, Φ is equal to Φ in the h−th, dim(φ(s, a))-sized block. A choice of a single w ∈ R dim(Φ (s,a)) for Φ is given by simply concatenating w 1 , · · · , w h found earlier for Φ.</p><formula xml:id="formula_43">w = [w 1 , w 2 , · · · , w H ]<label>(33)</label></formula><p>It is easy to see that w T Φ satisfies assumption C.1. A fact that will be used in the proof for Theorem C.2, is that this construction of Φ also satisfies:</p><formula xml:id="formula_44">||I 2 H − Φ Φ T || ∞ ≤ .</formula><p>We now restate the theorem from Section 3 and provide a proof below.</p><p>Theorem C.2 (Exponential lower bound for on-policy distributions) There exists a family of MDPs parameterized by H &gt; 0, with |S| = 2 H , |A| = 2 and a set of features satisfying Assumption C.1, such that on-policy sampling distribution, i.e. D k = d π k , requires Ω γ −H exact fixed-point iteration steps in the generic algorithm (Algorithm 2) for convergence, if at all, the algorithm converges to an ε−accurate Q-function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>Tree Construction. Consider the family of tree MDPs like the one shown in <ref type="figure" target="#fig_16">Figure 10</ref>. Both the transition function T and the reward function r are deterministic, and there are two actions at each state: a 1 and a 2 . There are H level of states, thereby forming a full binary tree of depth H. Executing action a 1 transitions the state to its left child int he tree and executing action a 2 transitions the state to its right child. There are 2 h states in level h. Among the 2 H−1 states in level H − 1, there is one state, s * , such that action a * at this state yields a reward of r(s * , a * ) = 1. For other states of the MDP, r(s, a) = 0. This is a typical example of a sparse reward problem, generally used for studying exploration <ref type="bibr" target="#b42">[43]</ref>, however, we re-iterate that in this case, we are primarily interested in the number of iterations needed to learn, and thereby assume that the algorithm is given infinite access to the MDP, and all transitions are observed, and the algorithm just picks a distribution D k , in this case, the on-policy state-action marginal for performing backups.</p><p>Main Argument. Now, we are equipped with a family of the described tree MDPs and a corresponding set of features Φ which can represent an ε−accurate Q-function. Our aim is to show that on-policy Q-learning takes steps, exponential in the horizon for solving this task. For any stochastic policy π(a|s), andp defined asp = min s∈S,a∈A π(a|s), 0 &lt;p &lt; 0.5, the marginal state-action distribution satisfies:</p><formula xml:id="formula_45">d π (s * , a * ) ≤ γ H · (1 −p) H+1<label>(34)</label></formula><p>Since d π is a discounted state-action marginal distribution, another property that it satisfies is that:</p><formula xml:id="formula_46">c ≤ ||d π || 2 ≤ 1 1 − γ 2 H<label>(35)</label></formula><p>where c is a constant c &gt; 0. The above is true, since, there are 2 H states in this MDP, and the maximum values of any entry in d π can be 1 1−γ since, 1 − γ is the least eigenvalue of (I − γP π ) for any policy π, since ||P π || 2 = 1. Now, under an on-policy sampling scheme and a linear representation of the Q-function as assumed, the updates on the weights for each iteration of Algorithm 2 are given by (D π k represents Diag(d π k )):</p><formula xml:id="formula_47">w k+1 = Φ T D π k Φ −1 Φ T D π k (r + γP π k Φw k )<label>(36)</label></formula><p>Now, ||D π k r|| ≤ γ H (1 −p) H+1 ||φ(s * , a * )|| from the property Equation <ref type="bibr" target="#b33">34</ref>. Hence, the maximum 2-norm of the updated w k+1 is given by:</p><formula xml:id="formula_48">||w k+1 || 2 ≤ || Φ T D π k Φ −1 Φ T D π k R|| 2 + γ|| Φ T D π k Φ −1 Φ T D π k P π k Φw k || 2 ≤ γ H (1 −p) H+1 ||D π k || F · (1 − ε) · 2 H−1 + γ||w k || 2 ≤ γ H (1 −p) H+1 c (1 − ε) · 2 H−1 + ||w k || 2 = (γ) H · c · (1) (1 − ε) · 2 H−1 + ||w k || 2 .<label>(37)</label></formula><p>where the first inequality follows by an application of the triangle inequality, the second inequality follows by using the minimum value of the Frobenius norm of the matrix Φ to be (1 − ε) · 2 H−1 (using the ε−rank lemma used to satisfy Assumption C.1) in the denominator of the first term, bounding ||D π k r|| by Equation <ref type="bibr" target="#b33">34</ref>, and finally bounding the second term by γ||w k || 2 , since the maximum eigenvalue of the entire matrix in front of w k is ≤ 1, as it is a projection matrix with a discount γ valued scalar multiplier. The third inequality follows from lower bounding D π k by c using Equation <ref type="bibr" target="#b34">35</ref>.</p><p>The optimal w * is given by the fixed point of the Bellman optimality operator, and in this case satisfies the following via Cauchy-Schwartz inequality,</p><formula xml:id="formula_49">(I − γP * )Φw * = r =⇒ ||Φ|| F · ||w * || 2 ≥ ||(I − γP * ) −1 r|| ≥ 1 1 + γ ||r|| 2 =⇒ (1 + ε) · 2 H−1 · ||w * || 2 ≥ 1 1 + γ =⇒ ||w * || 2 ≥ 1 1 + γ · 2 −H+1 · (1 + ε) −1<label>(38)</label></formula><p>Thus, in order for w k to be equal to w * , it must satisfy the above condition (Equation <ref type="bibr" target="#b37">38</ref>). If we choose an initialization w 0 = 0 (or a vector sufficiently close to 0), we can compute the minimum number of steps it will take for on-policy ADP to converge in this setting by using 37 and 38:</p><formula xml:id="formula_50">k ≥ (1 + γ) −1 · 2 −H+1 · (1 + ε) −1 (γ) H · (1 −p) H · (c) (1−ε)·2 H−1 =⇒ k ≈ Ω γ −H<label>(39)</label></formula><p>for sufficiently small ε. Hence, the bound follows.</p><p>A note on the bound. Since typically RL problems usually assume discount factors γ close to 1, one might wonder the relevance is this bound in practice. We show via an example that this is indeed relevant. In particular, we compute the value of this bound for commonly used γ,p and H. For a discount γ = 0.99, and a minimum probability ofp = 0.01 (as it is common to use entropy bonuses that induce a minimum probability of taking each action), this bound is of the order of (γ · (1 −p)) H ≈ 10 9 for H = 1000 <ref type="bibr" target="#b39">(40)</ref> for commonly used horizon lengths of 1000 (example, on the gym benchmarks).</p><p>Corollary C.2.1 (Extension to replay buffers) There exists a family of MDPs parameterized by H &gt; 0, with |S| = 2 H , |A| = 2 and a set of features Φ satisfying assumption C.1, such that ADP with replay buffer distribution takes Ω(γ −H ) many steps of exact fixed-point iteration for convergence of ADP, if at all convergence happens to an −accurate Q-function.</p><p>Proof For replay buffers, we can prove a similar statement as previously. The steps in this proof follow exactly the steps in the proof for the previous theorem. With replay buffers, the distribution for the projection at iteration k is given by:</p><formula xml:id="formula_51">d k (s, a) = 1 k k i=1 d π k (s, a)<label>(41)</label></formula><p>Therefore, we can bound the probability of observing any state-action pair similar to Equation 34 as:</p><formula xml:id="formula_52">d k (s * , a * ) ≤ 1 k k i=1 γ H · (1 −p) H+1<label>(42)</label></formula><p>withp as defined previously. Note that this inequality is the same as the previous proof, and doesn't change. We next bound the 2-norm of the state-visitation distribution, in this case, the state-distribution in the buffer.</p><formula xml:id="formula_53">c ≤ ||d k || 2 ≤ 1 1 − γ · 2 H<label>(43)</label></formula><p>where c &gt; 0. The two main inequalities used are thus the same as the previous proof. Now, we can simply follow the previous proof to prove the result.</p><p>Practical Implications. In this example, both on-policy and replay buffer Q-learning suffer from the problem of exponentially many samples need to reach the optimal Q-function. Even in our experiments in Section 3, we find that on-policy distributions tend to reduce errors very slowly, at a rate that is very small. The above bound extends this result to replay buffers as well.</p><p>In our next result, however, we show that an optimal choice of distribution, including DisCor, can avoid the large iteration complexity in this family of MDPs. Specifically, using the errors against Q * , i.e. |Q k − Q * | can help provide a signal to improve the Q-function such that this optimal distribution / DisCor will take only poly(H) many iterations for convergence.</p><p>Corollary C.2.2 (Optimal distributions / DisCor) In the tree MDP family considered in Theorem 3.1, with linear function approximation for the Q-function, and with Assumption C.1 for the features Φ, DisCor takes poly(H) many exact iterations for ε−accurate convergence to the optimal Q-function.</p><p>Proof We finally show that the DisCor algorithm, which prioritizes states based on the error in target values, will take poly(H) many steps for convergence. Assume that Q-values are initialized randomly, for example via a normal random variable with standard deviation σ, i.e., Q 0 (s, a) ∼ N (0, σ 2 ), however, σ is very small, but is more than 0 (σ &gt; 0) (this proof is still comparable to the proof for on-policy distributions, since Q-values can also be initialized very close to 0 even in that case, and the proof of Theorem C.2 still remains valid.). Now we reason about a run of DisCor in this case.</p><p>Iteration 1. In the first iteration, among all nodes in the MDP, the leaf nodes (depth H-1) have 0 error at the corresponding target values, since an episode terminates once a rollout reaches a leaf node. Hence, the algorithm will assign equal mass to all leaf node states, and exactly update the Q-values for nodes in this level (upto ε-accuracy).</p><p>Iteration 2. In the second iteration, the leaf nodes at level H −1 have accurate Q-values, therefore, the algorithm will pick nodes at the level H − 2, for which the target values, i.e. Q-values for nodes at level H − 1, have 0 error along with nodes at level H − 1. The algorithm will update Q-values at these nodes at level H − 2, while ensuring that the incurred error at the nodes at level H − 1 isn't beyond ε, since nodes at both levels are chosen. Since, the optimal value function Q * can be represented upto ε−accuracy, we can satisfy this criterion.</p><p>Iteration k. In iteration k, the algorithm updates Q-values for nodes at level H − k, while also ensuring Q-values for all nodes at a level higher than H − k are estimated within the range of ε−allowable error, since all the nodes below level H − k are updated. This is feasible since, Q * is expressible with ε−accuracy within the linear function class chosen. This iteration process continues, and progress level by level, from the leaves (level H − 1) to the root (level 0). At each iteration Q-values for all states at the same level, and below are learned together. Since learning progresses in a "one level at-a-time" fashion, with guaranteed correct target values (i.e. target values are equal to the optimal Q-function Q * ) for any update that the algorithm performs, it would take at most poly(H) many iterations (for example, multiple passes through the depth of the tree) for ε-accurate convergence to the optimal Q-function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Extended Related Work</head><p>Error propagation in ADP. A number of prior works have analysed error propagation in ADP methods. Most work in this area has been devoted to analysing how errors in Bellman error minimization propagate through the learning process of the ADP algorithm, typically focusing on methods such as fitted Q-iteration (FQI) <ref type="bibr" target="#b9">[10]</ref> or approximate policy iteration <ref type="bibr" target="#b43">[44]</ref>. Prior works in this area assume an abstract error model, and analyze how errors propagate. Typically these prior works only limitedly explore reasons for error propagation or present methods to curb error propagation. <ref type="bibr" target="#b44">[45]</ref> analyze error propagation in approximate policy iteration methods using quadratic norms. <ref type="bibr" target="#b18">[19]</ref> analyze the propagation of error across iterations of approximate value iteration (AVI) for L p -norm p = (1, 2). <ref type="bibr" target="#b20">[21]</ref> provide finite sample guarantees of AVI using error propagation analysis. Similar ideas have been used to provide error bounds for a number of different methods - <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> and many more. In this work, we show that ADP algorithms suffer from an absence of corrective feedback, which arises because the data distribution collected by an agent is insufficient to ensure that error propagation is eventually corrected for. We further propose an approach, DisCor, which can be used in conjunction with modern deep RL methods.</p><p>Offline / Batch Reinforcement Learning. Our work bears similarity to the recent body of literature on batch, or offline reinforcement learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. All of these works exploit the central idea of constraining the policy to lie in a certain neighborhood of the behavior, data-collection policy. While <ref type="bibr" target="#b29">[30]</ref> show that this choice can be motivated from the perspective of error propagation, we note that there are clear differences between our work and such prior works in batch RL. First, the problem statement of batch RL requires learning from completely offline experience, however, our method learns online, via on-policy interaction. While error propagation is a reason behind incorrect Q-functions in batch RL, we show that such error accumulation also happens in online reinforcement, which results in a lack of corrective feedback.</p><p>Generalization effects in deep Q-learning. There are a number of recent works that theoretically analyze and empirically demonstrate that certain design decisions for neural net architectures used for Q-learning, or ADP objectives can prove to be significant in deep Q-learning. For instance, <ref type="bibr" target="#b28">[29]</ref> point out that sparse representations may help Q-learning algorithms, which links back to prior literature on state-aliasing and destructive interference. <ref type="bibr" target="#b27">[28]</ref> uses an objective inspired from the neural tangent kernel (NTK) <ref type="bibr" target="#b50">[51]</ref> to "cancel" generalization effects in the Q-function induced across state-action pairs to mimic tabular and online Q-learning. Our approach, DisCor, can be interpreted as only indirectly affecting generalization via the target Q-values for state-action pairs that will be used as bootstrap targets for the Bellman backup, which are expected to be accurate with DisCor, and this can aid generalization, similar to how generalization can be achieved via abstention from training on noisy labels in supervised learning <ref type="bibr" target="#b17">[18]</ref>.</p><p>Replay Buffers and Generalization. There are some prior works performing analytical studies on the size of the replay buffer <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, which propose that larger replay buffers might hurt training with function approximation. Partly this problem goes away if smaller buffers are used, or if the algorithm chooses to replay recent experience more often. Our work indicates an absence of corrective feedback problem -online data collection might not be able to correct errors in the Q-function -which is distinct from the size of the replay buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Experimental Details</head><p>In this section, we provided experimental details, such as the DisCor algorithm in practice (Section E.1), and the hyperparameter choices (Section E.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 DisCor: Deep RL Version</head><p>1: Initialize online Q-network Q θ (s, a), target Q-network, Qθ(s, a), error network ∆ φ (s, a), target error network ∆φ, initial distribution p0(s, a), a replay buffer β and a policy π ψ (a|s), number of gradient steps G, target network update rate η, initial temperature for computing weights w k , τ0. 2: for step k in {1, . . . , } do <ref type="bibr">3:</ref> Collect M samples using π ψ (a|s), add them to replay buffer β, sample {(si, ai)} N i=1 ∼ β 4:</p><p>Evaluate Q θ (s, a) and ∆ φ (s, a) on samples (si, ai).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute target values for Q and ∆ on samples:</p><formula xml:id="formula_54">yi = ri + γE a ∼π ψ (a |s ) [Qθ(s i , a )] ∆i = |Q θ (s, a) − yi| + γEâ i ∼π(a i |s ) [∆φ(s i ,âi)] 6:</formula><p>Compute w k using Equation 8 with temperature τ k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Take G gradient steps on the Bellman error for training Q θ weighted by w k .</p><formula xml:id="formula_55">θ ← θ − α∇ θ 1 N N i=1 w k (si, ai) · (Q θ (si, ai) − yi) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Tale G gradient steps to minimize unweighted (regular) Bellman error for training φ.</p><formula xml:id="formula_56">φ ← φ − α∇ φ 1 N N i=1 (∆ θ (si, ai) −∆i) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Update the policy π ψ if it is explicitly modeled.</p><formula xml:id="formula_57">ψ ← ψ + α∇ ψ E s∼β,a∼π ψ (a|s) [Q θ (s, a)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Update target networks using soft updates (SAC), hard updates (DQN)</p><formula xml:id="formula_58">θ ← (1 − η)θ + ηθ φ ← (1 − η)φ + ηφ 11:</formula><p>Update temperature hyperparameter for DisCor:</p><formula xml:id="formula_59">τ k+1 ← (1 − η)τ k + η batch-mean(∆ φ (si, ai))</formula><p>12: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 DisCor in Practice</head><p>In this section, we provide details on the experimental setup and present the pseudo-code for the practical instantiation of our algorithm, DisCor. The pseudocode for the practical algorithm is provided in Algorithm 3. Like any other ADP algorithm, such as DQN or SAC, our algorithm maintains a pair of Q-functions -the online Q-network Q θ and a target network Qθ. For continuous control domains, we use the clipped double Q-learning trick <ref type="bibr" target="#b4">[5]</ref>, which is also referred to as the "twin-Q" trick, and it further parametrizes another pair of online and target Q-functions, and uses the minimum Q-value for backup computation. In addition to Q-functions, in a continuous control domain, we parametrize a separate policy network π ψ similar to SAC. In a discrete action domain, the policy is just given by a greedy maximization of the online Q-network.</p><p>DisCor further maintains a model for accumulating errors ∆ φ parameterized by φ and the corresponding target error network ∆φ. In the setting with two Q-functions, DisCor models two networks, one for modelling error in each Q-function. At every step, a few (depending upon the algorithm) gradient steps are performed on Q and ∆, and π -if it is explicitly modeled, for instance in continuous control domains. This is a modification of generalized ADP Algorithm 2 and the corresponding DisCor version (Algorithm 1), customized to modern deep RL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Experimental Hyperparameter Choices</head><p>We finally specify the hyperparameters we used for our experiments. These are as follows:  present some individual-environment learning curves for these environments comparing different methods in both exact ( <ref type="figure" target="#fig_20">Figure 13</ref>) and sampled ( <ref type="figure" target="#fig_5">Figure 14</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 MetaWorld Tasks</head><p>In this section, we first provide a pictorial description of the six hard tasks we tested on from meta-world, where SAC usually does not perform very well. <ref type="figure" target="#fig_7">Figure 15</ref> shows these tasks. We provide the trends for average return achieved during evaluation (not the success rate as shown in <ref type="figure" target="#fig_9">Figure 7</ref> in Section 7) for each of the six tasks. Note that DisCor clearly outperforms both the baseline SAC and the prior method PER in all six cases, achieving nearly 50% more than the returns achieved by SAC.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 OpenAI Gym Benchmarks</head><p>Here we present an evaluation on the standard OpenAI continuous control gym benchmark environments. Modern ADP algorithms such as SAC can already solve these tasks easily, without any issues, since these algorithms have been tuned on these tasks. A comparison of the three algorithms DisCor, SAC and PER, on three of these benchmark tasks is shown in <ref type="figure" target="#fig_9">Figure 17</ref>. We note that in this case, all the algorithms are roughly comparable to each other. For instance, DisCor performs better than SAC and PER on Walker2d, however, is outperformed by SAC on Ant.</p><p>Stochastic reward signals. That said, we also performed an experiment to verify the impact of stochasticity, such as noise in the reward signal, on the DisCor algorithm as compared to other baseline algorithms like SAC and PER. Analogous the diagnostic tabular experiments on low signal-to-noise ratio environments, such as those with sparse reward, we would expect a baseline ADP method to be impacted more due to an absence of corrective feedback in tasks with stochastic reward noise, since a noisy reward effectively reduces the signal-to-noise ratio. We would also expect a method that ensures corrective feedback to perform better. In order to test this hypothesis, we created stochastic reward tasks out of the OpenAI gym benchmarks. We modified the reward function r(s, a) in these gym tasks to be equal to: r (s, a) = r(s, a) + z, z ∼ N (0, 1) <ref type="bibr" target="#b43">(44)</ref> and the agent is only provided these noisy rewards during training. However, we only report the deterministic ground-truth reward during evaluation. We present the results in <ref type="figure" target="#fig_11">Figure 18</ref>. Observe that in this scenario, DisCor  From left to right: pull stick, push with wall, push with stick, turn dial, hammer and insert peg side tasks. Note that DisCor clearly achieves better returns or learns faster in most of the tasks. emerges as the best performing algorithm on these tasks, and outperforms other baselines SAC and PER both in terms of asymptotic performance (example, HalfCheetah) and sample efficiency (example, Ant). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Walker2d-v2</head><p>DisCor PER SAC <ref type="figure" target="#fig_9">Figure 17</ref>: Peformance of DisCor, SAC and PER on gym benchmarks. On an average, all methods perform roughly similarly on these settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Walker2d-v2</head><p>DisCor PER SAC <ref type="figure" target="#fig_11">Figure 18</ref>: Performance of DisCor, SAC and PER on continuous control gym benchmarks with stochastic reward noise. Note that DisCor learns slightly faster and performs better than SAC and PER on these stochastic problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 MT10 Multi-Task Experiments</head><p>In this section, we present the trend of returns, as a learning curve and as a comparative histogram (at 1M environment steps of training) for the multi-task MT10 benchmark, extending the results shown in Section 7.3, <ref type="figure" target="#fig_11">Figure 8</ref>. These plots are shown in <ref type="figure">Figure 19</ref>. Observe that DisCor achieves more than 30% of the return of SAC, and obtains an individually higher value of return on more tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT-10 Multi Task Benchmark</head><p>DisCor SAC (a) Average task return (b) Per-task return at 1M steps <ref type="figure">Figure 19</ref>: Performance of DisCor and SAC on the MT10 benchmark. Returns for DisCor are higher than SAC by around 30%; <ref type="formula">(2)</ref> DisCor achieves a non-trivial return on 7/10 tasks after 1000k steps, as compared to 3/10 for unweighted SAC, similar to the trend at 500k steps shown in <ref type="figure" target="#fig_11">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 MT50 Multi-Task Experiments</head><p>We further evaluated the performance of DisCor on the multi-task MT50 benchmark <ref type="bibr" target="#b13">[14]</ref>. This is an extremely challenging benchmark where the task is to learn a single policy that can solve 50 tasks together, with the same evaluation protocol as previously used in the MT10 experiments (Section 7.3 and Appendix F.4). We present the results (average task return and average success rate) in Figures 20. Note that while SAC tends to saturate/plateau in between 4M -8M steps, accounting for corrective feedback via the DisCor algorithm makes the algorithm continue learning in that scenario too.  into training, rounded off to the nearest integer. Note that DisCor clearly outperforms DQN with multi-step returns. We also find that adding n-step returns to DisCor can hurt, for instance, on Breakout, where the same hurts with DQN as well (for comparison, see <ref type="figure">Figure 9</ref> in the main paper), however, we still observe that DisCor, when applied with multi-step returns performs better than DQN with multi-step returns as well, indicating the benefits of DisCor even when methods such as multi-step returns are used.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.6 Comparison with AFM</head><p>In this section, we present a comparison of DisCor and AFM <ref type="bibr" target="#b5">[6]</ref>, a prior method similar to prioritized experience replay on the MuJoCo gym benchmarks. We find that DisCor clearly outperforms AFM in these scenarios. We present these results in <ref type="figure" target="#fig_15">Figure 21</ref> where the top row presents results in the case of regular gym benchmarks, and the bottom row presents results in the case of gym benchmarks with stochastic reward noise. F.7 DQN with multi-step returns N-step returns with DQN are hypothesized to stabilize learning since updates to the Q-function now depends on reward values spanning multiple steps, and the coefficient of the bootstrapped Q-value is γ T , which is exponentially smaller than γ used conventionally in Bellman backups, implying that the error accumulation process due to incorrect targets is reduced. Thus, we perform a comparison of DisCor and DQN with n-step backups, where n was chosen to be 3, n = 3, in accordance with commonly used multi-step return settings for Atari games. We present the average return obtained by DisCor and DQN (+n-step), with sticky actions, in <ref type="table" target="#tab_8">Table 1</ref>. We clearly observe that DisCor outperforms DQN with 3-step returns in all three games evaluated on. We also observe that n-step returns applied with DisCor also outperform n-step returns applied with DQN, indicating the benefits of using DisCor even when other techniques, such as n-step returns are used.  with stochastic reward noise (bottom row). Note that DisCor clearly out-performs AFM in both scenarios, on all three benchmarks tested on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.8 Code for the Method</head><p>The code is shown in <ref type="figure">Figure 22</ref>. It is a simplified version of the code from our implementation of DisCor on top of the official SAC repository <ref type="bibr" target="#b54">[55]</ref>.  <ref type="figure">Figure 22</ref>: Code for training the error function ∆ φ , and modified training for the Q-function Q(s, a) using ∆ φ to get weights w(s, a) for training. Code written in convention with regular Tensorflow guidelines, in the same style as the official SAC implementation <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>34</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>B * Q)(s, a) = r(s, a) + γE s ∼T [max a Q(s , a )]. The goal is to converge to the optimal value function, Q * , by applying successive Bellman projections. With function approximation, these algorithms project the values of the Bellman optimality operator B * onto a family of Q-function approximators Q (e.g., deep neural nets) under a sampling or data distribution µ, such that Q k+1 ← Π µ (B * Q k ) and Π µ (Q) def = argmin Q ∈Q E s,a∼µ [(Q (s, a) − Q(s, a)) 2 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 3 . 1 (</head><label>31</label><figDesc>Exponential lower bound for on-policy and replay buffer distributions) There exists a family of MDPs parameterized by H &gt; 0, with |S| = 2 H , |A| = 2, such that even with features, Φ that can represent the optimal Q-function near-perfectly, i.e., ||Q * − Φw|| ∞ ≤ ε, on-policy or replay-buffer Q-learning, i.e. D k = d π k , or D k = k i=1 d πi respectively, requires Ω γ −H exact Bellman projection steps for convergence to Q * , if at all the algorithm converges to Q * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Instability for replay buffer distributions: return (dashed) and value error (solid) over training iterations. Note the rapid increase in value error at multiple points, which co-occurs with instabilities in returns. Error (left) and returns (right) for sparse reward MDP with replay buffer distributions. Note the inability to learn, low return, and highly unstable value error E k , often increasing sharply, destabilizing the learning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Experiments showing various detrimental consequences of an absence of corrective feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Value Error E k / return for two runs of DisCor (blue) and DisCor(oracle) (red) in exact (left) and sampled (right) settings.Observe that (i) DisCor achieves similar performance as DisCor (oracle) generally, (ii) DisCor provides corrective feedback: value error decreases with both DisCor and DisCor(oracle).(b) Comparative Performance on tabular domains</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Performance of DisCor, DisCor (oracle), replay buffer, PER, on-policy and uniform sampling averaged across tabular domains with (right) and without (middle) sampling error. Note that: (1) DisCor generally ensures corrective feedback, (2) DisCor is generally comparable to DisCor (oracle), however, DisCor (oracle) outperforms DisCor, as expected, and (3) DisCor (DisCor and DisCor (oracle) generally outperform all distributions. Exact setup for these domains is described in Appendix F.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Visual description of the six MetaWorld tasks used in our experiments in Section 7. Figures taken from<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Evaluation success of DisCor, unweighted SAC and PER on six MetaWorld tasks. From left to right: pull stick, push with wall, push stick, turn dial, hammer and insert peg side. Note that DisCor achieves better final success rates or learns faster on most of the tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Average success rate (b) Per-task return at 500k environment steps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Performance of DisCor (blue) and unweighted SAC (green) on the MT10 benchmark. We observe that: (1) DisCor outperforms unweighted SAC by a factor of 1.5 in terms success rate; (2) DisCor achieves a non-trivial return on 7/10 tasks after 500k environment steps, as compared to 3/10 for unweighted SAC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Proof of Theorem 4 . 2 .</head><label>42</label><figDesc>We now present a Lemma B.0.1 which proves a recursive inequality for |Q k − Q * |, then show that the corresponding recursive estimator upper bounds |Q k − Q * | pointwise in Lemma B.0.2, and then finally show that our chosen estimator ∆ k is equivalent to this recursive estimator in Theorem B.1 therefore proving Theorem 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>follows from the definition of ∆ k ,<ref type="bibr" target="#b21">(22)</ref> follows by rearranging the recursive sum containing α i , for i ≤ m alongside ∆ m , (23) follows from the inductive hypothesis at k = m, and (24) follows from Lemma B.0.1.Thus, by using the principle of mathematical induction, we have shown that∆ k + i γ k−i α i ≥ |Q k − Q * | pointwise for each s, a, for every k ∈ N.The final piece in this argument is to show, that the estimator ∆ k used by the DisCor algorithm (Algorithm 1), which is initialized randomly, i.e. not initialized to ∆ 0 = |Q 0 − Q * |, still satisfies the property from Lemma B.0.2, possibly for certain k ∈ N.Therefore, we now show why: ∆ k + k i=1 α i γ k−i ≥ |Q k − Q * | point-wise for a sufficiently large k. We restate a slightly modified version of Theorem 4.2 for convenience.Theorem B.1 (Formal version of Theorem 4.2) For a sufficiently large k ≥ k 0 = log(1−γ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Algorithm 2 ADP algorithm 1 :</head><label>21</label><figDesc>Generic Initialize Q-values Q0.2:  for step t in {1, . . . , N} do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>Example element of the tree family of MDPs used to prove the lower bound in Theorem C.2. Here, the depth of the tree H = 2. r(s) = 0 implies that executing any action a 1 or a 2 , a reward of 0 is obtained as state s. (s * , a * ) is given by state marked r(s, a 1 ) = 1. any Φ satisfying the argument in the above paragraph, such that Φ (s, a) =    0, ..., 0 h×dim(φ(s,a)) , Φ(s, a) dim(φ(s,a))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 11 :</head><label>11</label><figDesc>Performance of different methods: DisCor (blue), DisCor (oracle) (red), Replay buffer Q-learning (green, on-policy (grey) and prioritized updates (orange), across different environments measured in terms of smooth normalized returns in the exact setting with all transitions. Note that DisCor and DisCor (oracle) generally tend to perform better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>Performance of different methods: DisCor (blue), DisCor (oracle) (red), Replay buffer Q-learning (green) and prioritized updates (orange). across different environments measured in terms of smooth normalized return with sampled transitions. Note that DisCor and DisCor (oracle) generally tend to perform better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 13 :</head><label>13</label><figDesc>Learning curves for different algorithms in the exact setting. Note that DisCor (blue) and DisCor (oracle) (red) are generally the best algorithms in these settings. Replay Buffers (green) help over on-policy (pink) distributions. Prioritizing transitions based on high Bellman error (orange) is performant in some cases, but hurts in the other cases -it is especially slow in cases with sparse rewards, note the speed of learning on grid16randomsparse and grid16smoothsparse (right of the vertical line) environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 14 :</head><label>14</label><figDesc>Learning curves for different algorithms in the sampled setting. Note that DisCor and DisCor (oralce) anre generally the best algorithms in these settings. Replay Buffers (green) help over on-policy (gray) distributions, but may the algorithm may still fail to reach optimal return. Prioritizing for high Bellman error (PER) may fail to learn in sparse-reward tasks as is evident from the curves for sparse reward environments (right of the vertical line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 15 :</head><label>15</label><figDesc>Visual description of the six MetaWorld tasks used in our experiments in Section 7. Figures taken from<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 16 :</head><label>16</label><figDesc>Evaluation average return achieved by DisCor (blue), SAC (green) and PER (orange) on six Metaworld benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 20 :</head><label>20</label><figDesc>Performance of DisCor and SAC on the MT50 benchmark. Note that, DisCor clearly keeps learning unlike SAC which tends to plateau for about 3M steps in the middle (the stretch between 4M and 7M steps on the x-axis, where SAC exhibits a small gradient in the learning progress, whereas DisCor continuously keeps learning).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 21 :</head><label>21</label><figDesc>Performance of DisCor, SAC, PER and AFM on continuous control gym benchmarks (top row) and gym benchmarks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head># # Train Q -function 22 Q_training_ops 26 # 29 # 33 # 36 # 40 #</head><label>222629333640</label><figDesc>= tf . contrib . layers . optimize_loss ( loss , learning_rate = self . _Q_lr , 23 optimizer = self . _Q_optimizer , variables = self . _Q . trainable_variables ) 24 training_ops . update ({ 'Q ': tf . group ( Q_training_ops ) }) 25 # Training the error function 27 err_values = self . _error_fns ([ self . _observations_ph , self . _actions_ph ]) 28 # Mean Bellman error used to compute target values for error 30 bellman_errors = tf . abs ( Q_values -Q_target ) 31 err_targets = tf . stop_gradient ( self . _get_error_target ( bellman_errors ) ) 32 # This is used to update the moving mean , self . _error_model_tau_ph 34 self . _mean_error_values = tf . reduce_mean ( err_values ) 35 # Simple mean squared error loss for \ delta_ \ phi 37 err_losses = tf . losses . mean_squared_error ( 38 labels = err_targets , predictions = err_values , weights =0.5) 39 # Update error function : \ delta_ \ phi 41 err_training_ops = tf . contrib . layers . optimize_loss ( err_losses , 42 learning_rate = self . _dist_lr , 43 optimizer = self . _err_optimizer , variables = self . _error_fns . t r ai n ab l e_variables ) 44 training_ops . update ({ ' Error ': tf . group ( err_training_ops ) })45</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 :</head><label>1</label><figDesc>Average Performance of DQN + 3-step returns, DisCor and Discor + 3-step returns on Pong and Breakout at 60M steps</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Xinyang Geng and Aurick Zhou for helpful discussions. We thank Vitchyr Pong, Greg Kahn, Xinyang Geng, Aurick Zhou, Avi Singh, Nicholas Rhinehart, and Michael Janner for feedback on an earlier version of this paper, and all the members of the RAIL lab for their help and support. We thank Tianhe Yu, Kristian Hartikainen, and Justin Yu for help with debugging and setting up the implementations. This research was supported by: the National Science Foundation, the Office of Naval Research, and the DARPA Assured Autonomy program. We thank Google, Amazon and NVIDIA for providing compute resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An mdp-based recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1265" to="1295" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hado Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 23rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning and the deadly triad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modayil</surname></persName>
		</author>
		<idno>abs/1812.02648</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Addressing function approximation error in actor-critic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Herke Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diagnosing bottlenecks in deep q-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning. PMLR</title>
		<meeting>the 36th International Conference on Machine Learning. PMLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>abs/1801.01290</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C H</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforcement learning with deep energy-based policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Actor-Critic Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaymohan</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<title level="m">Continuous control with deep reinforcement learning. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convex analysis. Princeton Mathematical Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tyrrell Rockafellar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, N. J.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The implicit function theorem: History, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">R</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combating label noise in deep learning using abstention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 35th International Conference on Machine Learning</title>
		<meeting>35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Error bounds for approximate value iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial intelligence (AAAI)</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1006" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Error propagation for approximate policy and value iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Amir-Massoud Farahmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finite-time bounds for fitted value iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="815" to="857" />
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A convergent o(n) temporal-difference algorithm for off-policy learning with linear function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Reza</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast gradient-descent methods for temporal-difference learning with linear function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Reza</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalabh</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wiewiora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convergent temporal-difference learning with arbitrary smooth function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalabh</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 22nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic approximation and q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1994-09" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="185" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zap q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adithya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">P</forename><surname>Devraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">22322241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An analysis of temporal-difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards characterizing divergence in deep q-learning. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The utility of sparse representations for control in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raksha</forename><surname>Kumaraswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
		<idno>abs/1811.06626</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stabilizing off-policy q-learning via bootstrapping error reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The fixed points of off-policy td</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An emphatic approach to the problem of off-policy temporal-difference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Rupam</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">26032631</biblScope>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ray interference: a source of plateaus in deep reinforcement learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Borsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<title level="m">Prioritized experience replay. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Provably efficient q-learning with function approximation via distribution shift error checking oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">253279</biblScope>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Dopamine: A Research Framework for Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Samuel Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">523562</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Provably efficient maximum entropy exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><forename type="middle">Van</forename><surname>Soest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding the impact of entropy on policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafarali</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning. PMLR</title>
		<meeting>the 36th International Conference on Machine Learning. PMLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the existence of fixed points for approximate value iteration and temporal-difference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="589" to="608" />
			<date type="published" when="2000-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Is a good representation sufficient for sample efficient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A convergent form of approximate policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Theodore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Precup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">02</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Error bounds for approximate policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML03</title>
		<meeting>the Twentieth International Conference on International Conference on Machine Learning, ICML03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page">560567</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Approximate modified policy iteration and its application to the game of tetris</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gabillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Lesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="1629" to="1676" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tight performance bounds for approximate modified policy iteration with non-stationary policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Lesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
		<idno>abs/1304.5610</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Approximate policy iteration schemes: A comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1314" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Off-policy deep reinforcement learning without exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11361</idno>
		<title level="m">Behavior regularized offline reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A deeper look at experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangtong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01275</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The effects of memory replay in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruishan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">56th Annual Allerton Conference on Communication, Control, and Computing</title>
		<meeting><address><addrLine>Allerton</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06782</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Soft actor-critic algorithms and applications</title>
		<editor>Kristian Hartikainen George Tucker Sehoon Ha Jie Tan Vikash Kumar Henry Zhu Abhishek Gupta Pieter Abbeel Tuomas Haarnoja, Aurick Zhou and Sergey Levine</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">DisCor mainly introduces one hyperparameter, the temperature τ used to compute the weights w k in Equation 8. As shown in Line 11 of Algorithm 3, DisCor maintains a moving average of the temperatures and uses this average to perform the weighting. This removes the requirement for tuning the temperature values at all. For initialization, we chose τ 0 = 10.0 for all our experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>• Temperature Τ</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>irrespective of the domain or task</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">For the design of the error network, ∆ φ , we utilize a network with 1 extra hidden layer than the corresponding Q-network. For instance, in metaworld domains, the standard Q-network used was [256, 256, 256] in size, and thus we used an error network of size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>• Architecture</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>256, 256, 256, 256. and for MT10 tasks we used [160, 160, 160, 160, 160, 160] sized Q-networks [54] and 1-extra layer error networks ∆ φ</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">• Target net updates: We performed target net updates for ∆φ in the same manner as standard Q-functions, in all domains. For instance, in MetaWorld, we update the target network ∆φ with a soft update rate of 0.005 at each environment step</title>
		<imprint/>
	</monogr>
	<note>as is standard with SAC [55], whereas in DQN [11], we use hard target resets</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">• Learning rates for ∆ φ : These were chosen to be the same as the corresponding learning rate for the Q-function</title>
		<imprint/>
	</monogr>
	<note>which is 3e − 4 for SAC and 0.0025 for DQN</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<ptr target="https://github.com/google/dopamine/tree/master/baselines" />
		<title level="m">• Official Implementation repositories used for our work: 1. Soft-Actor-Critic</title>
		<imprint/>
	</monogr>
	<note>Offical DQN implementation. and the baseline DQN numbers were reported from the logs available at</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">• We perform self-normalized importance sampling across a batch, instead of regular importance sampling, since that gives rise to more stable training, and suffers less from the curse of variance in importance sampling</title>
		<ptr target="https://github.com/justinjfu/diagnosing_qlearning" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">For DQNs on atari, we were only able to run 3 seeds for each game for our method, however, we found similar performances, and less variance across seeds, as is evident from the variance bands in the corresponding results. For baseline DQN, we just used the log files provided by the dopamine repository for our results. environments and a suite of algorithms based on fitted Q-iteration [10], which forms the basis of modern deep RL algorithms based on ADP. We evaluated performance on different variants of the (16, 16) gridworld provided, with different reward styles (sparse, dense), different observation functions (one-hot, random features, locally smooth observations), and different amounts of entropy coefficients (0.01, 0.1). We evaluated on five different kinds of environments: grid16randomobs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Seeds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">all our experiments, we implemented our methods on top of the official repositories, ran each experiment for 4 randomly chosen seeds from the interval</title>
		<imprint/>
	</monogr>
	<note>OpenAI gym and tabular environments. grid16onehot, grid16smoothobs, grid16smoothsparse, grid16randomsparse -which cover a wide variety of combinations of feature and reward types. We also evaluated on CliffWalk, Sparsegraph and MountainCar MDPs in Figures 11 and 12</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">We evaluated in two modes -(1) exact mode, in the absence of sampling error, where an algorithm is provided with all transitions in the MDP and simply chooses a weighting over the states rather than sampling transitions from the environment, and (2) sampled mode</title>
		<imprint/>
	</monogr>
	<note>Sampling Modes. which is the conventional RL paradigm, where the algorithm performs online data collection to collect its own data</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Setup for Figures 1 and 4a. For Figures 1 and 4a, we used the grid16randomobs MDP (which is a 16 × 16 gridworld with randomly initialized vectors as observations), with an entropy penalty of 0.01 to the policy. For Figure 4b we used the grid16smoothobs MDP with locally smooth observations, with an entropy penalty of 0.01 as well, and for Figure 4c, we used grid16smoothsparse environment</title>
		<imprint/>
	</monogr>
	<note>with sparse reward and smooth features</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">We provide some individual environment performance curves showing the smoothed normalized return achieved at the end of 300 steps of training in both exact (Figure 11) and sampled (Figure 12) settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Results</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>We also</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
