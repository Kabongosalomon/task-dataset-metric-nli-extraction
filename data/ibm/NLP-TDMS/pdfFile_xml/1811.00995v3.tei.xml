<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Invertible Residual Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
						</author>
						<title level="a" type="main">Invertible Residual Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both stateof-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.</p><p>1 Official code release:</p><p>https://github.com/ jhjacobsen/invertible-resnet Algorithm 1. Inverse of i-ResNet layer via fixed-point iteration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the main appeals of neural network-based models is that a single model architecture can often be used to solve a variety of related tasks. However, many recent advances are based on special-purpose solutions tailored to particular domains. State-of-the-art architectures in unsupervised learning, for instance, are becoming increasingly domainspecific <ref type="bibr" target="#b42">(Van Den Oord et al., 2016b;</ref><ref type="bibr" target="#b25">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b33">Parmar et al., 2018;</ref><ref type="bibr" target="#b23">Karras et al., 2018;</ref><ref type="bibr" target="#b41">Van Den Oord et al., 2016a)</ref>. On the other hand, one of the most successful feed-forward architectures for discriminative learning are deep residual networks <ref type="bibr">(He et al., 2016;</ref><ref type="bibr" target="#b45">Zagoruyko &amp; Komodakis, 2016)</ref>, which differ considerably from their generative counterparts. This divide makes it complicated to choose or design a suitable architecture for a given task. It also makes it hard for discriminative tasks to benefit from * Equal contribution 1 University of Bremen, Center for Industrial Mathematics 2 Vector Institute and University of Toronto. Correspondence to: Jens Behrmann &lt;jensb@uni-bremen.de&gt;, Jörn-Henrik Jacobsen &lt;j.jacobsen@vectorinstitute.ai&gt;.  <ref type="figure">Figure 1</ref>. Dynamics of a standard residual network (left) and invertible residual network (right). Both networks map the interval [−2, 2] to: 1) noisy x 3 -function at half depth and 2) noisy identity function at full depth. Invertible ResNets describe a bijective continuous dynamics while regular ResNets result in crossing and collapsing paths (circled in white) which correspond to nonbijective continuous dynamics. Due to collapsing paths, standard ResNets are not a valid density model. unsupervised learning. We bridge this gap with a new class of architectures that perform well in both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings</head><p>To achieve this, we focus on reversible networks which have been shown to produce competitive performance on discriminative <ref type="bibr" target="#b15">(Gomez et al., 2017;</ref><ref type="bibr" target="#b21">Jacobsen et al., 2018)</ref> and generative <ref type="bibr" target="#b10">(Dinh et al., 2014;</ref><ref type="bibr" target="#b25">Kingma &amp; Dhariwal, 2018)</ref> tasks independently, albeit in the same model paradigm. They typically rely on fixed dimension splitting heuristics, but common splittings interleaved with non-volume conserving elements are constraining and their choice has a significant impact on performance <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b11">Dinh et al., 2017)</ref>. This makes building reversible networks a difficult task. In this work we show that these exotic designs, necessary for competitive density estimation performance, can severely hurt discriminative performance.</p><p>To overcome this problem, we leverage the viewpoint of ResNets as an Euler discretization of ODEs <ref type="bibr" target="#b28">Lu et al., 2017;</ref><ref type="bibr" target="#b8">Ciccone et al., 2018)</ref> and prove that invertible ResNets (i-ResNets) can be constructed by simply changing the normalization scheme of standard ResNets. arXiv:1811.00995v3 <ref type="bibr">[cs.</ref>LG] 18 May 2019</p><p>As an intuition, <ref type="figure">Figure 1</ref> visualizes the differences in the dynamics learned by standard and invertible ResNets.</p><p>This approach allows unconstrained architectures for each residual block, while only requiring a Lipschitz constant smaller than one for each block. We demonstrate that this restriction negligibly impacts performance when building image classifiers -they perform on par with their noninvertible counterparts on classifying MNIST, CIFAR10 and CIFAR100 images.</p><p>We then show how i-ResNets can be trained as maximum likelihood generative models on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian determinant of a residual block. Like FFJORD <ref type="bibr" target="#b17">(Grathwohl et al., 2019)</ref>, i-ResNet flows have unconstrained (free-form) Jacobians, allowing them to learn more expressive transformations than the triangular mappings used in other reversible models. Our empirical evaluation shows that i-ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, bringing general-purpose architectures one step closer to reality. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Enforcing Invertibility in ResNets</head><p>There is a remarkable similarity between ResNet architectures and Euler's method for ODE initial value problems:</p><formula xml:id="formula_0">x t+1 ← x t + g θt (x t ) x t+1 ← x t + hf θt (x t )</formula><p>where x t ∈ R d represent activations or states, t represents layer indices or time, h &gt; 0 is a step size, and g θt is a residual block. This connection has attracted research at the intersection of deep learning and dynamical systems <ref type="bibr" target="#b28">(Lu et al., 2017;</ref><ref type="bibr" target="#b7">Chen et al., 2018)</ref>. However, little attention has been paid to the dynamics backwards in time</p><formula xml:id="formula_1">x t ← x t+1 − g θt (x t ) x t ← x t+1 − hf θt (x t )</formula><p>which amounts to the implicit backward Euler discretization. In particular, solving the dynamics backwards in time would implement an inverse of the corresponding ResNet. The following theorem states that a simple condition suffices to make the dynamics solvable and thus renders the ResNet invertible:</p><p>Theorem 1 (Sufficient condition for invertible ResNets).</p><formula xml:id="formula_2">Let F θ : R d → R d with F θ = (F 1 θ • . . . • F T θ )</formula><p>denote a ResNet with blocks F t θ = I + g θt . Then, the ResNet F θ is Input: output from residual layer y, contractive residual block g, number of fixed-point iterations n Init: x 0 := y for i = 0, . . . , n do</p><formula xml:id="formula_3">x i+1 := y − g(x i ) end for invertible if Lip(g θt ) &lt; 1, for all t = 1, . . . , T,</formula><p>where Lip(g θt ) is the Lipschitz-constant of g θt .</p><p>Note that this condition is not necessary for invertibility. Other approaches <ref type="bibr" target="#b10">(Dinh et al., 2014;</ref><ref type="bibr" target="#b21">Jacobsen et al., 2018;</ref><ref type="bibr" target="#b6">Chang et al., 2018;</ref><ref type="bibr" target="#b25">Kingma &amp; Dhariwal, 2018</ref>) rely on partitioning dimensions or autoregressive structures to create analytical inverses.</p><p>While enforcing Lip(g) &lt; 1 makes the ResNet invertible, we have no analytic form of this inverse. However, we can obtain it through a simple fixed-point iteration, see Algorithm 1. Note, that the starting value for the fixed-point iteration can be any vector, because the fixed-point is unique. However, using the output y = x + g(x) as the initialization x 0 := y is a good starting point since y was obtained from x only via a bounded perturbation of the identity. From the Banach fixed-point theorem we have</p><formula xml:id="formula_4">x − x n 2 ≤ Lip(g) n 1 − Lip(g) x 1 − x 0 2 .<label>(1)</label></formula><p>Thus, the convergence rate is exponential in the number of iterations n and smaller Lipschitz constants will yield faster convergence.</p><p>Additional to invertibility, a contractive residual block also renders the residual layer bi-Lipschitz. Lemma 2 (Lipschitz constants of Forward and Inverse). Let F (x) = x + g(x) with Lip(g) = L &lt; 1 denote the residual layer. Then, it holds</p><formula xml:id="formula_5">Lip(F ) ≤ 1 + L and Lip(F −1 ) ≤ 1 1 − L .</formula><p>Hence by design, invertible ResNets offer stability guarantees for both their forward and inverse mapping. In the following section, we discuss approaches to enforce the Lipschitz condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Satisfying the Lipschitz Constraint</head><p>We implement residual blocks as a composition of contractive nonlinearities φ (e.g. ReLU, ELU, tanh) and linear mappings.</p><p>For example, in our convolutional networks g = W 3 φ(W 2 φ(W 1 )), where W i are convolutional layers. Hence,</p><formula xml:id="formula_6">Lip(g) &lt; 1, if W i 2 &lt; 1,</formula><p>where · 2 denotes the spectral norm. Note, that regularizing the spectral norm of the Jacobian of g <ref type="bibr" target="#b39">(Sokoli et al., 2017)</ref> only reduces it locally and does not guarantee the above condition. Thus, we will enforce W i 2 &lt; 1 for each layer.</p><p>A power-iteration on the parameter matrix as in <ref type="bibr" target="#b29">Miyato et al. (2018)</ref> approximates only a bound on W i 2 instead of the true spectral norm, if the filter kernel is larger than 1 × 1, see <ref type="bibr" target="#b40">Tsuzuku et al. (2018)</ref> for details on the bound. Hence, unlike <ref type="bibr" target="#b29">Miyato et al. (2018)</ref>, we directly estimate the spectral norm of W i by performing power-iteration using W i and W T i as proposed in <ref type="bibr" target="#b16">Gouk et al. (2018)</ref>. The power-iteration yields an under-estimateσ i ≤ W i 2 . Using this estimate, we normalize viã</p><formula xml:id="formula_7">W i = c W i /σ i , if c/σ i &lt; 1 W i , else ,<label>(2)</label></formula><p>where the hyper-parameter c &lt; 1 is a scaling coefficient. Sinceσ i is an under-estimate, W i 2 ≤ c is not guaranteed. However, after training <ref type="bibr" target="#b38">Sedghi et al. (2019)</ref> offer an approach to inspect W i 2 exactly using the SVD on the Fourier transformed parameter matrix, which will allow us to show Lip(g) &lt; 1 holds in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generative Modelling with i-ResNets</head><p>We can define a simple generative model for data x ∈ R d by first sampling z ∼ p z (z) where z ∈ R d and then defining x = Φ(z) for some function Φ : R d → R d . If Φ is invertible and we define F = Φ −1 , then we can compute the likelihood of any x under this model using the change of variables formula</p><formula xml:id="formula_8">ln p x (x) = ln p z (z) + ln | det J F (x)|,<label>(3)</label></formula><p>where J F (x) is the Jacobian of F evaluated at x. Models of this form are known as Normalizing Flows <ref type="bibr" target="#b35">(Rezende &amp; Mohamed, 2015)</ref>. They have recently become a popular model for high-dimensional data due to the introduction of powerful bijective function approximators whose Jacobian log-determinant can be efficienty computed <ref type="bibr" target="#b10">(Dinh et al., 2014;</ref><ref type="bibr" target="#b25">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b7">Chen et al., 2018)</ref> or approximated <ref type="bibr" target="#b17">(Grathwohl et al., 2019)</ref>.</p><p>Since i-ResNets are guaranteed to be invertible we can use them to parameterize F in Equation <ref type="formula" target="#formula_8">(3)</ref>. Samples from this model can be drawn by first sampling z ∼ p(z) and then computing x = F −1 (z) with Algorithm 1. In <ref type="figure" target="#fig_0">Figure 2</ref> we Data Samples Glow i-ResNet show an example of using an i-ResNet to define a generative model on some two-dimensional datasets compared to Glow <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scaling to Higher Dimensions</head><p>While the invertibility of i-ResNets allows us to use them to define a Normalizing Flow, we must compute ln | det J F (x)| to evaluate the data-density under the model. Computing this quantity has a time cost of O(d 3 ) in general which makes naïvely scaling to high-dimensional data impossible.</p><p>To bypass this constraint we present a tractable approximation to the log-determinant term in Equation <ref type="formula" target="#formula_8">(3)</ref>, which will scale to high dimensions d. Previously, <ref type="bibr" target="#b34">Ramesh &amp; LeCun (2018)</ref> introduced the application of log-determinant estimation to non-invertible deep generative models without the specific structure of i-ResNets.</p><p>First, we note that the Lipschitz constrained perturbations x + g(x) of the identity yield positive determinants, hence</p><formula xml:id="formula_9">| det J F (x)| = det J F (x),</formula><p>see Lemma 6 in Appendix A. Combining this result with the matrix identity ln det(A) = tr(ln(A)) for non-singular A ∈ R d×d (see e.g. <ref type="bibr" target="#b44">Withers &amp; Nadarajah (2010)</ref>), we have</p><formula xml:id="formula_10">ln | det J F (x)| = tr(ln J F ),</formula><p>where tr denotes the matrix trace and ln the matrix logarithm. Thus for z = F (x) = (I + g)(x), it is ln p x (x) = ln p z (z) + tr ln I + J g (x) .</p><p>The trace of the matrix logarithm can be expressed as a power series <ref type="bibr" target="#b19">(Hall, 2015)</ref> tr</p><formula xml:id="formula_11">ln I + J g (x) = ∞ k=1 (−1) k+1 tr J k g k ,<label>(4)</label></formula><p>which converges if J g 2 &lt; 1. Hence, due to the Lipschitz constraint, we can compute the log-determinant via the above power series with guaranteed convergence.</p><p>Before we present a stochastic approximation to the above power series, we observe following properties of i-ResNets: Due to Lip(g t ) &lt; 1 for the residual block of each layer t, we can provide a lower and upper bound on its log-determinant with</p><formula xml:id="formula_12">d T t=1 ln(1 − Lip(g t )) ≤ ln | det J F (x)| d T t=1 ln(1 + Lip(g t )) ≥ ln | det J F (x)|,</formula><p>for all x ∈ R, see Lemma 7 in Appendix A. Thus, both the number of layers T and the Lipschitz constant affect the contraction and expansion bounds of i-ResNets and must be taken into account when designing such an architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stochastic Approximation of log-determinant</head><p>Expressing the log-determinant with the power series in (4) has three main computational drawbacks: 1) Computing tr(J g ) exactly costs O(d 2 ), or approximately needs d evaluations of g as each entry of the diagonal of the Jacobian requires the computation of a separate derivative of g <ref type="bibr" target="#b17">(Grathwohl et al., 2019)</ref>. 2) Matrix powers J k g are needed, which requires the knowledge of the full Jacobian. 3) The series is infinite.</p><p>Fortunately, drawback 1) and 2) can be alleviated. First, vector-Jacobian products v T J g can be computed at approximately the same costs as evaluating g through reverse-mode automatic differentiation. Second, a stochastic approximation of the matrix trace of A ∈ R d×d</p><formula xml:id="formula_13">tr(A) = E p(v) v T Av ,</formula><p>known as the Hutchinsons trace estimator, can be used to estimate tr J k g . The distribution p(v) needs to fulfill E[v] = 0 and Cov(v) = I, see <ref type="bibr" target="#b20">(Hutchinson, 1990;</ref><ref type="bibr" target="#b4">Avron &amp; Toledo, 2011)</ref>.</p><p>While this allows for an unbiased estimate of the matrix trace, to achieve bounded computational costs, the power series (4) will be truncated at index n to address drawback 3). Algorithm 2 summarizes the basic steps. The truncation turns the unbiased estimator into a biased estimator, where the bias depends on the truncation error. Fortunately, this error can be bounded as we demonstrate below.</p><p>Algorithm 2. Forward pass of an invertible ResNets with Lipschitz constraint and log-determinant approximation, SN denotes spectral normalization based on (2).</p><p>Input: data point x, network F , residual block g, number of power series terms n for Each residual block do Lip constraint:</p><formula xml:id="formula_14">Ŵ j := SN(W j , x) for linear Layer W j . Draw v from N (0, I) w T := v T ln det := 0 for k = 1 to n do w T := w T J g (vector-Jacobian product) ln det := ln det +(−1) k+1 w T v/k end for end for</formula><p>To improve the stability of optimization when using this estimator we recommend using nonlinearities with continuous derivatives such as ELU <ref type="bibr" target="#b9">(Clevert et al., 2015)</ref> or softplus instead of ReLU (See Appendix C.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Error of Power Series Truncation</head><p>We estimate ln | det(I + J g )| with the finite power series</p><formula xml:id="formula_15">P S(J g , n) := n k=1 (−1) k+1 tr J k g k ,<label>(5)</label></formula><p>where we have (with some abuse of notation) P S(J g , ∞) = tr(ln(I + J g )). We are interested in bounding the truncation error of the log-determinant as a function of the data dimension d, the Lipschitz constant Lip(g) and the number of terms in the series n. Theorem 3 (Approximation error of Loss). Let g denote the residual function and J g the Jacobian as before. Then, the error of a truncated power series at term n is bounded as</p><formula xml:id="formula_16">|P S(J g , n) − ln det(I + J g )| ≤ − d ln(1 − Lip(g)) + n k=1 Lip(g) k k .</formula><p>While the result above gives an error bound for evaluation of the loss, during training the error in the gradient of the loss is of greater interest. Similarly, we can obtain the following bound. The proofs are given in Appendix A.</p><p>Theorem 4 (Convergence Rate of Gradient Approximation). Let θ ∈ R p denote the parameters of network F , let g, J g be as before. Further, assume bounded inputs and a Lipschitz activation function with Lipschitz derivative. Then, we obtain the convergence rate</p><formula xml:id="formula_17">∇ θ ln det I + J g ) − P S J g , n ∞ = O(c n )</formula><p>where c := Lip(g) and n the number of terms used in the power series.</p><p>In practice, only 5-10 terms must be taken to obtain a bias less than .001 bits per dimension, which is typically reported up to .01 precision (See Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reversible Architectures</head><p>We put our focus on invertible architectures with efficient inverse computation, namely NICE <ref type="bibr" target="#b10">(Dinh et al., 2014)</ref>, i-RevNet <ref type="bibr" target="#b21">(Jacobsen et al., 2018)</ref>, Real-NVP <ref type="bibr" target="#b11">(Dinh et al., 2017)</ref>, Glow <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref> and Neural ODEs <ref type="bibr" target="#b7">(Chen et al., 2018)</ref> and its stochastic density estimator FFJORD <ref type="bibr" target="#b17">(Grathwohl et al., 2019)</ref>. A summary of the comparison between different reversible networks is given in <ref type="table">Table 1</ref>.</p><p>The dimension-splitting approach used in NICE, i-RevNet, Real-NVP and Glow allows for both analytic forward and inverse mappings. However, this restriction required the introduction of additional steps like invertible 1 × 1 convolutions in Glow <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref>. These 1 × 1 convolutions need to be inverted numerically, making Glow altogether not analytically invertible. In contrast, i-ResNet can be viewed as an intermediate approach, where the forward mapping is given analytically, while the inverse can be computed via a fixed-point iteration.</p><p>Furthermore, an i-ResNet block has a Lipschitz bound both for forward and inverse (Lemma 2), while other approaches do not have this property by design. Hence, i-ResNets could be an interesting avenue for stability-critical applications like inverse problems <ref type="bibr" target="#b2">(Ardizzone et al., 2019)</ref> or invariancebased adversarial vulnerability <ref type="bibr" target="#b22">(Jacobsen et al., 2019)</ref>.</p><p>Neural ODEs <ref type="bibr" target="#b7">(Chen et al., 2018)</ref> allow free-form dynamics similar to i-ResNets, meaning that any architecture could be used as long as the input and output dimensions are the same.</p><p>To obtain discrete forward and inverse dynamics, Neural ODEs rely on adaptive ODE solvers, which allows for an accuracy vs. speed trade-off. Yet, scalability to very high input dimension such as high-resolution images remains unclear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ordinary Differential Equations</head><p>Due to the similarity of ResNets and Euler discretizations, there are many connections between the i-ResNet and ODEs, which we review in this section.</p><p>Relationship of i-ResNets to Neural ODEs: The view of deep networks as dynamics over time offers two fundamental learning approaches: 1) Direct learning of dynamics using discrete architectures like ResNets <ref type="bibr" target="#b28">Lu et al., 2017;</ref><ref type="bibr" target="#b8">Ciccone et al., 2018)</ref>. 2) Indirect learning of dynamics via parametrizing an ODE with a neural network as in <ref type="bibr" target="#b7">Chen et al. (2018)</ref>; <ref type="bibr" target="#b17">Grathwohl et al. (2019)</ref>.</p><p>The dynamics x(t) of a fixed ResNet F θ are only defined at time points t i corresponding to each block g θt i . However, a linear interpolation in time can be used to generate continuous dynamics. See <ref type="figure">Figure 1</ref>, where the continuous dynamics of a linearly interpolated invertible ResNet are shown against those of a standard ResNet. Invertible ResNets are bijective along the continuous path while regular ResNets may result in crossing or merging paths. The indirect approach of learning an ODE, on the other hand, adapts the discretization based on an ODE-solver, but does not have a fixed computational budget compared to an i-ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability of ODEs:</head><p>There are two main approaches to study the stability of ODEs, 1) behavior for t → ∞ and 2) Lipschitz stability over finite time intervals [0, T ]. Based on time-invariant dynamics f (x(t)), <ref type="bibr" target="#b8">(Ciccone et al., 2018)</ref> constructed asymptotically stable ResNets using anti-symmetric layers such that Re(λ(J x )) &lt; 0 (with Re(λ(·)) denoting the real-part of eigenvalues, ρ(·) spectral radius and J x g the Jacobian at point x). By projecting weights based on the Gershgorin circle theorem, they further fulfilled ρ(J x g) &lt; 1, yielding asymptotically stable ResNets with shared weights over layers. On the other hand,  considered time-dependent dynamics f (x(t), θ(t)) corresponding to standard ResNets. They induce stability by using anti-symmetric layers and projections of the weights. Contrarily, initial value problems on [0, T ] are well-posed for Lipschitz continuous dynamics <ref type="bibr" target="#b3">(Ascher, 2008)</ref>. Thus, the invertible ResNet with Lip(f ) &lt; 1 can be understood as a stabilizer of an ODE for step size h = 1 without a restriction to anti-symmetric layers as in ; ; <ref type="bibr" target="#b8">Ciccone et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Spectral Sum Approximations</head><p>The approximation of spectral sums like the log-determinant is of broad interest for many machine learning problems such as Gaussian Process regression   <ref type="bibr" target="#b10">(Dinh et al., 2014)</ref>, Real-NVP <ref type="bibr" target="#b11">(Dinh et al., 2017)</ref>, Glow <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref> and FFJORD <ref type="bibr" target="#b17">(Grathwohl et al., 2019)</ref>. Non-volume preserving refers to the ability to allow for contraction and expansions and exact likelihood to compute the change of variables (3) exactly. The unbiased estimator refers to a stochastic approximation of the log-determinant, see section 3.2.</p><p>determinant of Jacobian of deep neural networks in <ref type="bibr" target="#b34">Ramesh &amp; LeCun (2018)</ref> for density matching and evaluation of the likelihood of GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We complete a thorough experimental survey of invertible ResNets. First, we numerically verify the invertibility of i-ResNets. Then, we investigate their discriminative abilities on a number of common image classification datasets. Furthermore, we compare the discriminative performance of i-ResNets to other invertible networks. Finally, we study how i-ResNets can be used to define generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Validating Invertibility and Classification</head><p>To compare the discriminative performance and invertibility of i-ResNets with standard ResNet architectures, we train both models on CIFAR10, CIFAR100, and MNIST. The CIFAR and MNIST models have models have 54 and 21 residual blocks, respectively and we use identical settings for all other hyperparameters. We replace strided downsampling with "invertible downsampling" operations <ref type="bibr" target="#b21">(Jacobsen et al., 2018)</ref> to ensure bijectivity, see Appendix C.2 for training and architectural details. We increase the number of input channels to 16 by padding with zeros. This is analagous to the standard practice of projecting the data into a higher-dimensional space using a standard convolutional layer at the input of a model, but this mapping is reversible. To obtain the numerical inverse, we apply 100 fixed point iterations (Equation <ref type="formula" target="#formula_4">(1)</ref>) for each block. This number is chosen to ensure that the poor reconstructions for vanilla ResNets (see <ref type="figure" target="#fig_1">Figure 3</ref>) are not due to using too few iterations. In practice far fewer iterations suffice, as the trade-off between reconstruction error and number of iterations analyzed in Appendix D shows.</p><p>Classification and reconstruction results for a baseline preactivation ResNet-164, a ResNet with architecture like i-ResNets without Lipschitz constraint (denoted as vanilla) and five invertible ResNets with different spectral normalization coefficients are shown in <ref type="table" target="#tab_3">Table 2</ref>. The results illustrate that for larger settings of the layer-wise Lipschitz constant c, our proposed invertible ResNets perform competitively with the baselines in terms of classification performance, while being provably invertible. When applying very conservative normalization (small c), the classification error becomes higher on all datasets tested.</p><p>To demonstrate that our normalization scheme is effective and that standard ResNets are not generally invertible, we reconstruct inputs from the features of each model using Algorithm 1. Intriguingly, our analysis also reveals that unconstrained ResNets are invertible after training on MNIST (see <ref type="figure" target="#fig_6">Figure 7</ref> in Appendix B), whereas on CIFAR10/100 they are not. Further, we find ResNets with and without BatchNorm are not invertible after training on CIFAR10, which can also be seen from the singular value plots in Appendix B ( <ref type="figure" target="#fig_5">Figure 6</ref>). The runtime on 4 GeForce GTX 1080 GPUs with 1 spectral norm iteration was 0.5 sec for a forward and backward pass of batch with 128 samples, while it took 0.2 sec without spectral normalization. See section C.1 (appendix) for details on the runtime.</p><p>The reconstruction error decays quickly and the errors are already imperceptible after 5-20 iterations, which is the cost of 5-20 times the forward pass and corresponds to 0.15-0.75 seconds for reconstructing 100 CIFAR10 images.</p><p>ResNet-164 Vanilla c = 0.9 c = 0.8 c = 0.7 c = 0.6 c = 0.5  Computing the inverse is fast even for the largest normalization coefficient, but becomes faster with stronger normalization. The number of iterations needed for full convergence is approximately cut in half when reducing the spectral normalization coefficient by 0.2, see <ref type="figure">Figure 8</ref> (Appendix D) for a detailed plot. We also ran an i-RevNet <ref type="bibr" target="#b21">(Jacobsen et al., 2018)</ref> with comparable hyperparameters as ResNet-164 and it performs on par with ResNet-164 with 5.6%. Note however, that i-RevNets, like NICE <ref type="bibr" target="#b10">(Dinh et al., 2014)</ref>, are volume-conserving, making them less well-suited to generative modeling.</p><p>In summary, we observe that invertibility without additional constraints is unlikely, but possible, whereas it is hard to predict if networks will have this property. In our proposed model, we can guarantee the existence of an inverse without significantly harming classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Other Invertible Architectures</head><p>In this section we compare i-ResNet classifiers to the stateof-the-art invertible flow-based model Glow. We take the implementation of <ref type="bibr" target="#b25">Kingma &amp; Dhariwal (2018)</ref> and modify it to classify CIFAR10 images (with no generative modeling component). We create an i-ResNet that is as close as possible in structure to the default Glow model on CI-FAR10 (denoted as i-ResNet Glow-style) and compare it to two variants of Glow, one that uses learned (1 × 1 convolutions) and affine block structure, and one with reverse permutations (like Real-NVP) and additive block structure. Results of this experiment can be found in  <ref type="table" target="#tab_4">Table 3</ref>. CIFAR10 classification results compared to state-of-theart flow Glow as a classifier. We compare two versions of Glow, as well as an i-ResNet architecture as similar as possible to Glow in its number of layers and channels, termed "i-ResNet, Glow-Style". discriminative task, even when adapting the network depth and width to that of Glow. This indicates that i-ResNets have a more suitable inductive bias in their block structure for discriminative tasks than Glow.</p><p>We also find that i-ResNets are considerably easier to train than these other models. We are able to train i-ResNets using SGD with momentum and a learning rate of 0.1 whereas all version of Glow we tested needed Adam or Adamax <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2014)</ref> and much smaller learning rates to avoid divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generative Modeling</head><p>We run a number of experiments to verify the utility of i-ResNets in building generative models. First, we compare i-ResNet Flows with Glow <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref> on simple two-dimensional datasets. <ref type="figure" target="#fig_0">Figure 2</ref> qualitatively shows the density learned by a Glow model with 100 coupling layers and 100 invertible linear transformations. We compare against an i-ResNet where the coupling layers are replaced by invertible residual blocks with the same number of parameters and the invertible linear transformations are replaced by actnorm <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref>. This results in the i-ResNet model having slightly fewer parameters, while maintaining an equal number of layers. In this experiment we train i-ResNets using the brute-force computed log-determinant since the data is two-dimensional. We find that i-ResNets are able to more accurately fit these simple densities. As stated in <ref type="bibr" target="#b17">Grathwohl et al. (2019)</ref>, we believe this is due to our model's ability to avoid partitioning dimensions.</p><p>Next we evaluate i-ResNets as a generative model for images on MNIST and CIFAR10. Our models consist of multiple i-ResNet blocks followed by invertible downsampling or dimension "squeezing" to downsample the spatial dimensions. We use multi-scale architectures like those of <ref type="bibr" target="#b11">Dinh et al. (2017)</ref>; <ref type="bibr" target="#b25">Kingma &amp; Dhariwal (2018)</ref>. In these experiments we train i-ResNets using the log-determinant approximation, see Algorithm 2. Full architecture, experimental, and evaluation details can be found in Appendix C.3. Samples from our CIFAR10 model are shown in <ref type="figure" target="#fig_3">Figure 5</ref> and samples from our MNIST model can be found in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method MNIST CIFAR10</head><p>NICE <ref type="bibr" target="#b10">(Dinh et al., 2014)</ref> 4.36 4.48 † MADE <ref type="bibr" target="#b14">(Germain et al., 2015)</ref> 2.04 5.67 MAF <ref type="bibr" target="#b32">(Papamakarios et al., 2017)</ref> 1.89 4.31 Real NVP <ref type="bibr" target="#b11">(Dinh et al., 2017)</ref> 1.06 3.49 Glow <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref> 1.05 3.35 FFJORD <ref type="bibr" target="#b17">(Grathwohl et al., 2019)</ref> 0.99 3.40 i-ResNet 1.06 3.45 Compared to the classification model, the log-determinant approximation with 5 series terms roughly increased the computation times by a factor of 4. The bias and variance of our log-determinant estimator is shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>Results and comparisons to other generative models can be found in <ref type="table" target="#tab_5">Table 4</ref>. While our models did not perform as well as Glow and FFJORD, we find it intriguing that ResNets, with very little modification, can create a generative model competitive with these highly engineered models. We believe the gap in performance is mainly due to our use of a biased log-determinant estimator and that the use of an unbiased method <ref type="bibr">(Han et al., 2018)</ref> can help close this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Other Applications</head><p>In many applications, a secondary unsupervised learning or generative modeling objective is formulated in combination with a primary discriminative task. i-ResNets are appealing here, as they manage to achieve competitive performance on both discriminative and generative tasks. We summarize some application areas to highlight that there is a wide variety of tasks for which i-ResNets would be promising to consider:</p><p>• Hybrid density and discriminative models for joint classification and detection or fairness applications (Nalis-  • Semi-supervised learning from few labeled examples <ref type="bibr" target="#b31">(Oliver et al., 2018;</ref> • Solving inverse problems with hybrid regression and generative losses <ref type="bibr" target="#b2">(Ardizzone et al., 2019)</ref> • Adversarial robustness with likelihood-based generative models <ref type="bibr" target="#b37">(Schott et al., 2019;</ref><ref type="bibr" target="#b22">Jacobsen et al., 2019)</ref> Finally, it is plausible that the Lipschitz bounds on the layers of the i-ResNet could aid with the stability of gradients for optimization, as well as adversarial robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We introduced a new architecture, i-ResNets, which allow free-form layer architectures while still providing tractable density estimates. The unrestricted form of the Jacobian allows expansion and contraction via the residual blocks, while partitioning-based models <ref type="bibr" target="#b10">(Dinh et al., 2014;</ref><ref type="bibr" target="#b25">Kingma &amp; Dhariwal, 2018)</ref> must include affine blocks and scaling layers to be non-volume preserving.</p><p>Several challenges remain to be addressed in future work. First, our estimator of the log-determinant is biased. However, there have been recent advances in building unbiased estimators for the log-determinant <ref type="figure" target="#fig_0">(Han et al., 2018)</ref>, which we believe could improve the performance of our generative model. Second, learning and designing networks with a Lipschitz constraint is challenging. For example, we need to constrain each linear layer in the block instead of being able to directly control the Lipschitz constant of a block, see <ref type="bibr" target="#b1">Anil et al. (2018)</ref> for a promising approach for addressing this problem.</p><p>for spotting a mistake in one of the proofs. We also thank everyone else at Vector for helpful discussions and feedback.</p><p>Han, I., Malioutov, D., Avron, H., and Shin, J. Approximating the spectral sums of large-scale matrices using chebyshev approximations. SIAM Journal on Scientific Computing, 39, 06 2016.</p><p>Han, I., Avron, H., and Shin, J. Stochastic chebyshev gradient descent for spectral optimization. In Advances in Neural Information Processing Systems 31, pp. 7397-7407. 2018.</p><p>He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.</p><p>Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019.</p><p>Zhao, T., Zhang, D., Sun, Z., and Lee, H. Information regularized neural networks, 2019. URL https:// openreview.net/forum?id=BJgvg30ctX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Lemmas and Proofs</head><p>Proof. (Theorem 1) Since ResNet F θ is a composition of functions, it is invertible if each block F t θ is invertible. Let x t+1 ∈ R d be arbitrary and consider the backward Euler discretization x t = x t+1 − hf θt (x t ) = x t+1 − g θt (x t ). Re-writing as a iteration yields</p><formula xml:id="formula_18">x 0 t := x t+1 and x k+1 t := x t+1 − g θt (x k t ),<label>(6)</label></formula><p>where lim k→∞ x k t = x t is the fixed point if the iteration converges. As g θt : R d → R d is an operator on a Banach space, the contraction condition Lip(g θt ) &lt; 1 guarantees convergence due to the Banach fixed point theorem.</p><p>Remark 5. The condition above was also stated in Zhao et al. (2019) (Appendix D), however, their proof restricts the domain of the residual block g to be bounded and applies only to linear operators g, because the inverse was given by a convergent Neumann-series.</p><p>Proof. (Lemma 2) First note, that Lip(F ) ≤ 1 + L follows directly from the addition of Lipschitz constants. For the inverse, consider</p><formula xml:id="formula_19">F (x) − F (y) 2 = x − y + g(x) − g(y) 2 = x − y − (−g(x) + g(y)) 2 ≥ | x − y 2 − − g(x) + g(y) 2 | (7) ≥ | x − y 2 − (−1) (g(x) − g(y)) 2 | ≥ x − y 2 − g(x) − g(y) 2 ≥ x − y 2 − L x − y 2 ,</formula><p>where we apply the reverse triangular inequality in (7) and apply the Lipschitz constant of g. Denote x = F −1 (z) and y = F −1 (w) for z, w ∈ R d , which is possible since F −1 is surjective. Inserting above yields</p><formula xml:id="formula_20">F (F −1 (z)) − F (F −1 (w)) 2 ≥ (1 − L) F −1 (z) − F −1 (w) 2 ⇐⇒ 1 1 − L z − w 2 ≥ F −1 (z) − F −1 (w) 2 ,</formula><p>which holds for all z, w.</p><p>Lemma 6 (Positive Determinant of Jacobian of Residual Layer). Let F (x) = (I + g(·))(x) denote a residual layer and J F (x) = I + J g (x) its Jacobian at x ∈ R d . If Lip(g) &lt; 1, then it holds λ i of J F (x) are positive for all x and thus</p><formula xml:id="formula_21">| det[J F (x)]| = det[J F (x)],</formula><p>where λ i denotes the eigenvalues.</p><p>Proof. (Lemma 6) First, we have λ i (J F ) = λ i (J g )+1 and J g (x) 2 &lt; 1 for all x due to Lip(g) &lt; 1. Since the spectral radius ρ(J g ) ≤ J g 2 , it is |λ i (J g )| &lt; 1. Hence, Re(λ i (J F )) &gt; 0 and thus det J F = i (λ i (J g ) + 1) &gt; 0.</p><p>Lemma 7 (Lower and Upper Bounds of an invertible ResNet on log-determinant). Let F θ :</p><formula xml:id="formula_22">R d → R d with F θ = (F 1 θ • . . . • F T θ )</formula><p>denote an invertible ResNet with blocks F t θ = I + g θt . Then, we can obtain the following bounds</p><formula xml:id="formula_23">d T t=1 ln(1 − Lip(g t )) ≤ ln | det J F (x)| d T t=1 ln(1 + Lip(g t )) ≥ ln | det J F (x)|, for all x ∈ R d .</formula><p>Proof. <ref type="figure" target="#fig_6">(Lemma 7)</ref>) First, the sum over the layers is due to the function composition, because J F (x) = t J F t (x) and</p><formula xml:id="formula_24">ln | det J F (x)| = ln T t=1 det J F t (x) = T t=1 ln det J F t (x),</formula><p>where we use the positivity of the determinant, see Lemma 6. Furthermore, note that</p><formula xml:id="formula_25">σ d (A) d ≤ i σ i (A) = | det A| ≤ σ 1 (A) d</formula><p>for a matrix A and largest singular values σ 1 and smallest σ d . Furthermore, we have σ i (J F t ) ≤ (1 + Lip(g t )) and</p><formula xml:id="formula_26">σ d (J F t ) ≤ (1 − Lip(g t ))</formula><p>, which follows from Theorem 2. Inserting this and applying the logarithm rules finally yields the claimed bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. (Theorem 3)</head><p>We begin by noting that</p><formula xml:id="formula_27">|P S(J g , n) − tr ln(J g )| = ∞ k=n+1 (−1) k+1 tr J k g k ≤ ∞ k=n+1 (−1) k+1 tr J k g k ≤ ∞ k=n+1 tr J k g k ≤ d ∞ k=n+1 Lip(g) k k ,<label>(8)</label></formula><p>where inequality (8) follows from</p><formula xml:id="formula_28">| tr J k | ≤ d i=d λ i (J k ) ≤ d i=d |λ i (J k )| ≤ dρ(J k ) ≤ d J k 2 ≤ d J k 2 ≤ d Lip(g) k .<label>(9)</label></formula><p>We note that the full series ∞ k=1</p><p>Lip(g) k k = − ln(1 − Lip(g)) thus we can bound the approximation error by</p><formula xml:id="formula_29">|P S(J g , n) − tr ln(J g )| ≤ −d ln(1 − Lip(g)) + n k=1 Lip(g) k k</formula><p>Proof. (Theorem 4) First, we derive the by differentiating the power series and using the linearity of the trace operator. We obtain</p><formula xml:id="formula_30">∂ ∂θ i ln det I + J g (x, θ) = ∂ ∂θ i ∞ k=1 (−1) k+1 tr J k g (x, θ) k = tr ∞ k=1 k(−1) k+1 k J k−1 g (x, θ) ∂(J g (x, θ)) ∂θ i = tr ∞ k=0 (−1) k J k g (x, θ) ∂(J g (x, θ)) ∂θ i . By definition of · ∞ , ∇ θ P S J g (θ), ∞ − ∇ θ P S J g (θ), n ∞ = max i=1,...,p ∂ ∂θ i P S J g (θ), ∞ − ∂ ∂θ i P S J g (θ), n ,</formula><p>which is why, we consider an arbitrary i from now on. It is</p><formula xml:id="formula_31">∂ ∂θ i P S J g (θ), ∞ − ∂ ∂θ i P S J g (θ), n = ∞ k=n+1 (−1) k tr J k g (x, θ) ∂(J g (x, θ)) ∂θ i ≤ d ∞ k=n+1 Lip(g) K ∂J g (x, θ) ∂θ i 2 ,<label>(10)</label></formula><p>where we used the same arguments as in estimation <ref type="formula" target="#formula_28">(9)</ref>.</p><p>In order to bound ∂Jg(x,θ) ∂θi 2</p><p>, we need to look into the design of the residual block. We assume contractive and elementwise activation functions (hence φ (·) &lt; 1) and N linear layers W i in a residual block. Then, we can write the Jacobian as a matrix product</p><formula xml:id="formula_32">J g (x, θ) = W T N D N · · · W T 1 D 1 , where D i = diag(φ (z i−1 )) with pre-activations z i−1 ∈ R d .</formula><p>Since we need to bound the derivative of the Jacobian with respect to weights θ i , double backpropagation <ref type="bibr" target="#b13">(Drucker &amp; Lecun, 1992)</ref> is necessary. In general, the terms , we bound the previous terms as follows</p><formula xml:id="formula_33">W T i 2 , D i 2 , D * i 2 := diag(φ (z i−1 )) 2 ,</formula><formula xml:id="formula_34">W T i 2 ≤ Lip(g) (11) D i 2 ≤ const,<label>(12)</label></formula><formula xml:id="formula_35">D * i 2 ≤ const (13) x 2 ≤ const (14) ∂W i ∂θ i T 2 ≤ W i F + s.<label>(15)</label></formula><p>In particular, (12) is due to the assumption of a Lipschitz activation function and (13) due to assuming a Lipschitz derivative of the activation function. Note, that we are using continuously differentiable activations functions (hence, not ReLU), where this assumptions holds for common functions like ELU, softplus and tanh. Furthermore, (14) holds by assuming bounded inputs and due to the network being Lipschitz. To understand the bound (15), we denote s as the amount of parameter sharing of θ i . For example, if θ i is a entry from a convolution kernel, s = w * h with w spatial width and h spatial height. Then</p><formula xml:id="formula_36">∂W i ∂θ i T 2 ≤ W i F + s, since ∂W lm (x, θ) ∂θ i = 1, if W lm = θ i 0, else .</formula><p>Hence, as each term appearing in the second derivative ∂Jg(x,θ) ∂θi 2 is bounded, we can introduce the constant a(g, θ, x) &lt; ∞ which depends on the parameters, the implementation of g and the inputs x. Note, that we do not give an exact bound on <ref type="bibr">∂Jg(x,θ)</ref> ∂θi 2 , since we are only interesting in the existence of such a bound in order to proof the convergence in the claim.</p><p>Inserting above in (10) and denoting c := Lip(g), yields</p><formula xml:id="formula_37">∂ ∂θ i P S J g (θ), ∞ − ∂ ∂θ i P S J g (θ), n ≤ d a(g, θ, x) ∞ k=n+1 c K =ã(d, g, θ, x) 1 1 − c − 1 − c n 1 − c + c n .</formula><p>Letting f (n) :=ã(d, g, θ, x) 1 1−c − 1−c n 1−c + c n and g(n) = c n , then</p><formula xml:id="formula_38">lim n→∞ f (n) g(n) = const &lt; ∞,</formula><p>which proves the claimed convergence rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Verification of Invertibility</head><p>Inveritibility of Learned Mappings In this experiment we train standard ResNets and i-ResNets with various layer-wise Lipschitz coefficients (c ∈ {.3, .5, .7, .9}). After training, we inspect the learned transformations at each layer by computing the largest singular value of each linear mapping based on the approach in <ref type="bibr" target="#b38">Sedghi et al. (2019)</ref>. It can be seen clearly ( <ref type="figure" target="#fig_5">Figure  6</ref> left) that the standard and BatchNorm models have many singular values above 1, making their residual connections non-invertible. Conversely, in the i-ResNet models <ref type="figure" target="#fig_5">(Figure 6 right)</ref>, all singular values are below 1 (and roughly equal to c) indicating their residual connections are invertible. Computing Inverses with Fixed-Point Iteration Here we numerically compute inverses in our trained models using the fixed-point iteration, see Algorithm 1. We invert each residual connection using 100 iterations (to ensure convergence). We see that i-ResNets can be inverted using this method whereas with standard ResNets this is not guaranteed <ref type="figure" target="#fig_6">(Figure 7 top)</ref>. Interestingly, on MNIST we find that standard ResNets are indeed invertible after training on MNIST <ref type="figure" target="#fig_6">(Figure 7 bottom)</ref>. . Surprisingly, MNIST reconstructions are close to exact for both models, even without explicitly enforcing the Lipschitz constant. On CIFAR10 however, reconstructions completely fail for the vanilla ResNet, but are qualitatively and quantitatively exact for our proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Runtime Comparison</head><p>Glow i-ResNet i-ResNet SN i-ResNet SN LogDet 0.72 sec 0.31 sec 0.57 sec 1.88 sec <ref type="table">Table 5</ref>. Timings for a forward and backward pass. Glow is the dimension-splitting baseline, it has the same number of layers and channels as all i-ResNets and batch size is identical. We compare Glow to: 1) plain, 2) spectral norm, 3) spectral norm and log determinant estimated i-ResNets. Discriminative i-ResNets are around 1.2 -2.1 times faster, while generative i-ResNets are around 2.6 times slower than the dimension splitting baseline. Thus, overall wall-clock times do not differ by much more than a factor of two in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Classification</head><p>Architecture We use pre-activation ResNets with 39 convolutional bottleneck blocks with 3 convolution layers each and kernel sizes of 3x3, 1x1, 3x3 respectively. All models use the ELU nonlinearity <ref type="bibr" target="#b9">(Clevert et al., 2015)</ref>. In the BatchNorm version, we apply batch normalization before every nonlinearity and in the invertible models we use ActNorm <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref> before each residual block. The network has 2 down-sampling stages after 13 and 26 blocks where a dimension squeezing operation is used to decrease the spatial resolution. This reduces the spatial dimension by a factor of two in each direction, while increasing the number of channels by a factor of four. All models transform the data to a 8x8x256 tensor to which we apply BatchNorm, a nonlinearity, and average pooling to a 256-dimensional vector. A linear classifier is used on top of this representation.</p><p>Injective Padding Since our invertible models are not able to increase the dimension of their latent representation, we use injective padding <ref type="bibr" target="#b21">(Jacobsen et al., 2018)</ref> which concatenates channels of 0's to the input, increasing the size of the transformed tensor. This is analagous to the standard practice of projecting the data into a higher-dimensional space using a non-ResNet convolution at the input of a model, but this mapping is reversible. We add 13 channels of 0's to all models tested, thus the input to our first residual block is a tensor of size 32x32x16. We experimented with removing this step but found it led to approximately a 2% decrease in accuracy for our CIFAR10 models.</p><p>Training We train for 200 epochs with momentum SGD and a weight decay of 5e-4. The learning rate is set to 0.1 and decayed by a factor of 0.2 after 60, 120 and 160 epochs. For data-augmentation, we apply random shifts of upt to two pixels for MNIST and shifts/ random horizontal flips for CIFAR(10/100) during training. The inputs for MNIST are normalized to <ref type="bibr">[-0.5,0</ref>.5] and for CIFAR(10/100) normalize by subtracting the mean and dividing by the standard deviation of the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Generative Modeling</head><p>Toy Densities We used 100 residual blocks, where each residual connection is a multilayer perceptron with state sizes of 2-64-64-64-2 and ELU nonlinearities <ref type="bibr" target="#b9">(Clevert et al., 2015)</ref>. We used ActNorm <ref type="bibr" target="#b25">(Kingma &amp; Dhariwal, 2018)</ref> after each residual block. The change in log density was computed exactly by constructing the full Jacobian during training and visualization.</p><p>MNIST and CIFAR The structure of our generative models closely resembles that of Glow. The model consists of "scale-blocks" which are groups of i-ResNet blocks that operate at different spatial resolutions. After each scale-block, apart from the last, we perform a squeeze operation which decreases the spatial resolution by 2 in each dimension and multiplies the number of channels by 4 (invertible downsampling).</p><p>Our MNIST and CIFAR10 models have three scale-blocks. Each scale-block has 32 i-ResNet blocks. Each i-ResNet block consists of three convolutions of 3 × 3, 1 × 1, 3 × 3 filters with ELU <ref type="bibr" target="#b9">(Clevert et al., 2015)</ref> nonlinearities in between. Each convolutional layer has 32 filters in the MNIST model and 512 filters in the CIFAR10 model.</p><p>We train for 200 epochs using the Adamax <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2014)</ref> optimizer with a learning rate of .003. Throughout training we estimate the log-determinant in Equation <ref type="formula" target="#formula_8">(3)</ref> using the power-series approximation (Equation <ref type="formula" target="#formula_11">(4)</ref>) with ten terms for the MNIST model and 5 terms for the CIFAR10 model.</p><p>Evaluation During evaluation we use the bound presented in Section 3.3 to determine the number of terms needed to give an estimate with bias less than .0001 bit/dim. We then average over enough samples from Hutchinson's estimator such that the standard error is less than .0001 bit/dim, thus we can safely report our model's bit/dim accurate up to a tolerance of .0002.</p><p>Choice of Nonlinearity Differentiating our log-determinant estimator requires us to compute second derivatives of our neural network's output. If we were to use a nonlinearity with discontinuous derivatives (i.e. ReLU), then these values are not defined in certain regions. This can lead to unstable optimization. To guarantee the quantities required for optimization always exist, we recommend using nonlinearities which have continuous derivatives such as ELU <ref type="bibr" target="#b9">(Clevert et al., 2015)</ref> or softplus. In all of our experiments we use ELU.  <ref type="figure">Figure 8</ref>. Trade-off between number of fixed point iterations and reconstruction error (log scale) for computing the inverse for different normalization coefficients of trained invertible ResNets (on CIFAR10). The reconstruction error decays quickly. 5-20 iterations are sufficient respectively to obtain visually perfect reconstructions. Note that one iteration corresponds to the time for one forward pass, thus inversion is approximately 5-20 times slower than inference. This corresponds to a reconstruction time of 0.15-0.75 seconds for a batch of 100 CIFAR10 images with 5-20 iterations and 4.3 seconds with 100 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fixed Point Iteration Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invertible Residual Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluating the Bias of Our Log-determinant Estimator</head><p>Here we numerically evaluate the bias of the log-determinant estimator used to train our generative models (Equation <ref type="formula" target="#formula_11">(4)</ref>). We compare the true value (computed via brute-force) with the estimator's mean and standard deviation as the number of terms in the power series is increased. After 10 terms, the estimator's bias is negligible and after 20 terms it is numerically 0. This is averaged over 1000 test examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Terms</head><p>Bits/Dim <ref type="figure">Figure 9</ref>. Convergence of approximation error of log-determinant estimator when varying the number of terms used in the power series. The variance is due to the stochastic trace estimator.</p><p>F. Additional Samples of i-ResNet flow CIFAR10 samples.</p><p>MNIST samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Visual comparison of i-ResNet flow and Glow. Details of this experiment can be found in Appendix C.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Original images (top) and reconstructions from i-ResNet with c = 0.9 (middle) and a standard ResNet with the same architecture (bottom), showing that the fixed point iteration does not recover the input without the Lipschitz constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Bias and standard deviation of our log-determinant estimator as the number of power series terms increases. Variance is due to the stochastic trace estimator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>CIFAR10 samples from our i-ResNet flow. More samples can be found in Appendix F. nick et al., 2018; Louizos et al., 2016)• Unsupervised learning for downstream tasks(Hjelm  et al., 2019;<ref type="bibr" target="#b43">Van Den Oord et al., 2018)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the derivative. Hence, in order to bound ∂Jg(x,θ) ∂θi 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Maximal singular value of each layers convolutional operator for various trained ResNets on Cifar10. Left: Vanilla and Batchnorm ResNet singular values. It is likely that the baseline ResNets are not invertible as roughly two thirds of their layers have singular values fairly above one, making the blocks non-contractive. Right: Singular values for our 4 spectrally normalized ResNets. The regularization is effective and in every case the single ResNet block remains a contraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Original images (top), i-ResNets with c = 0.9 (middle) and reconstructions from vanilla (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Standard ResNet Output Invertible ResNet</head><label></label><figDesc>of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table><row><cell>Output</cell><cell></cell></row><row><cell>Depth</cell><cell></cell></row><row><cell>Input</cell><cell>Input</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Among others, Taylor approximation<ref type="bibr" target="#b5">(Boutsidis et al., 2017)</ref> of the log-determinant similar to our approach or Chebyshev polynomials(Han et al., 2016)  are used. In<ref type="bibr" target="#b5">Boutsidis et al. (2017)</ref>, error bounds on the estimation via truncated power series and stochastic trace estimation are given for symmetric positive definite matrices. However, I + J g is not symmetric and thus, their analysis does not apply here.</figDesc><table><row><cell>Method</cell><cell cols="4">ResNet NICE/ i-RevNet Real-NVP Glow FFJORD i-ResNet</cell></row><row><cell>Free-form</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Analytic Forward</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Analytic Inverse</cell><cell>N/A</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Non-volume Preserving</cell><cell>N/A</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Exact Likelihood</cell><cell>N/A</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Unbiased Stochastic Log-Det Estimator</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row></table><note>Recently, unbiased estimates (Adams et al., 2018) and unbi- ased gradient estimators (Han et al., 2018) were proposed for symmetric positive definite matrices. Furthermore, Cheby- shev polynomials have been used to approximate the log-Table 1. Comparing i-ResNet and ResNets to NICE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of i-ResNet to a ResNet-164 baseline architecture of similar depth and width with varying Lipschitz constraints via coefficients c. Vanilla shares the same architecture as i-ResNet, without the Lipschitz constraint.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>. We can</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>MNIST and CIFAR10 bits/dim results. † Uses ZCA preprocessing making results not directly comparable.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Rich Zemel for very helpful comments on an earlier version of the manuscript. We thank Yulia Rubanova</p><p>We gratefully acknowledge the financial support from the German Science Foundation for RTG 2224 "π 3 : Parameter Identification -Analysis, Algorithms, Applications"</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating the spectral density of large implicit matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saunderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03451</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sorting out lipschitz function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analyzing inverse problems with invertible neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Numerical methods for evolutionary differential equations. Computational science and engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Randomized algorithms for estimating the trace of an implicit symmetric positive semidefinite matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toledo</surname></persName>
		</author>
		<idno>8:1-8:34</idno>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A randomized algorithm for approximating the log determinant of a symmetric positive definite matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-M</forename><surname>Kontopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zouzias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="page" from="95" to="117" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reversible architectures for arbitrarily deep residual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Begert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holtham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nais-net: Stable deep networks from nonautonomous differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gallieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3029" to="3039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nice</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable log determinants for gaussian process kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6327" to="6337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving generalization performance using double backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="991" to="998" />
			<date type="published" when="1992-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Made</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cree</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04368</idno>
		<title level="m">Regularisation of neural networks by enforcing lipschitz continuity</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ffjord: Scalable reversible generative models with free-form continuous dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lie groups, lie algebras, and representations: An elementary introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graduate Texts in Mathematics</title>
		<imprint>
			<biblScope unit="page">222</biblScope>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics -Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="450" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<title level="m">Deep invertible networks. In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Excessive invariance causes adversarial vulnerability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<title level="m">Generative flow with invertible 1x1 convolutions. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The variational fair autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10121</idno>
		<title level="m">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hybrid models with deep and invertible features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS workshop on Bayesian deep learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Backpropagation for implicit spectral densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00499</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04272</idno>
		<title level="m">Deep neural networks motivated by partial differential equations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards the first adversarially robust neural network model on MNIST</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The singular values of convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust large margin deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sokoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R D</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4265" to="4280" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuzuku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Withers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nadarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">log det a = tr log a. International Journal of Mathematical Education in Science and Technology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1121" to="1124" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
