<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Liao</surname></persName>
							<email>zhibin.liao@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ARC Centre of Excellence for Robotic Vision</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<email>gustavo.carneiro@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ARC Centre of Excellence for Robotic Vision</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets (e.g., CIFAR-10, CIFAR-100, MNIST, and SVHN). The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The use of piecewise linear activation units in deep learning models <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, such as deep convolutional neural network (CNN) <ref type="bibr" target="#b6">[7]</ref>, has produced models that are showing state-of-the-art results in several public databases (e.g., CIFAR-10 <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b7">[8]</ref>, MNIST <ref type="bibr" target="#b8">[9]</ref> and * This research was supported by the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016) SVHN <ref type="bibr" target="#b9">[10]</ref>). These piecewise linear activation units have been the subject of study by Montufar et al. <ref type="bibr" target="#b0">[1]</ref> and by Srivastava et al. <ref type="bibr" target="#b10">[11]</ref>, and the main conclusions achieved in these works are: 1) the use of a multi-layer composition of piecewise linear activation units allows for an exponential division (in terms of the number of network layers) of the input space <ref type="bibr" target="#b0">[1]</ref>; 2) given that the activation units are trained based on a local competition that selects which region of the activation function a training sample will use, "specialized" subnetworks will be formed by the consistency that they respond to similar training samples (i.e., samples lying in one of the regions produced by the exponential division above) <ref type="bibr" target="#b10">[11]</ref> and 3) even though subnetworks are formed and trained with a potentially small number of training samples, these models are not prone to overfitting because these subnetworks share their parameters, resulting in an implicit regularization of the training process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>An assumption made by these works is that a large proportion of the regions of the piecewise linear activation units are active during training and inference. For instance, in the Rectifier Linear Unit (ReLU) <ref type="bibr" target="#b1">[2]</ref>, Leaky-ReLu (LReLU) <ref type="bibr" target="#b2">[3]</ref> and Parametric-ReLU (PReLU) <ref type="bibr" target="#b3">[4]</ref>, there must be two sets of points: one lying in the negative side and another on the positive side of the activation function domain (see region 1 covering the negative side and region 2 on the positive side in {P,L}ReLU cases of <ref type="figure">Fig. 1</ref>). Moreover, in the Maxout <ref type="bibr" target="#b5">[6]</ref> and Local winner takes all (LWTA) <ref type="bibr" target="#b4">[5]</ref> activation units, there must be k sets of points -each set lying in one of the k regions of the activation function domain (see Maxout case in <ref type="figure">Fig. 1</ref>). This assumption is of utmost importance because if violated, then the activation units may degenerate into simple linear functions that are not capable of exponentially dividing the input space or training the "specialized" subnetworks (i.e., the model capacity is reduced). Moreover, in learning models that have very deep architectures, the violation of this assumption makes the model ill-conditioned, as shown in the toy example below. In this paper, we propose the introduction of batch normalisation units <ref type="bibr" target="#b11">[12]</ref> before the piecewise linear activation units to guarantee that the input data is evenly distributed with respect to the activation function domain, which results in a more balanced use of all the regions of the piecewise linear activation units and pre-conditions the model. Note that Goodfellow et al. <ref type="bibr" target="#b5">[6]</ref> have acknowledged this assumption and proposed the use of dropout <ref type="bibr" target="#b12">[13]</ref> to regularize the training process, but dropout cannot guarantee a more balanced distribution of the input <ref type="figure">Figure 1</ref>. Piecewise linear activation functions: ReLU <ref type="bibr" target="#b1">[2]</ref>, LReLU <ref type="bibr" target="#b2">[3]</ref>, PReLU <ref type="bibr" target="#b3">[4]</ref>, and Maxout <ref type="bibr" target="#b5">[6]</ref>.</p><p>data in terms of the activation function domain. Furthermore, dropout is a regularization technique that does not help pre-condition the model. Therefore, the issues that we have identified remains with dropout.</p><p>In order to motivate our observation and the model being proposed in this paper, we show a toy problem that illustrates well our points. Assume that we have a 2-D binary problem, where samples are drawn (12K for training and 2K for testing) using a uniform distribution between [−10, 10] (in each dimension) from the partition shown in <ref type="figure" target="#fig_0">Fig. 2</ref>-(a) (leftmost image), with the colors blue and yellow indicating the class labels. We train a multi-layer perceptron (MLP) with varying number of nodes per layer n l ∈ {2, 4} and varying number of layers L ∈ {2, 3, 4, 5, 6}, and it is possible to place two types of piecewise linear activation functions after each layer: ReLU <ref type="bibr" target="#b1">[2]</ref> and maxout <ref type="bibr" target="#b5">[6]</ref>, where for maxout we can vary the number of regions k ∈ {2, 4} (e.g., <ref type="figure">Fig. 1</ref> shows a maxout with 4 regions). Also, before each activation function, we have the option of placing a batch normalisation unit <ref type="bibr" target="#b11">[12]</ref>. Training is based on backpropagation <ref type="bibr" target="#b13">[14]</ref> using mini-batches of size 100, learning rate of 0.0005 for 20 epochs then 0.0001 for another 20 epochs, momentum of 0.9 and weight decay of 0.0001, where we run five times the training (with different training and test samples) and report the mean train and test errors. Finally, the MLP weights are initialized with Normal distribution scaled by 0.01 for all layers.</p><p>Analysing the mean train and test error in <ref type="figure" target="#fig_0">Fig. 2</ref>-(b), we first notice that all models have good generalization capability, which is a characteristic already identified for deep networks that use piecewise linear activation units <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. Looking at the curves for the networks with 2 and 3 layers, where all models seem to be trained properly (i.e., they are pre-conditioned), the models containing batch normalisation units (denoted by "with normalisation") produce the smallest train and test errors, indicating the higher capacity of these models. Beyond 3 layers, the models that do not use the batch normalisation units become ill-conditioned, producing errors of 0.39, which effectively means that all points are classified as one of the binary classes. In general, batch normalisation allows the use of maxout in deeper MLPs that contain more nodes per layer, and the maxout function contains more regions (i.e., larger k). The best result (in terms of mean test and train error) is achieved with an MLP of 5 or more layers, where each layer contains 4 nodes and maxout has 4 regions (test error saturates at 0.07). The best results with ReLU are also achieved with batch normalisation, using a large number of layers (5 or more), and 4 nodes per layer, but notice that the smallest ReLU errors (around 0.19 on test set) are significantly higher than the maxout ones, indicating that maxout has larger capac-ity. The images in <ref type="figure" target="#fig_0">Fig. 2-(a)</ref> show the division of the input space (into linear regions) used to train the subnetworks within the MLP model (we show the best performing models of ReLU with and without normalisation and maxout with and without normalisation), where it is worth noticing that the best maxout model (bottom-right image) produces a very large number of linear regions, which generate class regions that are similar to the original classification problem. The input space division, used to train the subnetworks, are generated by clustering the training points that produce the same activation pattern from all nodes and layers of the MLP. We also run these same experiments using dropout (of 0.2), and the relative results are similar to the ones presented in <ref type="figure" target="#fig_0">Fig. 2-(b)</ref>, but the test errors with dropout are around 2× larger, which indicate that dropout does not pre-condition the model (i.e., the models that do not have the batch normalisation units still become ill-conditioned when having 3 or more layers), nor does it balance the input data for the activation units (i.e., the capacity of the model does not increase with dropout).</p><p>This toy experiment motivates us to propose a model that: 1) contains a large number of layers and nodes per layer, 2) uses maxout activation function <ref type="bibr" target="#b5">[6]</ref>, and 3) uses a batch normalisation unit <ref type="bibr" target="#b11">[12]</ref> before each maxout layer. More specifically, we extend the Network in Network (NIN) model <ref type="bibr" target="#b14">[15]</ref>, where we replace the original ReLU units by batch normalisation units followed by maxout units. Replacing ReLU by maxout has the potential to increase the capacity of the model <ref type="bibr" target="#b5">[6]</ref>, and as mentioned before, the use of batch normalisation units will guarantee a more balanced distribution of this input data for those maxout units, which increases the model capacity and pre-conditions the model. We call our proposed model the maxout network in maxout network (MIM) -see <ref type="figure" target="#fig_1">Fig. 3</ref>. We assess the performance of our model on the following datasets: CIFAR-10 <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b7">[8]</ref>, MNIST <ref type="bibr" target="#b8">[9]</ref> and SVHN <ref type="bibr" target="#b9">[10]</ref>. We first show empirically the improvements achieved with the introduction of maxout and batch normalisation units to the NIN model <ref type="bibr" target="#b14">[15]</ref>, forming our proposed MIM model, then we show a study on how this model provides a better pre-conditioning for the proposed deep learning model, and finally we show the final classification results on the datasets above, which are compared to the state of the art and demonstrated to be the best in the field in two of these datasets (CIFAR-10, CIFAR-100) and competitive on MNIST and SVHN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Batch Normalised Deep Learning with Piecewise Linear Activation Units</head><p>In this section, we first explain the piecewise linear activation units, followed by an introduction of how the batch normalisation unit works and a presentation of the proposed MIM model, including its training and inference procedures.</p><p>The nomenclature adopted in this section is the same as the one introduced by Montufar et al. <ref type="bibr" target="#b0">[1]</ref>, where a feedforward neural network is defined by the function F : R n0 → R out :  where f (.) represents a preactivation function, the parameter θ is formed by the input weight matrices W l ∈ R k.n l ×n l−1 , bias vectors b l ∈ R k.n l and normalisation parameters γ l and β l in (3) for each layer l ∈ {1, ..., L}, h l (.)</p><formula xml:id="formula_0">F (x, θ) = f out • g L • h L • f L • ... • g 1 • h 1 • f 1 (x),<label>(1)</label></formula><p>represents a normalisation function, and g l (.) is a non-linear activation function. The preactivation function is defined by</p><formula xml:id="formula_1">f l (x l−1 ) = W l x l−1 + b l , where the output of the (l − 1) th layer is x l = [x l,1 , ..</formula><p>., x l,n l ], denoting the activations x l,i of the units i ∈ {1, ..., n l } from layer l. This output is computed from the activations of the preceding layer by</p><formula xml:id="formula_2">x l = g l (h l (f l (x l−1 ))). Also note that f l = [f l,1 , ..., f l,n l ]</formula><p>is an array of n l preactivation vectors f l,i ∈ R k , which after normalisation, results in an array of n l normalised vectors h l,i ∈ R k produced by h l,i (f l,i (x l−1 )), and the activation of the i th unit in the l th layer is represented by</p><formula xml:id="formula_3">x l,i = g l,i (h l,1 (f l,i (x l−1 ))).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Piecewise Linear Activation Units</head><p>By dropping the layer index l to facilitate the nomenclature, the recently proposed piecewise linear activation units ReLU <ref type="bibr" target="#b1">[2]</ref>, LReLU <ref type="bibr" target="#b2">[3]</ref>, PReLU <ref type="bibr" target="#b3">[4]</ref>, and Maxout <ref type="bibr" target="#b5">[6]</ref> are represented as follows <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_4">ReLU: g i (h i ) = max{0, h i }, LReLU or PReLU: g i (h i ) = max{α.h i , h i }, Maxout: g i (h i ) = max{h i,1 , ..., h i,k }.<label>(2)</label></formula><p>where h i ∈ R and k = 1 for ReLU, LReLU <ref type="bibr" target="#b2">[3]</ref>, and PReLU <ref type="bibr" target="#b3">[4]</ref>, α is represented by a small constant in LReLU, but a learnable model parameter in PReLU, k denotes the number of regions of the maxout activation function, and</p><formula xml:id="formula_5">h i = [h i,1 , ..., h i,k ] ∈ R k .</formula><p>According to Montufar et al. <ref type="bibr" target="#b0">[1]</ref>, the network structure is defined by the input dimensionality n 0 , the number of layers L and the width n l of each layer. A linear region of the function F : R n0 → R m is a maximal connected subset of R n0 . Note from (2) that rectifier units have two behaviour types: 1) constant 0 (ReLU) or linear (LReLU or PReLU) with a small slope when the input is negative; and 2) linear with slope 1 when input is positive. These two behaviours are separated by a hyperplane (see <ref type="figure">Fig. 1</ref>) and the set of all hyperplanes within a rectifier layer forms a hyperplane arrangement, which split the input space into several linear regions. A multi-layer network that uses rectifier linear units with n 0 inputs and L hidden layers with n ≥ n 0 nodes can compute functions that have Ω (n/n 0 ) L−1 n n0 linear regions, and a multi-layer network that uses maxout activation units with L layers of width n 0 and rank k can compute functions that have k L−1 k n0 linear regions <ref type="bibr" target="#b0">[1]</ref>. These results indicate that multi-layer networks with maxout and rectifier linear units can compute functions with a number of linear regions that grows exponentially with the number of layers <ref type="bibr" target="#b0">[1]</ref>. Note that these linear regions can be observed as the colored polygons in <ref type="figure" target="#fig_0">Fig. 2-(a)</ref>, where the number of linear regions is denoted by "# SUBNETS".</p><p>The training process of networks containing piecewise linear activation units uses a divide and conquer strategy where ∂ ∂W l moves the classification boundary for layer l according to the loss function with respect to the points in its current linear region (similarly for the bias term b l ), and ∂ ∂x l−1 moves the offending points (i.e., points being erroneously classified) away from their current linear regions. Dividing the data points into an exponentially large number of linear regions is advantageous because the training algorithm can focus on minimizing the loss for each one of these regions almost independently of others -this is why we say it uses a divide and conquer algorithm. We also say that it is an almost independent training of each linear region because the training parameters for each region are shared with all other regions, and this helps the regularization of the training process. However, the initialization of this training process is critical because if the data points are not evenly distributed at the beginning, then all these points may lie in only one of the regions of the piecewise linear unit. This will drive the learning of the classification boundary for that specific linear region, where the loss will be minimized for all those points in that region, and the boundary for the other linear regions will be trained less effectively with much fewer points. This means that even the points with relatively high loss will remain in that initial region because the other regions have been ineffectively trained, and consequently may have a larger loss. This issue is very clear with the use of maxout units, where in the extreme case, only one of the k regions is active, which means that the maxout unit will behave as a simple linear unit. If a large amount of maxout units behave as linear units, then this will reduce the ability of these networks to compute functions that have an exponential number of linear regions, and consequently decrease the capacity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Batch Normalisation Units</head><p>In order to force the initialization to distribute the data points evenly in the domain of the piecewise activation functions, such that a large proportion of the k regions is used, we propose the use of batch normalisation by Ioffe and Szegedy's <ref type="bibr" target="#b11">[12]</ref>. This normalisation has been proposed because of the difficulty in initializing the network parameters and setting the value for the learning rate, and also because the inputs for each layer are affected by the parameters of the previous layers. These issues lead to a complicated learning problem, where the input distribution for each layer changes continuously -an issue that is called covariate shift <ref type="bibr" target="#b11">[12]</ref>. The main contribution of this batch normalisation is the introduction of a simple feature-wise centering and normalisation to make it have mean zero and variance one, which is followed by a batch normalisation (BN) unit that shifts and scales the normalised value. For instance, assuming that the input to the normalisation unit is f = [f 1 , ..., f n l ], where f i ∈ R k , the BN unit consists of two stages:</p><formula xml:id="formula_6">Normalisation:f i,k = f i,k −E[f i,k ] √ Var[f i,k ] Scale and shift: h i,k = γ ifi,k + β i ,<label>(3)</label></formula><p>where the shift and scale parameters {γ i , β i } are new network parameters that participate in the training procedure <ref type="bibr" target="#b11">[12]</ref>. Another important point is that the BN unit does not process each training sample independently, but it uses both the training sample and other samples in a mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Maxout Network in Maxout Network Model</head><p>As mentioned in Sec. 2.1, the number of linear regions that networks with piecewise linear activation unit can have grows exponentially with the number of layers, so it is important to add as many layers as possible in order to increase the ability of the network to estimate complex functions. For this reason, we extend the recently proposed Network in Network (NIN) <ref type="bibr" target="#b14">[15]</ref> model, which is based on a CNN that uses a multi-layer perceptron (MLP) as its activation layer (this layer is called the Mlpconv layer). In its original formulation, the NIN model introduces the Mlpconv with ReLU activation after each convolution layer, and replaces the fully connected layers for classification in CNN (usually present at the end of the whole network) by a spatial average of the feature maps from the last Mlpconv layer, which is fed into a softmax layer. In particular, we extend the NIN model by replacing the ReLU activation after each convolution layer of the Mlpconv by a maxout activation unit, which has the potential to increase even further the model capacity. In addition, we also add the BN unit before the maxout units. These two contributions form our proposed model, we give it a simple name Maxout Network in Maxout Network Model (MIM), which is depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>. Finally, we include a dropout layer <ref type="bibr" target="#b12">[13]</ref> between MIM blocks for regularizing the model.  <ref type="table">Table 1</ref>. The proposed MIM model architectures used in the experiments. In each maxout-conv unit (m-conv), the convolution kernel is defined by the first row in the block: (height)x(width)x(num of units). The second row of the block contains the information of convolution stride (stride), padding (pad), and maxout rank (k). The third row contains the BN units. Each layer of the maxout-mlp unit (m-mlp) is equivalent to a maxout-conv unit with 1x1 convolution kernel size. A softmax layer is present as the last layer of the model (but not shown in this table). The model on top row is used on CIFAR-10 and 100 and SVHN, while the model on the bottom row is for MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate our proposed method on four common deep learning benchmarks: CIFAR-10 <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b7">[8]</ref>, MNIST <ref type="bibr" target="#b8">[9]</ref> and SVHN <ref type="bibr" target="#b9">[10]</ref>. The CIFAR-10 [8] dataset contains 60000 32x32 RGB images of 10 classes of common visual objects (e.g., animals, vehicles, etc.), where 50000 images are for training and the rest 10000 for testing. The CIFAR-100 <ref type="bibr" target="#b7">[8]</ref> is an extension of CIFAR-10, where the difference is that CIFAR-100 has 100 classes with 500 training images and 100 testing images for each class. In both CIFAR-10 and 100, the visual objects are well-centred in the images. The MNIST <ref type="bibr" target="#b8">[9]</ref> dataset is a standard benchmark for comparing learning methods. It contains 70000 28x28 grayscale images of numerical digits from 0 to 9, divided as 60000 images for training and 10000 images for testing. Finally, the Street View House Number (SVHN) <ref type="bibr" target="#b9">[10]</ref> dataset is a real-word digit dataset with over 600000 32x32 RGB images containing images of house numbers (i.e., digits 0-9). The cropped digits are well-centred and the original aspect ratio is kept, but some distracting digits are present next to the centred digits of interest. The dataset is partitioned into training, test and extra sets, where the extra 530000 images are less difficult samples to be used as extra training data.</p><p>For each of these datasets, we validate our algorithm using the same training and validation splitting described by Goodfellow et al. <ref type="bibr" target="#b5">[6]</ref> in order to estimate the model hyperparameters. For the reported results, we run 5 training processes, each with different model initializations, and the test results consist of the mean and standard deviation of the errors in these 5 runs. Model initialization is based on randomly producing the MIM weight values using a Normal distribution, which is multiplied by 0.01 in the first layer of the first MIM block and by 0.05 in all remaining layers. Moreover, we do not perform data augmentation for any of these datasets and only compare our MIM model with the state-of-the-art methods that report non data-augmented results. For the implementation, we use the MatConvNet <ref type="bibr" target="#b15">[16]</ref> CNN toolbox and run our experiments on a standard PC</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) NIN <ref type="bibr" target="#b14">[15]</ref> 10.41% NIN with maxout (without BN)</p><p>10.95 ± 0.21% NIN with ReLU (with BN)</p><p>9.43 ± 0.21% MIM (= NIN with maxout and BN -our proposal)</p><p>8.52 ± 0.20%  <ref type="bibr" target="#b14">[15]</ref> with the introduction of maxout and BN units, which comprise our contributions in this paper that form the MIM model. Then, we show s study on how the BN units pre-conditions the NIN model. Finally, we show a comparison between our proposed MIM model against the current state of the art on the aforementioned datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Introducing Maxout and Batch Normalisation to NIN model</head><p>In this section, we use CIFAR-10 to show the contribution provided by each component proposed in this paper. The first row of Tab. 2 shows the published results of NIN <ref type="bibr" target="#b14">[15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ill-conditioning Study in Real Datasets</head><p>The study of how the BN units pre-conditions the proposed model (on CIFAR-10 and MNIST) is shown in <ref type="figure">Fig. 4</ref>. For this evaluation, we use the combination of NIN and <ref type="figure">Figure 4</ref>. The ill-conditioning, measured by the model's inability to converge as a function of the learning rate. The training error (blue curve) and test error (orange curve) of the models trained without BN stay at the initial error when the learning rate is above a certain value, showing no sign of convergence (the results in terms of these learning rates are therefore omitted).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) Stochastic Pooling <ref type="bibr" target="#b16">[17]</ref> 15.13% Maxout Networks <ref type="bibr" target="#b5">[6]</ref> 11.68% Network in Network <ref type="bibr" target="#b14">[15]</ref> 10.41% Deeply-supervised nets <ref type="bibr" target="#b17">[18]</ref> 9.69% RCNN-160 <ref type="bibr" target="#b18">[19]</ref> 8.69% MIM (our proposal)</p><p>8.52 ± 0.20% <ref type="table">Table 3</ref>. Comparison between MIM and the state-of-the-art methods on CIFAR-10 <ref type="bibr" target="#b7">[8]</ref>.</p><p>maxout units as the standard model, and ensure that the learning rate is the only varying parameter. We train five distinct models with learning rates in [10 −2 , 10 1 ] for CIFAR-10, and [10 −3 , 10 1 ] for MNIST, and plot the error curves with the mean and standard deviation values. From <ref type="figure">Fig. 4</ref>, we can see that without BN, a deep learning model can become ill-conditioned. It is also interesting to see that these un-normalized models give best performance right before the learning rate drives it into the ill-conditioning mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with the State of the Art</head><p>We compare the proposed MIM model (first row of Tab. 1) with Stochastic Pooling <ref type="bibr" target="#b16">[17]</ref>, Maxout Networks <ref type="bibr" target="#b5">[6]</ref>, Network in Network <ref type="bibr" target="#b14">[15]</ref>, Deeply-supervised nets <ref type="bibr" target="#b17">[18]</ref>, and recurrent CNN <ref type="bibr" target="#b18">[19]</ref> on CIFAR-10 <ref type="bibr" target="#b7">[8]</ref> and show the results in Tab. 3. The comparison on CIFAR-100 <ref type="bibr" target="#b7">[8]</ref> against the same state-of-the-art models above, and also the Tree based Priors <ref type="bibr" target="#b19">[20]</ref>, is shown in Tab. 4. The performance on MNIST <ref type="bibr" target="#b8">[9]</ref> of the MIM model (second row of Tab. 1) is compared against Stochastic Pooling <ref type="bibr" target="#b16">[17]</ref>, Conv. Maxout+Dropout <ref type="bibr" target="#b5">[6]</ref>, Network in Network <ref type="bibr" target="#b14">[15]</ref>, Deeplysupervised nets <ref type="bibr" target="#b17">[18]</ref>, and recurrent CNN <ref type="bibr" target="#b18">[19]</ref> in Tab. 5. It is important to mention that the best result we observed with the MIM model on MNIST over the 5 runs is 0.32%. Finally, our MIM model in the first row of Tab. 1 is compared against the same models above, plus Dropconnect <ref type="bibr" target="#b20">[21]</ref> on SVHN <ref type="bibr" target="#b9">[10]</ref>, and results are displayed in Tab. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) Stochastic Pooling <ref type="bibr" target="#b16">[17]</ref> 42.51% Maxout Networks <ref type="bibr" target="#b5">[6]</ref> 38.57% Tree based Priors <ref type="bibr" target="#b19">[20]</ref> 36.85% Network in Network <ref type="bibr" target="#b14">[15]</ref> 35.68% Deeply-supervised nets <ref type="bibr" target="#b17">[18]</ref> 34.57% RCNN-160 <ref type="bibr" target="#b18">[19]</ref> 31.75% MIM <ref type="table">(our proposal)</ref> 29.20 ± 0.20% <ref type="table">Table 4</ref>. Comparison between MIM and the state-of-the-art methods on CIFAR-100 <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) Stochastic Pooling <ref type="bibr" target="#b16">[17]</ref> 0.47% Conv. Maxout+Dropout <ref type="bibr" target="#b5">[6]</ref> 0.47% Network in Network <ref type="bibr" target="#b14">[15]</ref> 0.45% Deeply-supervised nets <ref type="bibr" target="#b17">[18]</ref> 0.39% MIM (our proposal) 0.35 ± 0.03% RCNN-96 <ref type="bibr" target="#b18">[19]</ref> 0.31% <ref type="table">Table 5</ref>. Comparison between MIM and the state-of-the-art methods on MNIST <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) Stochastic Pooling <ref type="bibr" target="#b16">[17]</ref> 2.80% Conv. Maxout+Dropout <ref type="bibr" target="#b5">[6]</ref> 2.47% Network in Network <ref type="bibr" target="#b14">[15]</ref> 2.35% MIM (our proposal)</p><p>1.97 ± 0.08% Dropconnect <ref type="bibr" target="#b20">[21]</ref> 1.94% Deeply-supervised nets <ref type="bibr" target="#b17">[18]</ref> 1.92% RCNN-192 <ref type="bibr" target="#b18">[19]</ref> 1.77% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion and Conclusion</head><p>The results in Sec. 3.1 show that the replacement of ReLU by maxout increases the test error on CIFAR-10, similarly to what has been shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The introduction of BN with ReLU activation units provide a significant improvement of the test error, and the introduction of BN units before the maxout units produce the smallest error, which happens due to the even input data distribution with respect to the activation function domain, resulting in a more balanced use of all the regions of the maxout units. The study in Sec. 3.2 clearly shows that the introduction of BN units pre-conditions the model, allowing it to use large learning rates and produce more accurate classification. The comparison against the current state of the art (Sec. 3.3) shows that the proposed MIM model produces the best result in the field on CIFAR-10 and CIFAR-100. On MNIST, our best result over five runs is comparable to the best result in the field. Finally, on SVHN, our result is slightly worse than the current best result in the field. An interesting point one can make is with respect to the number of regions k that we set for the MIM maxout units. Note that we set k = 2 because we did not notice any significant improvement with bigger values of k, and also because the computational time and memory requirements of the training become intractable.</p><p>This paper provides an empirical demonstration that the combination of piecewise linear activation units with BN units provides a powerful framework to be explored in the design of deep learning models. More specifically, our work shows how to guarantee the assumption made in the use of piecewise linear activation units about the balanced distribution of the input data for these units. This empirical ev-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>a) Original classification problem (left) with the liner regions found by each model (represented by the color of each subnet) and classification division of the original space (class regions). b) Train and test error as a function of the number of layers, number of nodes per layer, piecewise linear activation function, number of regions in the activation function, and the use of normalisation Toy problem with the division of the space into linear regions and classification profile produced by each model (a), and a quantitative comparison between models (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Proposed MIM model. The MIM model is based on the NIN<ref type="bibr" target="#b14">[15]</ref> model. This model contains three blocks that have nearly identical architectures, with small differences in terms of the number of filters and stride in convolution layers. The first two blocks use max pooling and the third block uses average pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on CIFAR-10 of the introduction of maxout and BN units to NIN, which produce our proposed MIM model (last row).</figDesc><table><row><cell>equipped with Intel i7-4770 CPU and Nvidia GTX TITAN</cell></row><row><cell>X GPU. Finally, Tab. 1 specifies the details of the proposed</cell></row><row><cell>MIM architecture used for each dataset.</cell></row><row><cell>Below, we first show experiments that demonstrate the</cell></row><row><cell>performance of the original NIN model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>In our first experiment, we replace all ReLU units from the NIN model by the maxout units (with k = 2) and run the training and test experiments described above (test results are shown in the second row of Tab. 2). Second, we include the BN units before each ReLU unit in the NIN model and show the test results in the third row of Tab. 2. Finally, we include the maxout and BN units to the NIN model, which effectively forms our proposed MIM model, and test results are displayed in the fourth row of Tab. 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Comparison between MIM and the state-of-the-art methods on SVHN<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>idence can be shown more theoretically in a future work, following the results produced by Montufar, Srivastava and others <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Guido F Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compete to compute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohrob</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Kazerounian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 30th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Computer Science Department</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding locally competitive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<idno>abs/1412.4564</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3367" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2094" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Yann L Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
