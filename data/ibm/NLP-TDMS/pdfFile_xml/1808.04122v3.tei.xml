<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Quoc Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
							<email>2thanh.vu@csiro.au</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">The Australian e-Health Research Centre</orgName>
								<orgName type="institution" key="instit2">CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Nguyen</surname></persName>
							<email>tu.dinh.nguyen@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><forename type="middle">Quoc</forename><surname>Nguyen</surname></persName>
							<email>1dai.nguyen@monash.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
							<email>dinh.phung@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce an embedding model, named CapsE, exploring a capsule network to model relationship triples (subject, relation, object). Our CapsE represents each triple as a 3-column matrix where each column vector represents the embedding of an element in the triple. This 3-column matrix is then fed to a convolution layer where multiple filters are operated to generate different feature maps. These feature maps are reconstructed into corresponding capsules which are then routed to another capsule to produce a continuous vector. The length of this vector is used to measure the plausibility score of the triple. Our proposed CapsE obtains better performance than previous state-of-the-art embedding models for knowledge graph completion on two benchmark datasets WN18RR and FB15k-237, and outperforms strong search personalization baselines on SEARCH17.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) containing relationship triples <ref type="bibr">(subject, relation, object)</ref>, denoted as <ref type="bibr">(s, r, o)</ref>, are the useful resources for many NLP and especially information retrieval applications such as semantic search and question answering <ref type="bibr" target="#b30">(Wang et al., 2017)</ref>. However, large knowledge graphs, even containing billions of triples, are still incomplete, i.e., missing a lot of valid triples <ref type="bibr" target="#b32">(West et al., 2014)</ref>. Therefore, much research efforts have focused on the knowledge graph completion task which aims to predict missing triples in KGs, i.e., predicting whether a triple not in KGs is likely to be valid or not <ref type="bibr" target="#b3">(Bordes et al., 2011</ref><ref type="bibr" target="#b2">(Bordes et al., , 2013</ref><ref type="bibr" target="#b22">Socher et al., 2013)</ref>. To this end, many embedding models have been proposed to learn vector representations for entities (i.e., subject/head entity and object/tail entity) and relations in KGs, and obtained stateof-the-art results as summarized by <ref type="bibr" target="#b16">Nickel et al. (2016a)</ref> and <ref type="bibr" target="#b13">Nguyen (2017)</ref>. These embedding models score triples <ref type="bibr">(s, r, o)</ref>, such that valid triples have higher plausibility scores than invalid ones <ref type="bibr" target="#b3">(Bordes et al., 2011</ref><ref type="bibr" target="#b2">(Bordes et al., , 2013</ref><ref type="bibr" target="#b22">Socher et al., 2013)</ref>. For example, in the context of KGs, the score for (Melbourne, cityOf, Australia) is higher than the score for (Melbourne, cityOf, United Kingdom).</p><p>Triple modeling is applied not only to the KG completion, but also for other tasks which can be formulated as a triple-based prediction problem. An example is in search personalization, one would aim to tailor search results to each specific user based on the user's personal interests and preferences <ref type="bibr" target="#b23">(Teevan et al., 2005</ref><ref type="bibr" target="#b25">(Teevan et al., , 2009</ref><ref type="bibr" target="#b0">Bennett et al., 2012;</ref><ref type="bibr" target="#b7">Harvey et al., 2013;</ref><ref type="bibr" target="#b29">Vu et al., 2015</ref><ref type="bibr" target="#b28">Vu et al., , 2017</ref>. Here the triples can be formulated as (submitted query, user profile, returned document) and used to re-rank documents returned to a user given an input query, by employing an existing KG embedding method such as TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, as proposed by <ref type="bibr" target="#b28">Vu et al. (2017)</ref>. Previous studies have shown the effectiveness of modeling triple for either KG completion or search personalization. However, there has been no single study investigating the performance on both tasks.</p><p>Conventional embedding models, such as TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, DISTMULT <ref type="bibr" target="#b34">(Yang et al., 2015)</ref> and ComplEx <ref type="bibr" target="#b27">(Trouillon et al., 2016)</ref>, use addition, subtraction or simple multiplication operators, thus only capture the linear relationships between entities. Recent research has raised interest in applying deep neural networks to triplebased prediction problems. For example, <ref type="bibr" target="#b12">Nguyen et al. (2018)</ref> proposed ConvKB-a convolutional neural network (CNN)-based model for KG completion and achieved state-of-the-art results. Most of KG embedding models are constructed to modeling entries at the same dimension of the given triple, where presumably each dimension captures some relation-specific attribute of entities. To the best of our knowledge, however, none of the existing models has a "deep" architecture for modeling the entries in a triple at the same dimension. <ref type="bibr" target="#b21">Sabour et al. (2017)</ref> introduced capsule networks (CapsNet) that employ capsules (i.e., each capsule is a group of neurons) to capture entities in images and then uses a routing process to specify connections from capsules in a layer to those in the next layer. Hence CapsNet could encode the intrinsic spatial relationship between a part and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints. Each capsule accounts for capturing variations of an object or object part in the image, which can be efficiently visualized. Our high-level hypothesis is that embedding entries at the same dimension of the triple also have these variations, although it is not straightforward to be visually examined.</p><p>To that end, we introduce CapsE to explore a novel application of CapsNet on triple-based data for two problems: KG completion and search personalization. Different from the traditional modeling design of CapsNet where capsules are constructed by splitting feature maps, we use capsules to model the entries at the same dimension in the entity and relation embeddings. In our CapsE, v s , v r and v o are unique k-dimensional embeddings of s, r and o, respectively. The embedding triple [v s , v r , v o ] of (s, r, o) is fed to the convolution layer where multiple filters of the same 1×3 shape are repeatedly operated over every row of the matrix to produce k-dimensional feature maps. Entries at the same dimension from all feature maps are then encapsulated into a capsule. Thus, each capsule can encode many characteristics in the embedding triple to represent the entries at the corresponding dimension. These capsules are then routed to another capsule which outputs a continuous vector whose length is used as a score for the triple. Finally, this score is used to predict whether the triple (s, r, o) is valid or not.</p><p>In summary, our main contributions from this paper are as follows:</p><p>• We propose an embedding model CapsE using the capsule network <ref type="bibr" target="#b21">(Sabour et al., 2017)</ref> for modeling relationship triples. To our best of knowledge, our work is the first consideration of exploring the capsule network to knowledge graph completion and search personalization.</p><p>• We evaluate our CapsE for knowledge graph completion on two benchmark datasets WN18RR <ref type="bibr" target="#b5">(Dettmers et al., 2018)</ref> and FB15k-237 <ref type="bibr" target="#b26">(Toutanova and Chen, 2015)</ref>. CapsE obtains the best mean rank on WN18RR and the highest mean reciprocal rank and highest Hits@10 on FB15k-237.</p><p>• We restate the prospective strategy of expanding the triple embedding models to improve the ranking quality of the search personalization systems. We adapt our model to search personalization and evaluate on SEARCH17 <ref type="bibr" target="#b28">(Vu et al., 2017)</ref> -a dataset of the web search query logs. Experimental results show that our CapsE achieves the new state-of-the-art results with significant improvements over strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The proposed CapsE</head><p>Let G be a collection of valid factual triples in the form of (subject, relation, object) denoted as <ref type="bibr">(s, r, o)</ref>. Embedding models aim to define a score function giving a score for each triple, such that valid triples receive higher scores than invalid triples.</p><p>We denote v s , v r and v o as the k-dimensional embeddings of s, r and o, respectively. In our proposed CapsE, we follow <ref type="bibr" target="#b12">Nguyen et al. (2018)</ref> </p><formula xml:id="formula_0">to view each embedding triple [v s , v r , v o ] as a matrix A = [v s , v r , v o ] ∈ R k×3 , and denote A i,: ∈ R 1×3 as the i-th row of A.</formula><p>We use a filter ω ∈ R 1×3 operated on the convolution layer. This filter ω is repeatedly operated over every row of A to generate a feature map q = [q 1 , q 2 , ..., q k ] ∈ R k , in which q i = g (ω · A i,: + b) where · denotes a dot product, b ∈ R is a bias term and g is a non-linear activation function such as ReLU. Our model uses multiple filters ∈ R 1×3 to generate feature maps. We denote Ω as the set of filters and N =| Ω | as the number of filters, thus we have N k-dimensional feature maps, for which each feature map can capture one single characteristic among entries at the same dimension.</p><p>We build our CapsE with two single capsule layers for a simplified architecture. In the first layer, we construct k capsules, wherein entries at the same dimension from all feature maps are encapsulated into a corresponding capsule. Therefore, each capsule can capture many characteristics among the entries at the corresponding dimension in the embedding triple. These characteristics are generalized into one capsule in the second layer which produces a vector output whose length is used as the score for the triple.</p><p>The first capsule layer consists of k capsules, for which each capsule i ∈ {1, 2, ..., k} has a vector output u i ∈ R N×1 . Vector outputs u i are multiplied by weight matrices W i ∈ R d×N to produce vectorsû i ∈ R d×1 which are summed to produce a vector input s ∈ R d×1 to the capsule in the second layer. The capsule then performs the non-linear squashing function to produce a vector output e ∈ R d×1 :</p><formula xml:id="formula_1">e = squash (s) ; s = i c iûi ;û i = W i u i</formula><p>where squash (s) = s 2 1+ s 2 s s , and c i are coupling coefficients determined by the routing process as presented in Algorithm 1. Because there is one capsule in the second layer, we make only one difference in the routing process proposed by <ref type="bibr" target="#b21">Sabour et al. (2017)</ref>, for which we apply the softmax in a direction from all capsules in the previous layer to each of capsules in the next layer. 1 for all capsule i ∈ the first layer do b i ← 0</p><formula xml:id="formula_2">for iteration = 1, 2, ..., m do c ← softmax (b) s ← i c iûi e = squash (s) for all capsule i ∈ the first layer do b i ← b i +û i · e</formula><p>Algorithm 1: The routing process is extended from <ref type="bibr" target="#b21">Sabour et al. (2017)</ref>. <ref type="bibr">1</ref> The softmax in the original routing process proposed by <ref type="bibr" target="#b21">Sabour et al. (2017)</ref> is applied in another direction from each of capsules in the previous layer to all capsules in the next layer.</p><p>We illustrate our proposed model in <ref type="figure" target="#fig_0">Figure 1</ref> where embedding size: k = 4, the number of filters: N = 5, the number of neurons within the capsules in the first layer is equal to N, and the number of neurons within the capsule in the second layer: d = 2. The length of the vector output e is used as the score for the input triple.</p><p>Formally, we define the score function f for the triple (s, r, o) as follows:</p><formula xml:id="formula_3">f (s, r, o) = capsnet (g ([v s , v r , v o ] * Ω))</formula><p>where the set of filters Ω is shared parameters in the convolution layer; * denotes a convolution operator; and capsnet denotes a capsule network operator. We use the Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba, 2014)</ref> to train CapsE by minimizing the loss function <ref type="bibr" target="#b27">(Trouillon et al., 2016;</ref><ref type="bibr" target="#b12">Nguyen et al., 2018)</ref> as follows:</p><formula xml:id="formula_4">L = (s,r,o)∈{G∪G } log 1 + exp −t (s,r,o) · f (s, r, o) in which, t (s,r,o) = 1 for (s, r, o) ∈ G −1 for (s, r, o) ∈ G</formula><p>here G and G are collections of valid and invalid triples, respectively. G is generated by corrupting valid triples in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge graph completion evaluation</head><p>In the knowledge graph completion task <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, the goal is to predict a missing entity given a relation and another entity, i.e, inferring a head entity s given (r, o) or inferring a tail entity o given (s, r). The results are calculated based on ranking the scores produced by the score function f on test triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>Datasets: We use two recent benchmark datasets WN18RR <ref type="bibr" target="#b5">(Dettmers et al., 2018)</ref> and FB15k-237 <ref type="bibr" target="#b26">(Toutanova and Chen, 2015)</ref>. These two datasets are created to avoid reversible relation problems, thus the prediction task becomes more realistic and hence more challenging <ref type="bibr" target="#b26">(Toutanova and Chen, 2015)</ref>. <ref type="table" target="#tab_0">Table 1</ref>  Evaluation protocol: Following <ref type="bibr" target="#b2">Bordes et al. (2013)</ref>, for each valid test triple (s, r, o), we replace either s or o by each of all other entities to create a set of corrupted triples. We use the "Filtered" setting protocol <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, i.e., not taking any corrupted triples that appear in the KG into accounts. We rank the valid test triple and corrupted triples in descending order of their scores. We employ evaluation metrics: mean rank (MR), mean reciprocal rank (MRR) and Hits@10 (i.e., the proportion of the valid test triples ranking in top 10 predictions). Lower MR, higher MRR or higher Hits@10 indicate better performance. Final scores on the test set are reported for the model obtaining the highest Hits@10 on the validation set. Training protocol: We use the common Bernoulli strategy <ref type="bibr" target="#b31">(Wang et al., 2014;</ref><ref type="bibr" target="#b11">Lin et al., 2015b)</ref> when sampling invalid triples. For WN18RR, Pinter and Eisenstein (2018) 2 found a strong evidence to support the necessity of a WordNet-related semantic setup, in which they averaged pre-trained word embeddings for word surface forms within the WordNet to create synset embeddings, and then used these synset embeddings to initialize entity embeddings for training their TransE association model. We follow this evidence in using the pre-trained 100-dimensional Glove word embeddings <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref> to train a TransE model on WN18RR. <ref type="bibr">2</ref> Pinter and Eisenstein (2018) considered WN18RR and evaluated their M3GM model only for 7 relations as they employed the inverse rule model <ref type="bibr" target="#b5">(Dettmers et al., 2018)</ref> for 4 remaining symmetric relations. Regarding a fair comparison to other models, we use the M3GM implementation released by <ref type="bibr" target="#b20">Pinter and Eisenstein (2018)</ref> to re-train and re-evaluate the M3GM model for all 11 relations. We thank <ref type="bibr" target="#b20">Pinter and Eisenstein (2018)</ref> for their assistance running their code.</p><p>We employ the TransE and ConvKB implementations provided by <ref type="bibr" target="#b15">Nguyen et al. (2016b)</ref> and <ref type="bibr" target="#b12">Nguyen et al. (2018)</ref>. For ConvKB, we use a new process of training up to 100 epochs and monitor the Hits@10 score after every 10 training epochs to choose optimal hyper-parameters with the Adam initial learning rate in {1e −5 , 5e −5 , 1e −4 } and the number of filters N in {50, 100, 200, 400}. We obtain the highest Hits@10 scores on the validation set when using N= 400 and the initial learning rate 5e −5 on WN18RR; and N= 100 and the initial learning rate 1e −5 on FB15k-237.</p><p>Like in ConvKB, we use the same pre-trained entity and relation embeddings produced by TransE to initialize entity and relation embeddings in our CapsE for both WN18RR and FB15k-237 (k = 100). We set the batch size to 128, the number of neurons within the capsule in the second capsule layer to 10 (d = 10), and the number of iterations in the routing algorithm m in {1, 3, 5, 7}. We run CapsE up to 50 epochs and monitor the Hits@10 score after each 10 training epochs to choose optimal hyper-parameters. The highest Hits@10 scores for our CapsE on the validation set are obtained when using m = 1, N = 400 and the initial learning rate at 1e −5 on WN18RR; and m = 1, N = 50 and the initial learning rate at 1e −4 on FB15k-237. Following <ref type="bibr" target="#b2">Bordes et al. (2013)</ref>, for each relation r in FB15k-237, we calculate the averaged number η s of head entities per tail entity and the averaged number η o of tail entities per head entity. If η s &lt;1.5 and η o &lt;1.5, r is categorized one-to-one (1-1). If η s &lt;1.5 and η o ≥1.5, r is categorized one-to-many (1-M). If η s ≥1.5 and η o &lt;1.5, r is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head><p>FB15k-237 MR MRR H@10 MR MRR H@10 DISTMULT <ref type="bibr" target="#b34">(Yang et al., 2015)</ref> 5110 0.      <ref type="bibr">48.37 52.60 53.14 53.33 53.21 3 47.78 52.34 52.93 52.99 52.86 5 47.03 52.25 45.80 45.99 45.76 7 40.46 45.36 45.79 45.85 45.93</ref>   <ref type="figure" target="#fig_2">Figures 2 and 3</ref> are consistent. These also imply that our CapsE would be a potential candidate for applications which contain many M-M relations such as search personalization.</p><p>We see that the length and orientation of each capsule in the first layer can also help to model the important entries in the corresponding dimension, thus CapsE can work well on the "side M" of triples where entities often appear less frequently than others appearing in the "side 1" of triples. Additionally, existing models such as DISTMULT, ComplEx and ConvE can perform well for entities with high frequency, but may not for rare entities with low frequency. These are reasons why our CapsE can be considered as the best one on FB15k-237 and it outperforms most existing models on WN18RR.</p><p>Effects of routing iterations: We study how the number of routing iterations affect the performance. <ref type="table" target="#tab_4">Table 3</ref> shows the Hits@10 scores on the WN18RR validation set for a comparison w.r.t each number value of the routing iterations and epochs with the number of filters N = 50 and the Adam initial learning rate at 1e −5 . We see that the best performance for each setup over each 10 epochs is obtained by setting the number m of routing iterations to 1. This indicates the opposite side for knowledge graphs compared to images. In the image classification task, setting the number m of iterations in the routing process higher than 1 helps to capture the relative positions of entities in an image (e.g., eyes, nose and mouth) properly. In contrast, this property from images may be only right for the 1-1 relations, but not for the 1-M, M-1 and M-M relations in the KGs because of the high variant of each relation type (e.g., symmetric relations) among different entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Search personalization application</head><p>Given a user, a submitted query and the documents returned by a search system for that query, our approach is to re-rank the returned documents so that the more relevant documents should be ranked higher. Following <ref type="bibr" target="#b28">Vu et al. (2017)</ref>, we represent the relationship between the submitted query, the user and the returned document as a (s, r, o)-like triple (query, user, document). The triple captures how much interest a user puts on a document given a query. Thus, we can evaluate the effectiveness of our CapsE for the search personalization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Dataset: We use the SEARCH17 dataset <ref type="bibr" target="#b28">(Vu et al., 2017)</ref> of query logs of 106 users collected by a large-scale web search engine. A log entity consists of a user identifier, a query, top-10 ranked documents returned by the search engine and clicked documents along with the user's dwell time. <ref type="bibr" target="#b28">Vu et al. (2017)</ref> constructed short-term (session-based) user profiles and used the profiles to personalize the returned results. They then employed the SAT criteria <ref type="bibr" target="#b6">(Fox et al., 2005)</ref> to identify whether a returned document is relevant from the query logs as either a clicked document with a dwell time of at least 30 seconds or the last clicked document in a search session (i.e., a SAT click). After that, they assigned a relevant label to a returned document if it is a SAT click and also assigned irrelevant labels to the remaining top-10 documents. The rank position of the relevant labeled documents is used as the ground truth to evaluate the search performance before and after re-ranking.</p><p>The dataset was uniformly split into the training, validation and test sets. This split is for the purpose of using historical data in the training set to predict new data in the test set <ref type="bibr" target="#b28">(Vu et al., 2017)</ref>. The training, validation and test sets consist of 5,658, 1,184 and 1,210 relevant (i.e., valid) triples; and 40,239, 7,882 and 8,540 irrelevant (i.e., invalid) triples, respectively. Evaluation protocol: Our CapsE is used to rerank the original list of documents returned by a search engine as follows: (i) We train our model and employ the trained model to calculate the score for each (s, r, o) triple. (ii) We then sort the scores in the descending order to obtain a new ranked list. To evaluate the performance of our proposed model, we use two standard evalu-ation metrics: mean reciprocal rank (MRR) and Hits@1. 3 For each metric, the higher value indicates better ranking performance.</p><p>We compare CapsE with the following baselines using the same experimental setup: (1) SE: The original rank is returned by the search engine.</p><p>(2) CI <ref type="bibr" target="#b24">(Teevan et al., 2011)</ref>: This baseline uses a personalized navigation method based on previously clicking returned documents. (3) SP <ref type="bibr" target="#b0">(Bennett et al., 2012;</ref><ref type="bibr" target="#b29">Vu et al., 2015)</ref>: A search personalization method makes use of the sessionbased user profiles. (4) Following <ref type="bibr" target="#b28">Vu et al. (2017)</ref>, we use TransE as a strong baseline model for the search personalization task. Previous work shows that the well-known embedding model TransE, despite its simplicity, obtains very competitive results for the knowledge graph completion <ref type="bibr" target="#b10">(Lin et al., 2015a;</ref><ref type="bibr" target="#b17">Nickel et al., 2016b;</ref><ref type="bibr" target="#b27">Trouillon et al., 2016;</ref><ref type="bibr" target="#b14">Nguyen et al., 2016a</ref><ref type="bibr" target="#b12">Nguyen et al., , 2018</ref>. <ref type="formula">(5)</ref> The CNNbased model ConvKB is the most closely related model to our CapsE. Embedding initialization: We follow <ref type="bibr" target="#b28">Vu et al. (2017)</ref> to initialize user profile, query and document embeddings for the baselines TransE and ConvKB, and our CapsE.</p><p>We train a LDA topic model <ref type="bibr" target="#b1">(Blei et al., 2003)</ref> with 200 topics only on the relevant documents (i.e., SAT clicks) extracted from the query logs. We then use the trained LDA model to infer the probability distribution over topics for every returned document. We use the topic proportion vector of each document as its document embedding (i.e. k = 200). In particular, the z th element (z = 1, 2, ..., k) of the vector embedding for document d is: v d,z = P(z | d) where P(z | d) is the probability of the topic z given the document d.</p><p>We also represent each query by a probability distribution vector over topics. Let D q = {d 1 , d 2 , ..., d n } be the set of top n ranked documents returned for a query q (here, n = 10). The z th element of the vector embedding for query q is defined as in <ref type="bibr" target="#b28">(Vu et al., 2017)</ref>:</p><formula xml:id="formula_5">v q,z = n i=1 λ i P(z | d i ), where λ i = δ i−1 n j=1 δ j−1 is the exponential decay function of i which is the rank of d i in D q .</formula><p>And δ is the decay hyper-parameter (0 &lt; δ &lt; 1). Following <ref type="bibr" target="#b28">Vu et al. (2017)</ref>, we use δ = 0.8. Note that if we learn query and document embeddings during training, the models will overfit to the data and will not work for new queries <ref type="bibr">3</ref> We re-rank the list of top-10 documents returned by the search engine, so Hits@10 scores are same for all models. and documents. Thus, after the initialization process, we fix (i.e., not updating) query and document embeddings during training for TransE, <ref type="bibr">Con-vKB and CapsE.</ref> In addition, as mentioned by <ref type="bibr" target="#b0">Bennett et al. (2012)</ref>, the more recently clicked document expresses more about the user current search interest. Hence, we make use of the user clicked documents in the training set with the temporal weighting scheme proposed by <ref type="bibr" target="#b29">Vu et al. (2015)</ref> to initialize user profile embeddings for the three embedding models.</p><p>Hyper-parameter tuning: For our CapsE model, we set batch size to 128, and also the number of neurons within the capsule in the second capsule layer to 10 (d = 10). The number of iterations in the routing algorithm is set to 1 (m = 1). For the training model, we use the Adam optimizer with the initial learning rate ∈ {5e −6 , 1e −5 , 5e −5 , 1e −4 , 5e −4 }. We also use ReLU as the activation function g. We select the number of filters N ∈ {50, 100, 200, 400, 500}. We run the model up to 200 epochs and perform a grid search to choose optimal hyper-parameters on the validation set. We monitor the MRR score after each training epoch and obtain the highest MRR score on the validation set when using N = 400 and the initial learning rate at 5e −5 .</p><p>We employ the TransE and ConvKB implementations provided by <ref type="bibr" target="#b15">Nguyen et al. (2016b)</ref> and <ref type="bibr" target="#b12">Nguyen et al. (2018)</ref> and then follow their training protocols to tune hyper-parameters for TransE and ConvKB, respectively. We also monitor the MRR score after each training epoch and attain the highest MRR score on the validation set when using margin = 5, l 1 -norm and SGD learning rate at 5e −3 for TransE; and N = 500 and the Adam initial learning rate at 5e −4 for ConvKB.   <ref type="bibr" target="#b28">(Vu et al., 2017)</ref>. Hits@1 (H@1) is reported in %. In information retrieval, Hits@1 is also referred to as P@1. The subscripts denote the relative improvement over our TransE results. Specifically, our CapsE achieves the highest performances in both MRR and Hits@1 (our improvements over all five baselines are statistically significant with p &lt; 0.05 using the paired t-test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head><p>To illustrate our training progress, we plot performances of CapsE on the validation set over epochs in <ref type="figure" target="#fig_4">Figure 4</ref>. We observe that the performance is improved with the increase in the number of filters since capsules can encode more useful properties for a large embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Other transition-based models extend TransE to additionally use projection vectors or matrices to translate embeddings of s and o into the vector space of r, such as: TransH <ref type="bibr" target="#b31">(Wang et al., 2014)</ref>, TransR <ref type="bibr" target="#b11">(Lin et al., 2015b)</ref>, TransD <ref type="bibr" target="#b8">(Ji et al., 2015)</ref> and <ref type="bibr">STransE (Nguyen et al., 2016b)</ref>. Furthermore, DISTMULT <ref type="bibr" target="#b34">(Yang et al., 2015)</ref> and ComplEx <ref type="bibr" target="#b27">(Trouillon et al., 2016)</ref> use a tri-linear dot product to compute the score for each triple. Moreover, ConvKB (Nguyen et al., 2018) applies convolutional neural network, in which feature maps are concatenated into a single feature vector which is then computed with a weight vector via a dot product to produce the score for the input triple.</p><p>ConvKB is the most closely related model to our CapsE. See an overview of embedding models for KG completion in <ref type="bibr" target="#b13">(Nguyen, 2017)</ref>.</p><p>For search tasks, unlike classical methods, personalized search systems utilize the historical interactions between the user and the search system, such as submitted queries and clicked documents to tailor returned results to the need of that user <ref type="bibr" target="#b23">(Teevan et al., 2005</ref><ref type="bibr" target="#b25">(Teevan et al., , 2009</ref>. That historical information can be used to build the user profile, which is crucial to an effective search personalization system. Widely used approaches consist of two separated steps: (1) building the user profile from the interactions between the user and the search system; and then (2) learning a ranking function to re-rank the search results using the user profile <ref type="bibr" target="#b0">(Bennett et al., 2012;</ref><ref type="bibr" target="#b33">White et al., 2013;</ref><ref type="bibr" target="#b7">Harvey et al., 2013;</ref><ref type="bibr" target="#b29">Vu et al., 2015)</ref>. The general goal is to re-rank the documents returned by the search system in such a way that the more relevant documents are ranked higher. In this case, apart from the user profile, dozens of other features have been proposed as the input of a learning-to-rank algorithm <ref type="bibr" target="#b0">(Bennett et al., 2012;</ref><ref type="bibr" target="#b33">White et al., 2013)</ref>. Alternatively, <ref type="bibr" target="#b28">Vu et al. (2017)</ref> modeled the potential user-oriented relationship between the submitted query and the returned document by applying TransE to reward higher scores for more relevant documents (e.g., clicked documents). They achieved better performances than the standard ranker as well as competitive search personalization baselines <ref type="bibr" target="#b24">(Teevan et al., 2011;</ref><ref type="bibr" target="#b0">Bennett et al., 2012;</ref><ref type="bibr" target="#b29">Vu et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose CapsE-a novel embedding model using the capsule network to model relationship triples for knowledge graph completion and search personalization. Experimental results show that our CapsE outperforms other state-of-the-art models on two benchmark datasets WN18RR and FB15k-237 for the knowledge graph completion. We then show the effectiveness of our CapsE for the search personalization, in which CapsE outperforms the competitive baselines on the dataset SEARCH17 of the web search query logs. In addition, our CapsE is capable to effectively model many-to-many relationships. Our code is available at: https://github.com/daiquocnguyen/CapsE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example illustration of our CapsE with k = 4, N = 5, and d = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Hits@10 (in %) and MRR on the FB15k-237 test set w.r.t each relation category. Hits@10 and MRR on the WN18RR test set w.r.t each relation. The right y-axis is the percentage of triples corresponding to relations.categorized many-to-one (M-1). If η s ≥1.5 and η o ≥1.5, r is categorized many-to-many (M-M). As a result, 17, 26, 81 and 113 relations are labelled 1-1, 1-M, M-1 and M-M, respectively. And 0.9%, 6.3%, 20.5% and 72.3% of the test triples in FB15k-237 contain 1-1, 1-M, M-1 and M-M relations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>shows the Hits@10 and MRR results for predicting head and tail entities w.r.t each relation category on FB15k-237. CapsE works better than ConvKB in predicting entities on the "side M" of triples (e.g., predicting head entities in M-1 and M-M; and predicting tail entities in 1-M and M-M), while ConvKB performs better than CapsE in predicting entities on the "side 1" of triples (i.e., predicting head entities in 1-1 and 1-M; and predicting tail entities in 1-1 and M-1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>shows the Hits@10 and MRR scores w.r.t each relation on WN18RR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Learning curves on the validation set with the initial learning rate at 5e −5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>presents the statistics of WN18RR and FB15k-237. Statistics of the experimental datasets. #E is the number of entities. #R is the number of relations.</figDesc><table><row><cell>Dataset</cell><cell>#E</cell><cell>#R #Triples in train/valid/test</cell></row><row><cell cols="3">WN18RR 40,943 11 86,835 3,034 3,134</cell></row><row><cell cols="3">FB15k-237 14,541 237 272,115 17,535 20,466</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>compares the experimental results of our CapsE with previous state-of-the-art published results, using the same evaluation protocol. Our CapsE performs better than its closely related CNN-based model ConvKB on both experimental datasets (except Hits@10 on WN18RR and MR on FB15k-237), especially on FB15k-237 where our CapsE gains significant improvements of 0.523 − 0.418 = 0.105 in MRR (which is about 25.1% relative improvement), and 59.3% − 53.2% = 6.1% absolute improvement in Hits@10.Table 2also shows that our CapsE obtains the best MR score on WN18RR and the highest MRR and Hits@10 scores on FB15k-237.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on the WN18RR and FB15k-237 test sets. Hits@10 (H@10) is reported in %. Results of DISTMULT, ComplEx and ConvE are taken from<ref type="bibr" target="#b5">Dettmers et al. (2018)</ref>. Results of TransE on FB15k-237 are taken from<ref type="bibr" target="#b12">Nguyen et al. (2018)</ref>. Our CapsE Hits@1 scores are 33.7% on WN18RR and 48.9% on FB15k-237. Formulas of MRR and Hits@1 show a strong correlation, so using Hits@1 does not really reveal any additional information for this task. The best score is in bold, while the second best score is in underline. denotes our new results for TransE and ConvKB, which are better than those published by<ref type="bibr" target="#b12">Nguyen et al. (2018)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Predicting head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell cols="4">CapsE ConvKB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80.26</cell><cell>83.58</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hits@10</cell><cell>40 60</cell><cell>47.4</cell><cell>51.56</cell><cell>47.72</cell><cell>57.15</cell><cell>59.78</cell><cell>49.51</cell><cell>57.28</cell><cell>48.85</cell><cell>40 60</cell><cell>47.92</cell><cell>50</cell><cell></cell><cell></cell><cell>58.59</cell><cell>53.46</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell>17.48</cell><cell>11.37</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1-1</cell><cell cols="2">1-M</cell><cell cols="2">M-1</cell><cell cols="2">M-M</cell><cell></cell><cell cols="2">1-1</cell><cell cols="2">1-M</cell><cell>M-1</cell><cell>M-M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Hits@10 on the WN18RR validation set with N = 50 and the initial learning rate at 1e −5 w.r.t each number of iterations in the routing algorithm m and each 10 training epochs.</figDesc><table><row><cell>derivationally related f orm are symmet-</cell></row><row><cell>ric relations which can be considered as M-M</cell></row><row><cell>relations. Our CapsE also performs better than</cell></row><row><cell>ConvKB on these 4 M-M relations. Thus, results</cell></row><row><cell>shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>presents the experimental results of the baselines and our model. Embedding models TranE, ConvKB and CapsE produce better ranking performances than traditional learning-to-rank search personalization models CI and SP. This indicates a prospective strategy of expanding the triple embedding models to improve the ranking quality of the search personalization systems. In particular, our MRR and Hits@1 scores are higher than those of TransE (with relative improvements of 14.5% and 22% over TransE, respectively).</figDesc><table><row><cell>Method</cell><cell>MRR</cell><cell>H@1</cell></row><row><cell>SE [ ]</cell><cell>0.559</cell><cell>38.5</cell></row><row><cell>CI [ ]</cell><cell>0.597</cell><cell>41.6</cell></row><row><cell>SP [ ]</cell><cell>0.631</cell><cell>45.2</cell></row><row><cell>TransE [ ]</cell><cell>0.645</cell><cell>48.1</cell></row><row><cell cols="2">TransE (ours) 0.669</cell><cell>50.9</cell></row><row><cell>ConvKB</cell><cell cols="2">0.750 +12.1% 59.9 +17.7%</cell></row><row><cell>Our CapsE</cell><cell cols="2">0.766 +14.5% 62.1 +22.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on the test set. [ ] denotes the results reported in</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was partially supported by the ARC Discovery Projects DP150100031 and DP160103934. The authors thank Yuval Pinter for assisting us in running his code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling the impact of shortand long-term behavior on search personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multirelational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Structured Embeddings of Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">KBGAN: Adversarial Learning for Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating implicit measures to improve web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Karnawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mydland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="168" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building user profiles from topic models for personalised search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Carman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information and Knowledge Management</title>
		<meeting>the ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2309" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding via Dynamic Mapping Matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling Relation Paths for Representation Learning of Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learning</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An overview of embedding models of entities and relationships for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neighborhood Mixture Model for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">STransE: a novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Review of Relational Machine Learning for Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Holographic Embeddings of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge Graphs</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting Semantic Relations using Global Graph Properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1741" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reasoning With Neural Tensor Networks for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Personalizing search via automated analysis of interests and activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding and predicting personal navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Liebling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayathri Ravichandran</forename><surname>Geetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Web Search and Data Mining</title>
		<meeting>the ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discovering and using groups to improve personalized search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Bush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Web Search and Data Mining</title>
		<meeting>the ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Observed Versus Latent Features for Knowledge Base and Text Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Search personalization with embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Willis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Information Retrieval</title>
		<meeting>the European Conference on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="598" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal latent topic user profiles for search personalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><forename type="middle">Ngoc</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Information Retrieval</title>
		<meeting>the European Conference on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="605" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding: A Survey of Approaches and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge Base Completion via Searchbased Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on World Wide Web</title>
		<meeting>the 23rd International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="515" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhancing personalized search by mining and modeling task behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ryen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web conference</title>
		<meeting>the World Wide Web conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1411" to="1420" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Yang Song, and Hongning Wang</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
