<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Jen</forename><surname>Tsai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Tsung</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Chengchi University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Chiao Tung University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Qualcomm Technologies, Inc. * equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image motion blur usually results from moving objects or camera shakes. Such blur is generally directional and non-uniform. Previous research efforts attempt to solve non-uniform blur by using self-recurrent multi-scale or multi-patch architectures accompanying with self-attention. However, using self-recurrent frameworks typically leads to a longer inference time, while inter-pixel or inter-channel self-attention may cause excessive memory usage. This paper proposes blur-aware attention networks (BANet) that accomplish accurate and efficient deblurring via a single forward pass. Our BANet utilizes region-based selfattention with multi-kernel strip pooling to disentangle blur patterns of different degrees and with cascaded parallel dilated convolution to aggregate multi-scale content features. Extensive experimental results on the GoPro and HIDE benchmarks demonstrate that the proposed BANet performs favorably against the state-of-the-art in blurred image restoration and can provide deblurred results in realtime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dynamic scene deblurring or blind motion deblurring aims to restore a blurred image with little knowledge about the blur kernel. Scene blurring caused by camera shakes, moving objects, low shutter speeds, or low frame rates not only degrades the quality of the taken images/videos but also results in information loss. Therefore, removing such blurring artifacts to recover image details becomes essential to many subsequent vision applications where clean and sharp images are appreciated. Although significant progress has been made in both conventional and deep-learningbased approaches, we observe a compromise between accuracy and speed. Owing to this observation, we target at developing an efficient and effective algorithm in this paper for blurred image restoration with its current performance in accuracy and speed shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Deep-learning-based approaches usually reach superior results, given their better feature representation capability toward dynamic scenes. Among the state-of-the-art architectures for deblurring, the self-recurrent module is widely used to leverage blurred image repeatability in either multiple scales (MS) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>, multiple patch levels (MP) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>, or multiple temporal behaviors (MT) <ref type="bibr" target="#b14">[15]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a)-(c). MS models distill multi-scale blur information in a self-recurrent manner and restore blurred images using the resultant coarse-to-fine features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>. However, scaling a blurred image to a lower resolution often results in blur information loss, so it may not help deblurring much. MP models split an input blurred image into multiple patches to estimate and remove motion blur of different scales <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>. Yet, patches heuristically separated are sub-optimal for handling non-uniform blur in dynamic scenes. In <ref type="bibr" target="#b14">[15]</ref>, an MT structure is proposed to eliminate non-uniform blur and can generate better results progressively. Despite their effectiveness, existing recurrent models are generally far from real-time deblurring.</p><p>In addition to model architectures, research studies [17,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv Layers</head><p>Conv Features Blur-Aware Moduel 19] further exploit self-attention to address blur nonuniformity. Suin et al. <ref type="bibr" target="#b18">[19]</ref> utilize MP-based processing with self-attention to extract features for areas with global and local motion. However, using the self-recurrent mechanism to generate multi-scale features often leads to a longer inference time. To shorten the latency, Purohit and Rajagopalan <ref type="bibr" target="#b16">[17]</ref> selectively aggregate features through learnable attention which is enabled by deformable convolutions for modeling local blur in a single forward pass. Despite its effectiveness, self-attention exploring pixel-wise or channel-wise correlations via trainable filters often causes high memory usage, thus only applicable to small-scale features <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>. Furthermore, motion blur coming from moving objects manifests smeared effects and produces directional and local averaging artifacts that cannot be handled by inter-pixel/channel correlations.</p><formula xml:id="formula_0">(a) (b) 2 iter 1 iter (c) (d)</formula><p>This paper proposes a blur-aware attention networks (BANet) to overcome the aforementioned issues. BANet is an efficient yet effective single-forward-pass model, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(d), which achieves the state-of-the-art deblurring performance and works in real time, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, our model stacks multiple layers of the blur-aware attention module (BAM) for removing motion blur. The proposed BAM computes region-based attention by using fast and computationally inexpensive local averaging to globally and locally capture blurred patterns of different degrees, and leverages cascaded parallel dilated convolution to extract features without suffering from blur information loss as an MS model does. As a result, the proposed model possesses a better deblurring capability and can support subsequent real-time applications. In short, our contributions are two-fold: First, we design a novel BAM module which is capable of disentangling blur contents of different degrees in dynamic scenes. Second, our efficient single-forward-pass deep networks perform favor-ably against the state-of-the-art methods while running 27x faster than the best competitor <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conventional Methods. Single-image deblurring is highly ill-posed. Conventional image deblurring studies often make different assumptions, e.g., uniform, nonuniform, or depth-aware, to model blur characteristics. Namely, these methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> apply different constraints to the estimated blur kernels, latent images, or both with handcrafted regularization terms for blur removal. Unfortunately, these methods often lead to solving a non-convex optimization problem and involve heuristic parameter tuning that is entangled with the camera pipeline; thus, they cannot generalize well for complex real-world examples.</p><p>Deblurring via Learning. Learning-based approaches with self-recurrent modules gain great success in singleimage deblurring; namely, the corarse-to-fine scheme can gradually restore the sharp image on different resolutions (MS) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>, fields of view (MP) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>, or temporal characteristics (MT) <ref type="bibr" target="#b14">[15]</ref>. Despite the success, selfrecurrent models usually lead to longer runtime. Recently, non-recurrent based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> are proposed for efficient deblurring. For instance, Kupyn et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> suggest using conditional generative adversarial networks to restore blurred images. However, their methods do not well address non-uniform blur in dynamic scenes, often causing blur artifacts in the deblurring results. <ref type="bibr">Yuan</ref>   Self-attention. Self-attention (SA) <ref type="bibr" target="#b20">[21]</ref> has been widely adopted to advance the fields of image processing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> and computer vision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>. Recent studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> show that attention is beneficial for learning inter-pixel correlations to emphasize different local features for removing non-uniform blur. Purohit et al. <ref type="bibr" target="#b16">[17]</ref> propose to deblur using SA to explore pixel-wise correlation for a non-local feature adaptation. However, since SA requires much memory in O((HW ) 2 ) space, where H and W are the height and width of the input to SA. Thus, their method only applies SA to the smallest-scale features (from a 1280×720 blurred input to 160 × 90 SA's input), limiting the efficacy of SA. Also, motion blur causes directional and local averaging artifacts, which may not be well addressed by merely pixelwise SA. Suin et al. <ref type="bibr" target="#b18">[19]</ref> employ an MP architecture with less memory-intensive SA by using global average pooling, resulting in the space complexity O(d a d c HW ) where d a is the channel dimension of the components key and value in SA; d c is the dimension of the component query and d a d c HW . Although their method produces highquality deblurring results, it suffers from longer inference time. In contrast, we propose an efficient and low-memorycomplexity regional averaging SA to capture blur information more accurately, which can be applied to input images of full resolution and gets better performance in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>We present the blur-aware attention networks (BANet) to address the potential issues in two commonly used techniques for deblurring: self-recurrence and self-attention. Self-recurrent algorithms result in longer inference time due to repeatedly accessing input blurred images. Self-attention based on inter-pixel or inter-channel correlations is memory intensive and cannot explicitly capture regional blurring information. Instead, the proposed BANet is a one-pass residual network consisting of a series of the stacked blur-aware modules (BAM), which serve as the building blocks, to effectively disentangle different degrees of blurriness.</p><p>As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, BANet starts with two convolutional layers, which contain a stride of 2 to downsample the input image to half resolution. BANet employs one transposed convolutional layer to upsample features to the original size. In between, we stack a set of BAMs to correlate regions with similar blur and extract multi-scale content features. A BAM consists of two components: bluraware attention (BA) and cascaded parallel dilated convolution (CPDC). The BAM is also a residual-like architecture that concatenates two output to capture both global and local blurring features in a learnable manner. We detail the two key components, BA and CPDC, in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Blur-aware Attention (BA)</head><p>To accurately restore the motion area displaying directional and averaging artifacts caused by moving objects and camera shakes, we propose a region-based self-attention module, called blur-aware attention (BA), to capture such effects in global (image) and local (patch) scales. As shown in <ref type="figure">Figure 4</ref>, BA contains two parts including multi-kernel strip pooling (MKSP) and attention refinement (AR). We describe their details as follows.</p><p>Multi-Kernel Strip Pooling (MKSP). Strip pooling (SP) has been broadly used to capture region-based information. Hou et al. <ref type="bibr" target="#b3">[4]</ref> present an SP method that uses horizontal and vertical one-pixel long kernels to extract long-range context information for scene parsing. SP averages the input features within a row or a column individually and then fuses the two thin-strip features to discover global crossregion dependencies. Let the input feature map be a threedimensional (3D) tensor x ∈ R H×W ×C , where C denotes the number of channels. Applying SP to x generates a vertical and a horizontal tensor followed by a 1D convolution layer with the kernel size of 3. This produces y v ∈ R H×C </p><formula xml:id="formula_1">M sp = σ sig (f 1 (y)),<label>(1)</label></formula><p>where f 1 is a 1 × 1 convolution and σ sig is the sigmoid function.</p><p>Motivated by SP, we propose MKSP that adopts strip pooling with different kernel sizes to filter out irrelevant parts from attended features more precisely. MKSP constructs horizontal and vertical n-pixel long kernels to average input features within rows or columns, where n ∈ {1, 3, 5, 7}. Thus, MKSP generates four pairs of tensors, each of which has a vertical and a horizontal tensor followed by a 1D (for the special case n = 1) or 2D (for the rest cases) convolution layer with the kernel size of 3 or 3 × 3, respectively. This produces y v,n ∈ R H×n×C and y h,n ∈ R n×W ×C , where the vertical tensor is</p><formula xml:id="formula_2">y v,n i,j,c = 1 K h K h −1 k=0 x i,(j·S h +k),c ,<label>(2)</label></formula><p>where the horizontal stride S h = W n and the horizontalstrip kernel size K h = W − (n − 1)S h . Symmetrically, the horizontal tensor is defined by</p><formula xml:id="formula_3">y h,n i,j,c = 1 K v Kv−1 k=0 x (i·Sv+k),j,c ,<label>(3)</label></formula><p>where the vertical stride S v = H n and the vertical-strip kernel size </p><formula xml:id="formula_4">K v = H − (n − 1)S v . (a) (b) (c)</formula><p>Similar to SP, we concatenate all the fused tensors to yield an attention mask as M mksp = f AR (y 1 ⊕ y 3 ⊕ y 5 ⊕ y 7 ), where ⊕ stands for concatenation operation, and f AR (·) represents two 3 × 3 convolutions with a ReLU function in between followed by a sigmoid function. The proposed MKSP can generate attention masks that better fit objects or local scenes than those by using SP with only H × 1 and 1 × W kernels used, which yields rough band-shape masks. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example of mask comparison between SP and the proposed MKSP.</p><p>Attention Refinement (AR). After obtaining the attended features by element-wise multiplication of the attention masks M mksp and the input tensor x, we further refine these features via a simple attention mechanism using f AR (·). The final output of our BA block through the MKSP and AR stages is computed as</p><formula xml:id="formula_6">f AR (x) ⊗x,<label>(5)</label></formula><p>where ⊗ represents element-wise multiplication, andx = M mksp ⊗ x. <ref type="figure">Figure 6</ref> demonstrates that cascading MKSP with AR can refine the attended feature maps. The proposed BA facilitates the attention mechanism applied to deblurring since it requires less memory, i.e. O(HW C) where C represents channel dimensions, than those adopted in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>. It disentangles blurred contents with different degrees. <ref type="figure">Figure 7</ref> showcases three examples of blur content disentanglement by using the proposed BA, where we witness that background scenes are differentiated from the foreground scenes because objects closer to the camera move faster, thus more blurred. <ref type="figure">Figure 9</ref> shows more examples of attention maps yielded by BA, which implicitly acts as a gate for propagating relevant blur contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cascaded Parallel Dilated Convolution (CPDC)</head><p>Atrous convolution, also called dilated convolution, has been widely applied to computer-vision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> for enlarging receptive fields and extracting features from objects with different scales without increasing the kernel size. Inspired by this, we design a cascaded parallel dilated con-</p><formula xml:id="formula_7">D = 1 [ , , ] [ , ,<label>3 2 ]</label></formula><p>[ , , ] [ , , <ref type="bibr">3 2 ]</ref> (a) volution (CPDC) block with multiple dilation rates to capture multi-scale blurred objects. Instead of stacking dilated convolution layers with different rates in parallel, which we call parallel dilated convolution (PDC), our CPDC block cascades two sets of PDC with a convolutional layer. As an example, <ref type="figure" target="#fig_6">Figure 8</ref>(a) shows a PDC block that has three 3 × 3 dilated convolutional layers, each of which has a dilation rate D, where D = 1, 3, and 5, and each of which outputs features with half the number of input channels. After concatenation, the number of the output channel of the PDC block increases by 1.5 times. As shown in <ref type="figure" target="#fig_6">Figure 8(b)</ref>, our CPDC block consists of two PDC blocks bridged by a 3 × 3 convolutional layer, which would be more effective in aggregating multi-scale content information for deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We evaluate the blur-aware attention networks (BANet) on two image deblurring benchmark datasets: 1) GoPro dataset <ref type="bibr" target="#b11">[12]</ref> consists of 3214 pairs of blurred and sharp images of resolution 720 × 1280, where 2103 pairs are used for training and the rest are for testing, and 2) HIDE dataset <ref type="bibr" target="#b17">[18]</ref> that contains 2025 pairs of HD images, all for testing. We train our model for 3, 000 epochs using Adam optimizer with parameters β 1 = 0.9 and β 2 = 0.999. We set the initial learning rate to 10 −4 . After the first 50 epochs, it starts to linearly decay until 10 −7 after another 150 epochs. Following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, we randomly crop the input into 256 × 256 patches, along with random flipping or rotation for data augmentation. Lastly, we implement our model with PyTorch library on the Intel Xeon Silver 4210 CPU and NVIDIA 2080ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>Quantitative Analysis: We compare our method with eleven recently published works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> that also handle dynamic deblurring on the <ref type="figure">Figure 9</ref>. Visualization of the blur-aware attention masks where moving objects in the blurred images are highlighted while background is mostly excluded. These blur-aware masks are crucial for handling blurry images with diverse blur patterns. <ref type="table">Table 1</ref>. Evaluation results on the benchmark GoPro test dataset. A value in bold is the best score in its column and underlined the second best. Symbol * represents that the code was not released; thus we cite the results from the original papers or evaluate on the released deblurring images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>PSNR↑ SSIM↑ Time (ms) MSCNN <ref type="bibr" target="#b11">[12]</ref> 30.40 0.936 943 SRN <ref type="bibr" target="#b19">[20]</ref> 30.25 0.934 650 DSD <ref type="bibr" target="#b2">[3]</ref> 30.96 0.942 1300 DeblurGanv2 <ref type="bibr" target="#b8">[9]</ref> 29.55 0.934 42 DMPHN <ref type="bibr" target="#b23">[24]</ref> 31.36 0.947 354 LEBMD * <ref type="bibr" target="#b5">[6]</ref> 31.79 0.949 -EDSD * <ref type="bibr" target="#b22">[23]</ref> 29.81 0.934 10 DBGAN+ * <ref type="bibr" target="#b25">[26]</ref> 31.10 0.942 -MTRNN <ref type="bibr" target="#b14">[15]</ref> 31  <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> methods depending on compared methods' availability. <ref type="table">Table 1</ref> lists the metric scores i.e. PSNR, SSIM and running time obtained on the GoPro dataset from different approaches, including two variants of our model: one stacking eight (stack-8) and the other stacking ten (stack-10) BAMs, respectively. We can observe that the self-recurrent types of models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref> take comparatively longer times than non-recurrent ones <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>, including ours. We recorded the average runtime of all the models using a single GPU. As reported in <ref type="table">Table 1</ref>, our BANet runs significantly faster than the others while achieving the best metric scores. In particular, BANet outperforms the nearest competitor <ref type="bibr" target="#b18">[19]</ref> by 0.42 in terms of PSNR while running about 27x faster. Similarly, in <ref type="table" target="#tab_2">Table 2</ref>, the BANet works significantly better than all the compared methods. Method PSNR↑ SSIM↑ Time (ms) DeblurGanv2 <ref type="bibr" target="#b8">[9]</ref> 27.40 0.882 42 SRN <ref type="bibr" target="#b19">[20]</ref> 28.36 0.904 424 HAdeblur * <ref type="bibr" target="#b17">[18]</ref> 28.87 0.930 -DSD <ref type="bibr" target="#b2">[3]</ref> 29.10 0.913 1.200 DMPHN <ref type="bibr" target="#b23">[24]</ref> 29.10 0.918 341 MTRNN <ref type="bibr" target="#b14">[15]</ref> 29 Qualitative Analysis: <ref type="figure" target="#fig_0">Figure 10</ref> shows the qualitative comparisons on the GoPro test dataset with previous state-ofthe-art <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>, and <ref type="figure" target="#fig_0">Figure 11</ref> on the HIDE dataset with <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. Thanks to the designed blur-aware module, our model can effectively restore results with sharper edges and richer details in dynamic scenes containing camera shake and object motion. For instance, MTRNN <ref type="bibr" target="#b14">[15]</ref> and RADN <ref type="bibr" target="#b16">[17]</ref> can not well recover the text region in the first example and the brick pattern in the second example in <ref type="figure" target="#fig_0">Figure 10</ref>. In <ref type="figure" target="#fig_0">Figure 11</ref>, both MTRNN <ref type="bibr" target="#b14">[15]</ref> and DM-PHN <ref type="bibr" target="#b23">[24]</ref> do not work well on deblurring the text in the first example and the faces in the second and third examples. In contrast, BANet can recover those regions well with an even shorter runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>BAM with Different Components: In <ref type="table" target="#tab_4">Table 3</ref>, we explore the performance change with different component unions in our Blur-Aware Module (BAM) based on the GoPro test dataset. Adding a simple attention mechanism (AR) to both PDC (Net1 vs. Net2) or PDC+MKSP (Net3 vs. Net4) Input MTRNN RADN Ours <ref type="figure" target="#fig_0">Figure 10</ref>. Qualitative comparisons of deblurring results on testing images from the GoPro <ref type="bibr" target="#b11">[12]</ref> dataset. The first row shows the input blurred images and the following rows show results from MTRNN <ref type="bibr" target="#b14">[15]</ref>, RADN <ref type="bibr" target="#b16">[17]</ref>, and ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTRNN DMPHN</head><p>Ours <ref type="figure" target="#fig_0">Figure 11</ref>. Qualitative comparisons of deblurring results on testing images from the HIDE <ref type="bibr" target="#b17">[18]</ref> dataset. The first row contains the input blurred images. The second row contains MTRNN <ref type="bibr" target="#b14">[15]</ref>'s deblurred results, the third DMPHN <ref type="bibr" target="#b23">[24]</ref>'s and the last our BANet's.  for deblurring increases PSNR by around 0.2. Substituting PDC in Net4 with CPDC (Net5), our proposed version of the BAM, leads to a significant performance gain. Our proposed Net5 consists of using global attention and local convolution for locating blur regions; thus, this combination can generate the best performance while achieving real-time practical significance. Numbers of Stacked BAMs: Using more layers to enlarges the receptive field may improve performance for computer vision or image processing tasks. However, for deblurring, stacking more layers does not guarantee better performance <ref type="bibr" target="#b18">[19]</ref>, and might additionally cost extra inference time. However, using our residual learning-based BAM design, we can stack multiple layers to expand the effective receptive field for better deblurring. In <ref type="table">Table 4</ref>, we show performance comparisons with varying numbers of BAMs stacked in our model based on the GoPro test dataset. We list four versions: stack-4, stack-8, stack-10, and stack-12, corresponding four, eight, ten, and twelve BAMs used in the BANet. Although the quantitative results improved with the increase in the number of BAMs, the improvement became marginal after 12. Therefore, we choose 10 resblocks for the excellent balance between efficiency and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Blur-aware Attention vs. Self-Attention:</head><p>Purohit et al. <ref type="bibr" target="#b16">[17]</ref> utilized a similar self-attention (SA) mechanism proposed in <ref type="bibr" target="#b24">[25]</ref> for deblurring. It helps connect regions with similar blur to facilitate global access to relevant features across the entire input feature maps. However, its high memory usage makes applying it to images of high resolution infeasible. Thus, SA can only be employed in network layers on a smaller scale like <ref type="bibr" target="#b16">[17]</ref>, where important blur information would be lost due to down-sampling. In contrast, our proposed region-based attention is more suitable to correlate regions with similar blur characteristics. Moreover, it can be applied to images with a larger resolution thanks to its low memory consumption.</p><p>To further demonstrate our BA's efficacy, we compare the SA <ref type="bibr" target="#b24">[25]</ref> with BA using our BANet (stack-4) as a backbone network, shown in <ref type="figure" target="#fig_0">Figure 12</ref> (b). Due to high memory demand for SA (O((HW ) 2 )) to process 720×1280 images, we adopted our stack-4 model for training. When testing the networks, we separated the input image into eight subimages for SA to process each with a single 2080ti GPU. We provided deblurring results using BA with or without splitting the input image into eight sub-images for a fair comparison. <ref type="table" target="#tab_5">Table 5</ref> shows the scenario of the input divided into eight sub-images; BA still works better than SA for deblurring based on our BANet. Also, dividing the input image for deblurring using a single GPU increases the runtime, and adopting BA runs significantly faster. Last but not least, since our BA has lower memory usage, we can process the original input image without division to attain better quality (PSNR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper proposes novel blur-aware attention networks (BANet), which consists of the stacked blur-aware modules (BAMs) to disentangle blur contents of different degrees and the cascaded parallel dilated convolution (CPDC) module to aggregate multi-scale content features, for more accurate and efficient dynamic scene deblurring. We investigate and examine our design through demonstrations of attention masks and attended feature maps as well as extensive ablation studies and performance comparisons. It turns out that the proposed BANet achieves real-time deblurring and performs favorably against the state-of-the-art deblurring methods on the GoPro and HIDE benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Performance comparison on the GoPro test dataset in deblurring quality and runtime. The proposed BANet performs favorably against the state-of-the-art methods in both accuracy and efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Network architecture comparisons among (a) MS, (b) MP, (c) MT, and (d) our BANet. Recurrent models are typically less efficient. BANet completes deblurring via a single forward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Network architecture of the proposed blur-aware attention networks (BANet). The blur-aware modules (BAM) serve as the building blocks of BANet. The first BAM is detailed in the purple dotted box while the rest are represented by solid purple boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 . 1 W W − 1 j=0</head><label>411</label><figDesc>Architecture of blur-aware attention (BA). It cascades two parts, incuding multi-kernel strip pooling (MKSP) and attention refinement (AR). It is developed to disentangle blurred contents in an efficient way. See text for details. and y h ∈ R W ×C , where y v i,c = x i,j,c and y h j,c = 1 H H−1 i=0 x i,j,c . The SP operation fuses the two tensors into a 2D tensor y ∈ R H×W ×C , where y i,j,c = y v j,c + y h i,c , and then turns the fused tensor into an attention mask M sp as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of the attention masks in our deblurring model. (a) An input blurred image. (b) &amp; (c) The attention masks obtained using (b) strip pooling (SP) and (c) our MKSP. MKSP then fuses each pair of tensors (y v,n , y h,n ) into a 2D tensor y n ∈ R H×W ×C by y n i,j,c = y v,n i, n×j W ,c + y h,n n×i H ,j,c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>(a) Input blurred image. (b)-(d) Comparisons among the attended feature maps by using different components of the proposed BA including (b) AR, (c) MKSP, and (d) MKSP + AR. Three disentanglement examples of blurred patterns of different degrees using our BA. (a) Input blurred images and (b) attended feature maps on different regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Architectures of (a) parallel dilated convolution (PDC) and (b) cascaded parallel dilated convolution (CPDC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Architecture comparisons between (a) our original BAM and (b) BA replaced by SA [25] in BAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results on the benchmark HIDE dataset. A value in bold is the best score in its column and underlined the second best. * represents that the code was not released; thus we cite the results from the original papers or evaluate on the released deblurring images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study based on the GoPro test dataset for using different component combinations in the BAM of our BANet (stack-8).</figDesc><table><row><cell cols="5">Method PDC AR MKSP CPDC PSNR Time (ms) Net1 √ 31.29 7 Net2 √ √ 31.47 9 Net3 √ √ 31.83 13 Net4 √ √ √ 32.02 16 Net5 √ √ √ 32.23 23</cell></row><row><cell cols="5">Table 4. Performance comparisons of BANet stacking different</cell></row><row><cell cols="4">numbers of BAMs based on the GoPro test dataset.</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">stack-4 stack-8 stack-10 stack-12</cell></row><row><cell>PSNR</cell><cell>31.44</cell><cell>32.23</cell><cell>32.44</cell><cell>32.46</cell></row><row><cell>Time (ms)</cell><cell>12</cell><cell>23</cell><cell>28</cell><cell>35</cell></row><row><cell>BA CPDC</cell><cell>Concat</cell><cell></cell><cell>SA CPDC</cell><cell>Concat</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>A performance comparison between BA with SA<ref type="bibr" target="#b24">[25]</ref> using BANet (stack-4) as a backbone based on the GoPro test dataset. " * " represents deblurring eight sub-images instead of an entire image.SA</figDesc><table><row><cell>√</cell><cell>√</cell><cell>BA √</cell><cell>PSNR 30.59 31.30 31.44</cell><cell>Time (ms) 1240 670 12</cell></row></table><note>* BA*</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Strip Pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning event-based motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation-free dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Computer Vision</title>
		<meeting>Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Computer Vision</title>
		<meeting>Euro. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Computer Vision</title>
		<meeting>Euro. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Soft-segmentation guided object motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deblurring text images via l0-regularized intensity and gradient prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Computer Vision</title>
		<meeting>Euro. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Region-adaptive dense network for efficient motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nat&apos;l Conf. Artificial Intelligence</title>
		<meeting>Nat&apos;l Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Computer Vision</title>
		<meeting>Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatiallyattentive patch-hierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient dynamic scene deblurring using spatially variant deconvolution network with optical flow guided training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deblurring by realistic blurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
