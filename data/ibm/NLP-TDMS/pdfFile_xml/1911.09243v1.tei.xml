<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Label Classification with Label Graph Superimposing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Wang $</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical Sciences and LMAM</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma $</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical Sciences and LMAM</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
							<email>wenshilei@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Label Classification with Label Graph Superimposing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images or videos always contain multiple objects or actions. Multi-label recognition has been witnessed to achieve pretty performance attribute to the rapid development of deep learning technologies. Recently, graph convolution network (GCN) is leveraged to boost the performance of multi-label recognition. However, what is the best way for label correlation modeling and how feature learning can be improved with label system awareness are still unclear. In this paper, we propose a label graph superimposing framework to improve the conventional GCN+CNN framework developed for multi-label recognition in the following two aspects. Firstly, we model the label correlations by superimposing label graph built from statistical co-occurrence information into the graph constructed from knowledge priors of labels, and then multilayer graph convolutions are applied on the final superimposed graph for label embedding abstraction. Secondly, we propose to leverage embedding of the whole label system for better representation learning. In detail, lateral connections between GCN and CNN are added at shallow, middle and deep layers to inject information of label system into backbone CNN for label-awareness in the feature learning process. Extensive experiments are carried out on MS-COCO and Charades datasets, showing that our proposed solution can greatly improve the recognition performance and achieves new state-of-the-art recognition performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Multi-label is a natural property of images or videos, it is usually the case that a image or video contains multiple objects or actions. In the computer vision community, multilabel recognition is a fundamental and practical task, and has attracted increasing research efforts. Given the great success of single label image/video classification brought by deep convolutional networks <ref type="bibr" target="#b6">(He et al. 2015;</ref><ref type="bibr" target="#b0">Carreira and Zisserman 2017;</ref><ref type="bibr" target="#b7">He et al. 2016a;</ref><ref type="bibr" target="#b2">Feichtenhofer et al. 2018;</ref><ref type="bibr" target="#b28">Wu et al. 2019)</ref>, multi-label recognition can achieve pretty performance by naively treating each label as an independent individual and applying multiple binary classification (a) illustrates the co-occurrence of "Sports Ball" and "Tennis Racket" on the MS-COCO datasets, we can see the frequency that "Tennis Racket" co-occurs with "Sports Ball" is as high as 0.42. Similarly, (b) showcases an example of "Sitting on Couch" and "Watching Television" from the Charades dataset.</p><p>to predict whether a label presents or not. However, we argue that the following two aspects should be taken into consideration for such a task.</p><p>First of all, labels co-occur in images or videos with priors. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, with great chance, "Sports Ball" comes together with "Tennis Racket" and a man "Sitting on Couch" is "Watching Television" simultaneously. Then, a question is naturally raised, how to model the re-lation among labels to leverage such priors for better performance? Secondly, given input X, the common practice for predicting its labels can be formulated as a two-stage mapping y = F 1 • F 0 (X), where F 0 : X → f denotes the CNN feature extraction process and F 1 : f → y is the mapping from feature space to label space. Labels are only explicitly involved in the last stage as supervision in the training phase. Therefore, the further question is, for a specific multi-label classification task, whether and how the mutual-related label space can explicitly help the feature learning process F 0 ?</p><p>To take into account the label correlations, some approaches have been proposed. For example, probabilistic graph model was used in <ref type="bibr" target="#b13">(Li et al. 2016;</ref><ref type="bibr" target="#b14">Li, Zhao, and Guo 2014)</ref> and RNN was used in <ref type="bibr" target="#b22">(Wang et al. 2016a</ref>) to capture dependencies among labels. However, probabilistic graph models may suffer from scalability issues given their computational cost. RNN model relies on predefined or learned label sequential order and fails to well capture the global dependencies. Recently, graph convolutional network <ref type="bibr" target="#b11">(Kipf and Welling 2016)</ref>, aka GCN, has witnessed prevailing success in modeling relationship among vertices of a graph. Such a tool was leveraged to model the relation of the label system for multi-label recognition in <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>. Meanwhile, the label graph was built simply by utilizing the frequency of label co-occurrence. Another direction is to implicitly model label correlations via local image regions attention, as was done in <ref type="bibr" target="#b24">(Wang et al. 2017;</ref><ref type="bibr" target="#b31">Zhu et al. 2017a</ref>). In addition, all the aforementioned solutions follow the conventional practice of two-stage mapping and the whole structure of label system is ignored in learning the feature space.</p><p>In this paper, we attempt to find possible answers for the two questions. We propose a label graph superimposed deep convolution network called KSSNet for this task. The superimposing means the following two folds in our framework:</p><p>(1) to model the priors of co-occurrence of labels following the GCN paradigm, instead of using statistics of label co-occurrence alone to build the relation graph of the label system, we propose to superimpose knowledge based graph into statistics based graph for constructing the final one. (2) In order to learn better feature representations for a specific multi-label recognition task anchored on its label structures, we design a novel superimposed CNN and GCN network to extract label structure aware descriptors. Specifically, we first construct two adjacency matrices A S ∈ R N ×N and A K ∈ R N ×N to denote correlation graphs of labels, which is constructed by co-occurrence statistics and a knowledge graph named ConceptNet <ref type="bibr" target="#b20">(Speer, Chin, and Havasi 2017)</ref> respectively. The initial embedding of all nodes (namely, labels) is extracted from ConceptNet. The final adjacency matrix is a superimposed version. Then we apply multi-layer graph convolution on the final superimposed graph to model the label correlation. Besides, different from conventional graph augmented CNN solutions which utilize information of label system at the final recognition stage, we add lateral connections between CNN and GCN at shallow, middle and deep layers to inject information of the label system into backbone CNN for the purpose of labels awareness in feature learning. We have carried out extensive experiments on MS-COCO dataset <ref type="bibr" target="#b15">(Lin et al. 2014)</ref> for multi-label image recognition and Charades <ref type="bibr" target="#b18">(Sigurdsson et al. 2016</ref>) for multi-label video classification. Results show that our solution obtains absolute mAP improvement of 6.4% and 12.0% in MS-COCO and Charades with very limited computation cost overhead, when compared to its plain CNN counterpart. Our model achieves new state-of-the-art and outperforms current state-of-the-art solution by 1.3% and 2.4% in mAP on MS-COCO and Charades, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>State-of-the-art image or video classification frameworks <ref type="bibr" target="#b7">(He et al. 2016a;</ref><ref type="bibr" target="#b0">Carreira and Zisserman 2017;</ref><ref type="bibr" target="#b2">Feichtenhofer et al. 2018;</ref><ref type="bibr" target="#b9">He et al. 2019;</ref><ref type="bibr" target="#b28">Wu et al. 2019</ref>) can be directly applied for multi-label classification by replacing the cross-entropy loss with multi-binary classification loss. The straightforward extension leaves label correlation unexplored thus degrading the recognition performance. We propose our solution to alleviate this problem and it is closely related with the following jobs.</p><p>Many existing works on multi-label classification proposed to capture label relationship for performance improvement. The co-occurrence of labels can be well formulated by probabilistic graph models, in the literature, there have many methods based on such mathematical theory to model the labels <ref type="bibr" target="#b13">(Li et al. 2016;</ref><ref type="bibr" target="#b14">Li, Zhao, and Guo 2014)</ref>. To tackle the problem of computation cost burden of probabilistic graph models, the neural network based solution is becoming prevalence recently. In <ref type="bibr" target="#b22">(Wang et al. 2016a</ref>), recurrent network was used to encode labels into embedding vectors for label correlation modeling purpose. Context gating strategy was utilized in <ref type="bibr" target="#b16">(Lin, Xiao, and Fan 2018)</ref> to integrate the post processing of label re-ranking into the whole network architecture. There are also works done by leveraging the attention mechanism in order for modeling label relationship. In <ref type="bibr" target="#b24">(Wang et al. 2017)</ref> and <ref type="bibr" target="#b31">(Zhu et al. 2017a</ref>), either image region-level spatial attention map or attentive semantic-level label correlation modeling was used to boost the final recognition performance. <ref type="bibr" target="#b26">(Wang, Jia, and Breckon 2019)</ref> proposed to improve the performance by model ensemble.</p><p>Graph has been proved to be more effective for label structure modeling. Tree-structure label graph built with maximum spanning tree algorithm in <ref type="bibr" target="#b14">(Li, Zhao, and Guo 2014)</ref> and knowledge graph for describing label dependency in <ref type="bibr" target="#b12">(Lee et al. 2018</ref>) are two typical label graph solutions. Recently, GCN was introduced in (Kipf and Welling 2016) and it has been successfully utilized for non-grid structured data modeling. Researchers have leveraged GCN for many computer vision tasks and great performance was achieved. For instance, it was leveraged in <ref type="bibr" target="#b29">(Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b3">Gao et al. 2018)</ref> to model the relationship of skeletons of humans bodies for human action recognition and knowledgeaware GCN was applied for zero-shot video classification in <ref type="bibr" target="#b4">(Gao, Zhang, and Xu 2019)</ref>. Our work mostly relates to the one proposed in <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>, which used GCN to propagate information among labels and merges label information with CNN features at the final classification stage. Differently, our work builds GCN by superimposing the <ref type="figure">Figure 2</ref>: The overview of KSSNet with backbone of Inception-I3D. "LC" is our proposed lateral connection, 'S' and 'L' denote Sigmoid and LeakyReLU operations, respectively. "Inc." is the Inception block in I3D <ref type="bibr" target="#b0">(Carreira and Zisserman 2017)</ref>. KSSNet takes videos and initial label embeddings as input, and outputs the predicted labels of these videos. "GConv" is the abbreviation of "Graph Convolution". graph built from statistical co-occurrence information into the graph built with knowledge priors. The label information is absorbed into the backbone network for better feature learning.</p><formula xml:id="formula_0">Conv Pool Conv × 2 Inc. × 2 Pool Inc. × 5 Pool Inc. × 2 Average Pool Conv Pool Initial Label Embeddings GConv L GConv L S GConv X L GConv + Predicted Labels LC LC LC GCN " ($) &amp; ($) ' ($) " (() &amp; (() " ()) ' ()) &amp; ()) " (*) ' (*) &amp; (*) &amp; (+) " (+)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>In this paper, We propose a knowledge and label graph superimposing framework for multi-label classification. We provide a new label correlation modeling method of superimposing statistical label graph and knowledge prior oriented label graph. Better feature learning network architecture by absorbing label structure information generated by GCN at shallow, middle and deep layers of backbone CNN is designed. We call our model as KSSNet (Knowledge and Statistics Superimposing Network). Taking the KSSNet with backbone of Inception-I3D (Carreira and Zisserman 2017) designed for multi-label video classification as example, we show its block-diagram in <ref type="figure">Figure 2</ref>. When it comes to multi-label image classification, the framework can be easily constructed by superimposing GCN with stateof-the-art 2D CNN such as ResNet <ref type="bibr" target="#b7">(He et al. 2016a</ref>). In the following subsections, we firstly introduce in detail how label graph are constructed and superimposed, and then we show what is our proposed GCN and CNN superimposing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Construction</head><p>Our final graph is constructed by superimposing statistical label graph into knowledge prior oriented graph. Graph constructed with such statistical information as label cooccurrence frequencies and conditional probabilities of different labels is termed as statistical graph in our paper. Statistical information is determined by the distribution of samples in training set. The statistical graph can be influenced significantly by noise and disturbance. Meanwhile, knowledge graph, such as ConceptNet <ref type="bibr" target="#b20">(Speer, Chin, and Havasi 2017)</ref>, is built with human knowledge by several methods, such as expert-created resources and games with a purpose. It is more authentic for representing the relationship of labels, especially for small scale datasets. However it has three drawbacks: Firstly, the graph is so dense that it represents too much trivial relationship of nodes. When used into deeper GCNs, it will result in more heavy negative effect of over-smoothed label embeddings, compared with sparse graphs. Secondly, it is datasets independent and neglects the characteristics of specific tasks. Thirdly, as knowledge graph can hardly contain all labels in a dataset, the edges of these labels are lost. Our proposed method combines statistical information and human knowledge, which can overcome their drawbacks to some extent. We formally present its details as follows.</p><p>A graph is usually denoted as</p><formula xml:id="formula_1">G = (V, E, A), where V, E,</formula><p>A are the set of nodes, set of edges and adjacency matrix. A is an N × N matrix with (i, j) entry equaling to the weight of edges between nodes V i and V j . N = |V| is the number of vertices. E ∈ R N ×F denotes the feature (label embeddings in our case) matrix for all N nodes. We denote the statistical graph as G S = (V, E S , A S ), knowledge graph as G K = (V, E K , A K ), where A S and A K are adjacency matrices obtained with statistical information and knowledge priors respectively. A S is constructed by following <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>. A K is obtained according to the human created knowledge graph ConceptNet <ref type="bibr" target="#b20">(Speer, Chin, and Havasi 2017)</ref>. Specifically,</p><formula xml:id="formula_2">[A K ] ij = max{w r |r ∈ S ij }, if |S ij | &gt; 0 0, if |S ij | = 0 (1)</formula><p>where S ij is a set of relations (such as "used for" and "is a") between nodes V i and V j extracted from ConceptNet. w r is the weight of relation r. |S ij | is the number of elements in S ij . Denoting A S and A K as the normalized versions of</p><formula xml:id="formula_3">A S and A K , respectively. The normalized A S is A S = D −1/2 S A S D −1/2 S , where D S is diagonal and [D S ] ii = j [A S ] ij .</formula><p>A S is normalized analogously. Weighted average of A S and A K is used to superimpose the prior knowledge into statistical graph and the resulted new adjacency matrix is normalized.</p><formula xml:id="formula_4">A = λA S + (1 − λ)A K (2) where λ ∈ [0, 1] is a weight coefficient.</formula><p>Meanwhile, as the elements of A S and A K are nonnegative, A has more nonzero elements compared with A S and A K . That is, the graph constructed with A has more redundant edges than G S or G K , as is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. In order to suppress these edges, we use a threshold τ ∈ R to filter the elements of A</p><formula xml:id="formula_5">[A τ ] ij = 0, if A ij &lt; τ A ij , if A ij ≥ τ<label>(3)</label></formula><p>As is known to us, when the number of GCN layers increases, the performance of models drops in some tasks. The reason is possibly the over-smoothing of deeper GCN layers <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>. Inspired by such fact, we further adjust the entries in the adjacency matrix of the superimposed graph and obtain the final matrix A KS :</p><formula xml:id="formula_6">A KS = ηA τ + (1 − η)I N<label>(4)</label></formula><p>where I N is an N × N identity matrix. η ∈ R is a weight coefficient. With the adjacency matrix A KS , we construct the set of edges as</p><formula xml:id="formula_7">E KS = {(V i , V j )|[A KS ] ij = 0, and 0 ≤ i, j ≤ N } (5) (V i , V j )</formula><p>denotes the edge (directed or undirected) of nodes V i and V j . The graph we proposed is defined as G KS = (V, E KS , A KS ), which is called KS graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Superimposing of GCN and CNN</head><p>Unlike conventional convolutions, GCN is designed for non-Euclidean topological structure. In GCN, the label embeddings of each node is a mixture of the embeddings of its neighbors from the previous layer. We follow a common practice as was done in <ref type="bibr" target="#b11">(Kipf and Welling 2016;</ref><ref type="bibr" target="#b1">Chen et al. 2019)</ref> to apply graph convolution. Every GCN layer can be formulated as a non-linear function:</p><formula xml:id="formula_8">E (l+1) = σ(A KS E (l) W (l) ),<label>(6)</label></formula><p>where A KS is the normalized adjacency matrix. E (l) ∈ R N ×C (l) denotes the label embedding at the l-th layer for all N nodes. Note that E (0) is the initial label embeddings and it is extracted from semantic networks like ConceptNet <ref type="bibr" target="#b20">(Speer, Chin, and Havasi 2017)</ref>.</p><formula xml:id="formula_9">W (l) ∈ R C (l) ×C (l+1)</formula><p>is a transformation matrix and is learnable in the training phase. σ(·) denotes a non-linear activation operation.</p><p>Instead of only superimposing information of label relationship at the final recognition stage, we propose to inject label information into backbone 2D/3D CNNs at different stages by lateral connection (LC operation). <ref type="figure">Figure 4</ref> shows 2D and 3D versions of our proposed LC operation. Take 3D version for example, we define an LC operation in deep neural networks as:</p><formula xml:id="formula_10">y = g(R N ×T ×H×W (R T HW ×C (x) ⊗ σ(E T ))) + x (7)</formula><p>Here x ∈ R C×T ×H×W is CNN feature, C is the number of channels. T , H and W denote the frames, height and width of feature tensor. N is the number of labels. E ∈ R N ×C indicates the hidden label embeddings of GCN. g is a 1×1×1 convolution g : R N ×T ×H×W → R C×T ×H×W , whose parameters are to be learnt for the downstream tasks. '⊗' denotes matrix multiplication and (·) T is transpose operation. σ(·) denotes a non-linear activation operation. Both R N ×T ×H×W (·) and R T HW ×C (·) are defined as reshape operations, which rearrange the input array as the shape noted at their subscripts.</p><p>The motivation of LC is to push the CNN network to learn label-system anchored feature representations for better recognition. As stated in <ref type="formula">(7)</ref>, it first calculates crosscorrelation of CNN features and label embeddings and outputs how each CNN feature point is correlated with a label embedding. Such correlation tensor is then mapped to a hidden space by 1x1x1 convolution to encode the relationship of CNN features and label embeddings. At last, the relationship tensor generated from 1 × 1 × 1 convolution are added into the original CNN feature tensor. With the lateral connection, the relationship of label system and CNN feature maps is modeled and the learned CNN feature is kind of label-system anchored.</p><p>Our KSSNet superimposes labels embeddings into CNN features not only in the classification layer but also in hidden layers. There are several advantages of this strategy. (1) The hidden embeddings in GCN can help the feature learning process of CNN, making hidden CNN features aware of label relationship. (2) As for the learning process of hidden embeddings, the extra gradients from LC operation can  <ref type="figure">Figure 4</ref>: The block diagram of LC operation. 'R', "(·) T ", '×' and '+' denote matrix reshape, transpose, multiplication and sum operations respectively. x (l) and E (l) are CNN feature and GCN feature at the l th GCN layer. The shape of each tensor is marked in gray annotation.</p><formula xml:id="formula_11">! (#) % (#) R X R + T &amp; (#) ×(×)×* +×&amp; (#) ()*×&amp; (#) &amp; (#) ×+ ()*×+ +×(×)×* &amp; (#) ×(×)×* , (#) &amp; (#) ×(×)×* 3D version ! (#) % (#) R X R -: 1×1×1 + )*×&amp; (#) )*×+ +×)×* &amp; (#) ×)×* , (#) &amp; (#) ×)×* 2D version &amp; (#) ×)×* +×&amp; (#) -: 1×1×1 σ T &amp; (#) ×+ σ</formula><p>be seen as a special regularization, which forces hidden embeddings more adapt to CNN features. It can overcome the over-smoothing of deeper GCN to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>In this section, we conduct experiments to show that our proposed solution can achieve pretty good performance in both image and video multi-label recognition tasks. Then, we carry out ablation studies to evaluate the effectiveness of the proposed graph construction method in our KSSNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Evaluation Metrics</head><p>MS-COCO MS-COCO <ref type="bibr" target="#b15">(Lin et al. 2014</ref>) is a static image dataset, which is widely used for many tasks, such as multilabel image recognition, object localization and semantic segmentation. It contains about 82K images for training, 41K for validation and 41K for test. All images are involved with 80 object labels in the multi-label image recognition task. On average, each image has 2.9 labels. We evaluate all the methods on validation set, since the ground-truth labels of the test set are not available.</p><p>Charades Charades <ref type="bibr" target="#b18">(Sigurdsson et al. 2016</ref>) is a multilabel video dataset containing around 9.8K videos, among which about 8K for training and 1.8K for validation. The average length of videos in Charades is about 30 seconds. It has 157 action labels and 66.5K annotated activities, about 6.8 labels per video. Each action label is composed of a noun (object) and a verb (action). In total, there are 38 different nouns and 33 different verbs. We also evaluate different methos using its validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>In order for evaluating our model on MS-COCO comprehensively and for convenience of comparison with other solutions, we report the average per-class precision (CP), recall (CR), F1 (CF1), the average overall precision (OP), overall recall (OR), overall F1 (OF1) and mean average precision (mAP), as is done in <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>. As for the Charades, we evaluate all models with mAP <ref type="bibr" target="#b18">(Sigurdsson et al. 2016)</ref> to show their effectiveness. Besides, we also report the value of FLOPs that each model consumes to depict model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Experiment on MS-COCO For image recognition, we choose state-of-the-art ResNet101 <ref type="bibr" target="#b8">(He et al. 2016b</ref>) as the backbone of our KSSNet, which is pre-trained on ImageNet. The GCN of KSSNet is built from four successive graph convolution layers and the number of channels of their outputs is 256, 512, 1024 and 2048, respectively. In order to deal with the "dead ReLU" problem, we use LeakyReLU as activation operation for graph convolution layers, with negative slope of 0.2. Three 2D version LC operations between GCN and the backbone ResNet101 are used and the label embeddings of four graph convolution layers are injected to res2, res3, res4 and res5 of ResNet101. The activation function in the LC operation is set to be Tanh. We adopt 300-dimentional GloVe text model (Pennington, Socher, and Manning 2014) to generate the initial label embeddings of labels. As for the labels whose names contain multiple words and have no corresponding keys in GloVe, we obtain the label representation by averaging embeddings of all words. In the process of constructing statistical matrix G S , we use the strategy proposed in <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>. We set λ in (2) to be 0.4, τ in (3) to be 0.02 and η in (4) to be 0.4. During training, the same data preprocessing procedure as <ref type="bibr" target="#b1">(Chen et al. 2019</ref>) is adopted. Adam is used as the optimizer with a momentum of 0.9, weight decay of 10 −4 and batch size of 80. The initial learning rate of Adam is 0.01. All models are trained for 100 epochs in total. Experiment on Charades Inception-I3D of KSSNet is initialized following the inflating mechanism proposed in I3D (Carreira and Zisserman 2017) with BN-Inception pretrained on ImageNet. We fine-tune our models using 64frame input clips. These clips are sampled following the strategy of <ref type="bibr" target="#b23">(Wang et al. 2016b)</ref>, where each clip consists of 64 snippets and each snippet contain only one frame. The spatial size is 224 × 224, randomly cropped from a scaled video whose spatial size is 256 × 256. λ, η and τ are set to 0.6, 0.4 and 0.03, respectively. We train all models with mini-batch size of 16 clips. Adam is used as the optimizer, starting with a momentum of 0.9 and weight decay of 10 −4 . The weight decays of all bias are set to zero. Dropout <ref type="bibr" target="#b10">(Hinton et al. 2012</ref>) with a ratio of 0.5 is added after the average pooled CNN features. The initial learning rate of GCN parameters is set to be 0.001, while others are set to be 10 −4 . We use the strategy proposed in <ref type="bibr" target="#b6">(He et al. 2015)</ref> to initialize the GCN and initial label embeddings are extracted with ConceptNet <ref type="bibr" target="#b20">(Speer, Chin, and Havasi 2017)</ref>. During inference, we evenly extract 64 frames from the original fulllength video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Baselines</head><p>In this part, we present comparisons with several state-ofthe-arts on MS-COCO and Charades, respectively to show the effectiveness of our proposed solution.</p><p>Results on MS-COCO We compare our KSSNet with the state-of-the-art methods, including CNN-RNN <ref type="bibr" target="#b22">(Wang et al. 2016a</ref>), SRN <ref type="bibr" target="#b32">(Zhu et al. 2017b</ref>), ResNet101 <ref type="bibr" target="#b8">(He et al. 2016b</ref>), Multi-Evidence <ref type="bibr" target="#b5">(Ge, Yang, and Yu 2018)</ref> and ML-GCN <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>. <ref type="table" target="#tab_0">Table 1</ref> records the quantitative results of all models on MS-COCO validation set. ML-GCN is a GCN+CNN framework based on statistical label graph and it is the current state-of-the-art. It can be observed that our KSSNet obtains the best performance at almost all evaluation matrices. Specially, compared with ML-GCN, its mAP is 1.3% higher, the improvement of overall precision is improved from 85.8% to 87.8%, the gain of overall recall is 1.7% and new state-of-the-art overall F1 score of 81.5% is achieved. The result demonstrates the effectiveness of our KSSNet framework. The comparison of KSSNet and its backbone ResNet101 shows that the absolute improvement in mAP is up to 6.4% and evidences that the label embeddings of GCN can explicitly take advantage of the label relationship, which is hard to be learned by plain CNN or even ignored by many frameworks.</p><p>Results on Charades <ref type="table" target="#tab_1">Table 2</ref> shows the comparison with state-of-the-art models for our proposed KSSNet on Charades. Compared with backbone I3D model, KSSNet provides 12.0% higher mAP at the cost of very little computation overhead (from 108 GFLOPs to 127 GFLOPs). We can see that the gain is even larger than what achieved on MS-COCO (12.0% v.s. 6.4%). The origin beneath is possibly the characteristics of Charades dataset. On the one hand, each video has 6.8 labels on average, which is even more than MS-COCO. The correlation among different labels has significant influence on multi-label video recognition task. On the other hand, the dataset is not sufficiently large, so the impact of extra label correlation introduced by GCN is more obvious. It can be concluded from such observation that our proposed GCN and CNN superimposing framework can significantly improve baseline result, especially when the training data is not so sufficient. We can also see that although no pretraining on extra large scale video dataset, KSSNet (KS graph) achieves the best performance, which is 2.4% higher than the current state-of-the-art method LFB and SlowFast(NL) which are pretrained on Kinetics-400. It should be noted that the GFLOPs of our KSSNet is much smaller than SlowFast(NL), which means KSSNet has remarkable potential in fast multi-label video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In this section, we perform ablation studies to evaluate the effectiveness of our KS graph and to analyze the influence of GCN depth in KSSNet framework.</p><p>Label graphs of KSSNet In order to evaluate the influence of different graph, we implement three versions of KSSNet with statistical graph, knowledge graph and our proposed KS graph. All of them have the same framework with four GCN layers. <ref type="table" target="#tab_2">Table 3</ref> summarizes the results of KSSNet (statistical graph), KSSNet (knowledge graph) and KSSNet (KS graph). The experiment on MS-COCO shows that knowledge graph performs worse than statistical graph  and KS graph, which is caused by the relationship missing of uncovered labels in knowledge graph and by the oversmoothing impact introduced by the presence of many trivial edges. However, the experiment on Charades exhibits a contrary result, KSSNet (knowledge graph) outperforms KSSNet (statistical graph) by a mAP of 0.4. The cause can attribute to the characteristics of Charades. In Charades, labels are more complex and training samples are not so sufficient. Graph constructed from co-occurrence information is not so reliable while knowledge priors are always valid, so the contradiction between complex label relationship modeling and the lack of samples in Charades makes knowledge graph more effective than statistical graph. Both experiments show that our KS graph performs the best, which validates the effectiveness of superimposing statistical graph and knowledge graph.</p><p>Influence of GCN Depth in KSSNet As we know, conventional GCN suffers from over-smoothing. In this part, we conduct multi-label image and video recognition experiments to demonstrate that our KSSNet framework can deal with this problem effectively. The backbone of KSSNet is ResNet101 and Inception-I3d for MS-COCO and Charades, respectively. In this experiment, we modify the GCN pathway of KSSNet to be with three and two graph convolution layers. The modification can be simply done in two steps: 1) delete the first one or two graph convolution layer(s) and the corresponding LC operation(s) from the GCN pathway; 2) then the first graph convolution layer of the rest ones is adapted to take the initial label embeddings E (0) as input by adjusting its number of input channel C to the channel number of E (0) .</p><p>Experimental results are shown in <ref type="table" target="#tab_3">Table 4</ref>. It is obvious, with our KSSNet, more GCN layers lead to better classification results at the cost of small increase of computational cost and model size. KSSNet (3 layers) achieves better performance than KSSNet (2 layers) by absolute mAP improvements of 0.6% and 1.9% in MS-COCO and Charades. KSS-Net (4 layers) outperforms KSSNet (3 layers) by 0.2% and 1.1% in mAP. As is reported in ML-GCN <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>, when GCN has no less than 2 layers, performance of conventional GCN+CNN solution degrades as long as the number of graph convolution layers gets larger. Our model performs on the contrary. This is because that (1) more GCN layers bring more LC operations which guide CNN to learn better label structure aware features at shallow, middle and higher CNN layers. (2) The extra gradients from LC operation can regularize the learning of label embeddings in GCN. <ref type="formula" target="#formula_5">(3)</ref> We have proposed such strategies as redundant removal to tackle the over-smoothing issue of GCN.</p><p>Impact of Super-Parameters This experiment is conducted on Charades. When λ varies from 0 to 1 by step of 0.2 and keep other parameters as described above, mAP is 41. <ref type="bibr">05, 41.7, 43.44, 44.93, 44.83 and 41.57</ref>. When we fix λ as 0.6, τ varies from 0.01 to 0.04 by step of 0.01, the mAP is 44.6, 44.62, 44.93 and 44.77.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Capturing label relationship takes a crucial position on multi-label recognition. In order to better model this information, we propose to construct the KS graph for label correlation modeling by superimposing knowledge graph into statistical graph. Then the LC operation is presented for injecting GCN embeddings into CNN features, resulting in a novel neural network KSSNet. LC operation acts as label-feature correlation modeling and helps the model learn label-anchored feature representations. The KSSNet is proven to be capable of learning better feature representations for a specific multi-label recognition task anchored on its label relationship. Experiments on MS-COCO and Charades have demonstrated the effectiveness of our proposed KS graph and KSSNet for both multi-label image and video recognition tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of label relationship in multi-label datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>A subgraph with five nodes on MS-COCO. The number on each edge denotes its weight. Yellow dashed lines with red numbers nearby highlight the redundant edges when taking the threshold of 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons between baselines and KSSNet on MS-COCO. KSSNet is based on our proposed KS graph and has four GCN layers.</figDesc><table><row><cell>Method</cell><cell>mAP</cell><cell>CP</cell><cell>CR</cell><cell>CF1</cell><cell>OP</cell><cell>OR</cell><cell>OF1</cell></row><row><cell>CNN-RNN (Wang et al. 2016a)</cell><cell>61.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRN (Zhu et al. 2017a)</cell><cell cols="7">77.1 81.6 65.4 71.2 82.7 69.9 75.8</cell></row><row><cell>ResNet101 (He et al. 2016b)</cell><cell cols="7">77.3 80.2 66.7 72.8 83.9 70.8 76.8</cell></row><row><cell>Multi-Evidence (Ge, Yang, and Yu 2018)</cell><cell>-</cell><cell cols="6">80.4 70.2 74.9 85.2 72.5 78.4</cell></row><row><cell>ML-GCN (Chen et al. 2019)</cell><cell cols="7">82.4 84.4 71.4 77.4 85.8 74.5 79.8</cell></row><row><cell>KSSNet</cell><cell cols="7">83.7 84.6 73.2 77.2 87.8 76.2 81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results of baselines and KSSNet on Charades validation set. The KSSNet bellow has 4 GCN layers and its adjacency matrix is from our proposed KS graph.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Modality</cell><cell>Pretrain</cell><cell cols="2">mAP GFLOPs</cell></row><row><cell>Two-stream (Wu et al. 2018)</cell><cell>VGG16</cell><cell>RGB+Flow</cell><cell>ImageNet,UCF101</cell><cell>14.3</cell><cell>-</cell></row><row><cell>CoViAR (Wu et al. 2018)</cell><cell>-</cell><cell cols="3">Compressed (Wu et al. 2018) ILSVRC2012-CLS 21.9</cell><cell>-</cell></row><row><cell>CoViAR (Wu et al. 2018)</cell><cell>-</cell><cell>Compressed+Flow</cell><cell cols="2">ILSVRC2012-CLS 24.1</cell><cell>-</cell></row><row><cell>Asyn-TF (Sigurdsson et al. 2017)</cell><cell>VGG16</cell><cell>RGB+Flow</cell><cell>ImageNet</cell><cell>22.4</cell><cell>-</cell></row><row><cell>MultiScale (TR) (Zhou et al. 2018)</cell><cell>Inception-I3D</cell><cell>RGB</cell><cell>ImageNet</cell><cell>25.2</cell><cell>-</cell></row><row><cell>I3D (Carreira and Zisserman 2017)</cell><cell>Inception</cell><cell>RGB</cell><cell>Kinetics-400</cell><cell>32.9</cell><cell>108</cell></row><row><cell>ResNet-101(NL) (Wang et al. 2018)</cell><cell>ResNet101-I3D</cell><cell>RGB</cell><cell>Kinetics-400</cell><cell>37.5</cell><cell>544</cell></row><row><cell>STRG (NL) (Wang and Gupta 2018)</cell><cell>ResNet101-I3D</cell><cell>RGB</cell><cell>Kinetics-400</cell><cell>39.7</cell><cell>630</cell></row><row><cell>SlowFast (Feichtenhofer et al. 2018)</cell><cell>ResNet101</cell><cell>RGB</cell><cell>Kinetics-400</cell><cell>42.1</cell><cell>213</cell></row><row><cell>SlowFast(NL) (Feichtenhofer et al. 2018)</cell><cell>ResNet101</cell><cell>RGB</cell><cell>Kinetics-400</cell><cell>42.5</cell><cell>234</cell></row><row><cell>LFB(NL) (Wu et al. 2019)</cell><cell>ResNet101-I3D</cell><cell>RGB</cell><cell>Kinetics-400</cell><cell>42.5</cell><cell>-</cell></row><row><cell>KSSNet</cell><cell>Inception-I3D</cell><cell>RGB</cell><cell>ImageNet</cell><cell>44.9</cell><cell>127</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparisons of different label graphs on MS-COCO and Charades. "KSSNet (statistical graph)", "KSS-Net (knowledge graph)" and "KSSNet (KS graph)" are three versions of our proposed KSSNet which have the same framework and different graphs on each dataset.All our variants have four GCN layers. 84.2 73.1 77.6 87.2 76.4 81.2 Inception-I3D 40.7 KSSNet (knowledge graph) ResNet101 81.0 82.8 69.5 75.6 84.5 73.4 78.6 Inception-I3D 41.1</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>mAP</cell><cell>CP</cell><cell>MS-COCO CR CF1</cell><cell>OP</cell><cell>OR</cell><cell>OF1</cell><cell>Charades Backbone</cell><cell>mAP</cell></row><row><cell cols="10">KSSNet (statistical graph) ResNet101 83.1 KSSNet (KS graph) ResNet101 83.7 84.6 73.2 77.2 87.8 76.2 81.5 Inception-I3D 44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of different GCN depths of KSSNet. On each experiment, all versions of KSSNet use KSS graph as adjacency matrix.</figDesc><table><row><cell>Methods</cell><cell>MS-COCO Backbone mAP #Params</cell><cell>Backbone</cell><cell cols="2">Charades mAP GFLOPs #Params</cell></row><row><cell cols="4">KSSNet (2 layers) ResNet101 82.9 172.1MB Inception-I3D 41.9</cell><cell>110</cell><cell>47.6MB</cell></row><row><cell cols="4">KSSNet (3 layers) ResNet101 83.5 173.2MB Inception-I3D 43.8</cell><cell>115</cell><cell>49.1MB</cell></row><row><cell cols="4">KSSNet (4 layers) ResNet101 83.7 173.8MB Inception-I3D 44.9</cell><cell>127</cell><cell>49.8MB</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was supported by the Joint Laboratory of Intelligent Sports of China Institute of Sport Science (CISS).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilabel image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03982</idno>
		<title level="m">Slowfast networks for video recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generalized graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12013</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1277" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stnet: Local and global spatial-temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional graphical lasso for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2977" to="2986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-label image classification with a probabilistic label enhancement model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nextvlad: An efficient neural network to aggregate frame-level features for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A baseline for multilabel image classification using an ensemble of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6026" to="6035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with image-level supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5513" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with image-level supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
