<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Evolved Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
						</author>
						<title level="a" type="main">The Evolved Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments -the Evolved Transformer -demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-ofthe-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original "big" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of ∼7M parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past few years, impressive advances have been made in the field of neural architecture search. Reinforcement learning and evolution have both proven their capacity to produce models that exceed the performance of those designed by humans <ref type="bibr" target="#b28">(Real et al., 2019;</ref><ref type="bibr" target="#b42">Zoph et al., 2018)</ref>. These advances have mostly focused on improving vision models, although some effort has also been invested in searching for sequence models <ref type="bibr" target="#b23">Pham et al., 2018)</ref>. In these cases, it has always been to find improved recurrent neural networks (RNNs), which were long established as the de facto neural model for sequence problems <ref type="bibr" target="#b31">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>.</p><p>However, recent works have shown that there are better alternatives to RNNs for solving sequence problems. Due to the success of convolution-based networks, such as Convolution Seq2Seq <ref type="bibr" target="#b13">(Gehring et al., 2017)</ref>, and full attention networks, such as the Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>, feed-forward networks are now a viable option for solving sequence-to-sequence (seq2seq) tasks. The main strength of feed-forward networks is that they are faster, and easier to train than RNNs.</p><p>The goal of this work is to examine the use of neural architecture search methods to design better feed-forward architectures for seq2seq tasks. Specifically, we apply tournament selection architecture search and warm start it with the Transformer, considered to be the state-of-art and widely-used, to evolve a better and more efficient architecture. To achieve this, we construct a search space that reflects the recent advances in feed-forward seq2seq models and develop a method called Progressive Dynamic Hurdles (PDH) that allows us to perform our search directly on the computationally demanding WMT 2014 English-German (En-De) translation task. Our search produces a new architecture -called the Evolved Transformer (ET)which demonstrates consistent improvement over the original Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French (En-Fr), WMT 2014 English-Czech (En-Cs) and the 1 Billion Word Language Model Benchmark (LM1B). At a big model size, the Evolved Transformer establishes a new stateof-the-art BLEU score of 29.8 on WMT'14 En-De. It is also effective at smaller sizes, achieving the same quality as the original "big" Transformer with 37.6% less parameters and outperforming the Transformer by 0.7 BLEU at a mobile-friendly model size of ∼7M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>RNNs have long been used as the default option for applying neural networks to sequence modeling <ref type="bibr">(Sutskever arXiv:1901.11117v4 [cs.</ref>LG] 17 May 2019 <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>, with LSTM <ref type="bibr" target="#b15">(Hochreiter &amp; Schmidhuber, 1997)</ref> and GRU <ref type="bibr" target="#b7">(Cho et al., 2014)</ref> architectures being the most popular. However, recent work has shown that RNNs are not necessary to build state-ofthe-art sequence models. For example, many high performance convolutional models have been designed, such as WaveNet <ref type="bibr" target="#b32">(Van Den Oord et al., 2016)</ref>, Gated Convolution Networks , Conv Seq2Seq <ref type="bibr" target="#b13">(Gehring et al., 2017)</ref> and the Dynamic Lightweight Convolution model <ref type="bibr" target="#b35">(Wu et al., 2019)</ref>. Perhaps the most promising architecture in this direction is the Transformer architecture <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>, which relies only on multi-head attention to convey spatial information. In this work, we use both convolutions and attention in our search space to leverage the strengths of both of these layer types.</p><p>The recent advances in sequential feed-forward networks are not limited to architecture design. Various methods, such as BERT <ref type="bibr" target="#b11">(Devlin et al., 2018)</ref> and <ref type="bibr" target="#b25">Radford et. al's (2018)</ref> pre-training technique, have demonstrated how models such as the Transformer can improve over RNN pre-training <ref type="bibr" target="#b8">(Dai &amp; Le, 2015;</ref><ref type="bibr" target="#b22">Peters et al., 2018)</ref>. For translation specifically, work on scaling up batch size <ref type="bibr" target="#b20">(Ott et al., 2018;</ref><ref type="bibr" target="#b35">Wu et al., 2019)</ref>, using relative position representations <ref type="bibr" target="#b29">(Shaw et al., 2018)</ref>, and weighting multi-head attention <ref type="bibr" target="#b0">(Ahmed et al., 2017)</ref> have all pushed the state-of-the-art for WMT'14 En-De and En-Fr. However, these methods are orthogonal to this work, as we are only concerned with improving the neural network architecture itself, and not the techniques used for improving overall performance.</p><p>The field of neural architecture search has also seen significant recent progress. The best performing architecture search methods are those that are computationally intensive <ref type="bibr" target="#b3">Baker et al., 2016;</ref><ref type="bibr" target="#b27">Real et al., 2017;</ref><ref type="bibr" target="#b37">Xie &amp; Yuille, 2017;</ref><ref type="bibr" target="#b42">Zoph et al., 2018;</ref><ref type="bibr" target="#b28">Real et al., 2019)</ref>. Other methods have been developed with speed in mind, such as DARTS <ref type="bibr">(Liu et al., 2018b)</ref>, ENAS <ref type="bibr" target="#b23">(Pham et al., 2018)</ref>, SMASH <ref type="bibr" target="#b4">(Brock et al., 2018)</ref>, and SNAS <ref type="bibr" target="#b38">(Xie et al., 2019)</ref>. These methods radically reduce the amount of time needed to run each search by approximating the performance of each candidate model, instead of investing resources to fully train and evaluate each candidate separately. However, these methods also have several disadvantages that make them hard to apply in our case: (1) It is hard to warm start these methods with the Transformer, which we found to be necessary to yield strong results. (2) ENAS and DARTS require too much memory at the model sizes we are searching for.</p><p>(3) The best architecture in the vision domain (e.g., Amoe-baNet <ref type="bibr" target="#b28">(Real et al., 2019)</ref>) was discovered by evolutionary NAS, not these efficient methods, and we optimize for best architecture over best search efficiency here. <ref type="bibr" target="#b40">Zela et. al's (2018)</ref> utilization of Hyperband <ref type="bibr" target="#b18">(Li et al., 2017)</ref> and PNAS's <ref type="bibr" target="#b19">(Liu et al., 2018a)</ref> incorporation of a surro-gate model are examples of approaches that try to both increase efficiency via candidate performance estimation and maximize search quality by training models to the end when necessary. The Progressive Dynamic Hurdles (PDH) method we introduce here is similar to these approaches in that we train our best models to the end, but optimize efficiency by discarding unpromising models early on. However, it is critically different from comparable algorithms such as Hyperband and Successive Halving <ref type="bibr" target="#b16">(Jamieson &amp; Talwalkar, 2016)</ref> in that it allows the evolution algorithm to dynamically select new promising candidates as the search progresses; Hyperband and Successive Halving establish their candidate pool a priori, which we demonstrate is ineffective in our large search space in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We employ evolution-based architecture search because it is simple and has been shown to be more efficient than reinforcement learning when resources are limited <ref type="bibr" target="#b28">(Real et al., 2019)</ref>. We use the same tournament selection <ref type="bibr" target="#b14">(Goldberg &amp; Deb, 1991)</ref> algorithm as <ref type="bibr" target="#b28">Real et al. (2019)</ref>, with the aging regularization omitted, and so encourage the reader to view their in-depth description of the method. In the interest of saving space, we will only give a brief overview of the algorithm here.</p><p>Tournament selection evolutionary architecture search is conducted by first defining a gene encoding that describes a neural network architecture; we describe our encoding in the following Search Space subsection. An initial population is then created by randomly sampling from the space of gene encodings to create individuals. These individuals, each corresponding to a neural architecture, are trained and assigned fitnesses, which in our case are the models' negative log perplexities on the WMT'14 En-De validation set. The population is then repeatedly sampled from to produce subpopulations, from which the individual with the highest fitness is selected as a parent. Selected parents have their gene encodings mutated -encoding fields randomly changed to different values -to produce child models. These child models are then assigned a fitness via training and evaluating on the target task, as the initial population was. When this fitness evaluation concludes, the population is sampled from once again, and the individual in the subpopulation with the lowest fitness is killed, meaning it is removed from the population. The newly evaluated child model is then added to the population, taking the killed individual's place. This process is repeated and results in a population with high fitness individuals, which in our case represent well-performing architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space</head><p>Our encoding search space is inspired by the NASNet search space , but is altered to allow it to express architecture characteristics found in recent state-of-the-art feed-forward seq2seq networks. Crucially, we ensured that the search space can represent the Transformer, so that we could seed the initial population with it. Our search space consists of two stackable cells, one for the model encoder and one for the decoder (see <ref type="figure">Figure 1</ref>). Each cell contains NASNet-style blocks, which receive two hidden state inputs and produce new hidden states as outputs; the encoder contains six blocks and the decoder contains eight blocks, so that the Transformer can be represented exactly. The blocks perform separate transformations to each input and then combine the transformation outputs together to produce a single block output; we will refer to the transformations applied to each input as a branch. Our search space contains five branch-level search fields (input, normalization, layer, output dimension and activation), one block-level search field (combiner function) and one celllevel search field (number of cells </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Seeding the Search Space with Transformer</head><p>While previous neural architecture search works rely on well-formed hand crafted search spaces , we intentionally leave our space minimally tuned, in a effort to alleviate our manual burden and emphasize the role of the automated search method. To help navigate the large search space, we find it easier to warm start the search process by seeding our initial population with a known strong model, in this case the Transformer. This anchors the search to a known good starting point and guarantees at least a single strong potential parent in the population as the generations progress. We offer empirical support for these claims in our Results section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evolution with Progressive Dynamic Hurdles</head><p>The evolution algorithm we employ is adapted from the tournament selection evolutionary architecture search proposed by <ref type="bibr" target="#b28">Real et al. (2019)</ref>, described above. Unlike <ref type="bibr" target="#b28">Real et al. (2019)</ref> who conducted their search on CIFAR-10, our search is conducted on a task that takes much longer to train and evaluate on. Specifically, to train a Transformer to peak performance on WMT'14 En-De requires ∼300K training steps, or 10 hours, in the base size when using a single Google TPU V.2 chip, as we do in our search. In contrast, <ref type="bibr" target="#b28">Real et al. (2019)</ref> used the less resource-intensive CIFAR-10 task <ref type="bibr" target="#b17">(Krizhevsky &amp; Hinton, 2009)</ref>, which takes about two hours to train on, to assess their models during their search, as it was a good proxy for ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009</ref>) performance . However, in our preliminary experimentation we could not find a proxy task that gave adequate signal for how well each child model would perform on the full WMT'14 En-De task; we investigated using only a fraction of the data set and various forms of aggressive early stopping.</p><p>To address this problem we formulated a method to dynamically allocate resources to more promising architectures according to their fitness. This method, which we refer to as Progressive Dynamic Hurdles (PDH), allows models that are consistently performing well to train for more steps. It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small s 0 number of steps before being evaluated for fitness. However, after a predetermined number of child models, m, have been evaluated, a hurdle, h 0 , is created by calculating the the mean fitness of the current population. For the next m child models produced, models that achieve a fitness greater than h 0 after s 0 train steps are granted an additional s 1 steps of training and then are evaluated again to determine their final fitness. Once another m models have been considered this way, another hurdle, h 1 , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next m child models, training and evaluation continues in the same fashion, except models with fitness greater than h 1 after s 0 + s 1 steps of training are granted an additional s 2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached. Algorithm 1 (Supplementary Materials) formalizes how the fitness of an individual model is calculated with hurdles and Algorithm 2 (Supplementary Materials) describes tournament selection augmented with Progressive Dynamic Hurdles. Points to the left of the purple dashed line were generated using unaltered tournament selection. Between the purple and green dashed lines, models with a fitness above the solid purple line were granted additional train steps, forming a higher fitness cluster. To the right of the green dashed line, models with a fitness greater than the solid green line were granted a second round of additional train steps.</p><p>Although different child models may train for different numbers of steps before being assigned their final fitness, this does not make their fitnesses incomparable. Tournament selection evolution is only concerned with relative fitness rank when selecting which subpopulation members will be killed and which will become parents; the margin by which one candidate is better or worse than the other members of the subpopulation does not matter. Assuming no model overfits during its training, which is what we observed in our experiments, and that its fitness monotonically increases with respect to the number of train steps it is allocated, a comparison between two child models can be viewed as a comparison between their fitnesses at the lower of the two's cumulative train steps. Since the model that was allocated more train steps performed, by definition, above the fitness hurdle for the lower number of steps and the model that was allocated less steps performed, by definition, at or below that hurdle at the lower number of steps, it is guaranteed that the model with more train steps was better when it was evaluated at the lower number of train steps.</p><p>The benefit of altering the fitness algorithm this way is that poor performing child models will not consume as many resources when their fitness is being computed. As soon as a candidate's fitness falls below a tolerable amount, its evaluation immediately ends. This may also result in good candidates being labeled as bad models if they are only strong towards the latter part of training. However, the resources saved as a result of discarding many bad models improves the overall quality of the search enough to justify potentially also discarding some good ones; this is supported empirically in our Results section. Language Modeling For language modeling we used the 1 Billion Word Language Model Benchmark (LM1B) <ref type="bibr" target="#b5">(Chelba et al., 2013)</ref>, also using its "packed" Tensor2Tensor implementation. Again the tokens are split into a vocabulary of approximately 32K word-pieces and the sentences are shuffled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Details and Hyperparameters</head><p>Machine Translation All of our experiments used Ten-sor2Tensor's Transformer TPU hyperparameter settings 2 . These are nearly identical to those used by <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>, but modified to use the memory-efficient Adafactor <ref type="bibr" target="#b30">(Shazeer &amp; Stern, 2018)</ref> optimizer. Aside from using the optimizer itself, these hyperparameters also set the warmup to a constant learning rate of 10 −2 over 10K steps and then uses inverse-square-root learning-rate decay. For our experiments, we make only one change, which is to alter this decay so that it reaches 0 at the final step of training, which for our non-search experiments is uniformly 300K. We found that the our search candidate models, the Transformer, and the Evolved Transformer all benefited from this and so experimented with using linear decay, single-cycle cosine decay <ref type="bibr">(Loshchilov &amp; Hutter, 2017</ref>) and a modified inverse-square-root decay to 0 at 300K steps: lr = step −0.00303926 − .962392; every decay was paired with the same constant 10 −2 warmup. We used WMT En-De validation perplexity to gauge model performance and found that the Transformer preferred the modified inversesquare-root decay. Therefore, this is what we used for both all our Transformer trainings and the architecture searches themselves. The Evolved Transformer performed best with cosine decay and so that is what we used for all of its trainings. Besides this one difference, the hyperparameter settings across models being compared are exactly the same. Because decaying to 0 resulted in only marginal weight changes towards the end of training, we did not use checkpoint averaging, except where noted.</p><p>Per-task there is one additional hyperparameter difference, which is dropout rate. For ET and all search child models, dropout was applied uniformly after each layer, approximating the Transformer's more nuanced dropout scheme. For En-De and En-Cs, all "big" and "deep" sized models were given a higher dropout rate of 0.3, keeping in line with <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>, and all other models with an input embedding size of 768 are given a dropout rate of 0.2. Aside from this, hyperparameters are identical across all translation tasks.</p><p>For decoding we used the same beam decoding configuration used by <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>. That is a beam size of 4, length penalty (α) of 0.6, and maximum output length of input length + 50. All BLEU is calculated using casesensitive tokenization 3 and for WMT'14 En-De we also use the compound splitting that was used in <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>.</p><p>Language Modeling Our language model training setup is identical to our machine translation setup except we remove label smoothing and lower the intra-attention dropout rate to 0. This was taken from the Tensor2Tensor hyperparameters for LM1B 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Search Configurations</head><p>All of the architecture searches we describe were run on WMT'14 En-De. They utilized the search space and tournament selection evolution algorithm described in our Methods section. Unless otherwise noted, each search used 200 workers, which were equipped with a single Google TPU V.2 chip for training and evaluation. We maintained a population of size 100 with subpopulation sizes for both killing and reproducing set to 30. Mutations were applied independently per encoding field at a rate of 2.5%. For fitness we used the negative log perplexity of the validation set instead of BLEU because, as demonstrated in our Results section, perplexity is more consistent and that reduced the noise of our fitness signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we will first benchmark the performance of our search method, Progressive Dynamic Hurdles, against other evolutionary search methods <ref type="bibr" target="#b27">(Real et al., 2017;</ref>. We will then benchmark the Evolved Transformer, the result of our search method, against the Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study of Search Techniques</head><p>We tested our evolution algorithm enhancements -using PDH and warm starting by seeding the initial population with the Transformer -against control searches that did not use these techniques; without our enhancements, these controls function the same way as <ref type="bibr" target="#b28">Real et. al's (2019)</ref>   <ref type="figure">Figure 3</ref>. Transformer and Evolved Transformer architecture cells. The four most notable aspects of the found architecture are the use of 1) wide depth-wise separable convolutions, 2) Gated Linear Units , 3) branching structures and 4) swish activations <ref type="bibr" target="#b26">(Ramachandran et al., 2017)</ref>. Both the ET encoder and decoder independently developed a branched lower portion of wide convolutions. Also in both cases, the latter portion is almost identical to the Transformer.</p><p>Our proposed search (  <ref type="table" target="#tab_4">(Table 1</ref> column 2). For these we used the step increments (30K), the maximum number of steps our proposed search ultimately reaches (180K), the total number of steps each top model receives when fully trained to gauge its final performance (300K), and half the step increments (15K), recognizing the gains from evaluating a larger number of models in the 30K steps control case. To determine the number of child models each of these searches would be able to train, we selected the value that would make the total amount of resources used by each control search equal to the maximum amount of resources used for our proposed searches, which require various amounts of resources depending on how many models fail to overcome hurdles. In the three trials we ran, our proposed search's total number of train steps used was 422M ± 21M, with a maximum of 446M. Thus the number of child models allotted for each non-PDH control search was set so that the total number of child model train steps used would be 446M.</p><p>As demonstrated in <ref type="table" target="#tab_4">Table 1</ref>, the search we propose, with PDH and Transformer seeding, has the best performance on average. It also is the most consistent, having the lowest standard deviation. Of all the searches conducted, only a single control run -"30K no hurdles" <ref type="table" target="#tab_4">(Table 1</ref> row 3)produced a model that was better than any of our proposed search's best models. At the same time, the "30K no hurdles" setup also produced models that were significantly worse, which explains its high standard deviation. This phenomenon was a chief motivator for our developing this method. Although aggressive early stopping has the potential to produce strong models for cheap, searches that utilize it can also venture into modalities in which top fitness child models are only strong early on; without running models for longer, whether or not this is happening cannot be detected. For example, the 15K search performed worse than the 30K setting, despite evaluating twice as many models. Although the 180K and 300K searches did have insight into long term performance, it was in a resource-inefficient manner that hurt these searches by limiting the number of generations they produced; for the "180K no hurdles" run to train as many models as PDH would require 1.08B train steps, over double what PDH used in our worst case.</p><p>Searching with random seeding also proved to be ineffective, performing considerably worse than every other configuration. Of the five searches run, random seeding was the only one that had a top model perplexity higher than the Transformer, which is 4.75 ± 0.01 in the same setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Main Search.</head><p>After confirming the effectiveness of our search procedure, we launched a larger scale version of our search using 270 workers. We trained 5K models per hurdle (m = 5000) and used larger step increments to get a closer approximation to 300K step performance: s =&lt; 60, 60, 120 &gt;. The setup was the same as the Search Techniques experiments, except after 11K models we lowered the mutation rate to 0.01 and introduced the NONE value to the normalization mutation vocabulary.</p><p>The search ran for 15K child models, requiring a total of 979M train steps. Over 13K models did not make it past the first hurdle, drastically reducing the resources required to view the 240 thousandth train step for top models, which would have cost 3.6B train steps for the same number of models without hurdles. After the search concluded, we then selected the top 20 models and trained them for the full 300K steps, each on a single TPU V.2 chip. The model that ended with the best perplexity is what we refer to as the Evolved Transformer (ET). <ref type="figure">Figure 3</ref> shows the ET architecture. The most notable aspect of the Evolved Transformer is the use of wide depth-wise separable convolutions in the lower layers of the encoder and decoder blocks. The use of depth-wise convolution and self-attention was previously explored in QANet <ref type="bibr" target="#b39">(Yu et al., 2018)</ref>, however the overall architectures of the Evolved Transformer and QANet are different in many ways: e.g., QANet has smaller kernel sizes and no branching structures. The performance and analysis of the Evolved Transformer will be shown in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">The Evolved Transformer: Performance and Analysis</head><p>To test the effectiveness of the found architecture -the Evolved Transformer -we compared it to the Transformer in its Tensor2Tensor training regime on WMT'14 En-De. <ref type="table">Table 3</ref> shows the results of these experiments run on the same 8 NVIDIA P100 hardware setup that was used by <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>. Observing ET's improved performance at parameter-comparable "base" and "big" sizes, we were also interested in understanding how small ET could be shrunk while still achieving the same performance as the Transformer. To create a spectrum of model sizes for each architecture, we selected different input embedding sizes and shrank or grew the rest of the model embedding sizes with the same proportions. Aside from embedding depths, these models are identical at all sizes, except the "big" 1024 input embedding size, for which all 8 head attention layers are upgraded to 16 head attention layers, as was done in <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>.</p><p>ET demonstrates stronger performance than the Transformer at all sizes, with the largest difference of 0.7 BLEU at the smallest, mobile-friendly, size of ∼7M parameters. Performance on par with the "base" Transformer was reached when ET used just 78.4% of its parameters and performance of the "big" Transformer was exceeded by the ET model that used 37.6% less parameters. <ref type="figure" target="#fig_1">Figure 4</ref> shows the FLOPS vs. BLEU performance of both architectures.</p><p>To test if ET's strong performance generalizes, we also compared it to the Transformer on an additional three wellestablished language tasks: WMT'14 En-Fr, WMT'14 En- Cs, and LM1B. 4 Upgrading to 16 TPU V.2 chips, we doubled the number of synchronous workers for these experiments, pushing both models to their higher potential <ref type="bibr" target="#b20">(Ott et al., 2018)</ref>. We ran each configuration 3 times, except WMT En-De, which we ran 6 times; this was a matter of resource availability and we gave priority to the task we searched on. As shown in <ref type="table">Table 2</ref>, ET performs at least one standard deviation above the Transformer in each of these tasks. Note that the Transformer mean BLEU scores in all of our experiments for WMT'14 En-Fr and WMT'14 En-De are higher than those originally reported by <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>.</p><p>As can be seen in <ref type="table">Tables 3 and 2</ref>, the Evolved Transformer is much more effective than the Transformer at smaller model sizes. At the "big" model size, its BLEU performance saturates and the gap between the Evolved Transformer and the Transformer becomes smaller. One explanation for this behavior is that overfitting starts to occur at big model sizes, but we expect that data augmentation <ref type="bibr" target="#b20">(Ott et al., 2018)</ref> or hyperparameter tuning could improve performance. For example, we found that simply increasing the embedding size was not the best way to grow ET from the "base" size we searched over to a larger size. Depth should also be tuned in conjunction with embedding size, when controlling for number of parameters. For both the Transformer and ET, we tried four additional embedding sizes, <ref type="bibr">[512,</ref><ref type="bibr">640,</ref><ref type="bibr">768,</ref><ref type="bibr">896]</ref>, adjusting the depth accordingly so that all resulting models had a similar number of parameters. Using validation BLEU to determine the best configuration, we found that ET performed best with an embedding depth of 640, increasing its number of encoder cells from 3 to 9 and its number of decoder cells from 4 to 10. The Transformer also benefited from additional depth, although not to the same degree, achieving maximum performance at the 768 embedding size, with 6 encoder cells and 6 decoder cells. These results are included in To compare with other previous results, we trained the ET Deep model three times in our TPU setup on WMT'14 En-De, selected the best run according to validation BLEU and did a single decoding on the test set. We also copied previous state-of-the-art result setups by averaging the last 20 model checkpoints from training and decoding with a beam width of 5 <ref type="bibr" target="#b33">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b20">Ott et al., 2018;</ref><ref type="bibr" target="#b35">Wu et al., 2019)</ref>. As a result, the Evolved Transformer achieved a new state-of-the-art BLEU score of 29.8 <ref type="table">(Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented the first neural architecture search conducted to find improved feed-forward sequence models. We first constructed a large search space inspired by recent advances in seq2seq models and used it to search directly on the computationally intensive WMT En-De translation task. To mitigate the size of our space and the cost of training child models, we proposed using both our Progressive Dynamic Hurdles method and warm starting, seeding our initial population with a known strong model, the Transformer.</p><p>When run at scale, our search found the Evolved Transformer. In a side by side comparison against the Transformer in an identical training regime, the Evolved Transformer showed consistent stronger performance on both translation and language modeling. On the task we searched over, WMT'14 En-De, the Evolved Transformer established a new state-of-the-art of 29.8 BLEU. It also proved to be efficient at smaller sizes, achieving the same quality as the original "big" Transformer with 37.6% less parameters and outperforming the Transformer by 0.7 BLEU at a mobilefriendly model size of ∼7M parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Search Algorithms</head><p>In the following, we describe the algorithm that we use to calculate child model fitness with hurdles (Algorithm 1) and evolution architecture search with Progressive Dynamic Hurdles (Algorithm 2).</p><p>Algorithm 1 Calculate Model Fitness with Hurdles inputs: model: the child model s: vector of train step increments h: queue of hurdles</p><formula xml:id="formula_0">append ∞ to h TRAIN N STEPS(model, s 0 ) f itness ← EVALUATE(model) i ← 0 hurdle ← h i while f itness &gt; hurdle do i ← i + 1 TRAIN N STEPS(model, s i ) f itness ← EVALUATE(model) hurdle ← h i end while return f itness</formula><p>Algorithm 1 Calculating fitness with hurdles takes as arguments a child model, a vector of train step increments (s) and a queue of hurdles(h). The child model is the candidate model in our neural architecture search. The vector of step increments describes the number of steps between each hurdle; its length must be greater than 0. The queue of hurdles describes what hurdles have already been established; its length must be in [0, length(s)).</p><p>The algorithm starts by first training the child model a fixed number of s 0 steps and evaluating on the validation set to produce a fitness, as is done in <ref type="bibr" target="#b28">Real et al. (2019)</ref>. After this baseline fitness is established, the hurdles (h) are compared against to determine if training should continue. Each h i denotes the fitness a child model must have after i j=0 s j train steps to continue training. Each time a hurdle h i is passed, the model is trained an additional s i+1 steps. If the model's fitness ever falls below the hurdle corresponding to the number of steps it was trained for, training ends immediately and the current fitness is returned. If the model never falls below a hurdle and all hurdles have been passed, the child model receives one final training of s length(h) steps before fitness is returned; this is expressed in Algorithm 1 with ∞ being appended to the end of h. Algorithm 2 Evolution architecture search with PDH takes as arguments a vector of train step increments (s) and a number of child models per hurdle (m). It begins as <ref type="bibr">Real et al.'s (2019)</ref> evolution architecture search with a fixed number of child model train steps, s 0 . However, after m child models have been produced, a hurdle is created by taking the mean fitness of the current population and it is added to the hurdle queue, h. Algorithm 1 is used to compute each child model's fitness and so if they overcome the new hurdle they will receive more train steps. This process is continued, with new hurdles being created using the mean fitness of all models that have trained the maximum number of steps and h growing accordingly. The process terminates when length(s) − 1 hurdles have been created and evolution is run for one last round of m models, using all created hurdles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Search Space Information</head><p>In our search space, a child model's genetic encoding is expressed as: [left input, left normalization, left layer, left relative output dimension, left activation, right input, right normalization, right layer, right relative output dimension, right activation, combiner function] × 14 + [number of cells] × 2, with the first 6 blocks allocated to the encoder and the latter 8 allocated to the decoder. In the following, we will describe each of the components.</p><p>Input. The first branch-level search field is input. This specifies what hidden state in the cell will be fed as input to the branch. For each i th block, the input vocabulary of its branches is [0, i), where the j th hidden state corresponds to the j th block output and the 0 th hidden state is the cell input.</p><p>Normalization. The second branch-level search field is normalization, which is applied to each input before the layer transformation is applied. The normalization vocabulary is [LAYER NORMALIZATION <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>, NONE].</p><p>Layers. The third branch-level search field is layer, which is the neural network layer applied after the normalization. It's vocabulary is: <ref type="bibr">5, 7, 9, 11}</ref> • LIGHTWEIGHT CONV wx1 r: for w ∈ {3, 5, 7, 15}, r ∈ {1, 4, 16} <ref type="bibr" target="#b35">(Wu et al., 2019)</ref>. r is the reduction factor, equivalent to d/H described in <ref type="bibr" target="#b35">Wu et al. (2019)</ref>. For decoder convolution layers the inputs are shifted by (w − 1)/2 so that positions cannot "see" later predictions.</p><formula xml:id="formula_1">• STANDARD CONV wx1: for w ∈ {1, 3} • DEPTHWISE SEPARABLE CONV wx1: for w ∈ {3,</formula><p>Relative Output Dimension. The fourth branch-level search field is relative output dimension, which describes the output dimension of the corresponding layer. The Transformer is composed mostly of layers that project to the original input embedding depth (512 in the "base" configuration), but also contains 1x1 convolutions that project up to a dimension of 4 times that depth (2048 in the "base" configuration). The relative output dimension search field accounts for this variable output depth. It's vocabulary consists of 10 relative output size options: <ref type="bibr">[1,</ref><ref type="bibr">10]</ref>.</p><p>Here "relative" refers to the fact that for every layer i and j, each of their absolute output dimensions, a, and relative output dimensions, d, will obey the ratio: a i /a j = d i /d j . We determine the absolute output dimensions for each model by finding a scaling factor, s, such that for every layer i, a i = d i * s and the resulting model has an appropriate number of parameters; at the end of this section, we describe our constraints on number of model parameters. There may be multiple values of s for any one model that satisfy this constraint, and so for our experiments we simply perform a binary search and use the first valid value found. If no valid value is found, we reject the child model encoding as invalid and produce a new one in its stead.</p><p>We chose a vocabulary of relative sizes instead of absolute sizes because we only allow models within a fixed parameter range, as described later in this section (Constraints). Using relative sizes allows us to increase the number of configurations that represent valid models in our search space, because we can dynamically shrink or grow a model to make it fit within the parameter bounds. We found that using absolute values, such as <ref type="bibr">[256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048]</ref>, increases the number of rejected models and thereby decreases the possible models that can be expressed.</p><p>This relative output dimensions field is ignored for both the IDENTITY and DEAD BRANCH layers.</p><p>Activations. The final branch-level search field is activation, which is the non-linearity applied on each branch after the neural network layer. The activation vocabulary is {SWISH <ref type="bibr" target="#b26">(Ramachandran et al., 2017;</ref><ref type="bibr" target="#b12">Elfwing et al., 2018)</ref>, RELU, LEAKY RELU (MAAS ET AL., 2013), NONE}.</p><p>Combiner Functions. The block-level search field, combiner function, describes how the left and right layer branches are combined together.</p><p>Its vocabulary is {ADDITION, CONCATENATION, MULTIPLICATION}. For MULTIPLICATION and ADDITION, if the right and left branch outputs have differing embedding depths, then the smaller of the two is padded so that the dimensionality matches. For ADDITION the padding is 0's; for MULTIPLI-CATION the padding is 1's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Cells.</head><p>The cell-level search field is number of cells and it describes the number of times the cell is repeated. Its vocabulary is [1,6].</p><p>Composition. Each child model is defined by two cells, one for the encoder and one for the decoder. The encoder cell contains 6 blocks and the decoder cell contains 8 blocks. Each block contains two branches, each of which takes a previous hidden layer as input, and then applies its normalization, layer (with specified relative output dimensions) and activation to it. The two branches are then joined with the combiner function. Any unused hidden states are automatically added to the final block output via addition. Both the encoder and decoder cells defined this way are repeated their corresponding number of cells times and connected to the input and output embedding portions of the network to produce the final model; we use the same embedding scheme described by <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref> for all models. See <ref type="figure">Figure 1</ref> for a depiction of this composition.</p><p>Constraints. In the interest of having a fair comparison across child models, we limit our search to only architectures configurations that can contain between 59.1 million and 64.1 million parameters when their relative output dimensions are scaled; in the Tensor2Tensor  implementation we use, the base Transformer has roughly 61.1 million parameters on WMT En-De, so our models are allowed 3 million less or more parameters than that. Models that cannot be represented within this parame-ter range are not included in our search space.</p><p>Additionally, in preliminary experiment runs testing the effectiveness of our search space, we discovered three trends that hurt performance in almost every case. Firstly and most obviously is when a proposed decoder contains no ATTEND TO ENCODER layers. This results in the decoder receiving no signal from the encoder and thus the model output will not be conditioned on the input. Therefore, any model that does not contain ATTEND TO ENCODER is not in our search space. The second trend that we noticed was that models that had layer normalization removed were largely worse than their parent models. For this reason, we remove NONE from the normalization mutation vocabulary for each experiment, unless otherwise specified. Lastly, we observed that an important feature of good models was containing an unbroken residual path from inputs to outputs; in our search space, this means a path of IDENTITY layers from cell input to output that are combined with ADDITION at every combination function along the way. Our final constraint is therefore that models that do not have unbroken residual paths from cell inputs to outputs are not in our search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study of the Evolved Transformer</head><p>To understand what mutations contributed to ET's improved performance we conducted two rounds of ablation testing. In the first round, we began with the Transformer and applied each mutation to it individually to measure the performance change each mutation introduces in isolation. In the second round, we began with ET and removed each mutation individually to again measure the impact of each single mutation. In both cases, each model was trained 3 times on WMT En-De for 300k steps with identical hyperparameters, using the inverse-square-root decay to 0 that the Transformer prefers. Each training was conducted on a single TPU V.2 chip. The results of these experiments are presented in <ref type="table">Table 5</ref>; we use validation perplexity for comparison because it was our fitness metric.</p><p>In all cases, the augmented ET models outperformed the the augmented Transformer models, indicating that the gap in performance between ET and the Transformer cannot be attributed to any single mutation. The mutation with the seemingly strongest individual impact is the increase from 3 to 4 decoder cells. However, even when this mutation is introduced to the Transformer and removed from ET, the resulting augmented ET model still has a higher fitness than the augmented Transformer model.</p><p>To highlight the impact of each augmented model's mutation, we present not only their perplexities but also the difference between their mean perplexity and their unaugmented base model mean perplexity in the "Mean Diff" columns:</p><p>base model mean perplexityaugmented mean perplexity</p><p>This delta estimates the change in performance each mutation creates in isolation. Red highlighted cells contain evidence that their corresponding mutation hurt overall performance. Green highlighted cells contain evidence that their corresponding mutation helped overall performance.</p><p>In half of the cases, both the augmented Transformer's and the augmented Evolved Transformer's performances indicate that the mutation was helpful. Changing the number of attention heads from 8 to 16 was doubly indicated to be neutral and changing from 8 head self attention to a GLU layer in the decoder was doubly indicated to have hurt performance. However, this and other mutations that seemingly hurt performance may have been necessary given how we formulate the problem: finding an improved model with a comparable number of parameters to the Transformer. For example, when the Transformer decoder cell is repeated 4 times, the resulting model has 69.6M parameters, which is outside of our allowed parameter range. Thus, mutations that shrank ET's total number of parameters, even at a slight degradation of performance, were necessary so that other more impactful parameter-expensive mutations, such as adding an additional decoder cell, could be used.</p><p>Other mutations have inconsistent evidence about how useful they are. This ablation study serves only to approximate what is useful, but how effective a mutation is also depends on the model it is being introduced to and how it interacts with other encoding field values.  <ref type="table">Table 5</ref>. Mutation Ablations: Each mutation is described by the first 5 columns. The augmented Transformer and augmented ET perplexities on the WMT'14 En-De validation set are given in columns 6 and 7. Columns 7 and 8 show the difference between the unaugmented base model perplexity mean and the augmented model perplexity mean. Red highlighted cells indicate evidence that the corresponding mutation hurts overall performance. Green highlighted cells indicate evidence that the corresponding mutation helps overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Evolution architecture search with hurdles. The yaxis represents architecture fitness and the x-axis represents the order in which candidate models were created. The solid purple and green lines represent the values of the first and second hurdles, respectively. The dashed purple and green lines represent the points at which each of the corresponding hurdles were introduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Performance comparison of the Evolved Transformer against the Transformer across number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>• h HEAD ATTENTION: for h ∈ {4, 8, 16} • GATED LINEAR UNIT(Dauphin et al., 2017) • ATTEND TO ENCODER: (Only available to decoder) • IDENTITY: No transformation applied to input • DEAD BRANCH: No output</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and concatenated together with padding to form uniform 256 length inputs and targets, with examples longer than 256 being discarded. This yielded batch sizes of 4096 tokens per GPU or TPU chip; accordingly, 16 TPU chip configurations had ∼66K tokens per batch and 8 GPU chip configurations had ∼33K tokens per batch.</figDesc><table><row><cell>Setup</cell></row></table><note>4.1. Datasets Machine Translation We use three different machine translation datasets to perform our experiments, all of which were taken from their Tensor2Tensor implementations 1 . The first is WMT English-German, for which we mimic Vaswani et al.'s (2017) setup, using WMT'18 En-De training data without ParaCrawl (ParaCrawl, 2018), yielding 4.5 million sentence pairs. In the same fashion, we use newstest2013 for development and test on newstest2014. The second trans- lation dataset is WMT En-Fr, for which we also replicate Vaswani et.al's (2017) setup. We train on the 36 million sen- tence pairs of WMT'14 En-Fr, validate on newstest2013 and test on newstest2014. The final translation dataset is WMT English-Czech (En-Cs). We used the WMT'18 training dataset, again without ParaCrawl, and used newstest2013 and newstest2014 as validation and test sets. For all tasks, tokens were split using a shared source-target vocabulary of about 32K word-pieces (Wu et al., 2016). All datasets were generated using Tensor2Tensor's "packed" scheme; sentences were shuffled</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>searches, without aging regularization. Each search we describe was run 3 times and the top model from each run was retrained on a single TPU V.2 chip for 300K steps. The performance of the models after retraining is given inTable 1.</figDesc><table><row><cell>Transformer Encoder Block</cell><cell cols="2">Evolved Transformer Encoder Block</cell><cell>Activation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Normalization</cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell>Wide Convolution</cell></row><row><cell>Conv 1x1 : 512</cell><cell cols="2">Conv 1x1 : 512</cell><cell>Attention Non-spatial Layer</cell></row><row><cell>RELU</cell><cell>RELU</cell><cell></cell><cell></cell></row><row><cell>Conv 1x1 : 2048</cell><cell cols="2">Conv 1x1 : 2048</cell><cell></cell></row><row><cell>Layer Norm</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell>8 Head Self Attention : 512</cell><cell cols="2">8 Head Self Attention : 512</cell><cell></cell></row><row><cell>Layer Norm</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell>Conv 1x1 : 512</cell><cell cols="2">Sep Conv 9x1 : 256</cell><cell></cell></row><row><cell>RELU</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>Conv 1x1 : 2048</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RELU</cell><cell>RELU</cell><cell></cell></row><row><cell>Layer Norm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+</cell><cell>Conv 1x1 : 2048</cell><cell>Conv 3x1 : 256</cell><cell></cell></row><row><cell>8 Head Self Attention : 512</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>Layer Norm</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Gated Linear Unit : 512</cell><cell></cell></row><row><cell></cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>Transformer Decoder Block</cell><cell cols="2">Evolved Transformer Decoder Block</cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell>Conv 1x1 : 512</cell><cell cols="2">Conv 1x1 : 512</cell><cell></cell></row><row><cell>RELU</cell><cell>Swish</cell><cell></cell><cell></cell></row><row><cell>Conv 1x1 : 2048</cell><cell cols="2">Conv 1x1 : 2048</cell><cell></cell></row><row><cell>Layer Norm</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell>8 Head Attend to Encoder : 512</cell><cell cols="2">8 Head Attend to Encoder : 512</cell><cell></cell></row><row><cell>Layer Norm</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell>8 Head Self Attention : 512</cell><cell cols="2">8 Head Self Attention : 512</cell><cell></cell></row><row><cell>Layer Norm</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell>Conv 1x1 : 512</cell><cell cols="2">Sep Conv 7x1 : 512</cell><cell></cell></row><row><cell>RELU</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>Conv 1x1 : 2048</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+</cell><cell></cell><cell></cell></row><row><cell>Layer Norm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+</cell><cell>RELU</cell><cell>Sep Conv 7x1 : 256</cell><cell></cell></row><row><cell>8 Head Attend to Encoder : 512</cell><cell>Sep Conv 11x1 : 1024</cell><cell></cell><cell></cell></row><row><cell>Layer Norm</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell></row><row><cell>8 Head Self Attention : 512</cell><cell>16 Head Self Attention : 512</cell><cell cols="2">8 Head Attend to Encoder : 512</cell></row><row><cell>Layer Norm</cell><cell cols="2">Layer Norm</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>SEED MODEL</cell><cell>TRAIN STEPS</cell><cell>NUM MODELS</cell><cell>TOP MODEL PERPLEXITY</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TRANSFORMER</cell><cell>PDH</cell><cell>6000</cell><cell>4.50 ± 0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RANDOM</cell><cell>PDH</cell><cell>6000</cell><cell>5.23 ± 0.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TRANSFORMER</cell><cell>15K</cell><cell>29714</cell><cell>4.57 ± 0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TRANSFORMER</cell><cell>30K</cell><cell>14857</cell><cell>4.53 ± 0.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TRANSFORMER</cell><cell>180K</cell><cell>2477</cell><cell>4.58 ± 0.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TRANSFORMER</cell><cell>300K</cell><cell>1486</cell><cell>4.61 ± 0.02</cell></row></table><note>Table 1. Top model validation perplexity of various search se- tups. Number of models were chosen to equalize resource con- sumption.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>row 1), which used both PDH</cell></row><row><cell>and Transformer seeding, was run first, with hurdles created</cell></row><row><cell>every 1K models (m = 1000) and six 30K train step (1</cell></row><row><cell>hour) increments (s =&lt; 30, 30, 30, 30, 30, 30 &gt;). To test</cell></row><row><cell>the effectiveness of seeding with the Transformer, we ran</cell></row><row><cell>an identical search that was instead seeded with random</cell></row><row><cell>valid encodings (Table 1 row 2). To test the effectiveness</cell></row><row><cell>of PDH, we ran four controls (Table 1 rows 3-6) that each</cell></row><row><cell>used a fixed number of train steps for each child model</cell></row><row><cell>instead of hurdles</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 Table 2 .</head><label>22</label><figDesc>Comparison between the Transformer and ET trained on 16 TPU V.2 chips. For Translation, perplexity was calculated on the validation set and BLEU was calculated on the test set. For LM1B, perplexity was calculated on the test set. ET shows consistent improvement by at least one standard deviation on all tasks. It excels at the base size the search was conducted in, with an improvement of 0.6 BLEU in both En-Fr and En-Cs.Table 3. WMT'14 En-De comparison on 8 NVIDIA P100 GPUs. Each model was trained 10 to 15 times, depending on resource availability. Perplexity is calculated on the validation set and BLEU is calculated on the test set.</figDesc><table><row><cell>TASK</cell><cell>SIZE</cell><cell cols="2">TRAN PARAMS</cell><cell>ET PARAMS</cell><cell cols="2">TRAN PERP</cell><cell></cell><cell>ET PERP</cell><cell>TRAN BLEU</cell><cell>ET BLEU</cell></row><row><cell cols="2">WMT'14 EN-DE BASE</cell><cell cols="2">61.1M</cell><cell>64.1M</cell><cell cols="2">4.24 ± 0.03</cell><cell cols="2">4.03 ± 0.02</cell><cell>28.2 ± 0.2</cell><cell>28.4 ± 0.2</cell></row><row><cell>WMT'14 EN-DE</cell><cell>BIG</cell><cell cols="2">210.4M</cell><cell>221.7M</cell><cell cols="2">3.87 ± 0.02</cell><cell cols="2">3.77 ± 0.02</cell><cell>29.1 ± 0.1</cell><cell>29.3 ± 0.1</cell></row><row><cell cols="4">WMT'14 EN-DE DEEP 224.0M</cell><cell>218.1M</cell><cell cols="2">3.86 ± 0.02</cell><cell cols="2">3.69 ± 0.01</cell><cell>29.2 ± 0.1</cell><cell>29.5 ± 0.1</cell></row><row><cell cols="2">WMT'14 EN-FR BASE</cell><cell>60.8</cell><cell></cell><cell>63.8M</cell><cell cols="2">3.61 ± 0.01</cell><cell cols="2">3.42 ± 0.01</cell><cell>40.0 ± 0.1</cell><cell>40.6 ± 0.1</cell></row><row><cell>WMT'14 EN-FR</cell><cell>BIG</cell><cell cols="2">209.8M</cell><cell>221.2M</cell><cell cols="2">3.26 ± 0.01</cell><cell cols="2">3.13 ± 0.01</cell><cell>41.2 ± 0.1</cell><cell>41.3 ± 0.1</cell></row><row><cell cols="2">WMT'14 EN-CS BASE</cell><cell cols="2">59.8M</cell><cell>62.7M</cell><cell cols="2">4.98 ± 0.04</cell><cell cols="2">4.42 ± 0.01</cell><cell>27.0 ± 0.1</cell><cell>27.6 ± 0.2</cell></row><row><cell>WMT'14 EN-CS</cell><cell>BIG</cell><cell cols="2">207.6M</cell><cell>218.9M</cell><cell cols="2">4.43 ± 0.01</cell><cell cols="2">4.38 ± 0.03</cell><cell>28.1 ± 0.1</cell><cell>28.2 ± 0.1</cell></row><row><cell>LM1B</cell><cell>BIG</cell><cell cols="2">141.1M</cell><cell>151.8M</cell><cell cols="4">30.44 ± 0.04 28.60 ± 0.03</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Model</cell><cell cols="2">Embedding Size</cell><cell>Parameters</cell><cell cols="2">Perplexity</cell><cell>BLEU</cell><cell>∆ BLEU</cell></row><row><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell>128</cell><cell>7.0M</cell><cell cols="3">8.62 ± 0.03 21.3 ± 0.1</cell><cell>-</cell></row><row><cell></cell><cell>ET</cell><cell></cell><cell></cell><cell>128</cell><cell>7.2M</cell><cell cols="3">7.62 ± 0.02 22.0 ± 0.1</cell><cell>+ 0.7</cell></row><row><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell>432</cell><cell>45.8M</cell><cell cols="3">4.65 ± 0.01 27.3 ± 0.1</cell><cell>-</cell></row><row><cell></cell><cell>ET</cell><cell></cell><cell></cell><cell>432</cell><cell>47.9M</cell><cell cols="3">4.36 ± 0.01 27.7 ± 0.1</cell><cell>+ 0.4</cell></row><row><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell>512</cell><cell>61.1M</cell><cell cols="3">4.46 ± 0.01 27.7 ± 0.1</cell><cell>-</cell></row><row><cell></cell><cell>ET</cell><cell></cell><cell></cell><cell>512</cell><cell>64.1M</cell><cell cols="3">4.22 ± 0.01 28.2 ± 0.1</cell><cell>+ 0.5</cell></row><row><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell>768</cell><cell>124.8M</cell><cell cols="3">4.18 ± 0.01 28.5 ± 0.1</cell><cell>-</cell></row><row><cell></cell><cell>ET</cell><cell></cell><cell></cell><cell>768</cell><cell>131.2M</cell><cell cols="3">4.00 ± 0.01 28.9 ± 0.1</cell><cell>+ 0.4</cell></row><row><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell>1024</cell><cell>210.4M</cell><cell cols="3">4.05 ± 0.01 28.8 ± 0.2</cell><cell>-</cell></row><row><cell></cell><cell>ET</cell><cell></cell><cell></cell><cell>1024</cell><cell>221.7M</cell><cell cols="3">3.94 ± 0.01 29.0 ± 0.1</cell><cell>+ 0.2</cell></row><row><cell>Model</cell><cell cols="3">Params BLEU</cell><cell cols="2">SacreBLEU (Post, 2018)</cell><cell></cell><cell></cell></row><row><cell>Gehring et al. (2017)</cell><cell>216M</cell><cell cols="2">25.2</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vaswani et al. (2017)</cell><cell>213M</cell><cell cols="2">28.4</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ahmed et al. (2017)</cell><cell>213M</cell><cell cols="2">28.9</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chen et al. (2018)</cell><cell>379M</cell><cell cols="2">28.5</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shaw et al. (2018)</cell><cell>213M</cell><cell cols="2">29.2</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ott et al. (2018)</cell><cell>210M</cell><cell cols="2">29.3</cell><cell>28.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wu et al. (2019)</cell><cell>213M</cell><cell cols="2">29.7</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Evolved Transformer</cell><cell>218M</cell><cell cols="2">29.8</cell><cell>29.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Table 4. Model comparison on WMT'14 En-De.</cell><cell></cell><cell></cell></row></table><note>, labeled as "Deep" size.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Algorithm 2 Evolution Architecture Search with PDH inputs: s: vector of train step increments m: number of child models per hurdle</figDesc><table><row><cell>h ← empty queue</cell></row><row><cell>i ← 0</cell></row><row><cell>population ← INITIAL POPULATION()</cell></row><row><cell>while i &lt; LENGTH(s) -1 do</cell></row><row><cell>population ← EVOL N MODELS(population,</cell></row><row><cell>m, s, h)</cell></row><row><cell>hurdle ← MEAN FITNESS OF MAX(population)</cell></row><row><cell>append hurdle to h</cell></row><row><cell>end while</cell></row><row><cell>population ← EVOL N MODELS(population,</cell></row><row><cell>m, s, h)</cell></row><row><cell>return population</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/tensor2tensor/tree/master/ tensor2tensor/data generators</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tensorflow/tensor2tensor/blob/master/ tensor2tensor/models/transformer.py 3 https://github.com/moses-smt/mosesdecoder/blob/master/ scripts/generic/multi-bleu.perl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For LM1B, we only use the decoder architecture, with attend to encoder layers removed.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Ashish Vaswani, Jakob Uszkoreit, Niki Parmar, Noam Shazeer, Lukasz Kaiser and Ryan Sepassi for their help with Tensor2Tensor and for sharing their understanding of the Transformer. We are also grateful to David Dohan, Esteban Real, Yanping Huang, Alok Aggarwal, Vijay Vasudevan, and Chris Ying for lending their expertise in architecture search and evolution.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Weighted transformer network for machine translation. CoRR, abs/1711.02132</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.02132" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<ptr target="http://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SMASH: One-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rydeCEhs-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<idno>abs/1312.3005</idno>
		<ptr target="http://arxiv.org/abs/1312.3005" />
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<idno>abs/1804.09849</idno>
		<ptr target="http://arxiv.org/abs/1804.09849" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparative analysis of selection schemes used in genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-stochastic best arm identification and hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hyperband: a novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Translation, Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paracrawl</surname></persName>
		</author>
		<ptr target="http://paracrawl.eu/download.html" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<idno>abs/1804.08771</idno>
		<ptr target="http://arxiv.org/abs/1804.08771" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://blog.openai.com/language-unsupervised/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Searching for activation functions. CoRR, abs/1710.05941</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.05941" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Selfattention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>abs/1803.02155</idno>
		<ptr target="http://arxiv.org/abs/1803.02155" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adafactor: adaptive learning rates with sublinear memory cost. CoRR, abs/1804.04235</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1804.04235" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">31043112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
		<title level="m">A generative model for raw</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast and accurate reading comprehension by combining self-attention and convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards automated deep learning: efficient joint neural architecture and hyperparameter search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on AutoML, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
