<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inpainting Transformer for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pirnay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Incubation</orgName>
								<orgName type="institution">Fujitsu Technology Solutions GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng</forename><surname>Chai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Incubation</orgName>
								<orgName type="institution">Fujitsu Technology Solutions GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inpainting Transformer for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneficial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When learning from scratch, InTra achieves better than state-of-the-art results on the MVTec AD [1] dataset for detection and localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection and localization in vision describe the problem of deciding whether a given image is atypical with respect to a set of normal samples, and to identify the respective anomalous subregions within the image. Although often viewed separately, both problems have strong implications for industrial inspection <ref type="bibr" target="#b0">[1]</ref> and medical applications <ref type="bibr" target="#b1">[2]</ref>. In practical industrial applications, anomalies occur rarely. Due to the lack of sufficient anomalous samples, and as anomalies can be of unexpected shape and texture, it is hard to deal with this problem with supervised methods. Current approaches follow unsupervised methods and try to model the distribution of normal data only. At test time an anomaly score is given to each image to indicate how much it deviates from normal samples. For anomaly localization a similar score is assigned to subregions or individual pixels of the image.</p><p>A common approach following this paradigm is to use deep convolutional autoencoders or generative models such as variational autoencoders <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> in order to model the manifold of normal training data. The difference between the input and reconstructed image is then used to compute the anomaly scores. This is based on the idea that by training on normal images only, the model will not be able to properly reconstruct anomalous images, leading to higher anomaly scores. In practice this approach often suffers from the drawback that convolutional autoencoders generalize strongly and anomalies are reconstructed well, leading to misdetection <ref type="bibr" target="#b7">[8]</ref>. Recent methods propose to mitigate this effect by posing the generative part as an inpainting problem: Parts of the input image are covered and the model is trained to reconstruct the covered parts in a self-supervised way <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. By conditioning on the neighborhood of the excluded part only, small anomalies get effectively retouched. Due to their limited receptive field, fully convolutional neural networks (CNNs) are partially ineffective in modeling dis- <ref type="figure">Figure 1</ref>: Schematic overview of the proposed method. a.) The image is split into square patches. An inpainting transformer model (InTra) is trained to reconstruct a covered patch (black) from a long sequence of surrounding patches (red). Positional embeddings are added to the patches to include spatial context. b.) By reconstructing all patches of an image (left), a full reconstruction is obtained (middle). The difference of original and reconstruction is used to compute a pixel-wise anomaly score (right) for detection and localization. tant contextual information, which makes the removal of larger anomalous regions difficult. In order to influence a pixel by information 64 pixels away, at least 6 layers of 3 × 3 convolutions with dilation factor 2 or equivalent are required <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. For inpainting in general settings, this can be effectively addressed by introducing contextual attention in the model <ref type="bibr" target="#b13">[14]</ref>. For inpainting in the context of anomaly detection we suggest it to be beneficial to learn the relevant patterns alone by combining information from large regions around the covered image part via attention. Inspired by the recent success of self-attention based models such as Transformers <ref type="bibr" target="#b14">[15]</ref> in image recognition <ref type="bibr" target="#b15">[16]</ref>, we pose anomaly detection as a patch-inpainting problem and propose to solve it without convolutions: images are split into square patches, and a Transformer model is trained to reconstruct covered patches on the basis of a long sequence of neighboring patches. By recovering the whole image in this way, a full reconstructed image is obtained where the reconstruction of an individual patch incorporates a larger global context and not only the appearance of its immediate neighborhood. Thus patches are not reconstructed by simply mimicking the local neighborhood, leading to high anomaly scores even for spacious anomalous regions.</p><p>Our contributions enfold the modeling of anomaly detection as a patch-sequence inpainting problem which we solve using a deep Transformer network consisting of a simple stack of multiheaded self-attention blocks. Within this network convolutional operations are removed entirely. Furthermore we propose to a.) employ long residual connections between the Transformer blocks b.) perform a dimension reduction for keys and queries with a small multilayer perceptron when computing self-attention in order to improve the network's reconstruction capabilities for difficult surfaces. By adding embeddings of the position of individual patches within an image to the sequence of patches, it is possible to perform the inpainting in a global context even if the sequence of patches does not cover the full image. This yields improved results especially for anomaly detection tasks where the composition of an inspected image may only differ slightly from the normal samples. We evaluate our method on the challenging MVTec AD dataset for both detection and segmentation. Although Transformer networks are usually trained on huge amounts of data, we effectively train our networks with ∼55M parameters from scratch only on the 200-300 images available for each category in MVTec AD. With our proposed method InTra we achieve better than state-of-the-art performance on MVTec AD in comparison to other methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref> which are not using extra additional training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image based anomaly detection and segmentation has improved significantly with the technological advances of deep learning <ref type="bibr" target="#b0">[1]</ref>. The following section will give a summary about most recent approaches. While CNNs have shown to be highly successful for vision based tasks and also anomaly detection and segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>, sequence-to-sequence Transformer models, originating from Natural Language Processing (NLP) have found application in computer vision tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Anomaly Detection and Segmentation</head><p>Anomaly detection is concerned about deciding if an image contains an unexpected deviation from a predefined norm, while in segmentation the goal is to find and localize these deviations accurately on a pixel level to extract the regions where a defect occurred. For explainability a good overall performance in both tasks is needed which is not guaranteed if a method achieves good results in one of the tasks. Existing methods can be roughly categorized into two different approaches.</p><p>Reconstruction based. Reconstruction-based models try to model only normal, defect-free samples. For this, deep CNN autoencoders are widely used to model the manifold of defectfree images in a latent bottleneck. This has shown to achieve significant results in various domains. Given defective test data, these models should not be able to properly reconstruct the anomalous image since they only model normal data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8</ref>]. An anomaly map for segmentation is usually generated via pixel-wise difference between the input image and its model reconstruction, leading to noticeable anomalies. Modifications like integrating structural similarity index measure (SSIM) in the loss function during training are used to improve reconstruction quality by producing smoother images while focusing on retaining structural information such as edges <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>. Variational Autoencoders (VAEs) have also found usage in anomaly detection and segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24]</ref>. Their probabilistic latent space tries to capture a distribution capable of generating normal samples. This generative approach allows for the inclusion of the Kullback-Leibler divergence in the anomaly score to incorporate a probabilistic scoring <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. In general VAEs are not automatically superior to traditional autoencoder methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Adversarial models such as Generative Adversarial Networks (GANs) have been used for anomaly detection and segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29]</ref>. Although GANs often suffer from an unstable training procedure, highly realistic and almost natural images can be generated <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5]</ref>. Akcay et al. propose to additionally take the discriminator network into account for the calculation of the anomaly score <ref type="bibr" target="#b28">[29]</ref>. For this a feature loss between the input image and its reconstructed counterpart is employed based on the last convolutional layer of the discriminator model.</p><p>Embedding based. CNNs comprise of multiple hidden layers which produce meaningful latent features. Especially later stages lead to distinguishable and powerful intermediate representations. These feature vectors can be utilized to build the basis of an anomaly detection approach <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>. Popular CNN architectures such as ResNet have been used which were originally pretrained on the ImageNet classification benchmark <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Based on the feature representations e.g. a clustered codebook of good samples <ref type="bibr" target="#b33">[34]</ref> or a nearest neighbour algorithm can be used <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>. Another approach proposed by Rudolph et al. uses normalizing flows to estimate a standard multivariate normal distribution of good samples only <ref type="bibr" target="#b30">[31]</ref>.</p><p>While embedding based methods achieve good detection results, accurate localization of anomalies is not automatically included since no inherent mapping to the original image space exists. Defard et al. propose a patch based approach while maintaining the mapping of coordinates from original image space to feature maps <ref type="bibr" target="#b19">[20]</ref>. Based on different convolutional architectures (Resnet <ref type="bibr" target="#b31">[32]</ref>, Wide ResNet <ref type="bibr" target="#b34">[35]</ref>, EfficientNet <ref type="bibr" target="#b35">[36]</ref>) feature vectors are extracted for each patch and for each patch position a multivariate Gaussian is inferred to model the distribution of normal samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Inpainting in Anomaly Detection</head><p>Inpainting is a specific subtask in visual image understanding having found usage in various applications like image editing and synthesis. Given a partly covered image, the goal is to accurately reconstruct the original uncovered data. Earlier methods have found success using matching based on local image descriptors <ref type="bibr" target="#b36">[37]</ref>. Complex scenes and objects are harder to get consistent and realistic results for as the model has to understand the context and content of the image <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Even though in reconstruction-based methods for anomaly detection the models are trained on defect-free samples, they often generalize well to anomalies in practice <ref type="bibr" target="#b7">[8]</ref>. An inpainting scheme can be used to effectively hide anomalous regions to further restrict a model's capability to reconstruct anomalies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Since some part of the original image is covered, the reconstruction method needs to have semantic understanding of the image to be able to generate a coherent and realistic image.</p><p>Zavrtanik et al. propose to use a U-Net architecture <ref type="bibr" target="#b38">[39]</ref> taking advantage of long residual connections. Their reconstruction-based method randomly selects multiple parts of the image to inpaint <ref type="bibr" target="#b10">[11]</ref>. The additional use of gradient magnitude similarity (GMS) <ref type="bibr" target="#b39">[40]</ref> in the loss function yields the current state-of-the-art results for anomaly detection via inpainting for different benchmarks <ref type="bibr" target="#b10">[11]</ref>.</p><p>Anomalies which span over a large area may still cause problems as these will not be covered up sufficiently enough. As such we propose to add global context via replacing CNNs with a Transformer-based framework applied in vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transformers in Vision</head><p>Transformer models were originally introduced in NLP and have since evolved to be state-of-the-art design for various sequence tasks like text translation, generation and document classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Usually pretrained on huge amounts of data and afterwards finetuned on specific tasks such as human-like text synthesis impressive results can be achieved <ref type="bibr" target="#b41">[42]</ref>.</p><p>Attention is used to relate elements of a sequence to each other. Based on the relative weighted importance a shared representation is calculated taking into account the relative dependencies between sequence elements. This is able to replace recurrent neural networks in sequence-to-sequence modeling because long-range dependencies are processed globally. In practice a single attention module may fail to capture more complex relations inside a sequence. As such multiple attention passes are run in parallel, concatenated and unified with a linear transformation, resulting in multihead attention <ref type="bibr" target="#b14">[15]</ref>. The encoder and decoder architecture of the Transformer consists of a stack of multiple blocks. Each encoder and decoder block contains a combination of multihead self-attention and fully connected layers while each decoder block additionally processes the encoder output through another attention component. The general architecture can be found in the original work <ref type="bibr" target="#b14">[15]</ref>.</p><p>While Transformer architectures have been widely studied in NLP and sequence modeling, convolutional architectures have been essentially the standard tool in recent years due to weight sharing, translation equivariance and locality. Due to the induced bias in fully convolutional autoencoders, the restricted receptive field limits global context <ref type="bibr" target="#b13">[14]</ref>. Even though in theory the self-attention framework may mitigate this problem, running self-attention on the whole image without further simplifications is not feasible <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. First approaches restricted the neighborhood of pixels considered in the attention module <ref type="bibr" target="#b44">[45]</ref> while Ho et al. proposed axial attention in which the computation is split along the spatial axes.</p><p>Most recently Dosovitskiy et al. have proposed Vision Transformer (ViT) <ref type="bibr" target="#b15">[16]</ref>. The image data is split up into square nonoverlapping uniform patches <ref type="bibr" target="#b15">[16]</ref>. Each patch and position gets embedded into a latent space and every image is treated as a sequence of these embedded patches. A Transformer architecture is applied on the restructured data achieving comparable results to state of the art CNNs and even surpassing them on some tasks while reducing model bias with a more generic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Inpainting Transformer for Anomaly Detection</head><p>Our approach is based on a simple stack of Transformer blocks which are trained to inpaint covered image patches based on neighboring patches. The network is trained on normal samples only. An anomaly score is computed for each pixel based on the difference of the original image and its reconstruction obtained by inpainting all patches. An overview of the method is shown in <ref type="figure">Figure 1</ref>. In the following sections we describe the steps of our method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Embedding Patches and Positions</head><p>We use a similar notation as in <ref type="bibr" target="#b15">[16]</ref>. Let x ∈ R H×W×C be an input image, where (H, W) denotes the (height, width)resolution and C the number of channels of the image. Let K be the desired side length of a square patch and N := H K , M := W K (we resize the image such that K divides H and W). We split the image x into a N × M grid of flattened square patches</p><formula xml:id="formula_0">x p ∈ R (N × M) × (K 2 · C) , where x (i, j) p ∈ R K 2 ·C</formula><p>is the patch in the i-th row and j-th column. Our aim is to choose square subgrids of some side length L in this patch grid and train a network to reconstruct any covered patch in the subgrid based on the rest of the subgrid's patches. Formally, this inpainting problem is as follows:</p><p>Let</p><formula xml:id="formula_1">x (i, j) p (i, j) ∈ S</formula><p>be such a square subgrid ("window") of patches defined by some index set S = {r, . . . , r + L − 1} × {s, . . . , s + L − 1}. Here L is the side length of the window, and (r, s) is the grid position of the window's upper left patch. If (t, u) ∈ S is the position of some patch, the formal task to inpaint (t, u) given S is to approximate the patch x (t,u) p using <ref type="figure">Figure</ref>  As by definition Transformers are invariant with respect to reorderings of the input, we need to encode the positional information of the individual patches. The use of 2-dimensionalaware embeddings does not lead to significant performance gains <ref type="bibr" target="#b15">[16]</ref>, so we define two positional mappings</p><formula xml:id="formula_2">f local : S → N (i, j) → (i − r) · L + j − s + 1<label>(1)</label></formula><p>and</p><formula xml:id="formula_3">f global : S → N (i, j) → (i − 1) · N + j.<label>(2)</label></formula><p>Informally speaking, f global assigns to the patches in the window their 1-dimensional position when starting to count from the upper left patch in the whole image, whereas f local assigns the position "locally" when counting only within the window (see <ref type="figure" target="#fig_0">Figure 2</ref>). Depending on the image domain, local or global positioning can be chosen. The intuition behind this is that in some settings such as textures, the exact position of a patch window within the full image does not play an important role, whereas in other settings, global positions do carry important information.</p><p>To use as a sequence input to the Transformer model, we map the window of patches and their positional information into some latent space of dimension D. For position embeddings we use standard learnable one-dimensional position embeddings posemb : {1, . . . , L 2 } → R D for f local (resp. posemb : {1, . . . , N · M} → R D for f global ). For ease of notation, in the following we assume f = f local is chosen. The global case is analogous.</p><p>For each patch</p><formula xml:id="formula_4">x (i, j) p with (i, j) ∈ S \ {(t, u)</formula><p>} the position embeddings are added to a trainable linear projection via</p><formula xml:id="formula_5">y f (i, j) := x (i, j) p E + posemb( f (i, j)) ∈ R D (3) with weight matrix E ∈ R (K 2 ·C)×D .</formula><p>To account for the patch at position (t, u) to inpaint, we use posemb( f (t, u)) as above and add a single learnable embedding x inpaint ∈ R D , and set</p><formula xml:id="formula_6">z := x inpaint + posemb( f (t, u)) ∈ R D .<label>(4)</label></formula><p>The embedding x inpaint is comparable to the class token in <ref type="bibr" target="#b40">[41]</ref>.</p><p>The vectors in <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_6">(4)</ref> build the final sequence of embedded patches</p><formula xml:id="formula_7">y := [z; y 1 ; . . . ; y L 2 embeddings at S \ {(t, u)} ] ∈ R L 2 ×D (5)</formula><p>which serves as an input sequence of length L 2 to the Inpainting Transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multihead Feature Self-Attention</head><p>Self-attention is the main building block of the Transformer and will be successively applied to the projected patch sequence y in <ref type="bibr" target="#b4">(5)</ref>.</p><p>We briefly revisit multihead self-attention (MSA) as in <ref type="bibr" target="#b14">[15]</ref> before proposing a dimension-reduction extension. For this let</p><formula xml:id="formula_8">y = [y 1 ; . . . ; y L 2 ] ∈ R L 2 ×D be some input sequence of length L 2 .</formula><p>For standard MSA with h heads we compute queries q, keys k and values v via</p><formula xml:id="formula_9">q := yW q , k := yW k , v := yW v ∈ R L 2 ×D ,<label>(6)</label></formula><formula xml:id="formula_10">with learnable weight matrices W q , W k , W v ∈ R D×D . Each vec- tor in the sequence q, k, v is sliced into h pieces of dimension D := D h , i.e. we consider each q, k, v as elements in R h×L 2 ×D . For each head i ∈ {1, . . . , h}, self-attention is computed for the sequences q i , k i , v i ∈ R L 2 ×D as a weighted sum over v i as follows: A := softmax q i (k i ) √ D ∈ R L 2 ×L 2 ,<label>(7)</label></formula><formula xml:id="formula_11">z i := Av i ∈ R L 2 ×D .<label>(8)</label></formula><p>The sequencesz i are concatenated back to a sequence of dimension D which is unified via a linear map to obtain the MSA output</p><formula xml:id="formula_12">MSA(y) = [z 1 ; . . . ;z h ]U ∈ R L 2 ×D ,<label>(9)</label></formula><p>where U ∈ R D×D is another learnable weight matrix. For the inpainting problem MSA can be used as described above. In cases where the patches of the training images are very similar but indistinct the dot product of queries and keys in <ref type="bibr" target="#b6">(7)</ref> and hence the entries of the weighing matrix A are close to each other, leading to an almost uniform weighted sum. To mitigate this, we propose to perform a nonlinear dimension reduction when computing q and k. I.e. instead of only linear maps in (6) we set</p><formula xml:id="formula_13">q := MLP q (y), k := MLP k (y), v := yW v ∈ R L 2 ×D .<label>(10)</label></formula><p>Here</p><formula xml:id="formula_14">MLP q , MLP k : R D → R F</formula><p>are multilayer perceptrons with a single hidden layer with GELU non-linearity (in all our models we used F = D 2 and a hidden layer dimension of 2 · D). The rest of the multihead attention mechanism is applied as in <ref type="formula" target="#formula_10">(7)</ref>, (8), <ref type="bibr" target="#b8">(9)</ref>, except that the vectors in q, k are sliced into h pieces of dimension D := F h and D is replaced with D in <ref type="bibr" target="#b6">(7)</ref>. We refer to this modified MSA as multihead feature self-attention (MFSA). We experienced faster convergence and improved results with MFSA (see Section 4.3.2). However, depending on D and F, the number of learnable parameters increases strongly with MFSA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>Our network architecture for inpainting is composed of a simple stack of n Transformer blocks. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the architecture. The structure of each Transformer block mainly follows <ref type="bibr" target="#b15">[16]</ref> and consists of MFSA followed by a multilayer perceptron (MLP). Layer normalization is applied before ("prenorm" <ref type="bibr" target="#b45">[46]</ref>), and residual connections after MFSA and MLP. Each MLP has a single hidden layer with GELU nonlinearity and maps R D → R 4·D → R D . In particular the input and output of each Transformer block is a sequence in R L 2 ×D (see <ref type="figure" target="#fig_1">Figure  3)</ref>.</p><p>To obtain the inpainted patch, we average over the output sequence of the last Transformer block to get a single vector in R D . This vector is mapped back to the pixel space of the flattened patches R K 2 ·C via a learnable affine transformation. As an alternative to averaging, one could take the first vector of the sequence which corresponds to the patch-to-inpaint in the input sequence.</p><p>In early experiments an inspection of the attention weights showed that a large spatial context is present in earlier layers. In addition to that, Attention Rollout <ref type="bibr" target="#b46">[47]</ref> has been used in <ref type="bibr" target="#b15">[16]</ref> to illustrate that information across the entire input image is integrated already in the lowest layers. In order to carry this early information through deeper self-attention blocks of the network, we put additional long residual connections between early and late layers in a U-Net fashion <ref type="bibr" target="#b38">[39]</ref>. We found that the use of long residual connections leads to slightly more structural detail in the overall reconstruction, but also to more chromatic artifacts in the reconstruction of defective regions, improving detection (see Section 4.3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>The network is trained by randomly sampling batches of patch windows with a fixed side length L from normal image data. In each window a random patch position (t, u) is chosen, which is inpainted by the network as described in the previous sections.</p><p>For the loss function, we compare the original and reconstructed patch with pixel-wise L 2 loss. To account for percep-tual differences, we also include structural similarity (SSIM) <ref type="bibr" target="#b24">[25]</ref> and gradient magnitude similarity (GMS) <ref type="bibr" target="#b39">[40]</ref>. Preliminary for this, given any two images x,x ∈ R H×W×C , we define a gradient difference map</p><formula xml:id="formula_15">d g (x,x) := 1 − 1 3 3 c=1 GMS(x c ,x c ) ∈ R H×W .<label>(11)</label></formula><p>Here 1 is a matrix of ones and GMS(x c ,x c ) ∈ R H×W is the gradient magnitude similarity map for color channel c. We define an analogous difference map for SSIM which we denote by d s . Finally given an original and reconstructed patch x p ,x p ∈ R K×K×C , the full loss function L is given by</p><formula xml:id="formula_16">L(x p ,x p ) = L 2 (x p ,x p ) + α K 2 (i, j)∈K×K d g (x p ,x p ) (i, j) + β K 2 (i, j)∈K×K d s (x p ,x p ) (i, j)<label>(12)</label></formula><p>where α, β are individual scaling parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference and Anomaly Detection</head><p>The inferencing process is divided into two steps: First a complete inpainted image is generated, afterwards the difference between the reconstruction and original is used to compute a pixel-wise anomaly map for localization. The maximum pixel value of the anomaly map is taken as a global anomaly score for detection on image level. Let x ∈ R H×W×C be an input image with an N × M patch grid as introduced above. For each patch position (t, u) ∈ N × M, we choose an appropriate patch window of side length L which is used as a basis to inpaint the patch at position x (t,u) p . In particular we define the window by its upper left patch x (r,s)</p><formula xml:id="formula_17">p with r =g(t) − max(0, g(t) + L − N − 1),<label>(13)</label></formula><formula xml:id="formula_18">s =g(u) − max(0, g(u) + L − M − 1),<label>(14)</label></formula><p>where the map g is given by</p><formula xml:id="formula_19">g(c) = max 1, c − L 2 .<label>(15)</label></formula><p>The above equations simply choose (r, s) such that (t, u) is as much centered in the L × L patch-window as possible. Using this window, the patch x (t,u) p is reconstructed with our model as described. By reconstructing all patches in the N × M grid, we obtain a full reconstructionx of the whole image.</p><p>The generation of an expressive anomaly map from x andx is by itself a nontrivial problem. This is not part of our contributions, in particular for comparability we use a simplified variant of the GMS-based scheme proposed in <ref type="bibr" target="#b10">[11]</ref>. Generally speaking, we compute the gradient magnitude similarity of both images at half and quarter scale, smooth them with an averaging and gaussian blur operation and take the mean of the resulting maps resized to original scale.</p><p>Formally this translates to the following: We write x l for an image x resized to scale l. Now for original and reconstructed images x,x ∈ R H×W×C and scale l ∈ { 1 2 , 1 4 }, we set</p><formula xml:id="formula_20">m l (x,x) := bluravg l (d g (x l ,x l )) ∈ R l·H×l·W<label>(16)</label></formula><p>for a scaled and smoothed version of the gradient difference map d g in (11). To ease notation, we denote by bluravg l the application of an averaging filter followed by some Gaussian blur operation, both with a predefined kernel size and variance. As in <ref type="bibr" target="#b10">[11]</ref>, smoothing improves robustness with respect to small, poorly reconstructed anomalous regions. We resize the two-dimensional maps m 1 2 and m 1 4 back to the original size and take the pixel-wise mean which yields a difference map diff(x,x) ∈ R H×W .</p><p>To finally obtain an anomaly map for x during inference, we take the squared deviation of the difference map to the normal training data, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>anomap(x) :=</head><formula xml:id="formula_21">       diff(x,x) − 1 |T | z∈T diff(z,ẑ)        2 ∈ R H×W ≥0 ,<label>(17)</label></formula><p>where T is the set of normal training samples. As a scalar anomaly score for detection on image level we take the pixelwise maximum anoscore(x) := max</p><formula xml:id="formula_22">(i, j)∈H×W anomap(x) (i, j) ∈ R ≥0 .<label>(18)</label></formula><p>An example of an anomaly map can be seen in <ref type="figure">Figure 1b</ref>.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on the MVTec AD dataset which contains high resolution samples of 5 texture and 10 object categories stemming from manufacturing <ref type="bibr" target="#b0">[1]</ref>. For an overview of all categories we refer to <ref type="table">Table 1</ref>. The dataset has been a widely used benchmark for anomaly detection and localization in the manufacturing domain. Each category consists of around 60 to 400 normal, defect-free samples for training, and a mixture of normal and anomalous images for testing. Additionally for each image there is a ground-truth binary image labeled on pixel-level for segmentation of anomalous test images.</p><p>Based on an image's anomaly score <ref type="bibr" target="#b17">(18)</ref>, we report standard ROC AUC as a detection metric. For localisation, the image's anomaly map <ref type="formula" target="#formula_2">(17)</ref> is used for an evaluation of pixel-wise ROC AUC. Each category is evaluated separately to give an insight of advantages and problems of our method. Additionally we report mean AUC over all categories for detection and segmentation to give a comparison to current state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We train our model on each product category from scratch, with the same following strategy. We randomly choose 10% of images from the normal training data (however a maximum of 20) and use them as a validation set to control the quality of reconstructions. In each epoch 600 patch windows are sampled randomly per image. To augment the dataset, random rotation and flipping is used.</p><p>The choice of three parameters has an obvious significant impact on the performance: Side length K of square patches, side length L of a patch window and the choice of height H and width W (with H = W, as all images are square) to which the original image is resized during training and inference. The patch size determines how much of the image is covered, the size of the patch window determines the dilation of context we include during inpainting, the image size implicitly influences both. For all models we choose K = 16, L = 7. Transformers in vision usually take a long time to train as they lack the inductive bias of CNNs. For the choice of image size in our pipeline, a balance needs to be struck between enlarging the image context of the 7 × 7 window, quality of patch reconstructions and computation time. The heuristics is to choose the image as small as possible while keeping patch reconstructions at a high level of detail. Hence we train the model with image dimensions 256 × 256, 320 × 320, 512 × 512 for 200 epochs and compare the best (epoch-wise) validation losses (averaged over ±5 epochs). If there is no significant improvement in the validation loss of at least 10 −4 for an image dimension with the next in size, the smaller dimension is chosen. Table A.7 shows the resulting image sizes for each category. For comparison and generality this simple pipeline is applied employing the validation set, we note however that in practice K, L, H, W could be tuned for the detection task at hand if prior knowledge about possible defects is present. For the patch position embeddings we use f global as described in Section 3.1. The Inpainting Transformer model trained consists of 13 blocks with 8 feature attention heads each. We set the latent dimension to D = 512. For multihead feature self-attention, MLP q and MLP k have an output dimension of F = 256 and one hidden layer of dimension 1024. In total this amounts to ∼55M learnable parameters.</p><p>For computation of anomaly maps, given an image size of 512 × 512, a kernel size of 21 (resp. 11) is used for averaging and Gaussian blur (with σ = 2) for bluravg 1 2 (resp. bluravg <ref type="bibr">1 4</ref> ) in <ref type="bibr" target="#b15">(16)</ref>. The kernel sizes are simply scaled linearly for smaller image sizes. Before taking the maximum as anomaly score, the anomaly map is resized back to the original high image resolution for proper segmentation comparison. All resizing is performed with bilinear interpolation.</p><p>For the loss function L in <ref type="bibr" target="#b11">(12)</ref> we set α = β = 0.01. The network is trained using the Adam optimizer with a learning Patch-SVDD <ref type="bibr" target="#b16">[17]</ref> RIAD <ref type="bibr" target="#b10">[11]</ref> CutPaste <ref type="bibr" target="#b17">[18]</ref> PaDiM <ref type="bibr" target="#b19">[20]</ref>   rate of 0.0001 and a batch size of 256. Training of the Transformer network may take a long time (&gt; 500 epochs in some cases), depending on the chosen resolution, available data and difficulty of reconstruction. In particular the network is trained until no improvement on the validation loss is observed for 50 consecutive epochs, and the model at the epoch with the best validation loss is chosen for evaluation. Although common for training Transformers, we don't apply dropout at any point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Discussion</head><p>The following section presents the results of our method. In order to give an intuition of how well our approach performs on different tasks, we show the AUC scores for each category while discussing advantages and shortcomings of our model. We compare our method to current publicly available state-ofthe-art results on the MVTec AD benchmark. RIAD <ref type="bibr" target="#b10">[11]</ref> is directly comparable to our approach, as it also uses an inpainting reconstruction-based method and computes anomaly maps which are also based on GMS. In contrast to our approach, RIAD is trained on an image resolution of 256 × 256 for all categories and does not include a differentiation of image sizes in their pipeline.</p><p>Further we consider the embedding-based method Patch-SVDD <ref type="bibr" target="#b16">[17]</ref>. Their patch-based extension of support vector data description has been seen as state-of-the-art on MVTec AD until recently. In our comparison we also include most recent works such as PaDiM <ref type="bibr" target="#b19">[20]</ref> (see Section 2) and CutPaste <ref type="bibr" target="#b17">[18]</ref>. We note that PaDiM is based on pretrained CNNs using additional training data and is therefore not directly comparable to our method, however provides a good reference as it is to our knowledge the currently best performing model on MVTec AD both in detection and segmentation. The recent work CutPaste uses a special data augmentation strategy to train a one-class classifier in a self-supervised way. CutPaste also offers results using pretrained representations, however we focus on the results without extra training data in accordance to our training procedure. Also note that we use their best performing single model for a fair comparison instead of their ensemble. <ref type="figure" target="#fig_2">Figure 4</ref> shows a selection of qualitative results with reconstructions and anomaly maps. An extended selection can be seen in <ref type="figure" target="#fig_3">Figure A.5</ref>   The results for detection are reported in <ref type="table">Table 1</ref>. For texture categories we observe perfect scores for grid, which shows very structured patterns, and leather, which exhibits anomalies which are easily detected as our method learns to inpaint the leather pattern sufficiently well. The worst performing category with 98% AUC is wood, which can be attributed to hard test cases with color-based anomalies. Although there is an obvious visual difference between reconstruction and original, it does not lead to a sufficient difference in gradient magnitude. This can be seen in the first row of <ref type="figure" target="#fig_5">Figure A.7</ref>. We perform well on the category tile which exhibits heavy random patterns. This makes a good choice of the input image size in our pipeline indispensable as a masked patch may not cover too much of the pattern. Aggregated over all texture categories we see an overall improvement of 4.46% over Patch-SVDD and 3.9% over RIAD. Additionally we can outperform CutPaste and achieve on par results of 99.0% AUC with PaDiM while maintaining more restrictive requirements.</p><p>For object categories there are two underperforming categories: Cable contains many anomalous images where the defect lies in the overall constitution of the product (such as missing pieces). Combined with noise in large areas this makes these anomalies hard to detect via inpainting. Even though Patch-SVDD achieved 90.3% AUC most recent methods struggle as well, e.g. CutPaste reports an AUC of 81.2%. Although the defects in capsule are per-se easily visible on the generated anomaly maps, our method does not learn the reconstruction of the writing sufficiently well, leading to high anomaly scores also on normal samples. In effect we observe a worse performance in comparison to the other categories. An example is shown in <ref type="figure" target="#fig_5">Figure A.7</ref>.</p><p>Our method works well on structured, aligned data such as toothbrush, zipper and bottle. It clearly has difficulties with learning to reconstruct noisy areas such as the heads in hazelnut, leading to suboptimal results as seen in the third row of <ref type="figure" target="#fig_5">Figure A.7</ref>.</p><p>In summary on object categories our method performs on par with the current state-of-the-art which is reflected in the overall mean object AUC of 94.41% surpassing Patch-SVDD, RIAD and CutPaste.</p><p>Averaged over texture and object categories we report 95.94% AUC outperforming both Patch-SVDD and RIAD with a difference of 3.84% and 3.24% respectively. Additionally we can see an absolute improvement of 0.74% in comparison to the current state-of-the-art results of CutPaste while PaDiM still achieves the best detection results which might be explained by the additional training data.</p><p>The performance on the segmentation task is shown in <ref type="table" target="#tab_2">Table  2</ref>. Our method outperforms the current state-of-the-art methods on all texture categories except wood which we attribute to the same reason as in detection above. The same problem also affects tile.</p><p>In addition to cable, the localization performance for metal nut is not on par. The reconstructions for flipped metal nuts are too close to the original, so not every pixel in the object is assigned a high anomaly score. Averaged over all categories, our method achieves a mean AUC of 96.98% leading to an absolute improvement of 0.98% over CutPaste, also outperforming Patch-SVDD and RIAD. We can see from the segmentation results that by integrating information from distant patches into the inpainting problem, even large anomalous regions are localized well. Using InTra we have been able to obtain both very good detection and localization performance on the MVTec AD benchmark without extra training data while surpassing state-of-theart approaches on detection and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In the following we examine the influence of certain changes in the architecture. All chosen categories are trained for 200 epochs with settings as described in Section 4.1, if not stated otherwise. As before, all results are reported in ROC AUC, using for evaluation the epoch with best validation loss. We note that training for only 200 epochs for most categories does not lead to results comparable to <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Long residual connections</head><p>Long residual connections are used to add the output of earlier Transformer blocks to the input of later blocks (see <ref type="figure">Figure</ref> 3). We train two object categories (toothbrush, which is aligned, and hazelnut) and three texture categories without residual connections between the blocks. The results are reported in <ref type="table">Table 3</ref>, for examplary qualitative results see <ref type="figure">Figure  A</ref>.8. The aim is to benefit from the already large context of early self-attention in later layers. Comparable to what can be seen in U-Net, generally more structure can be found in the reconstructions when using long residual connections, which is helpful for low-level features. They do not help in noisy areas which are per-se difficult to inpaint due to noisy patterns, such as the head of hazelnuts.  <ref type="table">Table 3</ref>: AUC score for detection and segmentation for the architecture with and without long residual connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Feature Self-Attention</head><p>We examine the effect of using multihead feature selfattention (MFSA) and regular multihead self-attention (MSA) as described in Section 3.2. The results are listed in <ref type="table">Table 4</ref>. Except in one case, we see no significant quantitative gain for texture categories. Qualitatively, we found that learning a dimension reduction on the normal data before computing selfattention leads to a slighty lower validation loss, more detailed reconstructions and better retouching of defective areas (see <ref type="figure" target="#fig_7">Figure A.9</ref>). In effect we see a performance gain for object categories where consistent retouchings of defects have stronger segmentation implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Patch Position Embedding</head><p>We examine the effect of embedding the local position of a patch in a window with f local versus embedding the position with respect to the full image with f global (see Section 3.1). For categories such as textures, where the exact position of a patch window is not important, we experienced slightly faster training with f local . It takes ∼20 epochs longer until first details in the reconstructions start to show and the training loss drops significantly. There is however no difference in the final quantitative  <ref type="table">Table 4</ref>: AUC score for detection and segmentation using multihead feauture self-attention (MFSA) and regular multihead self-attention (MSA).</p><p>results. This comes as expected, as it takes a while until all global position embeddings have adjusted. Also as expected, the reverse is not true: Using f local in cases where the position of a patch within the whole image does carry important information, the performance drops significantly. See <ref type="table">Table 5</ref> for results on two such categories and <ref type="figure">Figure A.</ref>  <ref type="table">Table 5</ref>: AUC score for detection and segmentation using f local ("Local") and f global ("Global").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Patch Window Size</head><p>We test how the side length L of a patch window influences the performance, using L ∈ {5, 7, 9}. We can see in <ref type="table">Table 6</ref> that detection and segmentation improve with growing patch windows, as more information from distant pixels can be used for the inpainting task. For a resolution of 256 × 256 pixels, a 5×5 window of patches with side length 16 already covers about one third of the image, and more than half of the image for a 9× 9 window. This confirms the hypothesis that during inpainting context from distant patches is used, and not only the immediate neigborhood, resulting in improved detection and segmentation capabilities. It comes with high computational cost however, as the length of the patch sequence for the Transformer model is quadratic in L, and the computation of the dot product in <ref type="formula" target="#formula_10">(7)</ref> is quadratic in the sequence length.  <ref type="table">Table 6</ref>: AUC score for detection and segmentation using different dimensions of the patch window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Inspired by the success of using attention-only methods in vision tasks, we have successfully used a Transformer model for visual anomaly detection by using an inpainting reconstruction approach while considering embeddings of patch sequences as input. We argued that by discarding convolutions and using only self-attention to incorporate global context into reconstructions, anomalies can be successfully detected and localized. As in natural language processing, the position embeddings of patches play a significant role, especially in cases where positional information is important to understand the global structure of objects. For our task, applying a nonlinear dimension reduction before computing self-attention has shown to improve retouching of anomalies. In addition we adopted long residual connections in our architecture to take advantage of low-level features and their context in later Transformer blocks.</p><p>Although Transformer models are usually trained on large amounts of data, our model is trained from scratch on the basis of only 60-400 images, which is sufficient to detect irregularities with a high confidence. Hyperparameters such as the input image size, patch sequence length and patch dimension have a strong impact on the overall performance, and including the detection of good values for them in the training pipeline is paramount. With a simple pipeline as proposed, we have shown that InTra can reach and outperform state-ofthe-art results on the popular MVTec AD dataset. With more prior knowledge about potential defects and more computation time, the results might even be improved, possibly by largescale pretraining. Further extensions may include the usage of hidden self-attention output embeddings in the anomaly map generation leading to a hybrid reconstruction-embedding based approach. As our anomaly map calculation is fairly basic we propose that a thorough investigation may improve the results even more.    In the first row a test sample of wood is shown where color-based anomalies are not detected correctly due to only small differences in gradient magnitude. Second row: As our model is not able to reconstruct certain text areas sufficiently enough, in certain cases also good samples are assigned strong anomaly maps. Third row: Difficult reconstructions of noisy areas such as the head of a hazelnut leads to incorrect results in the anomaly map.  In each row, the left column shows an original, reconstruction and anomaly map when using multihead feature self-attention (MFSA). The right column shows results for the same images using regular multihead self-attention. Anomalies are covered slightly better using MFSA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Experiment Details</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 :</head><label>2</label><figDesc>Illustration of f local (left) and f global (right) for a patch window (red) with L = 3. only the content and positions of all other patches x (i, j) p S \{(t,u)} in the window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the proposed architecture. Left: Parts of an individual Transformer block. Right: A stack of Transformer blocks builds the full architecture. Long residual connections are used to add information from earlier blocks to later ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Examples of anomalies for different categories, each presented in a group of three images. For each group the original image is displayed on the left. The center image shows the reconstruction while the right image presents the calculated anomaly map, where the anomaly score for each pixel is linearly scaled by a factor of 50. Anomalous regions are displayed in brighter color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A. 5 :</head><label>5</label><figDesc>Qualitative results on MVTec AD for our method across different categories. Each row shows examples of one category, each column a group of three images with original (left), reconstruction (center), anomaly map (right). The two left columns show examples of anomalous test images, the rightmost column shows an example of a good test image. Categories from top to bottom: carpet, grid, leather, tile, wood, bottle, cable, capsule. The rest of the categories are shown in Figure A.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A. 6 :</head><label>6</label><figDesc>Examples of qualitative results continued from Figure A.5. Categories from top to bottom: hazelnut, metal nut, pill, screw, toothbrush, transistor, zipper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A. 7 :</head><label>7</label><figDesc>Examples where our model fails to properly compute anomaly maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A. 8 :</head><label>8</label><figDesc>Reconstruction examples using long residual connections. The left column of the first two rows shows two examples from wood using long residual connections in the architecture. The right column shows results for the same images without long residual connections. Anomalies are covered slightly better on the left hand side, which shows in the anomaly maps. Third row: Reconstruction examples from toothbrush. In both columns from left to right: Original, with long residual connections, without long residual connections. Reconstructions in the center show more detailed low-level structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A. 9 :</head><label>9</label><figDesc>Reconstructions and anomaly maps from hazelnut using different types of self-attention. The defects in the samples stretch over a larger object region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A. 10 :</head><label>10</label><figDesc>Reconstructions and anomaly maps from transistor (top row) and metal nut (bottom row) using global (left column) and local (right column) position embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Segmentation results for MVTec AD. Results are presented in pixel-wise ROC AUC %.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Category Det. 5 × 5 Det. 7 × 7 Det. 9 × 9 Seg. 5 × 5 Seg. 7 × 7 Seg. 9 × 9</figDesc><table><row><cell>Hazelnut</cell><cell>91.1</cell><cell>91.0</cell><cell>92.5</cell><cell>96.4</cell><cell>97.3</cell><cell>97.3</cell></row><row><cell>Wood</cell><cell>96.1</cell><cell>96.8</cell><cell>97.3</cell><cell>89.4</cell><cell>89.9</cell><cell>90.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattleger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning for medical anomaly detection -a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02364</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="DOI">10.5220/0007364503720380</idno>
		<ptr target="http://dx.doi.org/10.5220/0007364503720380" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<meeting>the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Anomaly Detection for Skin Disease Images Using Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01349</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1145/3422622</idno>
		<title level="m">Generative adversarial networks, Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59050-9_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-59050-9_12" />
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging -25th International Conference</title>
		<meeting><address><addrLine>Boone, NC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-06-25" />
			<biblScope unit="volume">10265</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ganomaly: Semisupervised anomaly detection via adversarial training, in: Asian Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Detecting anomalous faces with &apos;no peeking&apos; autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno>ArXiv abs/1802.05798</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly detection using deep learning based image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tabatabai</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2018.00201</idno>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reconstruction by inpainting for visual anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Skočaj</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107706</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2020.107706.URLhttps://www.sciencedirect.com/science/article/pii/S0031320320305094" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">107706</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised region-based anomaly detection in brain MRI with adversarial image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01942</idno>
		<ptr target="https://arxiv.org/abs/2010.01942" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Patch svdd: Patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04015</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padim</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-68799-1_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-68799-1_35" />
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition. ICPR International Workshops and Challenges -Virtual Event</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12664</biblScope>
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/ruff18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Anomaly detection of defects on concrete structures with the convolutional autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aei.2020.101105</idno>
		<ptr target="https://doi.org/10.1016/j.aei.2020.101105.URLhttps://www.sciencedirect.com/science/article/pii/S1474034620300744" />
	</analytic>
	<monogr>
		<title level="j">Advanced Engineering Informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">101105</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yairi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2689746.2689747</idno>
		<ptr target="https://doi.org/10.1145/2689746.2689747" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis, MLSDA&apos;14</title>
		<meeting>the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis, MLSDA&apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11723-8_16</idno>
		<idno>169doi:10.1007/ 978-3-030-11723-8_16</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-030-11723-8_16" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page">161</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cremers, q-space novelty detection with variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meissner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sgarlata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tomassini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Diffusion MRI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="113" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery, in: Information Processing in Medical Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akçay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2019.8851808</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10445</idno>
		<title level="m">Deep nearest neighbor anomaly detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Same same but differnet: Semisupervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<idno>doi:10. 1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18010209</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks</title>
		<imprint>
			<publisher>BMVC</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efficientnet</forename></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene completion using millions of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073659</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073659" />
		<title level="m">Globally and locally consistent image completion</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Convolutional networks for biomedical image segmentation</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2013.2293423</idno>
		<idno>doi:10.1109/ TIP.2013.2293423</idno>
		<ptr target="https://doi.org/10.1109/TIP.2013.2293423" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Image</forename><surname>Transformer</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/parmar18a.html" />
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1176</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.385</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.385" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4190" to="4197" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
