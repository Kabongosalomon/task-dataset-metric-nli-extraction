<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and nonisomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parameterizing kernels in the spectral domain spanned by graph laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strive to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parameterization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested our SyncSpecCNN on various tasks, including 3D shape part segmentation and 3D keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As has already happened in the image domain, the wide availability of 3D models brings with it the need to associate semantic information with the 3D data. In this work we focus on the problem of annotating 3D models represented by 2D meshes with part information. Understanding of the parts of an object (e.g., the back, seat and legs of a chair) is essential to its geometric structure, to its style, and to its function. There has been significant recent progress <ref type="bibr" target="#b27">[28]</ref> in the large scale part annotation of 3D models (e.g., for a subset of the ShapeNet <ref type="bibr" target="#b3">[4]</ref> models) -our aim here is to leverage this rich data set so as to infer parts of new 3D object models. Our techniques can also be used to infer keypoints and other substructures within 3D models.  Our SyncSpecCNN takes a shape graph equipped with vertex functions (i.e. spatial coordinate function) as input and predicts a per-vertex label. The framework is general and not limited to a specific type of output. We show 3D part segmentation and 3D keypoint prediction as example outputs here.</p><p>It is not straightforward to apply traditional deep learning approaches to 3D models because a mesh representation can be combinatorially irregular and does not permit the optimizations exploited by convolutional approaches, such as weight sharing, which depend on regular grid structures. In this paper we take a functional approach to represent information about shapes, starting with the observation that a shape part is itself nothing but a 0-1 indicator function defined on the shape.</p><p>Our basic problem is to learn functions on shapes. We start with example functions provided on a given shape collection in the training set and build a neural net that can infer the same function when given a new 3D model. This suggests the use of a spectral formulation, based on a dual graph representing the shape, yielding bases for a function space built over the mesh.</p><p>With this graph representation we face multiple challenges in building a convolutional neural architecture. One is how to share coefficients and conduct multiscale analysis in different parts of the graph for a single shape. Another is how to share information across related but different shapes that may be represented by very different graphs. We introduce a novel architecture, the Synchronized Spectral CNN (SyncSpecCNN) to address these issues.</p><p>The basic architecture of our neural network is similar to the fully convolutional segmentation network of <ref type="bibr" target="#b15">[16]</ref>, namely, we repeat the operation of convolving a vertex function by kernels and applying a non-linear transformation. However, our network combines processing in the primal and dual (spectral) domains. We deal with the problem of weight sharing among convolution kernels at different scales in the primal domain by performing the convolutions in the spectral domain, where they become just pointwise multiplications by the kernel duals. Our key building block consists of passing to the dual, performing a pointwise multiplication and then returning to the primal representation in order to perform an appropriate non-linear step (such operations are not easily dualized).</p><p>The issue of information sharing across shapes is more challenging. Since different shapes give rise to different nearest neighbor graphs on their point clouds, the eigenbases we get for the graph laplacians are not directly comparable. We synchronize all these laplacians by applying a functional map in the spectral domain to align them to a common canonical space. The aligning functional maps succeed in encoding all the dual information on a common set of basis functions where global learning takes place. An initial version of the aligning maps is computed directly from the geometry and then is further refined during training, in the style of a data-dependent spatial transformer network.</p><p>We have tested our SyncSpecCNN on various tasks including 3D shape part segmentation and 3D keypoint prediction. We achieve state-of-the-art performance on all these tasks.</p><p>Key contributions of our approach are as follows:</p><p>• We are the first to target at non-isometric shapes in the family of spectral CNNs.</p><p>• To allow weight sharing across different non-isometric shapes, we learn a Spectral Transformer Network.</p><p>• We introduce an effective spectral multiscale kernel construction scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>3D Shape Segmentation An important application of our framework is to obtain semantic part segmentation of 3D shapes in a supervised fashion. Along this track, most previous methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref> employ traditional machine learning techniques and construct classifiers based on geometric features. In the domain of unsupervised shape segmentation, there is one family of methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> emphasizes the effectiveness of spectral analysis for 3D shape segmentation. Inspired by this, our framework aims to marry the powerfulness of deep neural network and spectral analysis for 3D shapes segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral analysis on graphs</head><p>We model a 3D shape S as a graph G = (V, E), whose vertices V are points in R 3 and edges E connect nearby points. At each vertex of the graph, we can assign a vector. In this way, we define a vectorvalued vertex function on G. For example, a segment on a shape can be represented as an indicator vertex function. this is the functional view of segmentation, as introduced in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. The space of functions F defined on G can be represented under different bases, i.e., f = i α i b i for f ∈ F. One way to construct bases of F is through spectral analysis -for each shape graph, the eigen vectors of its graph laplacian L form an orthogonal bases B = {b i }. One type of graph laplacian can be constructed as</p><formula xml:id="formula_0">L = I − D −1/2 W D −1/2 , where I is identity matrix, D</formula><p>is the degree matrix and W is the adjacency weight matrix of G. Under this construction, the eigenvalues λ = {λ i } corresponding to B satisfy 0 ≤ λ i ≤ 2.</p><p>As is in Fourier analysis, the spectral decomposition also introduces the concept of frequency. For each basis b i , the eigenvalue λ i in the decomposition defines its frequency, depicting its smoothness. By projecting f on each basis b i , the coefficient α i can be obtained. α = {α i } is the spectral representation of f , in analogy to the Fourier transform. The convolution theorem of Fourier analysis can be extended to the laplacian spectrum: the convolution between a kernel and a function on the shape graph is equivalent to the point wise multiplication of their spectral representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. Functional map Different shapes define shape graphs with varied bases and spectral domains, which results in incomparable graph vertex function. Inspired by the recent work on synchronization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, we propose to align these different spectral domains using functional map <ref type="bibr" target="#b19">[20]</ref>. Functional map is initially introduced for this purpose on shapes. Specifically, given a pair of shape graph G i and G j , a functional map from F i to F j is given by a matrix X ij , which maps a function f ∈ F i with coefficient vector α to the function f ∈ F j with coefficient vector α = X ij α. α and α are computed according to a pair of bases. We refer the reader to <ref type="bibr" target="#b19">[20]</ref> for detailed introduction and intuition. CNN on Graphs We call such CNNs as "graph CNNs". Graph CNNs takes a graph with vertex function as input. Conventional image CNN can be viewed as a graph CNN on 2D regular grids of pixels, with RGB values as the vertex function. There have been some previous work studying graph CNN on more general graphs instead of 2D regular grids <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b4">5]</ref>, and <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref> have a special focus on near-isometric 3D shape graphs like human bodies. To generalize image CNN, These work usually tries to tackle the following three challenges: defining translation structures on graphs to allow parameter sharing; designing compactly supported filters on graphs; aggregating multiscale information. Their constructions of deep neural network usually fall into two types: spatial construction and spectral construction. The approach we propose belongs to the family of spectral construction but with two key differences: we explicitly design an effective multi-scale information aggregation scheme; we synchronize different spectral domains to allow parameter sharing among very different shape graphs thus increasing generalizability of our SyncSpecCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem</head><p>Given a 3D shape S represented as a shape graph G = (V, E), we seek for a per-vertex label l, such as segmentation or keypoints. These labels are represented as vertex functions f on G, i.e., f : V → R K . We precompute a set of 3D features for each vertex v ∈ V and use them as input vertex functions. These features capture location, curvature, and local context properties of each vertex v and we use the publicly available implementation <ref type="bibr" target="#b11">[12]</ref>. To represent the functional space on shape graphs G, we also construct the graph laplacian L of each shape S, compute the spectral frequency λ = {λ i } and corresponding bases B = {b i } through eigendecomposition. We note that a basis b i is also a vertex function. Therefore, our neural network takes the laplacian L of a graph G and vertex functions of local geometric features as input, and predicts a vertex function f such as segmentation or keypoint indicator function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>The basic architecture of our SyncSpecCNN is similar to the fully convolutional segmentation network as in <ref type="bibr" target="#b15">[16]</ref>, namely, we repeat the operation of convolving the vertex function by kernels and applying non-linear transformation. However, we have several key differences. First, we achieve convolution by modulation in the spectral domain. Second, we parametrize kernels in the spectral domain following a dilated fashion, so that kernel sizes could be effectively enlarged to capture large context information without increasing the number of parameters. Last, we design a Spectral Transformer Network to synchronize the spectral domain of different shapes, allowing better parameter sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>Similar to conventional CNN, our SyncSpecCNN contains layers including ReLU, DropOut, 1×1 Convolution <ref type="bibr" target="#b22">[23]</ref>, and BatchNormalization, which all operate in the spatial domain on graph vertex functions. The difference comes from our graph convolution operation, which introduces the following modules: Forward Transform, Backward Transform, Spectral Multiplication, and Spectral Transformer Network, as is shown in <ref type="figure">Figure 2</ref> and summarized in <ref type="table">Table 1</ref> We provide more details about the newly introduces modules as below.</p><p>In a basic convolution block, a vertex function f defined on G is first transformed into its spectral representation α through Forward Transform α = B T f . Then the functional map C predicted by the Spectral Transformer Network will be applied to α and outputs α = Cα for spectral domain synchronization (Sec 4.4). A Spectral Multiplication layer is followed, pointwisely multiplying α by a set of multipliers and gettingα = W α , where W is a diagonal matrix with its diagonal being the set of multipliers, andα is used to denote the multiplication result. This is how we conduct convolution in the spectral domain, where spectral dilated kernels are used to capture multiscale information (Sec 4.3). Then we apply the inverse functional map C inv toα , so that we get the spectral representationα = C invα in the original spectral domain before canonicalization.α is then converted back to a graph vertex function through Backward Transformf = Bα. This building block was repeated for several times and forms the backbone of our deep architecture. We also add skip links into our SyncSpecCNN to better facilitate information flow across earlier and later layers.</p><p>One interesting observation is worth mentioning: small convolution kernels correspond to smoothly transiting multipliers in the spectral domain, therefore not very sensitive to bases misalignment among shapes graphs in a certain range of spectrum and are more generalizable across graphs. As a result, we omit the spectral transformer network when the convolution kernels are small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Spectral Dilated Kernel Parameterization</head><p>Yu et al. <ref type="bibr" target="#b28">[29]</ref> has proved the effectiveness of multi-scale kernels for aggregating context information at different scales in the task of image segmentation. They propose to use dilated kernels to increase the kernel size without increasing the number of parameters. We parametrize our convolution kernels in a similar flavor but in the spectral domain, which turns out to be straightforward and effective. Essentially, we find that multi-resolution analysis on graphs could be achieved without complicated hierarchical graph clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2.</head><p>Architecture of our SyncSpecCNN. Spectral convolution is done through first transforming graph vertex functions into their spectral representation and then pointwise modulating it with a set of multipliers. The multiplied signal is transformed back to spatial domain to perform nonlinear operations. We introduce spectral transformer network to synchronize different spectral domains and allow better parameter sharing in spectral convolution. Convolution kernels are parametrized in a dilated fashion for effective multi-scale information aggregation.</p><p>Before explaining what the exact parametrization is, we first discuss the intuition behind our design. The Spectral Multiplication layer modulates the spectral representation α = {α i } by a set of multipliers from the kernel, where α i is the spectral coordinate of vertex function at basis b i . Note that λ i can be interpreted as the frequency of its corresponding eigenbasis b i , and b i itself is a vertex function that captures the intrinsic geometry of the shape. We assume that λ i 's are sorted ascendingly and arrange b i 's accordingly.</p><p>The multiplers are the spectral representation of convolution kernel. Denote the set of multipliers as m = {m i }, each corresponds to one λ i . Regard m as a function of λ i .</p><p>Again, generalized from conventional Fourier analysis, if m is concentrated in the low-end of the spectrum, the corresponding spatial kernel function is smooth; conversely, if the corresponding spatial functions is localized, m is smooth. Therefore, to obtain a smoother kernel function as in <ref type="bibr" target="#b28">[29]</ref>, we constrain the bandwidth of m, enabling us to learn a smaller number of parameters; in addition, varying the smoothness of m would control the kernel size.</p><p>To be specific, we associate each Spectral Multiplication layer with a dilation parameter γ and parameterize m i as a combination of some modulated exponential window functions, namely</p><formula xml:id="formula_1">m i = n j=0 ω 2j+1 e −jγλi cos(jγλ i π) + n j=1 ω 2j e −jγλi sin(jγλ i π)</formula><p>Here ω is a set of 2n + 1 learnable parameters, n is a hyper-parameter controlling the number of learnable parameters. Large γ corresponds to rapidly changing multipliers with small bandwidth, thus a smooth kernel with large spatial support. On the other hand, small γ corresponds to slowly changing multipliers with large bandwidth, corresponding to kernels with small spatial support. Instead of using an exponential window only, we add sin/cos modulation to increase the expressive power of the kernel. <ref type="figure" target="#fig_2">Figure 3</ref> shows a visualization of modulated exponential window function with different dilation parameter. Our parametrization has three main advantages: First, it allows aggregating multi-scale information since the size of convolution kernels vary in different layers; Second, large kernels could be easily acquired with a compact set of parameters, which effectively increases the receptive field while mitigates overfitting; Third, reduced parameters allow more efficient computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Spectral Transformer Network</head><p>As is shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the same spectral parametrization of kernels could lead to very different vertex functions when the underlying spectral domains are different. This problem is especially prominent when the kernel size is large. Therefore, being able to synchronize different spectral domains is the key to allow large kernels sharing parameters across different shape graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Basic idea</head><p>According to <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b23">[24]</ref>, one way to synchronize the spectral domains of a group of shapes is through a tool named functional map. In the functional map framework, one can find a linear map to pull the spectral domain of each individual shape to a canonical space, so that representations in the individual spectral domains become comparable under a canonical set of bases. Indeed, given each shape S, this linear map is as simple as a matrix C, which linearly transforms the spectral representation α on one shape to its counterpart α in the canonical space. Note that, from the synchronization in the spectral domain, one induces a spatial correspondence on the graph, vice versa. Viewing the spectral domain as the dual space and spatial domain on graph as the primal space, this primaldual relationship is the pivotal idea behind functional map.</p><p>Inspired by this idea, we design a Spectral Transformer Network (SpecTN) for the spectral domain synchronization task. Our SpecTN takes a shape S as input and predicts a matrix C for it (see <ref type="figure">Figure 2</ref>), so that α = Cα. Thus, without SpecTN, α will be directly passed to subsequent modules of our network; with SpecTN, α will be passed. In <ref type="figure" target="#fig_3">Figure 4</ref>, we show an example of how different spectral domains are synchronized after applying the linear map C predicted from our SpecTN.</p><p>Our SpecTN draws inspiration from Spatial Transformer Network (STN) <ref type="bibr" target="#b9">[10]</ref>. From a high level, both SpecTN and STN are learned to align data to a canonical form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Input to SpecTN</head><p>A proper representation for shape S is needed as the input to our SpecTN. To allow SpecTN predicting a transform between different spectral domains, certain depiction about the underlying spectral domain is greatly helpful, i.e. graph laplacian eigenbases in our setting. In addition, since spectral synchronization couples with graph alignment, providing rough shape graph correspondences could facilitate good prediction.</p><p>Based on these, we use voxel functions B v that is computed from laplacian eigenbases as the input to SpecTN: C = SpecTN(B v ; Θ). Specifically, B v is a volumetric reparameterization of the graph laplacian eigenbases B, defined voxel-wise in 3D volumetric space. The volumetric reparameterization is conducted by converting graph vertex function B into voxel function B v in a straightforward manner -we simply assign a vertex function value to the voxel where the vertex lies. Since all B v live in the same 3D volumetric space, correspondences among them are associated accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Optimization of SpecTN</head><p>Ideally, SpecTN should be learned automatically along with the minimization of the prediction loss, as the case in STN; however, in practice we find that such optimization is extremely challenging. This is because the parameters of C in SpecTN is quadratic w.r.t the number of spectral bases, hundreds of times more than in the affine transformation matrix of STN.</p><p>We address this challenge from three aspects: limit our scope to a reduced set of prominent spectral bases to curtail the parameters of C; add regularization to constrain the optimization space; smartly initialize SpecTN with a good starting point.</p><p>Reduced bases Synchronizing the whole spectrum could be a daunting task given its high dimensionality. In particular, free parameters in C grows quadratically as the dimension of spectral domain increases. To favor optimization, we adopt a natural strategy that only synchronizes the prominent part of the spectrum. In our case, the spectral parametrization of large kernels are mainly determined by the low-frequency end of the spectrum, indicating that the synchronization in this part of spectrum is sufficient. In practice, we synchronize the top 15 bases sorted by the frequency. This idea has been verified to be effective by <ref type="bibr" target="#b19">[20]</ref>.</p><p>Regularization Regularizations are used during training to force the output C of SpecTN to be close to an orthogonal map, namely, in the overall loss function we add a term CC T − I 2 F . With this regularization, C T can be used to approximate the inverse map. Such a maneuver is more friendly to differentiation and easier to train.</p><p>Initialization by precomputed functional map Given the huge optimization space and the non-convex objective, a good starting point helps to avoid optimization from getting stuck in bad local minima. As stated above, our linear transformation C can be interpreted as a functional map; therefore, it is natural for us to initialize C accordingly and then refine it to better serve the end-task. To this end, we first precompute a set of function maps C pre for each shape by an external routine, which roughly align each individual spectral domain of S to a canonical domain. Then we pretrain the SpecTN separately in a supervised manner:</p><formula xml:id="formula_2">minimize Θ i SpecTN(B v,i ; Θ) − C pre,i 2</formula><p>where i indexes shapes. This pretrained SpecTN is plugged into the SyncSpecCNN pipeline and fine-tuned while optimizing a specific task such as shape segmentation. Validated by our experiment, the pretraining step is crucial. Next we introduce how the external routine precomputes a functional map for some shape S. This functional map aligns the spectral domain of S to a canonical one of an "average" shapeS. So we start from the construction of the "average" shape and then proceed to the computation of the functional map.</p><p>The geometry ofS is not generated explicitly. Instead, S is represented by its volumetric adjacency matrixW v , which depicts the connectivity of voxels in the volumetric space that all shapes are voxelized.W v is obtained by averaging the volumetric adjacency matrices W v of all shapes. The W v for each shape S is the adjacency matrix of the corresponding volumetric graph, whose vertices are all the voxels and edges indicate the adjacency of occupied voxels in the volumetric space.</p><p>The functional map C from S toS could be induced from the spatial correspondences between S andS, by the primal-dual relationship <ref type="bibr" target="#b19">[20]</ref>. Since we already have the bases of S andS, as well as the rough spatial correspondences between them from the volumetric occupancy, this map can then be discovered by the approach proposed in <ref type="bibr" target="#b19">[20]</ref>. To be specific, we use B v to denote the volumetric reparametrization of graph laplacian eigenbases B for each shape S, and useB v to denote the grahp laplacian eigenbases ofS. B v andB v both lie in the volumetric space and their spatial correspondence is natural to acquire. The functional map C pre aligning B v withB v could be computed through simple matrix multiplication C pre =B T v B v . The computed functional map will serve as supervision and SpecTN is pretrained to minimize the loss function ||C − C pre || 2 F . It is worth mentioning that, if the shapes under consideration are diverse in topology and geometry, i.e. shapes from different categories, aligning every shape to a single "average" shape might cause unwanted distortion. Therefore we leverage multiple "average" shapes {S i } n i=1 and use a combination of their spectral domains as the canonical domain. Specifically, we assign each shape S to its closest "average" shape under some global similarity measurement (i.e. lightfield descriptor) and use {a i } n i=1 to represent such assignment, namely a i = 1 if S is assigned toS i and a i = 0 otherwise. Also we useB vi to denote the spectral bases of S i . Then the functional map C pre for each shape S could be computed through C pre = [a 1Bv1 a 2Bv2 ... a nBvn ] T B v . The SpecTN is pretrained to predict a functional map which only synchronizes spectral domain of each shape to its most similar "average" shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Implementation Details</head><p>In most of our experiments, input shapes are represented as point cloud with around 2000 − 3000 points. Given an input shape point cloud, we build a k-nearest neighbor graph G first. We use k = 6 in all our experiments. Then a graph weight matrix W could be constructed in which</p><formula xml:id="formula_3">W i,j = 1 d 2 i,j</formula><p>if point i and j are connected, 0 otherwise. We then compute the symmetric normalized graph laplacian L as L = I − D −1/2 W D −1/2 , where D is the degree matrix and I denotes identity matrix. Since many natural functions we care about could be depicted by a small number of low-frequency laplacian eigenbases, we compute and use the smallest 100 eigenvalues as well as the corresponding eigenbases for each L in all our experiments.</p><p>The choice of dilation parameters γ, number of output channels after each convolution layer, number of learnable parameters in each convolution kernel are shown in <ref type="table">Table 1</ref>. We choose c = 50 in all of our experiments. As is mentioned, we only consider the problem of synchronizing the low-frequency end of different spectral domains, so we choose to predict a functional map C ∈ R 15×45 in our experiments, which maps the first 15 eigenbases of each individual spectral domain into a canonical domain of dimension 45. Notice the dimension of canonical domain is larger that each individual domain to allow very different shapes to be mapped into different subspaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>Our proposed SyncSpecCNN takes one graph vertex function as input and predicts another as output. As a generic framework, the prediction is not limited to a specific type of graph vertex function and can be tailored towards different goals. To evaluate the effectiveness of our framework, we divide our experiments into five parts. First, we evaluate on a benchmark of 3D shape segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>. Second, we evaluate on keypoint prediction task using a new large scale keypoint annotation dataset. Third, we leverage SyncSpecCNN to learn vertex normal functions and visualize the prediction results qualitatively. Fourth, we perform control experiments to compare different design choices of the framework and analyze the stability of our system under input sampling density variations. Last, we show qualitative results and analyze error patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>For 3D shape segmentation task, we use a large scale shape part annotation dataset introduced by <ref type="bibr">[</ref>  <ref type="table">Table 2</ref>. IoU for part segmentation on 16 categories. To compute mean IoU, per category IoU is weighted by the corresponding shape number and then averaged. Ours1 represents a variation of our framework without SpecTN and Ours2 corresponds to our full pipeline with SpecTN. On average, our approach outperforms all the baseline including both traditional machine learning and deep learning based methods by a large margin. We also achieves the highest IoU on most of the categories.</p><p>made shapes, with 2 to 6 parts per category. In total there are 16,881 models with expert verified part annotations. In addition, we use the official train/test split provided along with ShapeNet models. For the keypoint prediction task, we build a new large scale keypoint annotation dataset, containing 1,337 chair models with 10 keypoints per shape, in contrast to traditional small scale dataset <ref type="bibr" target="#b12">[13]</ref> which has at most 100 shapes annotated per category. These keypoints are all manually annotated by experts with consistency across different shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Shape Part Segmentation</head><p>Per-category shape part segmentation We first conduct part segmentation assuming the category label of each shape is known, as the setting in <ref type="bibr" target="#b27">[28]</ref>. The task is to predict a part label for each sample point on shapes. We compare our framework with traditional learning-based techniques <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> leveraging on local geometric features and shape alignment cues, as well as recent deep learning based approaches <ref type="bibr" target="#b1">[2]</ref> which also fall into the family of spectral CNNs. In addition we design an additional baseline using a 3D volumetric CNN architecture, denoted as Voxel CNN, which generalizes VoxNet <ref type="bibr" target="#b18">[19]</ref> for segmentation tasks. The network has 10 convolutional layers without downsampling and keeps a receptive field of 19 with spatial resolution of 32. We compute per-point features in the preprocessing step as is in <ref type="bibr" target="#b27">[28]</ref> and use the same set of input for all baselines except Voxel CNN. The set of input shapes are pre-aligned using a hierarchical joint alignment algorithm described in <ref type="bibr" target="#b3">[4]</ref>. Point intersection over union (IoU) is used as evaluation metric, averaged across all part classes. Cross-entropy loss is minimized during training.</p><p>We evaluate our framework in two settings, with or without SpecTN, and compare the results in <ref type="table">Table 2</ref>.</p><p>Note that on most categories our approach achieves the best performance and on average outperforms state of the art by a large margin. In comparison to <ref type="bibr" target="#b1">[2]</ref>, the state of the art in the family of spectral CNNs, our approach introduces spectral dilated kernel parametrization, which increases the effectiveness of spectral CNN framework. Moreover, the performance gain from SpecTN shows that synchronizing spectral domains would greatly increase the generalizibility across shapes of different topology and geometry. Cross-category shape part segmentation Next we evaluate our approach on the part segmentation task in a crosscategory setting. In this task, shape category label is not known during the test phase and for each point the network needs to select one of the part label from all possible part labels in all categories. Cross-category setting introduces larger geometric and topological variance among shapes, thus could help examining the spectral CNN's ability of recognizing objects. At the same time the impact of spectral domain misalignment becomes stronger, providing a better testbed for validating the effectiveness of SpecTN. Since this experiment is proposed to verify design choices of spectral CNN, we mainly compare with <ref type="bibr" target="#b1">[2]</ref>. We mix the 16 categories of shapes in <ref type="bibr" target="#b27">[28]</ref> and train a single network for all categories. After predicting point segmentation labels, one can classify shapes through a point-wise majority voting scheme. Point IoU and classification accuracy (Acc) are chosen as the evalution metric for part segmentation and object categorization, respectively. The results are shown in the 2nd and 3rd column of <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Our approach outperforms the baseline ACNN by a large margin on both segmentation and classification. Note that ACNN <ref type="bibr" target="#b1">[2]</ref> does not explicitly conduct multi-scale analysis and is designed for near-isometric 3D shapes with similar spectral domains, thus generalizes less well across a diverse set of shapes. Our framework, in contrast, could effectively capture multi-scale context information, a feature that is highly important for both segmentation and classification. The spectral domain synchronization ability of SpecTN further improves our generalizability, leading to an extra performance gain as is shown in <ref type="table" target="#tab_2">Table 3</ref>. Partial data part segmentation To evaluate the robustness of our approach to incomplete data, we conduct part segmentation on simulated scans of 3D shapes from a single viewpoint. To be specific, we generate N = 6 simulated scans for each 3D shape in the part annotation dataset <ref type="bibr" target="#b27">[28]</ref> from random viewpoints, and then use these partial point cloud with part annotations for train and test. All the cross cat IoU partial point clouds are normalized to fit into a unit cube.</p><p>Following the train/test split provided by <ref type="bibr" target="#b3">[4]</ref>, we train our network to segment shape parts for each category. Again we compare our method with ACNN <ref type="bibr" target="#b1">[2]</ref>. IoU is used as evaluation metric and the results are shown in the 4th and 5th column of <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Our approach outperforms the baseline on partial data part segmentation by a large margin. In particular, from complete shape to partial shape setting, the performance drop of our approach is less significantly than the baseline, reflected by the gap of mean IoU between the complete data setting and the partial setting. It verifies that our method is more robust to data incompleteness. We surmise that the performance of ACNN is heavily influenced by noisy and sensitive principal curvature estimation on partial scans since this step plays a crucial rule in determining its local frames; whereas our approach makes less assumption about quality of the underlying shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Keypoint Prediction</head><p>Our framework is not limited to part segmentation but could learn more general functions on graphs. In this section, we evaluate our framework on the keypoint prediction task. We associate each keypoint an individual label and assign all the non-keypoints a background class label. The keypoint prediction problem could be treated as a multi-class classification problem and the cross-entropy loss is optimized during training. We evaluate our approach against previous state-of-the-art method <ref type="bibr" target="#b8">[9]</ref>. <ref type="bibr" target="#b8">[9]</ref> first jointly aligns all the shapes in 3D space via free-form deformation and then propagates keypoint labels to test shapes from its K nearest training shapes. We manually tune K and report the best performance of this method. Five-folds cross validation is adopted during evaluation, and PCK (percentage of correct keypoints) is used as evaluation metric. We show the PCK curve for the two approaches in <ref type="figure">Figure 6</ref>. Each point on a curve indicates fraction of correctly predicted keypoints for a given Euclidean error threshold. Our approach outperforms <ref type="bibr" target="#b8">[9]</ref>, in particular, more precise predictions can be obtained by our method (see the region close to y-axis). ground truth prediction ground truth prediction <ref type="figure">Figure 5</ref>. We evaluate our framework on normal prediction task. The colors shown on the 3D shape are RGB-coded normals, namely putting XYZ components of normal directions into RGB channels. Our framework could predict reasonable normal directions even on very thin structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Normal Prediction</head><p>To further validate the generality of our framework, we leverage our proposed SyncSpecCNN to learn another type of graph vertex function, vertex normal function. Specifically, our SyncSpecCNN takes the XYZ coordinate function of graph vertices as network input and predicts vertex normal as output. The network is trained to minimize the L2 loss between ground truth normals and predicted normals. We use the official train/test split provided by <ref type="bibr" target="#b3">[4]</ref> and visualize some of the normal prediction results from test set in <ref type="figure">Figure 5</ref>.</p><p>It can be seen our predictions are very close to the ground truth at most of the time.Even on thin structures the normal predictions are still reasonable. One problem of our prediction is that it tends to generate smoothly transiting normals along the boundary while the ground truth is sharper. This is due to the fact that we are using a small number of eigenbases in our experiments, which is not friendly to regression tasks with very high frequency signal as target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Diagnosis</head><p>Spectral Dilated Kernel Parametrization We evaluate our dilated kernel parametrization from two aspects: the basis function choice and kernel scale choice. <ref type="table">Table 4</ref> summarizes all the comparison results, as explained below.</p><p>We explore the expressive power of different kernel basis. In the family of spectral CNN, convolution kernels are parametrized by a linear combination of basis functions, i.e. modulated exponential window in our case. Previous  <ref type="figure">Figure 6</ref>. Keypoint prediction comparison. We draw PCK curves for both methods while changing the error threshold. Our approach outperforms <ref type="bibr" target="#b8">[9]</ref> on average and has particularly high local accuracy when the error threshold is small, i.e. our approach reaches pck = 0.29 when error threshold equals 0.01, while <ref type="bibr" target="#b8">[9]</ref> reaches pck = 0.16 methods have proposed to use different basis functions such as cubic spline basis <ref type="bibr" target="#b2">[3]</ref> and exponential window basis <ref type="bibr" target="#b1">[2]</ref>. Each row of <ref type="table">Table 4</ref> corresponds to a basis choice. We also evaluate the effectiveness of multi-scale analysis by changing the spatial sizes of convolution kernels. We compare with two baseline choices: set all kernel size to be the smallest kernel size in the current network; set to be the largest one. Each column of <ref type="table">Table 4</ref> corresponds to a kernel scale choice.</p><p>All numbers are reported on the cross-category part segmentation task, by IoU. We only take the XYZ coordinate function of graph vertices as network input as opposed to handcrafted geometry features which may have already capture some multi-scale information. Also we remove the 7th and 8th layers from our network which involves SpecTN and is designed for very large convolution kernels.</p><p>It can be seen that modulated exponential window basis has a better expressive power compared with baselines for our segmentation task. Using multi-scale kernels also enables the aggregation of multi-scale information, thus producing better performance than small or large kernels alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to Sampling Density Variance</head><p>In this experiment, we evaluate the robustness of our approach w.r.t point cloud density variation. To be specific, we train our SyncSpecCNN for shape segmentation on the point cloud provided by <ref type="bibr" target="#b27">[28]</ref> first. Then we downsample the point cloud under different downsample ratio and evaluate our trained model to check how segmentation performance would change. Again we evaluate our approach with/without SpecTN and the result is shown in <ref type="figure">Figure 7</ref>.</p><p>By introducing SpecTN, our framework becomes more robust to sampling density variation. Our conjecture is that sampling density variation may result in large spectral space  <ref type="table">Table 4</ref>. We compare different kernel basis and kernel size choices, using cross category part segmentation task for evaluation. IoU is reported in the table. In particular, we compare cubic spline basis <ref type="bibr" target="#b2">[3]</ref>, exponential window basis <ref type="bibr" target="#b1">[2]</ref> and our modulated exponential window. All convolution kernels are parametrized by the same number of parameters and we tweak the hyper parameters of different basis functions so that their spatial sizes are comparable. We also compare three different kernel size choices. "small" indicates using small convolution kernel only; "large" indicates using large convolution kernel only; "multiscale" uses kernels of different sizes in different layers, as in our current design. It's not obvious how to parametrize multi-scale convolution kernels using cubic spline basis functions, therefore we evaluate cubic spline basis with small-sized kernels only.  <ref type="figure">Figure 7</ref>. We evaluate the robustness of our model to sampling density change. Test shapes are downsampled by different ratios and fed into our network. We compute the segmentation IoU for different downsample ratios and show it here. With SpecTN, our framework becomes more robust to sampling density change.</p><p>perturbation, therefore being able to synchronize different spectral domains becomes especially important. <ref type="figure">Figure 8</ref> shows segmentation results generated from our network on two categories, Chair and Lamp. Representative good results are shown in the first block and typical error patterns are summarized from the second to fourth blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative Results and Error Analysis</head><p>Most of our segmentation is very close to ground truth as is shown in the first block. We can accurately segment shapes with large geometric or topological variations like wide bench v.s. ordinary chair, pendant lamp v.s. table lamp. The lamp base on the first row and the lampshade on the second row are very similar regarding their local geometry; however, since our network is able to capture large scale context information, it could still differentiate the two and segment shapes correctly.</p><p>We observe several typical error patterns in our results. Most segmentation error occurs along part boundaries. fuzzy part boundaries semantic ambiguity part missing ground truth prediction ground truth prediction ground truth prediction correct segmentation ground truth prediction <ref type="figure">Figure 8</ref>. We visualize some segmentation results from our network prediction. The first block shows typical correct segmentations, notice the huge shape variation we can cover. The second to fourth blocks summarize different error patterns we observe in the results.</p><p>There are also cases where the semantic definition of parts has inherent ambiguities. We also observe a third type of error pattern, in which our prediction might miss a certain part completely, as is shown in the fourth block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce a novel neural network architecture, the Synchronized Spectral CNN (SyncSpecCNN), for semantic annotation on 3D shape graphs. To share coefficients and conduct multi-scale analysis in different parts of a single shape graph, we introduce a spectral parametrization of dilated convolutional kernels. To allow parameter sharing across related but different shapes that may be represented by very different graphs, we introduce a spectral transformer network to synchronize different spectral domains. The effectiveness of different components in our network is validated through extensive experiments. Jointly these contributions lead to state-of-the-art performance on various semantic annotation tasks including 3D shape part segmentation and 3D keypoint prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our SyncSpecCNN takes a shape graph equipped with vertex functions (i.e. spatial coordinate function) as input and predicts a per-vertex label. The framework is general and not limited to a specific type of output. We show 3D part segmentation and 3D keypoint prediction as example outputs here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of modulated exponential window function with different dilation parameters in both spectral domain and spatial domain. The same spectral representation could induce spatially different kernel functions, especially when the kernel size is large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of low frequency eigenbasis functions before and after spectral synchronization. Before synchronization, eigenbasis functions on different shapes are not aligned. After applying the transform predicted from SpecTN, different spectral domain could be synchronized and the eigenbasis functions align.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>80.96 78.37 77.68 75.67 87.64 61.89 91.79 85.36 80.59 95.58 70.59 91.85 85.94 53.13 69.81 75.33 ACNN [2] 79.63 76.35 72.89 70.80 72.72 86.12 71.14 87.84 81.98 77.43 95.49 45.68 89.49 77.41 49.23 82.05 76.71 Voxel CNN 79.37 75.14 72.80 73.28 70.00 87.17 63.50 88.35 79.58 74.43 93.92 58.67 91.79 76.41 51.16 65.25 77.08 Ours1 83.48 80.61 81.62 76.92 73.86 88.65 74.48 89.03 85.34 83.47 95.53 62.74 92.01 80.88 62.10 82.23 81.36 Ours2 84.74 81.55 81.74 81.94 75.16 90.24 74.88 92.97 86.10 84.65 95.61 66.66 92.73 81.61 60.61 82.86 82.13</figDesc><table><row><cell>category</cell><cell cols="5">mean plane bag cap car chair ear-</cell><cell cols="4">guitar knife lamp laptop motor-</cell><cell cols="4">mug pistol rocket skate-</cell><cell>table</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>phone</cell><cell></cell><cell></cell><cell></cell><cell>bike</cell><cell></cell><cell></cell><cell></cell><cell>board</cell><cell></cell></row><row><cell>Wu14 [26]</cell><cell>-</cell><cell>63.20 -</cell><cell>-</cell><cell>-</cell><cell>73.47 -</cell><cell>-</cell><cell>-</cell><cell>74.42 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.76</cell></row><row><cell>Yi16 [28]</cell><cell>81.43</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">28], which</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">augments a subset of ShapeNet models with semantic part</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">annotations. The dataset contains 16 categories of man-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The 2nd and 3rd column of the table reports IoU for cross category part segmentation along with an induced classification accuracy. 4th and 5th column of the table reports IoU for part segmentation on partial shapes and complete shapes correspondingly. Our1 and Our2 corresponds to our framework without and with SpecTN respectively. In all experiments we beat the baseline by a large margin.</figDesc><table><row><cell></cell><cell></cell><cell>Acc</cell><cell cols="2">partial complete</cell></row><row><cell>ACNN</cell><cell>69.22</cell><cell cols="2">93.99 69.21</cell><cell>79.63</cell></row><row><cell>Ours1</cell><cell>79.65</cell><cell cols="2">99.59 76.19</cell><cell>83.48</cell></row><row><cell>Ours2</cell><cell>81.97</cell><cell cols="2">99.71 78.02</cell><cell>84.74</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06437</idno>
		<title level="m">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d mesh labeling via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fine-grained semisupervised labeling of large shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">190</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning 3d mesh segmentation and labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shape2pose: human-centric shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning part-based templates from large collections of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diverdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation of 3d meshes through spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 12th Pacific Conference on</title>
		<meeting>12th Pacific Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
	<note>Computer Graphics and Applications</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mesh segmentation via spectral embedding and contour analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning 3d part detection from sparsely labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Yumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 2nd International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Functional maps: a flexible representation of maps between shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vertexfrequency analysis on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ricaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="291" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Angular synchronization by eigenvectors and semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
	<note>Applied and computational harmonic analysis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image co-segmentation via consistent functional maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised multi-class joint image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3142" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interactive shape cosegmentation via label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="248" to="254" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d shape segmentation and labeling via extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Asia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
