<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X3D: Expanding Architectures for Efficient Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">X3D: Expanding Architectures for Efficient Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents X3D, a family of efficient video networks that progressively expand a tiny 2D image classification architecture along multiple network axes, in space, time, width and depth. Inspired by feature selection methods in machine learning, a simple stepwise network expansion approach is employed that expands a single axis in each step, such that good accuracy to complexity trade-off is achieved.</p><p>To expand X3D to a specific target complexity, we perform progressive forward expansion followed by backward contraction. X3D achieves state-of-the-art performance while requiring 4.8× and 5.5× fewer multiply-adds and parameters for similar accuracy as previous work. Our most surprising finding is that networks with high spatiotemporal resolution can perform well, while being extremely light in terms of network width and parameters. We report competitive accuracy at unprecedented efficiency on video classification and detection benchmarks. Code will be available at: https: //github.com/facebookresearch/SlowFast.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural networks for video recognition have been largely driven by expanding 2D image architectures <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b71">71]</ref> into spacetime. Naturally, these expansions often happen along the temporal axis, involving extending the network inputs, features, and/or filter kernels into spacetime (e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b75">75]</ref>); other design decisions-including depth (number of layers), width (number of channels), and spatial sizes-however, are typically inherited from 2D image architectures. While expanding along the temporal axis (while keeping other design properties) generally increases accuracy, it can be sub-optimal if one takes into account the computation/accuracy trade-off -a consideration of central importance in applications.</p><p>In part because of the direct extension of 2D models to 3D, video recognition architectures are computationally heavy. In comparison to image recognition, typical video models are significantly more compute-demanding, e.g. an image ResNet <ref type="bibr" target="#b29">[29]</ref> can use around 27× fewer multiply-add operations than a temporally extended video variant <ref type="bibr" target="#b81">[81]</ref>. This paper focuses on the low-computation regime in terms of computation/accuracy trade-off for video recognition. We base our design upon the "mobile-regime" models <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b61">61]</ref> developed for image recognition. Our core idea is that while expanding a small model along the temporal axis can increase accuracy, the computation/accuracy trade-off may not always be best compared with expanding other axes, especially in the low-computation regime where accuracy can increase quickly along different axes.</p><p>In this paper, we progressively "expand" a tiny base 2D image architecture into a spatiotemporal one by expanding multiple possible axes shown in <ref type="figure">Fig. 1</ref>. The candidate axes are temporal duration γ t , frame rate γ τ , spatial resolution γ s , network width γ w , bottleneck width γ b , and depth γ d . The resulting architecture is referred as X3D (Expand 3D) for expanding from the 2D space into 3D spacetime domain.</p><p>The 2D base architecture is driven by the MobileNet <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b61">61]</ref> core concept of channel-wise 1 separable convolutions, but is made tiny by having over 10× fewer multiply-add operations than mobile image models. Our expansion then progressively increases the computation (e.g., by 2×) by expanding only one axis at a time, train and validate the resultant architecture, and select the axis that achieves the best computation/accuracy trade-off. The process is repeated until the architecture reaches a desired computational budget. This can be interpreted as a form of coordinate descent <ref type="bibr" target="#b83">[83]</ref> in the hyper-parameter space defined by those axes. <ref type="bibr" target="#b0">1</ref> Also referred as "depth-wise". We use the term "channel-wise" to avoid confusions with the network depth, which is also an axis we consider.</p><p>Our progressive network expansion approach is inspired by the history of image ConvNet design where popular architectures have arisen by expansions across depth, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b94">94]</ref>, resolution <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b73">73]</ref> or width <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b93">93]</ref>, and classical feature selection methods <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b44">44]</ref> in machine learning. In the latter, progressive feature selection methods <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b44">44]</ref> start with either a set of minimum features and aim to find relevant features to improve in a greedy fashion by including (forward selection) a single feature in each step, or start with a full set of features and aim to find irrelevant ones that are excluded by repeatedly deleting the feature that reduces performance the least (backward elimination).</p><p>To compare to previous research, we use Kinetics-400 <ref type="bibr" target="#b43">[43]</ref>, Kinetics-600 <ref type="bibr" target="#b3">[4]</ref>, Charades <ref type="bibr" target="#b62">[62]</ref> and AVA <ref type="bibr" target="#b24">[24]</ref>. For systematic studies, we classify our models into different levels of complexity for small, medium and large models.</p><p>Overall, our expansion produces a sequence of spatiotemporal architectures, covering a wide range of computation/accuracy trade-offs. They can be used under different computational budgets that are application-dependent in practice. For example, across different computation and accuracy regimes X3D performs favorably to state-of-theart while requiring 4.8× and 5.5× fewer multiply-adds and parameters for similar accuracy as previous work. Further, expansion is simple and cheap e.g. our low-compute model is completed after only training 30 tiny models that accumulatively require over 25× fewer multiply-add operations for training than one large state-of-the-art network <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b81">81,</ref><ref type="bibr" target="#b84">84]</ref>.</p><p>Conceptually, our most surprising finding is that very thin video architectures that are created by expanding spatiotemporal resolution perform well, while being light in terms of network width and parameters. X3D networks have lower width than image-design <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b71">71]</ref> based video models, making X3D similar to the high-resolution Fast pathway <ref type="bibr" target="#b15">[15]</ref> which has been designed in such fashion. We hope these advances will facilitate future research and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Spatiotemporal (3D) networks. Video recognition architectures are favorably designed by extending image classification networks with a temporal dimension, and preserving the spatial properties. These extensions include direct transformation of 2D models <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b71">71]</ref> such as ResNet or Inception to 3D <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b89">89]</ref>, adding RNNs on top of 2D CNNs <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b92">92]</ref>, or extending 2D models with an optical flow stream that is processed by an identical 2D network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b80">80]</ref> . While starting with a 2D image based model and converting it to a spatiotemporal equivalent by inflating filters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">17]</ref> allows pretraining on image classification tasks, it makes video architectures inherently biased towards their image-based counterparts.</p><p>The SlowFast <ref type="bibr" target="#b15">[15]</ref> architecture has explored the resolution trade-off across several axes, different temporal, spatial, and channel resolution in the Slow and Fast pathway. Interestingly the Fast pathway can be very thin and therefore only adds a small computational overhead; however, performs low in isolation. Further, these explorations were performed with the architecture of the computationally heavy Slow pathway held constant to a temporal extension of an image classification design <ref type="bibr" target="#b29">[29]</ref>. In relation to this previous effort, our work investigates whether the heavy Slow pathway is required, or if a lightweight network can be made competitive.</p><p>Efficient 2D networks. Computation-efficient architectures have been extensively developed for the image classification task, with MobileNetV1&amp;2 <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b61">61]</ref> and Shuf-fleNet <ref type="bibr" target="#b95">[95]</ref> exploring channel-wise separable convolutions and expanded bottlenecks. Several methods for neural architecture search in this setting have been proposed, also adding Squeeze-Excitation (SE) <ref type="bibr" target="#b33">[33]</ref> attention blocks to the design space in <ref type="bibr" target="#b72">[72]</ref> and more recently, MobileNetV3 <ref type="bibr" target="#b31">[31]</ref> Swish non-linearities <ref type="bibr" target="#b59">[59]</ref>. MobileNets <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b72">72]</ref> were scaled up and down by using a multiplier for width and input size (resolution). Recently, MnasNet <ref type="bibr" target="#b72">[72]</ref> is used to apply liner scaling factors to spatial, width and depth axes for creating a set of EfficientNets <ref type="bibr" target="#b73">[73]</ref> for image classification.</p><p>Our expansion is related to this, but requires fewer samples and handles more axes as we only train a single model for each axis in each step, while <ref type="bibr" target="#b73">[73]</ref> performs a grid-search on the initial regime which requires k d models to be trained where k is the gridsize and d the number of axes. Moreover, the model used for this search, MnasNet was found by sampling around 8000 models <ref type="bibr" target="#b72">[72]</ref>. For video, this is prohibitive as datasets can have orders of magnitude more images than image classification e.g. the largest version of Kinetics <ref type="bibr" target="#b4">[5]</ref> has ≈195M frames, 162.5× more images than ImageNet. By contrast, our approach only requires to train 6 models, one for each expansion axis, until a desired complexity is reached, e.g. for 5 steps, it requires 30 models to be trained. Efficient 3D networks. Several innovative architectures for efficient video classification have been proposed, e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b85">85,</ref><ref type="bibr" target="#b89">89,</ref><ref type="bibr" target="#b97">[97]</ref><ref type="bibr" target="#b98">[98]</ref><ref type="bibr" target="#b99">[99]</ref>. Channel-wise separable convolution as a key building block for efficient 2D ConvNets <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b95">95]</ref> has been explored for video classification in <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b76">76]</ref>, where 2D architectures are extended to their 3D counterparts, e.g. ShuffleNet and MobileNet in <ref type="bibr" target="#b45">[45]</ref>, or ResNet in <ref type="bibr" target="#b76">[76]</ref> by using a 3×3×3 channel-wise separable convolution in the bottleneck of a residual stage. Earlier, <ref type="bibr" target="#b9">[10]</ref> adopt 2D ResNets and Mo-bileNets from ImageNet and sparsifies connections inside each residual block similar to separable or group convolution. A temporal shift module (TSM) is introduced in <ref type="bibr" target="#b51">[51]</ref> that extends a ResNet to capture temporal information using memory shifting operations. There is also active research on adaptive frame sampling techniques, e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b86">86,</ref><ref type="bibr" target="#b87">87,</ref><ref type="bibr" target="#b91">91]</ref>, which we think can be complementary to our approach.</p><p>In relation to most of these works, our approach does not assume a fixed inherited design from 2D networks, but expands a tiny architecture across several axes in space, time, channels and depth to achieve a good efficiency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">X3D Networks</head><p>Image classification architectures have gone through an evolution of architecture design with progressively expanding existing models along network depth <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b94">94]</ref>, input resolution <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b73">73]</ref> or channel width <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b93">93]</ref>. Similar progress can be observed for the mobile image classification domain where contracting modifications (shallower networks, lower resolution, thinner layers, separable convolution <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b95">95]</ref>) allowed operating at lower computational budget. Given this history in image ConvNet design, a similar progress has not been observed for video architectures as these were customarily based on direct temporal extensions of image models. However, is single expansion of a fixed 2D architecture to 3D ideal, or is it better to expand or contract along different axes?</p><p>For video classification the temporal dimension exposes an additional dilemma, increasing the number of possibilities but also requiring it to be dealt differently than the spatial dimensions <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b77">77]</ref>. We are especially interested in the trade-off between different axes, more concretely:</p><p>• What is the best temporal sampling strategy for 3D networks? Is a long input duration and sparser sampling preferred over faster sampling of short duration clips?</p><p>• Do we require finer spatial resolution? Previous works have used lower resolution for video classification <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b77">77]</ref> to increase efficiency. Also, videos typically come at coarser spatial resolution than Internet images; therefore, is there a maximum spatial resolution at which performance saturates?</p><p>• Is it better to have a network with high frame-rate but thinner channel resolution, or to slowly process video with a wider model? E.g. should the network have heavier layers as typical image classification models (and the Slow pathway <ref type="bibr" target="#b15">[15]</ref>) or rather lighter layers with lower width (as the Fast pathway <ref type="bibr" target="#b15">[15]</ref>). Or is there a better trade-off, possibly between these extremes?</p><p>• When increasing the network width, is it better to globally expand the network width in the ResNet block design <ref type="bibr" target="#b29">[29]</ref> or to expand the inner ("bottleneck") width, as is common in mobile image classification networks using channel-wise separable convolutions <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b95">95]</ref>?</p><p>• Should going deeper be performed with expanding input resolution in order to keep the receptive field size large enough and its growth rate roughly constant, or is it better to expand into different axes? Does this hold for both the spatial and temporal dimension? <ref type="table" target="#tab_9">Table 1</ref>. X3D architecture. The dimensions of kernels are denoted by {T ×S 2 , C} for temporal, spatial, and channel sizes. Strides are denoted as {temporal stride, spatial stride 2 }. This network is expanded using factors {γτ , γt, γs, γw, γ b , γ d } to form X3D. Without expansion (all factors equal to one), this model is referred to as X2D, having 20.67M FLOPS and 1.63M parameters.</p><formula xml:id="formula_0">stage filters output sizes T ×S 2 data layer stride γ τ , 1 2 1γ t ×(112γ s ) 2 conv 1 1×3 2 , 3×1, 24γ w 1γ t ×(56γ s ) 2 res 2   1×1 2 , 24γ b γ w 3×3 2 , 24γ b γ w 1×1 2 , 24γ w   ×γ d 1γ t ×(28γ s ) 2 res 3   1×1 2 , 48γ b γ w 3×3 2 , 48γ b γ w 1×1 2 , 48γ w   ×2γ d 1γ t ×(14γ s ) 2 res 4   1×1 2 , 96γ b γ w 3×3 2 , 96γ b γ w 1×1 2 , 96γ w   ×5γ d 1γ t ×(7γ s ) 2 res 5   1×1 2 , 192γ b γ w 3×3 2 , 192γ b γ w 1×1 2 , 192γ w   ×3γ d 1γ t ×(4γ s ) 2 conv 5 1×1 2 , 192γ b γ w 1γ t ×(4γ s ) 2 pool 5 1γ t ×(4γ s ) 2 1×1×1 fc 1 1×1 2 , 2048 1×1×1 fc 2 1×1 2 , #classes 1×1×1</formula><p>This section first introduces the basis X2D architecture in Sec. 3.1 which is expanded with operations defined in Sec. 3.2 by using the progressive approach in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basis instantiation</head><p>We begin by describing the instantiation of the basis network architecture, X2D, that serves as baseline to be expanded into spacetime. The basis network instantiation follows a ResNet <ref type="bibr" target="#b29">[29]</ref> structure and the Fast pathway design of SlowFast networks <ref type="bibr" target="#b15">[15]</ref> with degenerated (single frame) temporal input. X2D is specified in <ref type="table" target="#tab_9">Table 1</ref>, if all expansion factors {γ τ , γ t , γ s , γ w , γ b , γ d } are set to 1.</p><p>We denote spatiotemporal size by T ×S 2 where T is the temporal length and S is the height and width of a square spatial crop. The X2D architecture is described next.</p><p>Network resolution and channel capacity. The model takes as input a raw video clip that is sampled with framerate 1/γ τ in the data layer stage. The basis architecture only takes a single frame of size T ×S 2 =1×112 2 as input and therefore can be seen as an image classification network. The width of the individual layers is oriented at the Fast pathway design in <ref type="bibr" target="#b15">[15]</ref> with the first stage, conv 1 , filters the 3 RGB input channels and produces 24 output features. This width is increased by a factor of 2 after every spatial sub-sampling with a stride = 1, 2 2 at each deeper stage from res 2 to res 5 . Spatial sub-sampling is performed by the center ("bottleneck") filter of the first res-block of each stage.</p><p>Similar to the SlowFast pathways <ref type="bibr" target="#b15">[15]</ref>, the model preserves the temporal input resolution for all features throughout the network hierarchy. There is no temporal downsampling layer (neither temporal pooling nor time-strided convolutions) throughout the network, up to the global pooling layer before classification. Thus, the activations tensors contain all frames along the temporal dimension, maintaining full temporal frequency in all features.</p><p>Network stages. X2D consists of a stage-level and bottleneck design that is inspired by recent 2D mobile image classification networks <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b95">95]</ref> which employ channelwise separable convolution that are a key building block for efficient ConvNet models. We adopt stages that follow MobileNet <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b61">61]</ref> design by extending every spatial 3×3 convolution in the bottleneck block to a 3×3×3 (i.e. 3×3 2 ) spatiotemporal convolution which has also been explored for video classification in <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b76">76]</ref>. Further, the 3×1 temporal convolution in the first conv 1 stage is channel-wise.</p><p>Discussion. X2D can be interpreted as a Slow pathway since it only uses a single frame as input, while the network width is similar to the Fast pathway in <ref type="bibr" target="#b15">[15]</ref> which is much lighter than typical 3D ConvNets (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b81">81]</ref>) that follow an ImageNet design. Concretely, it only requires 20.67M FLOPs which amounts to only 0.0097% of a recent state-of-the-art SlowFast network <ref type="bibr" target="#b15">[15]</ref>.</p><p>As shown in <ref type="table" target="#tab_9">Table 1</ref> and <ref type="figure">Fig. 1</ref>, X2D is expanded across 6 axes, {γ τ , γ t , γ s , γ w , γ b , γ d }, described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Expansion operations</head><p>We define a basic set of expansion operations that are used for sequentially expanding X2D from a tiny spatial network to X3D, a spatiotemporal network, by performing the following operations on temporal, spatial, width and depth dimensions.</p><p>• X-Fast expands the temporal activation size, γ t , by increasing the frame-rate, 1/γ τ , and therefore temporal resolution, while holding the clip duration constant.</p><p>• X-Temporal expands the temporal size, γ t , by sampling a longer temporal clip and increasing the frame-rate 1/γ τ , to expand both duration and temporal resolution.</p><p>• X-Spatial expands the spatial resolution, γ s , by increasing the spatial sampling resolution of the input video.</p><p>• X-Depth expands the depth of the network by increasing the number of layers per residual stage by γ d times.</p><p>• X-Width uniformly expands the channel number for all layers by a global width expansion factor γ w .</p><p>• X-Bottleneck expands the inner channel width, γ b , of the center convolutional filter in each residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Progressive Network Expansion</head><p>We employ a simple progressive algorithm for network expansion, similar to forward and backward algorithms for feature selection <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b44">44]</ref>. Initially we start with X2D, the basis model instantiation with a set of unit expanding factors X 0 of cardinality a. We use a = 6 factors, X ={γ τ , γ t , γ s , γ w , γ b , γ d }, but other axes are possible.</p><p>Forward expansion. The network expansion criterion function, which measures the goodness for the current expansion factors X , is represented as J(X ). Higher scores of this measure represent better expanding factors, while lower scores would represent worse. In our experiments, this corresponds to the accuracy of a model expanded by X . Furthermore, let C(X ) be a complexity criterion function that measures the cost of the current expanding factors X . In our experiments, C is set to the floating point operations of the underlying network instantiation expanded by X , but other measures such as runtime, parameters, or memory are possible. Then, the network expansion tries to find expansion factors X with the best trade-off X = argmax Z,C(Z)=c = J(Z) where Z are the possible expansion factors to be explored and c is the target complexity. In our case we perform expansion that only changes a single one of the a expansion factors while holding the others constant; therefore there are only a different subsets of Z to evaluate, where each of them alters in only one dimension from X . The expansion with the best computation/accuracy trade-off is kept for the next step. This is a form of coordinate descent <ref type="bibr" target="#b83">[83]</ref> in the hyper-parameter space defined by those axes.</p><p>The expansion is performed in a progressive manner with an expansion-rateĉ that corresponds to the stepsize at which the model complexity c is increased in each expansion step. We use a multiplicative increase ofĉ ≈ 2 of the model complexity in each step that corresponds to the complexityincrease for doubling the number of frames of the model. The stepwise expansion is therefore simple and efficient as it only requires to train a few models until a target complexity is reached, since we exponentially increase the complexity. Details on the expansion are in §A.3.</p><p>Backward contraction. Since the forward expansion only produces models in discrete steps, we perform a backward contraction step to meet a desired target complexity, if the target is exceeded by the forward expansion steps. This contraction is implemented as a simple reduction of the last expansion, such that it matches the target. For example, if the last step has increased the frame-rate by a factor of two, the backward contraction will reduce the frame-rate by a factor &lt; 2 to roughtly match the desired target complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments: Action Classification</head><p>Datasets. We perform our expansion on Kinetics-400 <ref type="bibr" target="#b43">[43]</ref> (K400) with ∼240k training, 20k validation and 35k testing videos in 400 human action categories. We report top-1 and top-5 classification accuracy (%). As in previous work, we train and report ablations on the train and val sets. We also report results on test set as the labels have been made available <ref type="bibr" target="#b3">[4]</ref>. We report the computational cost (in FLOPs) of a single, spatially center-cropped clip. <ref type="bibr" target="#b1">2</ref> Training. All models are trained from random initialization ("from scratch") on Kinetics, without using ImageNet <ref type="bibr" target="#b10">[11]</ref> or other pre-training. Our training recipe follows <ref type="bibr" target="#b15">[15]</ref>. All implementation details and dataset specifics are in §A.3.</p><formula xml:id="formula_1"> τ  s  d  b  t  w X3D  d  d  t  t  b  τ  τ X3D-L X3D-M X3D-S X3D-XS  s  s  s X2D  w X3D-XL</formula><p>For the temporal domain, we randomly sample a clip from the full-length video, and the input to the network are γ t frames with a temporal stride of γ τ ; for the spatial domain, we randomly crop 112γ s ×112γ s pixels from a video, or its horizontal flip, with a shorter side randomly sampled in [128γ s , 160γ s ] pixels which is a linearly scaled version of the augmentation used in <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b81">81]</ref>.</p><p>Inference. To be comparable with previous work and evaluate accuracy/complexity trade-offs we apply two testing strategies: (i) K-Center: Temporally, uniformly samples K clips (e.g. K=10) from a video and spatially scales the shorter spatial side to 128γ s pixels and takes a γ t ×112γ s ×112γ s center crop, comparable to <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b84">84]</ref>. (ii) K-LeftCenterRight is the same as above temporally, but takes 3 crops of γ t ×128γ s ×128γ s to cover the longer spatial axis, as an approximation of fully-convolutional testing, following <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b81">81]</ref>. We average the softmax scores for all individual predictions.  <ref type="table" target="#tab_9">Table 2</ref>. Expanded instances on K400-val. 10-Center clip testing is used. We show top-1 and top-5 classification accuracy (%), as well as computational complexity measured in GFLOPs (floating-point operations, in # of multiply-adds ×10 9 ) for a single clip input. Inference-time computational cost is proportional to 10× of this, as a fixed number of 10 of clips is used per video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Expanded networks</head><p>The accuracy/complexity trade-off curve for the expansion process on K400 is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Expansion starts from X2D that produces 47.75% top-1 accuracy (vertical axis) with 1.63M parameters 20.67M FLOPs per clip (horizontal axis), which is roughly doubled in each progressive step. We use 10-Center clip testing as our default test setting for expansion, so the overall cost per video is ×10. We will ablate different number of testing clips in Sec. 4.3. The expansion in <ref type="figure" target="#fig_1">Fig. 2</ref> provides several interesting observations:</p><p>(i) First of all, expanding along any one of the candidate axes increases accuracy. This justifies our motivation of taking multiple axes (instead of just the temporal axis) into account when designing spatiotemporal models.</p><p>(ii) Surprisingly, the first step selected by the expansion algorithm is not along the temporal axis; instead, it is a factor that grows the "bottleneck" width γ b in the ResNet block design <ref type="bibr" target="#b29">[29]</ref>. This echoes the inverted bottleneck design in <ref type="bibr" target="#b61">[61]</ref> (called "inverted residual" <ref type="bibr" target="#b61">[61]</ref>). This is possibly because these layers are lightweight (due to the channel-wise design of MobileNets) and thus are economical to expand at first. Another interesting observation is that accuracy varies strongly, with the bottleneck expansion γ b providing the highest top-1 accuracy of 55.0% and depth expansion γ d the lowest with 51.3% at same complexity of 41.4M FLOPs.</p><p>(iii) The second step extends the temporal size of the model from one to two frames (expanding γ τ and γ t is identical for this step as there exists only a single frame in the previous one). This is what we expected to be the most effective expansion already in the first step as it enables the network to model temporal information for recognition.</p><p>(iv) The third step increases the spatial resolution γ s and starts to show a pattern that is interesting. The expansion increases spatial and temporal resolution followed by depth (γ d ) in the fourth step. This is followed by multiple temporal expansions that increase temporal resolution (i.e. frame-rate) and input duration (γ τ &amp; γ t ), followed by two more expansions across the spatial resolution, γ s , in steps 8 and 9, while step 10 increases the depth of the network, γ d . An expansion of the depth after increasing input resolution is intuitive, as it allows to grow the filter receptive field resolution and size within each residual stage.  (v) Even though we start from a base model that is intentionally made tiny by having very few channels, the expansion does not choose to globally expand the width up to the 10 th step of the expansion process, making X3D similar to the Fast pathway design <ref type="bibr" target="#b16">[16]</ref> with high spatiotemporal resolution but low width. The last expansion step shown in the top-right of <ref type="figure" target="#fig_1">Fig. 2</ref> increases the width γ w . The final two steps, not shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, expand γ τ and γ d .</p><p>In the spirit of VGG models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b64">64]</ref> we define a set of networks based on their target complexity. We use FLOPs as this reflects a hardware agnostic measure of model complexity. Parameters are also possible, but as they would not be sensitive to the input and activation tensor size, we only report them as secondary metric. To cover the models from our expansion, <ref type="table" target="#tab_9">Table 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>defines complexity regimes by FLOPs, ranging from extra small (XS) to extra extra large (XXL).</head><p>Expanded instances. The smallest instance, X3D-XS is the output after 5 expansion steps. Expansion is simple and efficient as it requires to train few models that are mostly at a low compute regime. For X3D-XS each step trains models of around 0.04, 0.08, 0.15, 0.30, 0.60 GFLOPs. Since we train one model for each of the 6 axes the approximate cost for these five steps is roughly equal to training a single model of 6 ×1.17 GFLOPS (to be fair, this ignores overhead cost for data loading etc. as 6×5=30 models are trained overall). The next larger model is X3D-S which is defined by one backward contraction step after the 7 th expansion step. The contraction step simply reduces the expansion (γ t ) proportionally to roughly match the target regime of ≤ 2 GFLOPs. For this model we also tried to contract each other axis to match the target and found that γ t is best among the others.</p><p>The next models in <ref type="table" target="#tab_9">Table 2</ref> is X3D-M (≤ 2 GFLOPs) that achieves 74.6% top-1 accuracy, X3D-L (≤ 20 GFLOPs) with 76.8% top-1 and X3D-XL (≤ 40 GFLOPs) with 78.4% and X3D-XXL (≤ 150 GFLOPs) with 80.0% top-1 accuracy by expansion in the consecutive steps.</p><p>Further speed/accuracy comparisons are provided in §B. <ref type="table" target="#tab_2">Table 3</ref> shows three instantiations of X3D with varying complexity. It is interesting to inspect the differences of the models, X3D-S in <ref type="table" target="#tab_2">Table 3a</ref> is just a lower spatiotemporal resolution (γ t , γ τ , γ s ) version of <ref type="table" target="#tab_2">Table 3b</ref>; therefore has the same number of parameters, and X3D-XL in <ref type="table" target="#tab_2">Table 3c</ref> is created by expanding X3D-M 3b in spatial resolution (γ s ) and width (γ w ). See <ref type="table" target="#tab_9">Table 1</ref> for X2D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>Kinetics-400. <ref type="table" target="#tab_3">Table 4</ref> shows the comparison with state-ofthe-art results for three X3D instantiations.To be comparable to previous work, we use the same testing strategy, that is 10-LeftCenterRight (i.e. <ref type="bibr">30-view)</ref> inference. For each model, the table reports (from-left-to-right) ImageNet pretraining (pre), top-1 and top-5 validation accuracy, average test accuracy as (top-1+ top-5)/2 (i.e. official test-server metric), inference cost (GFLOPs×views) and parameters.</p><p>In comparison to the state-of-the-art, SlowFast <ref type="bibr" target="#b15">[15]</ref>, X3D-XL, provides comparable (slightly lower) performance (-0.7% top-1 and identical top-5 accuracy), while requiring 4.8× fewer multiply-add operations (FLOPs) and 5.5× fewer parameters than SlowFast 16×8, R101 + NL blocks <ref type="bibr" target="#b81">[81]</ref>, and better accuracy than SlowFast 8×8, R101+NL with 2.4× fewer multiply-add operations and 5.5× fewer parameters. When comparing X3D-L, we observe similar performance as Channel-Separated Networks (ip-CSN-152) <ref type="bibr" target="#b76">[76]</ref> and SlowFast 8×8, at 4.3× fewer FLOPs and 5.4× fewer parameters. Finally, in the lower compute regime X3D-M is comparable to SlowFast 4×16, R50 and Oct-I3D + NL <ref type="bibr" target="#b8">[9]</ref> while having 4.7× fewer FLOPs and 9.1× fewer parameters. We observe consistent results on the test set with the largest (and least efficient) X3D-XXL producing 86.7% average top1/5 accuracy, showing good generalization performance.  <ref type="table" target="#tab_9">Table 5</ref>. Comparison with the state-of-the-art on Kinetics-600.</p><p>Results are consistent with K400 in <ref type="table" target="#tab_3">Table 4</ref> above.</p><p>Kinetics-600 is a larger version of Kinetics that shall demonstrate further generalization of our approach. Results are shown in <ref type="table" target="#tab_9">Table 5</ref>. Our variants demonstrate similar performance as above, with the best model now providing slightly better performance than the previous state-of-theart SlowFast 16×8, R101+NL <ref type="bibr" target="#b15">[15]</ref>, again for 4.8× fewer FLOPs (i.e. multiply-add operations) and 5.5× fewer parameter. In the lower computation regime, X3D-M is comparable to SlowFast 4×16, R50 but requires 5.8× fewer FLOPs and 9.1× fewer parameters.</p><p>Charades <ref type="bibr" target="#b62">[62]</ref> is a dataset with longer range activities. <ref type="table" target="#tab_4">Table 6</ref> shows our results. X3D-XL provides higher performance (+0.9 mAP with K400 and +1.9mAP under K600 pretraining), while requiring 4.8× fewer multiply-add operations (FLOPs) and 5.5× fewer parameters than previous highest system, SlowFast <ref type="bibr" target="#b15">[15]</ref> with+ NL blocks <ref type="bibr" target="#b81">[81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Experiments</head><p>This section provides ablation studies on K400 val and test sets, comparing accuracy and computational complexity.</p><p>Comparison with EfficientNet3D. We first aim to compare X3D with a 3D extension of EfficientNet <ref type="bibr" target="#b73">[73]</ref>. This architecture uses exactly the same implementation extras such as channel-wise separable convolution <ref type="bibr" target="#b32">[32]</ref> as as X3D,  but was found by searching a large number of models for optimal trade-off on image-classification. This ablation studies if a direct extension of EfficientNet to 3D is comparable to X3D (which is expanded by only training few models). Effi-cientNet models are provided for various complexity ranges. We ablate three versions, B0, B3 and B4 that are extended in 3D using uniform scaling coefficients <ref type="bibr" target="#b73">[73]</ref> for the spatial and temporal resolution. In <ref type="table" target="#tab_5">Table 7</ref>, we compare three X3D models of similar complexity to EfficientNet3D on two sets, K400-val and K400-test (from top-to-bottom). Starting with K400-val (top rows), our model X3D-XS, corresponding to only 4 expansion steps in <ref type="figure" target="#fig_1">Fig. 2</ref>. is comparable in FLOPs (slightly lower) and parameters (slightly higher), to EfficientNet3D-B0, but achieves 1.9% higher top-1 and 1.3% higher top-1 accuracy.</p><p>Next, comparing X3D-M to EfficientNet3D-B3 shows a gain of 2.0% top-1 and 2.1% top-5, despite using 32% fewer FLOPs and 54% fewer parameters. Finally, comparing X3D-L to EfficientNet3D-B4 shows a gain of 2.3% top-1 and 1.9% top-5, while having 23% and 50% fewer FLOPs and parameters, respectively. Seeing larger gains for larger models underlines the benefit of progressive expansion, as more expansion steps have been performed for these.</p><p>Since our expansion is measured by validation set performance, it is interesting to see if this provides a benefit for X3D. Therefore, we investigate potential differences on the K400-test set, in the lower half of <ref type="table" target="#tab_5">Table 7</ref>, where similar, even slightly higher improvements in accuracy can be observed when comparing the same models as above, showing that our models generalize well to the test set. Inference cost. In many cases, like the experiments before, the inference procedure follows a fixed number of clips for testing. Here, we aim to ablate the effect of using fewer testing clips for video-level inference. In <ref type="figure" target="#fig_2">Fig. 3</ref> we show the trade-off for the full inference of a video, when varying the number of temporal clips used. The vertical axis shows the top-1 accuracy on K400-val and the horizontal axis the overall inference cost in FLOPs for different models. Each model experiences a large performance increment when going from K = 1 clip to 3-clip testing (which triples the FLOPs); this is expected as the 1-clip only covers the temporal center of an input video, while 3-clip covers start, center and end. Increasing the number of clips beyond 3 only marginally increases performance, signaling that efficient video inference can be performed with sparse clip sampling if highest accuracy is not crucial. Lastly, when comparing different models we observe that X3D architectures can achieve similar accuracy as SlowFast <ref type="bibr" target="#b15">[15]</ref>, CSN <ref type="bibr" target="#b76">[76]</ref> or TSM <ref type="bibr" target="#b51">[51]</ref> (for the latter two, only 10-clip testing results are available to us), while requiring 3-20× fewer multiply-add operations. Notably, the SlowFast 16×8 variant does not benefit from increasing 7 to 10 temporal clips, showing that the expensive per clip cost of longer-term models does not reflect full inference efficiency as they allow sparser temporal sampling. A log-scale version of <ref type="figure" target="#fig_2">Fig. 3</ref> and similar plots on K400-test are in §B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments: AVA Action Detection</head><p>Dataset. The AVA dataset <ref type="bibr" target="#b24">[24]</ref> comes with bounding box annotations for spatiotemporal localization of (possibly multiple) human actions. There are 211k training and 57k validation video segments. We follow the standard protocol reporting mean Average Precision (mAP) on 60 classes <ref type="bibr" target="#b24">[24]</ref>. Detection architecture. We exactly follow the detection architecture in <ref type="bibr" target="#b15">[15]</ref> to allow direct comparison of X3D with SlowFast networks as a backbone. The detector is similar to Faster R-CNN <ref type="bibr" target="#b60">[60]</ref> with minimal modifications adapted for video. Details on implementation and training are in §A.1.  <ref type="table" target="#tab_9">Table 8</ref>. Comparison with the state-of-the-art on AVA. All methods use single center crop inference; full testing cost is directly proportional to the the floating point operations (GFLOPs) by multiplying with the number of validation segments (57k) in the dataset.</p><p>Inference. We perform inference on a single clip with γ t frames sampled with stride γ τ centered at the frame that is to be evaluated. Spatially we use a single center crop of 128γ s ×128γ s pixels as in <ref type="bibr" target="#b84">[84]</ref>, to have a comparable measure for overall test costs, since fully-convolutional inference has variable cost depending on the input video size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Main Results</head><p>We compare with state-of-the-art methods on AVA in <ref type="table" target="#tab_9">Table 8</ref>. To be comparable to previous work, we report results on the older AVA version 2.1 and newer 2.2 (which provides more consistent annotations), for our models pretrained on K400 or K600. We compare against long-term feature banks (LFB) <ref type="bibr" target="#b84">[84]</ref> and SlowFast <ref type="bibr" target="#b15">[15]</ref> as these are stateof-the art and use the same detection architecture as ours, varying the backbone from LFB, SlowFast and X3D. Note there are other recent works on AVA e.g., <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b90">90,</ref><ref type="bibr" target="#b96">96]</ref>.</p><p>In the upper part of <ref type="table" target="#tab_9">Table 8</ref> we compare X3D-XL with LFB, that uses a heavy backbone architecture for short and long-term modeling. X3D-XL provides comparable accuracy (+0.3 mAP vs. LFB R50 and -0.7mAP vs. LFB R101) at greatly reduced cost by 10.9×/14× fewer multiply-adds and 6.7×/11.1× fewer parameters than LFB R50/R101. Comparing to SlowFast <ref type="bibr" target="#b15">[15]</ref> in the lower portion of the table we observe that X3D-M is lower than SlowFast 4×16, R50 by 1.5mAP, but requiring 8.5× less multiply-adds and 10.9×less parameters for this result. Comparing the larger X3D-XL to SlowFast 8×8 + NL we observe the same performance at 3× and 5.4× fewer multiply-adds and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper presents X3D, a spatiotemporal architecture that is progressively expanded from a tiny spatial network. Multiple candidate axes, in space, time, width and depth are considered for expansion under good computation/accuracy trade-off. A surprising finding of our progressive expansion is that networks with thin channel dimension and high spatiotemporal resolution can be effective for video recognition. X3D achieves competitive efficiency, and we hope that it can foster future research and applications in video recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Implementation Details</head><p>A.1. Details: AVA Action Detection Detection architecture. We exactly follow the detection architecture in <ref type="bibr" target="#b15">[15]</ref> to allow direct comparison of X3D with SlowFast networks as a backbone. The detector is similar to Faster R-CNN <ref type="bibr" target="#b60">[60]</ref> with minimal modifications adapted for video. Since our paper focuses on efficiency, by default, we do not increase the spatial resolution of res 5 by 2× <ref type="bibr" target="#b15">[15]</ref>. Region-of-interest (RoI) features <ref type="bibr" target="#b21">[21]</ref> are extracted at the last feature map of res 5 by extending a 2D proposal at a frame into a 3D RoI by replicating it along the temporal axis, similar as done in previous work <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b66">66]</ref>, followed by application of frame-wise RoIAlign <ref type="bibr" target="#b27">[27]</ref> and temporal global average pooling. The RoI features are then max-pooled and fed to a per-class, sigmoid classifier for prediction.</p><p>Training. For direct comparison, the training procedure and hyper-parameters for AVA follow <ref type="bibr" target="#b15">[15]</ref> without modification. The network weights are initialized from the Kinetics models and we use step-wise learning rate decay, that is reduced by 10× when validation error saturates. We train for 14k iterations (68 epochs for ∼211k data), with linear warm-up <ref type="bibr" target="#b23">[23]</ref> for the first 1k iterations and use a weight decay of 10 −7 , as in <ref type="bibr" target="#b15">[15]</ref>. All other hyper-parameters are the same as in the Kinetics experiments. Ground-truth boxes, and proposals overlapping with ground-truth boxes by IoU &gt; 0.9, are used as the samples for training. The inputs are instantiationspecific clips of size γ t ×112γ s ×112γ s with time stride γ τ .</p><p>The region proposal extraction also follows <ref type="bibr" target="#b15">[15]</ref> and is summarized here for completeness. We follow previous works that use pre-computed proposals <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b66">66]</ref>. Our region proposals are computed by an off-the-shelf person detector, i.e., that is not jointly trained with the action detection models. We adopt a person-detection model trained with Detectron <ref type="bibr" target="#b22">[22]</ref>. It is a Faster R-CNN with a ResNeXt-101-FPN <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b88">88]</ref> backbone. It is pre-trained on ImageNet and the COCO human keypoint images <ref type="bibr" target="#b53">[53]</ref>. We fine-tune this detector on AVA for person (actor) detection. The person detector produces 93.9 AP@50 on the AVA validation set. Then, the region proposals for action detection are detected person boxes with a confidence of &gt; 0.8, which has a recall of 91.1% and a precision of 90.7% for the person class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Details: Charades Action Classification</head><p>For Charades, we fine-tune the Kinetics models. All settings are the same as those of Kinetics, except the following. A per-class sigmoid output is used to account for the mutli-class nature. We train on a single machine for 24k iterations using a batch size of 16 and a base learning rate of 0.02 with 10× step-wise decay if the validation error saturates. We use weight decay of 10 -5 . We also increase the model temporal stride by ×2 as this dataset benefits from longer clips. For inference, we temporally max-pool prediction scores <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b81">81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Details: Kinetics Action Classification</head><p>Training details. We use the initialization in <ref type="bibr" target="#b28">[28]</ref>. We adopt synchronized SGD training on 128 GPUs following the recipe in <ref type="bibr" target="#b23">[23]</ref>. The mini-batch size is 8 clips per GPU (so the total mini-batch size is 1024). We train with Batch Normalization (BN) <ref type="bibr" target="#b38">[38]</ref>, and the BN statistics are computed within each 8 clips, unless noted otherwise. We adopt a halfperiod cosine schedule <ref type="bibr" target="#b54">[54]</ref> of learning rate decaying: the learning rate at the n-th iteration is η · 0.5[cos( n nmax π) + 1], where n max is the maximum training iterations and the base learning rate η is set as 1.6. We also use a linear warm-up strategy <ref type="bibr" target="#b23">[23]</ref> in the first 8k iterations. Unless specified, we train for 256 epochs (60k iterations with a total mini-batch size of 1024, in ∼240k Kinetics videos). We use momentum of 0.9, weight decay of 5×10 -5 and dropout <ref type="bibr" target="#b30">[30]</ref> of 0.5 is used before the final classifier.</p><p>For Kinetics-600, we extend the training epochs (and schedule) of above by 2×. All other hyper-parameters are exactly as for Kinetics-400.</p><p>Implementation details. Non-Local (NL) blocks <ref type="bibr" target="#b81">[81]</ref> are not used for X3D. For SlowFast results, we use exactly the same implementation details as in <ref type="bibr" target="#b15">[15]</ref>. Specifically, for SlowFast models involving NL, we initialize them with the counterparts that are trained without NL, to facilitate convergence. We only use NL on the (fused) Slow features of res 4 (instead of res 3 +res 4 <ref type="bibr" target="#b81">[81]</ref>). For X3D and EfficientNet3D, we follow previous work on 2D mobile architectures <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73]</ref>, using SE blocks <ref type="bibr" target="#b33">[33]</ref> (also found beneficial for efficient video classification in <ref type="bibr" target="#b89">[89]</ref>) and swish non-linearity <ref type="bibr" target="#b59">[59]</ref>. To conserve memory, we use SE with original reduction ratio of 1/16 only in every other residual block after the 3×3 2 conv; swish is only used before and after these layers and all other weight layers are followed by ReLU non-linearity <ref type="bibr" target="#b47">[47]</ref>. We do not employ the "linear-bottleneck" design used in mobile image networks <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73]</ref>, as we found it to sometimes cause instability in distributed training, as it does not allow to zero-initialize the final BN scaling <ref type="bibr" target="#b23">[23]</ref> of residual blocks.</p><p>Expansion details. To expand the model specified in <ref type="table" target="#tab_9">Table 1</ref>, we set all initial expansion factors, X 0 , to one i.e. γ t =γ s =γ w =γ b =γ d =1 resulting in the X2D base model. A temporal sampling rate γ τ is not defined for the X2D model as it does not have multiple frames. The smallest possible common expansion for this model is defined by increasing the number of frames from 1 to two; therefore we set the expansion-rateĉ to match the cost of increasing the temporal input length of the model by a factor of two (the smallest possible common increase in the first expansion step), which roughly doubles the cost of the model,ĉ = 2.</p><p>Then, in every step of our expansion we train a models, one for expanding each axis, such that its complexity doubles (ĉ = 2). For the individual axes this roughly 3 equals to the following operations:</p><p>• X-Fast: γ τ ← 0.5γ τ , reduces the sampling stride to double frame-rate while sampling the same input duration, this doubles the temporal size γ t ←2γ t .</p><p>• X-Temporal: Increases frame-rate by γ τ ← 0.75γ τ and input duration to double the input size γ t ←2γ t (i.e. 1.5× higher frame-rate and 1.5×longer input duration).</p><p>• X-Spatial: Expands the spatial resolution proportionally γ s ← √ 2γ s .</p><p>• X-Depth: Expands the depth of the network by around γ d ← 2.2γ d .</p><p>• X-Width: Expands the global width for all layers by γ w ← 2γ w .</p><p>• X-Bottleneck: Expands the bottleneck width by roughly</p><formula xml:id="formula_2">γ b ← 2.25γ b .</formula><p>The exact scaling factors slightly differ from one expansion step to the other due to rounding effects in network geometry (layers stride, activation size etc.).</p><p>Since the stepwise expansion also allows to elegantly integrate regularization (which is typically increased for larger models), we perform a regularization expansion if the training error of the previous expansion step starts to deviate from the validation error. Specifically, we start the expansion with double the batch-size and half learning schedule than described above, then the BN statistics are computed within each 16 clips which lowers regularization and improves performance on small models. The batch-size is then decreased by 2× at the 8 th step of expansion which increases generalization. We perform another regularization expansion at the 11 th /13 th step by using drop-connect with rate 0.1/0.2 <ref type="bibr" target="#b34">[34]</ref>. Inference cost. Here we aim to provide further ablations for the effect of using fewer testing clips for efficient videolevel inference. In <ref type="figure" target="#fig_4">Fig. A.4</ref> we show the trade-off for the full inference of a video, when varying the number of temporal clips used. The vertical axis shows the top-1 accuracy on K400-val and the horizontal axis the overall inference cost in FLOPs for different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results</head><p>First, for comparison, the plot on top-left is the same as the one shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The plot on top-right shows this same plot with a logarithmic scale applied to the FLOPs axis. Using this scaling it is clearer to observe that smaller models (X3D-S and X3D-M) can provide up to 20× reduction in terms of multiply-add operations used during inference.</p><p>For example, 3-clip X3D-S produces 71.4% top-1 at 5.9 GFLOPs, whereas 10-clip CSN-50 <ref type="bibr" target="#b76">[76]</ref>  The lower two plots in <ref type="figure" target="#fig_4">Fig. A.4</ref> show the identical results on the test set of Kinetics-400 (which has been publicly released with Kinetics-600 <ref type="bibr" target="#b3">[4]</ref>). Note that the test set is more challenging which leads to overall lower accuracy for all approaches <ref type="bibr" target="#b0">[1]</ref>. We observe consistent results on the test set, illustrating good generalization of the models.</p><p>Mobile components. Finally, we ablate the effect of mobile components employed in X3D and EfficientNet3D. Since the components can have different effects of models from the small and large computation regime, we ablate the effects on a small (X3D-S) and a large model (X3D-XL).</p><p>First, we ablate channel-wise separable convolution <ref type="bibr" target="#b32">[32]</ref>, a key component in mobile ConvNets. We ablate two versions: (i) A version that reduces the bottleneck ratio (γ b ) accordingly, such that the overall architecture preserves the multiple-add operations (FLOPs), and (ii) a version that keeps the originally, expanded bottleneck ratio. <ref type="table" target="#tab_9">Table A</ref>.1 shows the results. For case (i) we see that performance drops significantly by 4% top-1 accuracy for X3D-S and by 2.4% for X3D-XL. For case (ii), we see that the performance of the baselines increases by 0.3% and 0.8% top-1 accuracy for X3D-S and X3D-XL, respectively. This shows that separable convolution is important for smallcomputation budgets, however, for best-performance a nonseparable convolution can provide gains (at high cost).</p><p>Second, we ablate swish non-linearities <ref type="bibr" target="#b59">[59]</ref> (that are only implemented before and after the "bottleneck" convolution, to conserve memory). We observe that removing swish has a smaller performance decrease of 0.9% for X3D-S and 0.4% for X3D-XL, and therefore could be changed to ReLU (which can be implemented in-place) if memory is priority.</p><p>Third, we ablate SE blocks <ref type="bibr" target="#b33">[33]</ref> (that are only used in every other residual block, to conserve memory). We observe that removing SE has a larger effect on performance, decreasing accuracy by 1.6% for X3D-S and 1.3% for X3D-XL. These observed effects on performance are similar to the ones that have been shown Non-local (NL) attention blocks <ref type="bibr" target="#b81">[81]</ref>, and also in line with <ref type="bibr" target="#b89">[89]</ref>, where SE attention blocks have been found beneficial for efficient video classification.  Ablations of mobile components for video classification on K400-val. We show top-1 and top-5 classification accuracy (%), parameters, and computational complexity measured in GFLOPs (floating-point operations, in # of multiply-adds ×10 9 ) for a single clip input of size γt×112γs×112γx. Inference-time computational cost is reported GFLOPs ×10, as a fixed number of 10-Center views is used. The results show that removing channel-wise separable convolution (CW conv) with unchanged bottleneck expansion ratio, γ b , drastically increases mutliply-adds and parameters at slightly higher accuracy, while swish has a smaller effect on performance than SE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 Figure 1 .</head><label>41</label><figDesc>X3D networks progressively expand a 2D network across the following axes: Temporal duration γt, frame rate γτ , spatial resolution γs, width γw, bottleneck width γ b , and depth γ d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Model capacity in GFLOPs (# of multiply-adds x 10 9 Progressive network expansion of X3D. The X2D base model is expanded 1 st across bottleneck width (γ b ), 2 nd temporal resolution (γτ ), 3 rd spatial resolution (γs), 4 th depth (γ d ), 5 th duration (γt), etc. The majority of models are trained for small computation cost, making the expansion economical in practice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>video in TFLOPs (# of multiply-adds x 10 12 ) Accuracy/complexity trade-off on Kinetics-400 for different number of inference clips per video. The top-1 accuracy (vertical axis) is obtained by K-Center clip testing where the number of temporal clips K ∈ {1, 3, 5, 7, 10} is shown in each curve. The horizontal axis shows the full inference cost per video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig</head><label></label><figDesc>. A.4 shows a series of extra plots on Kinetics-400, analyzed next (this extends Sec. 4 of the main paper):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A. 4 .</head><label>4</label><figDesc>Inference cost per video in TFLOPs (# of multiply-adds x 10 12 ) Inference cost per video in FLOPs (# of multiply-adds), video in TFLOPs (# of multiply-adds x 10 12 ) video in FLOPs (# of multiply-adds), Accuracy/complexity trade-off on K400-val (top) &amp; test (bottom) for varying # of inference clips per video. The top-1 accuracy (vertical axis) is obtained by K-Center clip testing where the number of temporal clips K ∈ {1, 3, 5, 7, 10} is shown in each curve. The horizontal axis measures the full inference cost per video. The left-sided plots show a linear and the right plots a logarithmic (log) scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>X3D-S with 1.96G FLOPs, 3.76M param, and 72.9% top-1 accuracy using expansion of γτ = 6, γ t = 13, γs= √ 2, γw= 1, γ b = 2.25, γ d = 2.2. X3D-M with 4.73G FLOPs, 3.76M param, and 74.6% top-1 accuracy using expansion of γτ = 5, γ t = 16, γs= 2, γw= 1, γ b = 2.25, γ d = 2.2. X3D-XL with 35.84G FLOPs &amp; 10.99M param, and 78.4% top-1 acc. using expansion of γτ = 5, γ</figDesc><table><row><cell>stage data layer conv1 res2 res3 res4 res5 conv5 pool5 fc1 fc2 (a) stage filters output sizes T ×H×W stride 6, 1 2 13×160×160 1×3 2 , 3×1, 24 13×80×80   1×1 2 , 54 3×3 2 , 54 1×1 2 , 24   ×3 13×40×40   1×1 2 , 108 3×3 2 , 108 1×1 2 , 48   ×5 13×20×20   1×1 2 , 216 3×3 2 , 216 1×1 2 , 96   ×11 13×10×10   1×1 2 , 432 3×3 2 , 432 1×1 2 , 192   ×7 13×5×5 1×1 2 , 432 13×5×5 13×5×5 1×1×1 1×1 2 , 2048 1×1×1 1×1 2 , #classes 1×1×1 data layer conv1 res2 res3 res4 res5 conv5 pool5 fc1 fc2 (b) stage filters output sizes T ×H×W stride 5, 1 2 16×224×224 1×3 2 , 3×1, 24 16×112×112   1×1 2 , 54 3×3 2 , 54 1×1 2 , 24   ×3 16×56×56   1×1 2 , 108 3×3 2 , 108 1×1 2 , 48   ×5 16×28×28   1×1 2 , 216 3×3 2 , 216 1×1 2 , 96   ×11 16×14×14   1×1 2 , 432 3×3 2 , 432 1×1 2 , 192   ×7 16×7×7 1×1 2 , 432 16×7×7 16×7×7 1×1×1 1×1 2 , 2048 1×1×1 1×1 2 , #classes 1×1×1 data layer conv1 res2 res3 res4 res5 conv5 pool5 fc1 fc2 (c)</cell><cell>       </cell><cell>filters stride 5, 1 2 1×3 2 , 3×1, 32 1×1 2 , 72 3×3 2 , 72 1×1 2 , 32   ×5 1×1 2 , 162 3×3 2 , 162 1×1 2 , 72   ×10 1×1 2 , 306 3×3 2 , 306 1×1 2 , 136   ×25 1×1 2 , 630 3×3 2 , 630 1×1 2 , 280   ×15 1×1 2 , 630 16×10×10 1×1 2 , 2048 1×1 2 , #classes</cell><cell>output sizes T ×H×W 16×312×312 16×156×156 16×78×78 16×39×39 16×20×20 16×10×10 16×10×10 1×1×1 1×1×1 1×1×1</cell></row></table><note>t = 16, γs= 2 √ 2, γw= 2.9, γ b = 2.25, γ d = 5.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Three instantiations of X3D with varying complexity. The top-1 accuracy corresponds to 10-Center view testing on K400. The models in (a) and (b) only differ in spatiotemporal resolution of the input and activations (γt, γτ , γs), and (c) differs from (b) in spatial resolution, γs, width, γw, and depth, γ d . See Table 1 for X2D. Surprisingly X3D-XL has a maximum width of 630 feature channels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison to the state-of-the-art on K400-val &amp; test.</figDesc><table><row><cell>model</cell><cell cols="4">pre top-1 top-5 test GFLOPs×views Param</cell></row><row><cell>I3D [7]</cell><cell></cell><cell cols="2">71.1 90.3 80.2 108 × N/A</cell><cell>12M</cell></row><row><cell>Two-Stream I3D [7] Two-Stream S3D-G [89] MF-Net [10] TSM R50 [51]</cell><cell>ImageNet</cell><cell cols="2">75.7 92.0 82.8 216 × N/A 77.2 93.0 143 × N/A 72.8 90.4 11.1 × 50 74.7 N/A 65 × 10</cell><cell>25M 23.1M 8.0M 24.3M</cell></row><row><cell>Nonlocal R50 [81]</cell><cell></cell><cell>76.5 92.6</cell><cell>282 × 30</cell><cell>35.3M</cell></row><row><cell>Nonlocal R101 [81]</cell><cell></cell><cell>77.7 93.3 83.8</cell><cell>359 × 30</cell><cell>54.3M</cell></row><row><cell>Two-Stream I3D [7]</cell><cell cols="2">-71.6 90.0</cell><cell>216 × NA</cell><cell>25.0M</cell></row><row><cell>R(2+1)D [77]</cell><cell cols="2">-72.0 90.0</cell><cell>152 × 115</cell><cell>63.6M</cell></row><row><cell>Two-Stream R(2+1)D [77]</cell><cell cols="2">-73.9 90.9</cell><cell cols="2">304 × 115 127.2M</cell></row><row><cell>Oct-I3D + NL [9]</cell><cell cols="2">-75.7 N/A</cell><cell>28.9 × 30</cell><cell>33.6M</cell></row><row><cell>ip-CSN-152 [76]</cell><cell cols="2">-77.8 92.8</cell><cell>109 × 30</cell><cell>32.8M</cell></row><row><cell>SlowFast 4×16, R50 [15]</cell><cell cols="2">-75.6 92.1</cell><cell>36.1 × 30</cell><cell>34.4M</cell></row><row><cell>SlowFast 8×8, R101 [15]</cell><cell cols="2">-77.9 93.2 84.2</cell><cell>106 × 30</cell><cell>53.7M</cell></row><row><cell cols="3">SlowFast 8×8, R101+NL [15] -78.7 93.5 84.9</cell><cell>116 × 30</cell><cell>59.9M</cell></row><row><cell cols="3">SlowFast 16×8, R101+NL [15] -79.8 93.9 85.7</cell><cell>234 × 30</cell><cell>59.9M</cell></row><row><cell>X3D-M</cell><cell cols="2">-76.0 92.3 82.9</cell><cell>6.2 × 30</cell><cell>3.8M</cell></row><row><cell>X3D-L</cell><cell cols="3">-77.5 92.9 83.8 24.8 × 30</cell><cell>6.1M</cell></row><row><cell>X3D-XL</cell><cell cols="3">-79.1 93.9 85.3 48.4 × 30</cell><cell>11.0M</cell></row><row><cell>X3D-XXL</cell><cell cols="3">-80.4 94.6 86.7 194.1 × 30</cell><cell>20.3M</cell></row><row><cell>model</cell><cell cols="4">pretrain top-1 top-5 GFLOPs×views Param</cell></row><row><cell>I3D [4]</cell><cell>-</cell><cell>71.9 90.1</cell><cell>108 × N/A</cell><cell>12M</cell></row><row><cell>Oct-I3D + NL [9]</cell><cell cols="2">ImageNet 76.0 N/A</cell><cell>25.6 × 30</cell><cell>12M</cell></row><row><cell>SlowFast 4×16, R50 [15]</cell><cell>-</cell><cell>78.8 94.0</cell><cell>36.1 × 30</cell><cell>34.4M</cell></row><row><cell>SlowFast 16×8, R101+NL [15]</cell><cell>-</cell><cell>81.8 95.1</cell><cell>234 × 30</cell><cell>59.9M</cell></row><row><cell>X3D-M</cell><cell>-</cell><cell>78.8 94.5</cell><cell>6.2 × 30</cell><cell>3.8M</cell></row><row><cell>X3D-XL</cell><cell>-</cell><cell>81.9 95.5</cell><cell>48.4 × 30</cell><cell>11.0M</cell></row></table><note>We report the inference cost with a single "view" (temporal clip with spatial crop) × the numbers of such views used (GFLOPs×views). "N/A" indicates the numbers are not available for us. The "test" column shows average of top1 and top5 on the Kinetics-400 testset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Comparison with the state-of-the-art on Charades. SlowFast variants are based on T ×τ = 16×8.</figDesc><table><row><cell>model</cell><cell></cell><cell>pretrain</cell><cell cols="4">mAP GFLOPs×views Param</cell></row><row><cell>Nonlocal [81]</cell><cell cols="3">ImageNet+Kinetics400 37.5</cell><cell cols="2">544 × 30</cell><cell>54.3M</cell></row><row><cell>STRG, +NL [82]</cell><cell cols="3">ImageNet+Kinetics400 39.7</cell><cell cols="2">630 × 30</cell><cell>58.3M</cell></row><row><cell>Timeception [36]</cell><cell></cell><cell>Kinetics-400</cell><cell>41.1</cell><cell cols="2">N/A×N/A</cell><cell>N/A</cell></row><row><cell>LFB, +NL [84]</cell><cell></cell><cell>Kinetics-400</cell><cell>42.5</cell><cell cols="2">529 × 30</cell><cell>122M</cell></row><row><cell cols="2">SlowFast, +NL [15]</cell><cell>Kinetics-400</cell><cell>42.5</cell><cell cols="2">234 × 30</cell><cell>59.9M</cell></row><row><cell cols="2">SlowFast, +NL [15]</cell><cell>Kinetics-600</cell><cell>45.2</cell><cell cols="2">234 × 30</cell><cell>59.9M</cell></row><row><cell>X3D-XL</cell><cell></cell><cell>Kinetics-400</cell><cell>43.4</cell><cell cols="3">48.4 × 30 11.0M</cell></row><row><cell>X3D-XL</cell><cell></cell><cell>Kinetics-600</cell><cell>47.1</cell><cell cols="3">48.4 × 30 11.0M</cell></row><row><cell>model</cell><cell cols="2">data top-1</cell><cell>top-5</cell><cell cols="3">FLOPs (G) Params (M)</cell></row><row><cell>EfficientNet3D-B0</cell><cell></cell><cell>66.7</cell><cell>86.6</cell><cell>0.74</cell><cell>3.30</cell></row><row><cell>X3D-XS</cell><cell></cell><cell cols="5">68.6 (+1.9) 87.9 (+1.3) 0.60 (−1.4) 3.76 (+0.5)</cell></row><row><cell>EfficientNet3D-B3</cell><cell>K400</cell><cell>72.4</cell><cell>89.6</cell><cell>6.91</cell><cell>8.19</cell></row><row><cell>X3D-M</cell><cell>val</cell><cell cols="5">74.6 (+2.2) 91.7 (+2.1) 4.73 (−2.2) 3.76 (−4.4)</cell></row><row><cell>EfficientNet3D-B4</cell><cell></cell><cell>74.5</cell><cell>90.6</cell><cell>23.80</cell><cell cols="2">12.16</cell></row><row><cell>X3D-L</cell><cell></cell><cell cols="5">76.8 (+2.3) 92.5 (+1.9) 18.37 (−5.4) 6.08 (−6.1)</cell></row><row><cell>EfficientNet3D-B0</cell><cell></cell><cell>64.8</cell><cell>85.4</cell><cell>0.74</cell><cell>3.30</cell></row><row><cell>X3D-XS</cell><cell></cell><cell cols="5">66.6 (+1.8) 86.8 (+1.4) 0.60 (−1.4) 3.76 (+0.5)</cell></row><row><cell>EfficientNet3D-B3</cell><cell>K400</cell><cell>69.9</cell><cell>88.1</cell><cell>6.91</cell><cell>8.19</cell></row><row><cell>X3D-M</cell><cell>test</cell><cell cols="5">73.0 (+2.1) 90.8 (+2.7) 4.73 (−2.2) 3.76 (−4.4)</cell></row><row><cell>EfficientNet3D-B4</cell><cell></cell><cell>71.8</cell><cell>88.9</cell><cell>23.80</cell><cell cols="2">12.16</cell></row><row><cell>X3D-L</cell><cell></cell><cell cols="5">74.6 (+2.8) 91.4 (+2.5) 18.37 (−5.4) 6.08 (−6.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table /><note>Comparison to EfficientNet3D: We compare to a 3D version of EfficientNet on K400-val and test. 10-Center clip testing is used. EfficientNet3D has the same mobile components as X3D.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>CW conv γ b = 0.6 68.9 88.8 1.95 3.16 − CW conv 73.2 90.4 17.6 22.1 − swish 72.0 90.4 1.96 3.76 − SE 71.3 89.9 1.96 3.60 (a) Ablating mobile components on a Small model. model top-1 top-5 FLOPs (G) Param (M) X3D-XL 78.4 93.6 35.84 11.0 − CW conv, γ b = 0.56 76.0 92.6 34.80 9.73 − CW conv 79.2 93.5 365.4 95.1 − swish 78.0 93.4 35.84 11.0 − SE 77.1 93.0 35.84 10.4 (b) Ablating mobile components on an X-Large model.</figDesc><table><row><cell></cell><cell cols="3">model top-1 top-5 FLOPs (G) Param (M)</cell></row><row><cell>X3D-S</cell><cell>72.9 90.5</cell><cell>1.96</cell><cell>3.76</cell></row><row><cell>−</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A .</head><label>A</label><figDesc></figDesc><table /><note>1.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use single-clip, center-crop FLOPs as a basic unit of computational cost. Inference-time computational cost is roughly proportional to this, if a fixed number of clips and crops is used, as is for our all models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The exact expansion factors slightly vary across steps to match the complexity increase,ĉ (which is observable inFig. 2of the main paper).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: I thank Kaiming He, Jitendra Malik, Ross Girshick, and Piotr Dollár for valuable discussions and encouragement.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix provides further details as referenced in the main paper: Sec. A contains additional implementation details for: AVA Action Detection ( §A.1), Charades Action Classification ( §A.2), and Kinetics Action Classification ( §A.3). Sec. B contains further results and ablations on Kinetics-400.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Activitynet-Challenge</surname></persName>
		</author>
		<ptr target="http://activity-net.org/challenges/2017/evaluation.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action search: Spotting actions in videos and its application to temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A short note about Kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Massively parallel video networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Mazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05049</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahman</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SlowFast networks for video recognition in ActivityNet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://static.googleusercontent.com/media/research.google.com/en//ava/2019/fair_slowfast.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno>2018. 9</idno>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatiotemporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02244</idno>
		<title level="m">Searching for MobileNetV3</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature selection: Evaluation, application, and small sample performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Zongker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Irrelevant features and the subset selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George H John</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Köpüklü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02422</idno>
		<title level="m">Resource efficient 3d convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ima-geNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyutae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">VideoLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Grouped spatial-temporal aggregation for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Leaving some stones unturned: dynamic feature prioritization for activity detection in streaming video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Lattice long short-term memory for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03349</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Appearanceand-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc Val</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Coordinate descent algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Mathematical Programming</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Step: Spatio-temporal progressive learning for video action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A structured model for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Faster recurrent networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04226</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Hidden two-stream convolutional networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00389</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">ECO: efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
