<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polysemy Deciphering Network for Robust Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China Uni-versity of Technology</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
							<email>chxding@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China Uni-versity of Technology</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pazhou Lab</orgName>
								<address>
									<postCode>510330</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China Uni-versity of Technology</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Dacheng Tao</surname></persName>
							<email>dacheng.tao@jd.com</email>
							<affiliation key="aff2">
								<orgName type="institution">JD Explore Academy at JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China Uni-versity of Technology</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China Uni-versity of Technology</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">JD Explore Academy at JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Polysemy Deciphering Network for Robust Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human-object interaction · Verb polysemy · Language priors · Attention model</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-Object Interaction (HOI) detection is important to human-centric scene understanding tasks. Existing works tend to assume that the same verb has similar visual characteristics in different HOI categories, an approach that ignores the diverse semantic meanings of the verb. To address this issue, in this paper, we propose a novel Polysemy Deciphering Network (PD-Net) that decodes the visual polysemy of verbs for HOI detection in three distinct ways. First, we refine features for HOI detection to be polysemyaware through the use of two novel modules: namely, Language Prior-guided Channel Attention (LPCA) and Language Prior-based Feature Augmentation (LPFA). LPCA highlights important elements in human and object appearance features for each HOI category to be identified; moreover, LPFA augments human pose and spatial features for HOI detection using language priors, enabling the verb classifiers to receive language hints that reduce intra-class variation for the same verb. Second, we introduce a novel Polysemy-Aware Modal Fusion module (PAMF), which guides PD-Net to make decisions based on feature types deemed more important according to the language priors. Third, we propose to relieve the verb polysemy problem through sharing verb classifiers for semantically similar HOI categories. Furthermore, to ex-Changxing Ding 1,2 pedite research on the verb polysemy problem, we build a new benchmark dataset named HOI-VerbPolysemy (HOI-VP), which includes common verbs (predicates) that have diverse semantic meanings in the real world. Finally, through deciphering the visual polysemy of verbs, our approach is demonstrated to outperform state-of-the-art methods by significant margins on the HICO-DET, V-COCO, and HOI-VP databases. Code and data in this paper are available at https://github.com/MuchHair/PD-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, researchers working in the field of computer vision have begun to pay increasing attention to scene understanding tasks <ref type="bibr" target="#b59">(Zheng et al. 2015;</ref><ref type="bibr" target="#b32">Lu et al. 2016;</ref><ref type="bibr" target="#b57">Zhang et al. 2017;</ref><ref type="bibr" target="#b58">Zhao et al. 2020;</ref><ref type="bibr" target="#b31">Lin et al. 2020</ref>). Since human beings are often central to real-world scenes, Human-Object Interaction (HOI) detection has become a fundamental problem in scene understanding. HOI detection involves not only identifying the classes and locations of objects in the images, but also the interactions (verbs) between each human-object pair. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, an interaction between a human-object pair can be represented by a triplet &lt;person verb object&gt;, herein referred to as one HOI category. One human-object pair may comprise multiple triplets, e.g. &lt;person f ly airplane&gt; and &lt;person ride airplane&gt;.</p><p>The HOI detection task is notably challenging <ref type="bibr" target="#b1">(Chao et al. 2018;</ref><ref type="bibr" target="#b5">Gao et al. 2018)</ref>. One major reason is that verbs can be polysemic. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, a verb may convey substantially different semantic meanings and visual characteristics with respect to different objects, as these objects may have diverse functions and attributes. One pair of examples can be found in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>  more discriminative parts of the human figure for &lt;person play soccer ball&gt; while "hands" are more important for describing &lt;person play f risbee&gt;. A second pair of examples is presented in <ref type="figure" target="#fig_0">Fig. 1(c)</ref> and <ref type="bibr">(d)</ref>. The human-object pairs in (c) and (d), despite being tagged with the same verb, present dramatically different human-object spatial features. Another more serious consideration is that the importance of the same type of visual feature may vary dramatically as the objects of interest change. For example, the human pose plays a vital role in describing &lt;person f ly kite&gt; in <ref type="figure" target="#fig_0">Fig.  1(e)</ref>; by contrast, the human pose is invisible and therefore useless for characterizing &lt;person f ly airplane&gt; in <ref type="figure" target="#fig_0">Fig.  1(f)</ref>. Verb polysemy therefore presents a significant challenge in the HOI detection. The problem of verb polysemy is relatively underexplored, and sometimes even ignored, in existing works <ref type="bibr" target="#b51">(Xu et al. 2019;</ref><ref type="bibr" target="#b25">Li et al. 2019b;</ref><ref type="bibr" target="#b28">Liao et al. 2020;</ref><ref type="bibr" target="#b48">Wang et al. 2020)</ref>. Most contemporary approaches tend to assume that the same verb will have similar visual characteristics across different HOI categories, and accordingly opt to design object-shared verb classifiers. When the verb classifier is shared among all objects, each verb obtains more training samples, thereby promoting the robustness of the classification for HOI categories with a small sample size. However, due to the polysemic nature of the verbs, a dramatic semantic gap may exist between instances of the same verb across different HOI categories. <ref type="bibr" target="#b1">Chao et al. (2018)</ref> constructed objectspecific verb classifiers for each HOI category, which are able to overcome the polysemy problem for HOI categories that have sufficient training samples. However, this approach lacks few-and zero-shot learning abilities for HOI categories where only small amounts of training data are available.</p><p>In this paper, we propose a novel Polysemy Deciphering Network (PD-Net) to address the challenging verb polysemy problem. As illustrated in <ref type="figure">Fig. 2</ref>, PD-Net transforms the multi-label verb classifications for each human-object pair into a set of binary classification problems. Here, each binary classifier is used for the verification of one verb category. The classifiers share the majority of their parameters; the main difference lies in the input features. Next, we decode the verb polysemy in the following three ways.</p><p>First, we enable features sent for each binary classifier to be polysemy-aware using two novel modules, namely Language Prior-guided Channel Attention (LPCA) and Language Prior-based Feature Augmentation (LPFA). The language priors are two word embeddings made up of one verb and one object. The object class is predicted by one object detector; the verb is the one to be determined by one specific binary verb classifier. For its part, LPCA is applied to both the human and object appearance features. The two appearance features are usually redundant, as only part of their information is involved for one specific HOI category (see <ref type="figure" target="#fig_0">Fig.  1</ref>). Therefore, LPCA is used to highlight important elements in the appearance features for each binary classifier. Moreover, both human-object spatial and human pose features are often vague and can vary dramatically for the same verb, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref> and (b), (c) and (d); we therefore propose LPFA, which concatenates the two features with language priors, respectively. In this way, the classifiers can receive hints to reduce the intra-class variation of the same verb for the pose and spatial features.</p><p>We further design a novel Polysemy-Aware Modal Fusion module (PAMF), which produces attention scores based on the above language priors in order to dynamically fuse multiple feature types. The language priors provide hints regarding the importance of the features for each HOI category. As can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>, the human pose feature is discriminative when the language prior is "fly kite" <ref type="figure" target="#fig_0">(Fig.  1(e)</ref>), but is less useful when the language prior is "fly airplane" <ref type="figure" target="#fig_0">(Fig. 1(f)</ref>). Therefore, our proposed PAMF deciphers the verb polysemy problem by highlighting the features that are more important for each HOI category.  <ref type="figure">Fig. 2</ref> Visual features of each human-object pair are duplicated multiple times so that polysemy-aware visual features can be obtained under the guidance of language priors. Each polysemy-aware feature is sent to a specific verb classifier, which can be any type of the three verb classifiers mentioned in this paper. To reduce the number of duplicated human-object pairs, meaningless HOI categories (e.g. &lt;person eat book&gt; and &lt;person ride book&gt;) are ignored. Meaningful and common HOI categories (e.g. &lt;person hold book&gt; and &lt;person read book&gt;) are available in each popular HOI detection database. Multiple verbs are "checked" in this figure as HOI detection performs multi-label verb classification for each human-object pair.</p><p>Moreover, as mentioned above, both object-shared and object-specific verb classifiers have limitations. We therefore propose a novel clustering-based object-specific verb classifier, which combines the advantages of object-shared and object-specific classifiers. The main motivation is to ensure that semantically similar HOI categories containing the same verb, e.g. &lt;person hold cow&gt; and &lt;person hold elephant&gt; can share the same verb classifier. HOIs that are semantically very different (e.g. &lt;person hold book&gt; and &lt;person hold backpack&gt;) are identified using another verb classifier. In this way, the verb polysemy problem is mitigated. Meanwhile, clustering-based object-specific classifiers has the capacity to handle the few-and zero-shot learning problems that arise in HOI detection, since we merge the training data of semantically similar HOI categories.</p><p>To the best of our knowledge, our proposed PD-Net is the first approach to explicitly handle the verb polysemy problem in HOI detection. More impressively, our experimental results on three databases demonstrate that our approach consistently outperforms state-of-the-art methods by considerable margins. A preliminary version of this paper has been published in <ref type="bibr" target="#b60">(Zhong et al. 2020)</ref>. Compared with the conference version, this version further proposes a novel Language Prior-guided Channel Attention module, simplifies the architecture of PD-Net by using Clustering-based Object-Specific classifiers, builds a new database (named HOI-VP) to facilitate the research on the verb polysemy problem, and includes further experimental investigations.</p><p>The remainder of this paper is organized as follows. Section 2 briefly reviews related works. The details of the proposed components of PD-Net are described in Section 3. The databases and implementation details are introduced in Section 4, while the experimental results are presented in Section 5. Finally, we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Human-Object Interaction Detection. HOI detection performs multi-label verb classification for each human-object pair, meaning that the interaction between the same humanobject pair may be described using multiple verbs. Depending on the order of verb classification and target object association, existing HOI detection approaches can be divided into two categories. The first category of methods infer the verb actions being performed by one person, then associate each verb with a single object in the image. Multiple target object association approaches have been proposed. For example, <ref type="bibr" target="#b43">Shen et al. (2018)</ref> proposed an approach based on the value of object detection scores, while <ref type="bibr" target="#b9">Gkioxari et al. (2018)</ref> fitted a distribution density function of the target object locations based on the human appearance feature. Moreover, <ref type="bibr" target="#b41">Qi et al. (2018)</ref> adopted a graph parsing network to associate the target objects. <ref type="bibr" target="#b28">Liao et al. (2020)</ref> and <ref type="bibr" target="#b48">Wang et al. (2020)</ref> first defined interaction points for HOI detection; next, they locate the interaction points and associate each point with one human-object pair.</p><p>The second category of methods first pair each human instance with all object instances as candidate human-object pairs, then recognize the verb for each candidate pair <ref type="bibr" target="#b12">(Gupta et al. 2019</ref>). Many types of features have been employed to promote the verb classification performance. For example, <ref type="bibr" target="#b45">Wan et al. (2019)</ref> employed both human parts and poseaware features for verb classification, while <ref type="bibr" target="#b52">Xu et al. (2020)</ref> exploited human gaze and intention to assist HOI detection. Furthermore, <ref type="bibr" target="#b47">Wang et al. (2019a)</ref> extracted context-aware human and object appearance features to promote HOI detection performance. <ref type="bibr" target="#b26">Li et al. (2020a)</ref> utilized 3D pose models and 3D object location to assist HOI detection. Moreover, <ref type="bibr" target="#b27">Li et al. (2020b)</ref> annotated large amounts of part-level human-object interactions and trained a PaStaNet, which is helpful for HOI detection models to make use of fine-grained human part features. A large number of novel model architectures for HOI detection have been also developed. For example, <ref type="bibr" target="#b25">Li et al. (2019b)</ref> introduced a Transferable Interactiveness Network that suppresses candidate pairs without interactions. <ref type="bibr" target="#b40">Peyre et al. (2019)</ref> constructed a multi-stream model that projects visual features and word embeddings to a joint space, which is helpful for unseen HOI category detection. <ref type="bibr" target="#b51">Xu et al. (2019)</ref> constructed a graph neural network to promote the quality of word embeddings by utilizing the correlation between semantically similar verbs, while <ref type="bibr" target="#b63">Zhou et al. (2020)</ref> proposed a cascade architecture that facilitates coarse-to-fine HOI detection.</p><p>Besides, HOI recognition <ref type="bibr" target="#b80">(Chao et al. 2015)</ref> is one similar task to HOI detection. Briefly, HOI recognition methods predict all possible HOI categories in an image but they do not detect the location of involved human-object pairs. For example, <ref type="bibr" target="#b72">Kato et al. (2018)</ref> proposed to compose classifiers for unseen verb-noun pairs by leveraging an external knowledge graph and graph convolutional networks. The Polysemy Problem. This problem is very common in our daily life. There have been some researches exploring the polysemy problem, e.g. natural language processing <ref type="bibr" target="#b78">(Ma et al. 2020)</ref>, <ref type="bibr" target="#b77">(Huang et al. 2012)</ref>, <ref type="bibr" target="#b81">(Oomoto et al. 2017</ref>) and recommendation systems . For example, the polysemy problem in natural language processing is mainly due to different usages of a word in grammar, e.g., functioned as a verb or a noun <ref type="bibr" target="#b78">(Ma et al. 2020)</ref>. Besides, each node in a recommendation system could have multiple facets because of the different links with its neighbour nodes, which brings in the node polysemy problem .</p><p>In this paper, we address the verb polysemy problem in HOI detection, i.e., a verb may convey substantially different visual characteristics when associated with different objects. There are also some related areas that may face the same verb polysemy problem, e.g. Action Recognition <ref type="bibr" target="#b73">(Simonyan et al. 2014;</ref><ref type="bibr" target="#b74">Tran et al. 2015;</ref><ref type="bibr" target="#b75">Damen et al. 2018)</ref> and Visual Relationship Detection <ref type="bibr" target="#b32">(Lu et al. 2016;</ref><ref type="bibr" target="#b21">Krishna et al. 2017;</ref><ref type="bibr" target="#b22">Kuznetsova et al. 2020;</ref><ref type="bibr" target="#b19">Ji et al. 2020)</ref>.</p><p>Action recognition methods aim to recognize human actions from an image or a video. In particular, the EPIC-KITCHENS Dataset <ref type="bibr" target="#b75">(Damen et al. 2018</ref>) is a new largescale egocentric video benchmark for action recognition tasks. Besides recognizing the human action from the video, the task on this dataset also involves identifying the interacted object category. It includes 125 verb and 352 noun cate-gories. There are also many verbs suffering from the polysemy problem, e.g. "hold", "open" and "close".</p><p>Visual relationship detection involves detecting and localizing pairs of objects in an image and also classifying the predicate or interaction between each subject-object pair. Different from HOI detection, the subject for each subjectobject pair in visual relationship detection can be any object category besides human. Polysemy verbs, e.g. "carry", "ride", "hold" are also common relationships in visual relationship detection. Therefore, visual relationship detection also suffers from the polysemy problem.</p><p>Furthermore, as the vast majority of HOI detection methods are based on single images <ref type="bibr" target="#b5">(Gao et al. 2018;</ref><ref type="bibr" target="#b51">Xu et al. 2019;</ref><ref type="bibr" target="#b25">Li et al. 2019b;</ref><ref type="bibr" target="#b12">Gupta et al. 2019;</ref><ref type="bibr" target="#b40">Peyre et al. 2019;</ref><ref type="bibr" target="#b41">Qi et al. 2018;</ref><ref type="bibr" target="#b28">Liao et al. 2020;</ref><ref type="bibr" target="#b48">Wang et al. 2020;</ref><ref type="bibr" target="#b44">Ulutan et al. 2020)</ref>, we only consider the verb polysemy problem for image-based HOI detection. However, image-based methods ignore the temporal information, which also provides rich cues to decipher the verb polysemy problem. Therefore, we expect more works on video-based HOI detection in the future. The Exploitation of Language Priors. Language priors have also been successfully utilized in many computer visionrelated fields, including Scene Graph Generation <ref type="bibr" target="#b32">(Lu et al. 2016;</ref><ref type="bibr" target="#b57">Zhang et al. 2017;</ref><ref type="bibr" target="#b10">Gu et al. 2019;</ref><ref type="bibr" target="#b49">Wang et al. 2019b</ref>), Image Captioning <ref type="bibr" target="#b54">Yao et al. 2019)</ref>, and Visual Question Answering <ref type="bibr" target="#b6">Gao et al. 2019;</ref><ref type="bibr" target="#b34">Marino et al. 2019)</ref>. Moreover, several works <ref type="bibr" target="#b51">(Xu et al. 2019;</ref><ref type="bibr" target="#b40">Peyre et al. 2019</ref>) have adopted language priors for HOI detection. All of these approaches project visual features and word embeddings to a joint space, which improves HOI detection by exploiting the semantic relationship between similar verbs or HOI categories (e.g. "drink" and "sip" or "ride horse" and "ride cow"). However, these works do not employ language priors to solve the challenging verb polysemy problem. Compared with the above methods, PD-Net aims to solve the verb polysemy problem by using three novel language prior-based components: Language Prior-guided Channel Attention, Language Prior-based Feature Augmentation, and Polysemy-Aware Modal Fusion. Attention Models. Attention mechanisms are becoming a popular component in computer vision tasks, including Image Captioning <ref type="bibr" target="#b53">Xu et al. 2015;</ref><ref type="bibr" target="#b55">You et al. 2016)</ref>, Action Recognition <ref type="bibr" target="#b7">(Girdhar et al. 2017;</ref><ref type="bibr" target="#b36">Meng et al. 2019)</ref>, and Pose Estimation <ref type="bibr" target="#b23">(Li et al. 2019a;</ref><ref type="bibr" target="#b0">Ye et al. 2016)</ref>. Existing studies on attention mechanisms can be roughly divided into three categories: namely, hard regional attention <ref type="bibr" target="#b18">(Jaderberg et al. 2015;</ref><ref type="bibr" target="#b24">Li et al. 2018;</ref><ref type="bibr" target="#b50">Wang et al. 2021)</ref>, soft spatial attention <ref type="bibr" target="#b46">(Wang et al. 2017;</ref><ref type="bibr" target="#b39">Pereira et al. 2019;</ref><ref type="bibr" target="#b65">Zhu et al. 2018)</ref>, and channel attention <ref type="bibr" target="#b39">(Pereira et al. 2019;</ref><ref type="bibr" target="#b15">Hu et al. 2018;</ref><ref type="bibr" target="#b76">Ding et al. 2020</ref>). Hard regional attention methods typically predict regions of interest (ROIs) first, and then only utilize features in ROIs for subsequent tasks.  <ref type="figure">Fig. 3</ref> Overview of the Polysemy Deciphering Network. For the sake of simplicity, only one binary CSP classifier (for "hold") is illustrated here. PD-Net takes four feature streams as input: the human appearance stream (H stream), the object appearance stream (O stream), the human-object spatial stream (S stream), and the human pose stream (P stream). These four feature streams are first processed by either LPCA or LPFA to be polysemy-aware. They are then sent to the H, O, S, and P blocks for binary classification, respectively. Subsequently, the classification scores from the four feature streams are fused using the attention scores produced by PAMF. Here, ⊗ and ⊕ denote the element-wise multiplication and addition operations, respectively.</p><p>In comparison, soft spatial attention and channel attention (CA) models use soft weights to highlight important features in the spatial and channel dimensions, respectively. There have also been existing works that adopted attention models to assist with HOI detection tasks. For example, <ref type="bibr" target="#b5">Gao et al. (2018)</ref> and <ref type="bibr" target="#b47">Wang et al. (2019a)</ref> employed an attention mechanism to enhance the human and object features by aggregating contextual information. <ref type="bibr" target="#b45">Wan et al. (2019)</ref> proposed the PMFNet model, which adopts human pose and spatial features as cues to infer the importance of each human part. <ref type="bibr" target="#b44">Ulutan et al. (2020)</ref> used cues derived from a human-object spatial configuration to highlight the important elements in appearance features.</p><p>To the best of our knowledge, few works thus far have made use of the attention mechanism to solve the verb polysemy problem in HOI detection. Moreover, existing attention models for HOI detection usually employ visual features (e.g. the appearance and pose features) as cues. By contrast, our proposed Language Prior-guided Channel Attention and Polysemy-Aware Modal Fusion adopt language priors as cues; these priors have clear semantic meanings, and are therefore well-suited to resolving the verb polysemy problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We first formulate a basic HOI detection scheme that is adopted by most existing works <ref type="bibr" target="#b12">(Gupta et al. 2019;</ref><ref type="bibr" target="#b25">Li et al. 2019b;</ref><ref type="bibr" target="#b45">Wan et al. 2019;</ref><ref type="bibr" target="#b62">Zhou and Chi 2019;</ref><ref type="bibr" target="#b26">Li et al. 2020a</ref>). Then, we explain the verb polysemy problem and formulate the PD-Net framework. Finally, we describe each of the key components in PD-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given an image I, human and object proposals are generated using Faster R-CNN <ref type="bibr" target="#b42">(Ren et al. 2015)</ref>. Each human proposal h and each object proposal o are paired as a candidate for verb classification. HOI detection models <ref type="bibr" target="#b12">(Gupta et al. 2019;</ref><ref type="bibr" target="#b25">Li et al. 2019b;</ref><ref type="bibr" target="#b45">Wan et al. 2019;</ref><ref type="bibr" target="#b62">Zhou and Chi 2019;</ref><ref type="bibr" target="#b26">Li et al. 2020a</ref>) then produce a set of verb classification scores for each candidate pair (h, o). The classification scores for verb v can be represented as follows:</p><formula xml:id="formula_0">S (h,o,v) = σ( i T (i,v) (G i (I, h, o))),<label>(1)</label></formula><p>where i is a subscript that stands for the i-th feature type. Function G i (·) denotes the model that produces the i-th type of features <ref type="bibr" target="#b12">(Gupta et al. 2019;</ref><ref type="bibr" target="#b25">Li et al. 2019b;</ref><ref type="bibr" target="#b45">Wan et al. 2019;</ref><ref type="bibr" target="#b62">Zhou and Chi 2019;</ref><ref type="bibr" target="#b26">Li et al. 2020a)</ref>. T (i,v) (·) represents the classifier for the verb v using features produced by G i (·). σ(·) denotes the sigmoid activation function. As described in Section 1, existing works <ref type="bibr" target="#b12">(Gupta et al. 2019;</ref><ref type="bibr" target="#b25">Li et al. 2019b;</ref><ref type="bibr" target="#b45">Wan et al. 2019;</ref><ref type="bibr" target="#b62">Zhou and Chi 2019;</ref><ref type="bibr" target="#b26">Li et al. 2020a</ref>) suffer from the verb polysemy problem.</p><p>First, owing to the large intra-class appearance variance for one verb, it is challenging for function G i (·) to learn discriminative visual features for a polysemic verb. Second, the importance of the same feature type may vary dramatically as the objects of interest change. Third, T (i,v) (·) is often shared by different HOI categories with the same verb, which makes it difficult for T (i,v) (·) to capture important visual cues to identify one polysemic verb. Therefore, Eq. (1) cannot well address the verb polysemy problem. Accordingly, in this paper, we propose PD-Net to address the above three problems. Given one human-object pair (h, o), the classification scores for verb v predicted by PD-Net can be represented as follows:</p><formula xml:id="formula_1">S PD (h,o,v) = σ( i a (i,o,v) T (i,o,v) (G i (I, h, o, w o , w v ))),<label>(2)</label></formula><p>where w o and w v represent the word embeddings for the object and verb categories, respectively. G i (·) can leverage word embeddings of one verb-object pair (HOI category) to generate polysemy-aware features. a <ref type="bibr">(i,o,v)</ref> denotes attention scores produced by the Polysemy-Aware Modal Fusion module. T <ref type="bibr">(i,o,v)</ref> represents the clustering-based objectspecific classifiers that are shared by semantically similar HOI categories with the same verb.</p><p>In the following, we introduce the framework of PD-Net based on Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of PD-Net</head><p>The framework of PD-Net is illustrated in <ref type="figure">Fig. 3</ref>. Similar to existing works <ref type="bibr" target="#b25">(Li et al. 2019b;</ref><ref type="bibr" target="#b12">Gupta et al. 2019;</ref><ref type="bibr" target="#b45">Wan et al. 2019;</ref><ref type="bibr" target="#b62">Zhou and Chi 2019;</ref><ref type="bibr" target="#b26">Li et al. 2020a</ref>), four types of visual features are adopted, i.e., human appearance, object appearance, human-object spatial, and human pose features. We construct these four types of features for PD-Net following <ref type="bibr" target="#b12">(Gupta et al. 2019)</ref>. In particular, the human and object appearance features are K A -dimensional vectors that are extracted from Faster R-CNN model using human and object bounding boxes, respectively. The human-object spatial feature is a 42-dimensional vector encoded using the bounding box coordinates of one human-object pair. Moreover, we use a pose estimation model <ref type="bibr" target="#b4">(Fang et al. 2017</ref>) to obtain the coordinates of 17 keypoints for each human instance. Subsequently, following <ref type="bibr" target="#b12">(Gupta et al. 2019)</ref>, the human keypoints and the bounding box coordinates of object proposal are then encoded into a 272-dimensional pose feature vector.</p><p>As outlined in <ref type="figure">Fig. 2</ref>, we transform the multi-label verb classification into a set of binary classification problems. Each of the binary classifiers is used for one verb category verification and only processes features that use this verb as language priors. Moreover, each binary classifier includes a set of H, O, S, and P blocks; Apart from the final layer which is used for verb prediction, the parameters of the other layers in each respective block are shared across different binary classifiers. Therefore, their overall model size is comparable to an ordinary multi-label classifier. The binary classifiers mainly differ in terms of their input features and the way in which they combine predictions from the four feature streams.</p><p>In the following, we propose four novel components to handle the verb polysemy problem in HOI detection. First, we introduce the Language Prior-guided Channel Attention and Language Prior-based Feature Augmentation modules, which facilitate the four types of features being polysemyaware. Second, we design the Polysemy-Aware Modal Fusion module that adaptively fuses the prediction scores produced by the four feature streams and obtains the final prediction score for each binary classifier. Finally, we propose a Clustering-based Object-Specific classification scheme that strikes a balance between resolving the verb polysemy problem and reducing the number of binary classifiers in PD-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Polysemy-Aware Feature Generation</head><p>We here introduce two novel components, i.e. Language Priorguided Channel Attention and Language Prior-based Feature Augmentation, that generate polysemy-aware features. The two components are denoted as G i (·) in Eq. (2). The language prior we used in this paper is the concatenated word embedding of two words: one word denotes the verb to be identified (w v in Eq. (2)), while the other one is the detected object in the human-object pair (w o in Eq. (2)). The word embeddings are generated using the word2vec tool, which was trained on the Google News dataset <ref type="bibr" target="#b37">(Mikolov et al. 2013)</ref>. The dimension of the language prior is 600. Language Prior-guided Channel Attention. Both the human and object appearance features are usually redundant as only part of their information is involved for a specific HOI category. One example can be found in <ref type="figure" target="#fig_0">Fig. 1(a)</ref> and (b), where the human body parts most relevant to "play soccer ball" and "play frisbee" are the "feet" and "hands", respectively. We therefore propose Language Prior-guided Channel Attention (LPCA) to highlight the important elements in human and object appearance features, based on the channel attention scheme and the guidance of language priors. LPCA is realized through two steps, which are outlined below.</p><p>First, we infer the important elements in the human or object appearance feature F A using language priors. F A is extracted from the Faster R-CNN model using a bounding box. As can be seen from <ref type="figure">Fig. 4</ref>, the language prior is projected to a K A -dimensional vector (denoted as L A ) via two successive fully-connected layers. The dimension of the first one is set to K A 2 ; K A is equal to the dimension of F A . Similar to <ref type="bibr" target="#b40">(Peyre et al. 2019)</ref>, L A is normalized via its L2 norm. To drive L A to pay attention to important elements in F A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hold book</head><formula xml:id="formula_2">· · · Fig. 4</formula><p>Model structure of LPCA. F A denotes the human or object appearance feature. P (·) and D(·) are realized by successive fullyconnected layers. ⊗ denotes the element-wise multiplication operation.</p><p>regarding to the verb-object pair, we perform element-wise multiplication between L A and F A , as follows:</p><formula xml:id="formula_3">L B = F A ⊗ L A ,<label>(3)</label></formula><p>and further compute the summation of elements in L B :</p><formula xml:id="formula_4">S au = σ(Sum(L B )),<label>(4)</label></formula><p>where σ denotes the sigmoid activation function and Sum(·) denotes the summation of all elements in one vector. During the training stage, we minimize the binary cross-entropy loss between S au and the binary label for the verb to verify. During inference, the operation in Eq. (4) can be ignored. By optimizing the fully-connected layers via the verb verification goal, the value of elements in L A can reflect the importance of the corresponding elements in F A . It is worth noting that the quality of both L A and L B can be affected by the discrepancy between visual features and word embeddings, since the word embeddings are not specifically designed for computer vision tasks <ref type="bibr" target="#b51">(Xu et al. 2019)</ref>. Therefore, directly using L B as representation for verb verification may be suboptimal. Consequently, to handle this problem, we propose the following strategy to further enhance the quality of the representations.</p><p>Second, we obtain attention scores based on L B via a plain channel attention module:</p><formula xml:id="formula_5">C att = σ(D(L B )),<label>(5)</label></formula><p>where C att stands for the final channel attention scores for F A . D(·) is realized by two successive fully-connected layers. By imposing a supervision on S au , elements in L B that are relevant to the verb to verify have large values, which facilitates the learning of an effective channel attention. For its part, the plain CA module then makes use of the correlation between elements in L B to promote the quality of attention scores. Finally, the polysemy-aware human or object appearance features can be obtained via the following equation:</p><formula xml:id="formula_6">F A = F A ⊗ C att ,<label>(6)</label></formula><p>Existing channel attention models, e.g. the SE network <ref type="bibr" target="#b15">(Hu et al. 2018)</ref>, usually utilize appearance features to generate channel attention scores for the features themselves. Besides, the VSGNet model <ref type="bibr" target="#b44">(Ulutan et al. 2020</ref>) adopted the human-object spatial features as cues to infer channel attention scores for the appearance feature. Compared with the above two works, LPCA contains clearer semantic information for each HOI. First, the input of LPCA includes language priors, which have clear semantic meaning. Second, LPCA imposes an auxiliary binary cross-entropy loss between S au and the verb to verify. This extra loss enables the visual features for different HOI categories with the same verb to be polysemy-aware according to their specific language priors. Moreover, language priors are adopted in <ref type="bibr" target="#b40">(Peyre et al. 2019)</ref> to construct verb classifiers. In their method, human and object features are fixed. In comparison, we utilize language priors to generate polysemy-aware features, which are adjustable for each verb-object pair to identify.</p><p>Finally, LPCA can also be regarded as cross-modal attention  or a conditioning module <ref type="bibr" target="#b70">(Perez et al. 2018</ref>). The differences between LPCA and existing cross-modal attention methods (e.g. ViLBERT ) and conditioning modules (e.g. FiLM <ref type="bibr" target="#b70">(Perez et al. 2018)</ref>) are illustrated as follows. First, ViLBERT utilizes a set of image region features and a sequence of word embeddings as input. Then, it conducts cross modal attention based on the standard transformer blocks <ref type="bibr" target="#b71">(Vaswani et al. 2017)</ref>, which compute query, key, and value matrices to produce the attentionpooled features. In comparison, LPCA only uses one visual feature vector and one text segment (the concatenation of the word embeddings of the verb and object category) as the input. The Hadamard product between the features F A and L A is utilized to generate the channel attention scores. Second, FiLM <ref type="bibr" target="#b70">(Perez et al. 2018</ref>) carries out a feature-wise affine transformation on a deep model's intermediate features, directly conditioned on the language prior. In comparison, in order to address the discrepancy between visual features and word embeddings, LPCA generates channel attention scores based on the correlation between visual feature and word embeddings. Language Prior-based Feature Augmentation. Language Prior-based Feature Augmentation (LPFA) is applied to humanobject spatial and human pose features. As illustrated in <ref type="figure" target="#fig_0">Fig.  1(c)</ref> and <ref type="formula">(d)</ref>, (e) and (f), the spatial and pose features are of-ten vague and vary dramatically for the same verb, meaning that they contain insufficient information. Therefore, we propose LPFA to augment the pose and spatial features. More specifically, we concatenate each of the two features with the 600-dimensional language prior As a result of this concatenation, the classifiers receive hints that can aid in reducing the intra-class variation of the same verb for the pose and spatial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Polysemy-Aware Modal Fusion</head><p>As illustrated in <ref type="figure">Fig. 3</ref>, the four feature streams are sent to the H, O, S and P blocks, respectively. The H and O blocks are constructed using two successive fully-connected layers, while the S and P blocks are constructed using three successive fully-connected layers. In the interests of simplicity, the dimension of each hidden fully-connected layer is set as the dimension of its input feature vector. The output dimension of these four blocks is set to K C , which is the number of binary classifiers in PD-Net.</p><p>As discussed in Section 1, one major challenge posed by the verb polysemy problem is that the relative importance of each of the four feature streams to the identification of the same verb may vary dramatically as the objects change. As shown in <ref type="figure" target="#fig_0">Fig. 1(e</ref>) and (f), the human appearance and pose features are most important for detecting &lt;person f ly kite&gt;; by contrast, these features are almost invisible and therefore less useful for detecting &lt;person f ly airplane&gt;. Therefore, we propose Polysemy-Aware Modal Fusion (PAMF) to generate attention scores that dynamically fuse the predictions of the four feature streams. To the best of our knowledge, PAMF is the first attention module that effectively uses language prior to address the verb polysemy problem in HOI detection. In more detail, we use the same 600-dimensional word embedding (e.g. "hold book") that is implemented in LPCA and LPFA. The language prior is fed into two successive fully-connected layers, the dimensions of which are 48 and 4, respectively. The first fully-connected layer is followed by a ReLU layer, while the second one is followed by a sigmoid activation function. The output of PAMF is used as attention scores for the four feature streams. In this way, the important feature streams with respect to each HOI category is highlighted, while those that are less important are suppressed. We can see that PAMF is a weight-light module that can be effectively optimized even with limited training data. Moreover, we use the pre-trained word embeddings <ref type="bibr" target="#b37">(Mikolov et al. 2013)</ref> as input for PAMF. These word embeddings have semantical relationships with each other as prior, which further reduces the optimization difficulty of PAMF.</p><p>Therefore, Eq.</p><p>(2) can be rewritten as follows:</p><formula xml:id="formula_7">S PD (h,o,v) = σ( i∈{H,O,S,P} a (i,o,v) s (i,o,v) ),<label>(7)</label></formula><p>where i denotes one feature stream, while a <ref type="bibr">(i,o,v)</ref> is the attention score generated by PAMF for the i-th feature stream. s <ref type="bibr">(i,o,v)</ref> is the output for verb v generated by the i-th feature stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Clustering-based Object Specific Verb Classifiers</head><p>Although the above proposed components, i.e. LPCA, LPFA, and PAMF, can help object-shared verb classifiers to relieve the verb polysemy problem, the defects of object-shared verb classifiers still remains. The essential reason is that HOIs with different objects share the same verb classifier. Under ideal circumstances, object-specific verb classifiers can overcome this problem if sufficient training data exists for each HOI category. However, if we assume that the number of object categories is |O| and the number of verb categories is |V |, the total number of their combinations will therefore be |O|×|V |, which is usually very large even if meaningless HOI categories are removed. Therefore, it is too difficult to obtain sufficient train samples for each HOI category. Moreover, due to the class imbalance problem for HOI categories, the object-specific classifiers lack few-and zero-shot learning abilities for HOI categories which have small amount of training data. Therefore, both types of verb classifiers have limitations.</p><p>In this subsection, we introduce a novel verb classifier, named Clustering-based object-SPecific (CSP) verb classifiers, which are denoted as T <ref type="bibr">(i,o,v)</ref> in Eq. (2). CSP classifiers can strike a balance between overcoming the verb polysemy problem and handling the zero-or few-shot learning problems. The main motivation behind CSP classifiers is that some HOIs tagged with the same verb are both semantically and visually similar, e.g. &lt;person hold sheep&gt;, &lt;person hold horse&gt;, and &lt;person hold cow&gt;; therefore, they can share the same verb classifier, meaning that the number of SP classifiers is consequently reduced. In more detail, we first obtain all meaningful and common HOI categories for each verb, which are available in popular databases such as HICO-DET <ref type="bibr" target="#b1">(Chao et al. 2018</ref>) and V-COCO <ref type="bibr" target="#b11">(Gupta and Malik 2015)</ref>. The number of meaningful HOI categories including the verb v is indicated by O v . We then use the Kmeans method <ref type="bibr" target="#b33">(MacQueen et al. 1967)</ref> to cluster the HOI categories with the same verb v into C v clusters according to the cosine distance between the word embeddings of the objects. We empirically set the C v for each verb as a rounded number of the square root of O v .</p><p>We provide visualization of clustering results for some polysemic verbs in the supplementary file. During both training and inference, only one CSP classifier is adopted to predict whether the verb is positive for one verb-object pair. The adopted CSP classifier is determined by the object category in the verb-object pair. This clustering strategy is also capable of handling the few-and zero-shot HOI detection problems <ref type="bibr" target="#b69">(Bansal et al. 2020)</ref>. For example, during testing, a new HOI category &lt;person hold elephant&gt; can share the same classifier with other HOI categories that have similar semantic meanings (e.g. &lt;person hold horse&gt;).</p><p>Besides our automatic way to cluster semantically similar HOI categories, one alternative is to find all possible semantic meanings for a verb from an English dictionary. A dictionary usually elaborates different semantic meanings for each verb. Therefore, the number of clusters for a verb can be determined. Then, one may manually associate each HOI category with one semantic meaning.</p><p>One recent work <ref type="bibr" target="#b69">(Bansal et al. 2020</ref>) also utilized clustering methods to achieve zero-shot HOI detection. There are two differences between the clustering strategies in this work and <ref type="bibr" target="#b69">(Bansal et al. 2020)</ref>. First, we utilize clustering to build new verb classifiers. In comparison, the clustering strategy in <ref type="bibr" target="#b69">(Bansal et al. 2020</ref>) is used to generate new training data. Second, we cluster the available HOI categories in one database for each respective verb. In comparison, clustering is conducted only once in <ref type="bibr" target="#b69">(Bansal et al. 2020</ref>). Our idea is that some HOI categories are meaningless. By clustering the available HOI categories for each verb, we can obtain more meaningful and fine-grained clustering result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training and Testing</head><p>Training. PD-Net can be conceptualized as a multi-task network. Its loss for the verification of the verb v in one HOI category (h, v, o) can be represented as follows:</p><formula xml:id="formula_8">L (h,o,v) = L BCE (S PD (h,o,v) , l v ) + i∈{H,O} L BCE (S i au , l v ),<label>(8)</label></formula><p>where L BCE represents binary cross-entropy loss, while l v denotes a binary label (l v ∈ {0, 1}) for one verb to verify. Moreover, S H au and S O au denote the output of Eq. (4) for the human and object appearance features, respectively. Testing. During testing, we use the same method as that utilized in the training stage to obtain the language priors. Here, the object category in the prior is predicted using Faster R-CNN (rather than the ground-truth); the verb category in the prior varies for each binary classifier of the verb. Following <ref type="bibr" target="#b25">(Li et al. 2019b</ref><ref type="bibr" target="#b26">(Li et al. , 2020a</ref><ref type="bibr" target="#b44">Ulutan et al. 2020;</ref><ref type="bibr" target="#b45">Wan et al. 2019)</ref>, we also construct an Interactiveness Network (INet) capable of suppressing pairs without interaction. Finally, the prediction score for one HOI category (h, v, o) is represented as follows:</p><formula xml:id="formula_9">S HOI (h,o,v) = S h × S o × S PD (h,o,v) × S I (h,o) ,<label>(9)</label></formula><p>where S h and S o are the detection scores of human and object proposals, respectively, while S I (h,o) denotes the prediction score generated by the pre-trained INet. In the experi- HOI-VerbPolysemy (HOI-VP) is a new database constructed in this paper. To the best of our knowledge, this is the first database to be designed explicitly for the verb polysemy problem in HOI detection. In more detail, it consists of 15 common verbs (predicates) that have rich and diverse semantic meanings. It also contains 517 common objects in real-world scenarios. Each verb is included in an average of 55 HOI categories, as detailed in <ref type="table" target="#tab_1">Table 1</ref>. In particular, "in" and "on" are two highly common predicates that are also polysemic in visual relationship detection tasks <ref type="bibr" target="#b32">(Lu et al. 2016;</ref><ref type="bibr" target="#b21">Krishna et al. 2017;</ref><ref type="bibr" target="#b22">Kuznetsova et al. 2020;</ref><ref type="bibr" target="#b19">Ji et al. 2020)</ref> and are thus both included in the HOI-VP database. There are 21,928 and 7,262 images used for training and testing, respectively. All images are collected from the VG database <ref type="bibr" target="#b21">(Krishna et al. 2017)</ref>, while the corresponding annotations are provided by the HCVRD database <ref type="bibr" target="#b67">(Zhuang et al. 2017</ref>). In the HOI-VP dataset, we only use images that were labelled with the 15 predicates listed in <ref type="table" target="#tab_1">Table 1</ref>. These images and their labels are collected based on the HCVRD dataset. Therefore, the images of HOI-VP can be considered as a subset of HCVRD.</p><p>It is worth noting here that the annotations in HCVRD contain noise. For example, the same verb may be annotated with different words, e.g., "hold", "holds", and "holding", while a similar problem exists for the objects, e.g., "camera", "digital camera", and "video camera". We therefore merge different annotations for the same verb or object categories, respectively. In the following, we take "hold" as example to explain the correction of annotations. We search for the highly relevant labels with the key word "hold". Then, we manually check the images with these labels, and make sure that these labels indeed have the same semantic meaning. Finally, we merge the labels of the same semantic meaning with "hold". The annotation noise for each object category is mainly from its fine-grained attributes. For example, the object "shirt" may be labelled as "black shirt", "blue shirt", and "stripe shirt". The merging steps for object labels are the same as those for the verbs. Some sample images from HOI-VP are illustrated in <ref type="figure">Fig. 9</ref>. This database will be made publicly available to expedite research into the verb polysemy problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>According to the official protocols <ref type="bibr" target="#b1">(Chao et al. 2018;</ref><ref type="bibr" target="#b11">Gupta and Malik 2015)</ref>, mean average precision (mAP) is used as the evaluation metric for HOI detection on both HICO-DET and V-COCO datasets. A positive human-object pair must meet the following requirements: first, the predicted HOI category must be the same type as the ground truth; second, both the human and object proposals must have an Intersection over Union (IoU) with the ground truth proposals of more than 0.5. Moreover, there are two mAP modes in HICO-DET, namely the Default (DT) mode and the Known-Object (KO) mode. In the DT mode, we calculate the average precision (AP) for each HOI category in all testing images. In the KO mode, the object categories in all images are known; therefore, we need only to compute the AP for each HOI category from images containing the interested object. For example, we evaluate the AP of &lt;person ride horse&gt; using only those testing images that contain a "horse". Since the images that contain the object category of interest are known, the KO mode is better able to reflect the verb classification ability. For V-COCO, the role mAP (Gupta and Malik 2015) (AP role ) is used for evaluation.</p><p>For the HOI-VP database, we use an evaluation protocol similar to that of HICO-DET. As there are as many as 517 object categories in HOI-VP, object detection becomes a challenging task. Accordingly, to reduce the impact  <ref type="bibr" target="#b16">(Huang et al. 2017)</ref> of object detection errors, the ground-truth bounding boxes and categories for both human and object instances are provided. This strategy is similar to the Predicate Classification (PREDCLS) protocol, which has been widely adopted in scene graph generation tasks <ref type="bibr" target="#b56">(Zellers et al. 2018;</ref><ref type="bibr" target="#b31">Lin et al. 2020)</ref>. It facilitates a clean comparison of verb classification ability between different HOI detection models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>To facilitate fair comparison with existing works, we consider two popular object detection models for PD-Net.  <ref type="bibr" target="#b12">(Gupta et al. 2019</ref>), for both human and each object category, we first select the top 10 proposals according to the detection scores after non-maximum suppression. Moreover, the bounding boxes whose detection scores are lower than 0.01 are removed. The dimension of appearance features for both object detectors, i.e. K A , is 2,048. Utilizing the same approach as existing works <ref type="bibr" target="#b5">(Gao et al. 2018;</ref><ref type="bibr" target="#b51">Xu et al. 2019;</ref><ref type="bibr" target="#b25">Li et al. 2019b;</ref><ref type="bibr" target="#b12">Gupta et al. 2019;</ref><ref type="bibr" target="#b40">Peyre et al. 2019;</ref><ref type="bibr" target="#b41">Qi et al. 2018;</ref><ref type="bibr" target="#b28">Liao et al. 2020;</ref><ref type="bibr" target="#b48">Wang et al. 2020;</ref><ref type="bibr" target="#b44">Ulutan et al. 2020;</ref><ref type="bibr" target="#b66">Zhong et al. 2021)</ref>, the HOI categories that appear in the training set are set as the meaningful and common HOI categories in each HOI database. The dimension of output layers of the H, O, S, and P blocks, i.e. K C , is set to 187, 45, and 83 on the HICO-DET, V-COCO, and HOI-VP databases, respectively; these figures are equal to the number of CSP classifiers on each respective database. We train PD-Net for 6 (10) epochs using Adam optimizer (Kingma and Ba 2014) with a learning rate of 1e-3 (1e-4) on HICO-DET (V-COCO), while on HOI-VP, we train PD-Net for 12 epochs using a learning rate of 1e-3. During testing, we rank the HOI candidate pairs according to their detection scores (obtained via Eq. (9)) and calculate mAP for evaluation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Studies</head><p>To demonstrate the effectiveness of each proposed component in PD-Net, we perform ablation studies on the HICO-DET database. In <ref type="table" target="#tab_4">Table 3</ref>, the baseline is constructed by removing Language Prior-guided Channel Attention (LPCA), Language Prior-based Feature Augmentation (LPFA), and Polysemy-Aware Modal Fusion (PAMF) from PD-Net; we also replace Clustering-based object-SPecific (CSP) classifiers with object-SHared (SH) classifiers. The other settings for the baseline remain the same as in PD-Net. For both models, the Faster R-CNN with ResNet-152 backbone is used for object detection. Experimental results are summarized in <ref type="table" target="#tab_4">Table 3</ref>. From these results, we can make the following observations. Effectiveness of PAMF. Polysemy-Aware Modal Fusion is designed to decipher the verb polysemy by assigning larger weights to more important feature types for each HOI category. As shown in <ref type="table" target="#tab_4">Table 3</ref>, PAMF promotes the performance of the baseline by 1.29% and 1.36% mAP in DT and KO modes, respectively. Effectiveness of LPFA. Language Prior-based Feature Augmentation is used to provide hints for the classifier in order to reduce the intra-class variation of the pose and spatial features by augmenting them with language priors. When LPFA is incorporated, HOI detection performance is promoted by 0.52% and 0.21% mAP in DT and KO modes, respectively. Effectiveness of LPCA. The appearance features are redundant for HOI detection. Language Prior-guided Channel Attention is proposed to generate polysemy-aware appearance features. As can be seen from <ref type="table" target="#tab_4">Table 3</ref>, LPCA promotes the HOI detection performance by a clear margin of 1.33% and 0.21% in DT and KO modes, respectively. Effectiveness of CSP Classifiers. Clustering-based object-SPecific classifiers can relieve the verb polysemy problem by assigning the same verb classifier to semantically similar HOI categories. As shown in <ref type="table" target="#tab_4">Table 3</ref>, CSP classifiers improve the HOI detection performance by 1.06% and 2.13% mAP in DT and KO modes, respectively.</p><p>Drop-one-out Study. We further perform a drop-one-out study in which each proposed component is removed individually. These experimental results further demonstrate that each component is indeed helpful to promote HOI detection performance. Finally, when INet is integrated, the mAP of PD-Net in the DT mode is further promoted by 0.60%. However, the mAP in the KO mode does not improve. This is because INet can assist PD-Net by suppressing candidate pairs without interactions, which are usually caused by incorrect or redundant object proposals in the DT mode. However, the KO mode is comparatively less affected by object detection errors; therefore, PD-Net can achieve high performance without the assistance of INet in this mode. This experiment demonstrates that the strong performance of PD-Net is primarily a result of its excellent verb classification ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparisons with Variants of PD-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Comparisons with Variants of the Language Prior</head><p>In this experiment, we remove the word embedding of the object category from the language prior so that only the word embedding of the verb category to identify is used as input for PAMF, LPFA, and LPCA. As shown in <ref type="table" target="#tab_5">Table  4</ref>, without the word embedding of the object category, the performance of PD-Net drops by a large margin of 2.39% (1.87%) mAP in DT (KO) mode. These experimental results indicate that the word embedding of the object category in the language prior is an important hint to decipher the verb polysemy problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparisons with Variants of LPCA</head><p>In this experiment, we compare the performance of Language Prior-guided Channel Attention (LPCA) with five possible variants: namely, Plain Channel Attention (CA), 'w/o S au ', 'w/o C att ', 'D([L A , F A ])' and FiLM <ref type="bibr" target="#b70">(Perez et al. 2018)</ref>. The other implementation details of PD-Net are kept the same for different variants. Plain CA means that we feed the appearance feature F A directly into a plain CA module, i.e. D(·) in <ref type="figure">Fig. 4, and</ref>   <ref type="figure">Fig. 4</ref> as the input of the H and O blocks in <ref type="figure">Fig. 3</ref>, without the further processing by the plain CA module. 'D([L A , F A ])' means that we use the concatenation of L A and F A as the input for function D(·) to generate channel attention scores in Eq.</p><p>(5). FiLM means that we replace LPCA with a FiLM layer <ref type="bibr" target="#b70">(Perez et al. 2018)</ref>. Experimental results are tabulated in Table 5. In this table, 'w/o LPCA' is a baseline that removes the entire LPCA module from PD-Net. From these results, we can make the following observations.</p><p>First, the plain CA module alone slightly promotes the performance of PD-Net. One main reason for this is that the plain CA module has very little ability to identify important elements in the appearance features for each HOI category. Second, without supervision from S au , the performance of LPCA degrades dramatically. Compared to the plain CA module, this setting adopts language priors to provide cues regarding the channel-wise importance of F A for each HOI category. However, it receives only implicit supervision from the binary score S PD <ref type="bibr">(h,o,v)</ref> in Eq. <ref type="formula" target="#formula_9">(9)</ref>, which is too weak to optimize LPCA's parameters. We therefore observe degraded performance after the extra supervision S au is removed.</p><p>Third, 'w/o C att ' obtains better performance than both the 'Plain CA' and 'w/o S au ' settings. However, its performance is still lower than that of our proposed LPCA by 0.38% and 0.52% mAP in DT and KO modes, respectively. This may be because L A is obtained via projection from the language prior. As word embeddings are not specifically designed for computer vision tasks, L A may not always be reliable and the quality of L B is affected <ref type="bibr" target="#b51">(Xu et al. 2019</ref>). Therefore, further processing L B using the plain CA module is helpful.</p><p>Fourth, LPCA outperforms D([L A , F A ]) by significant margins in both DT and KO modes. There are two main reasons for this. First, the concatenation operation significantly increases the model size of the channel attention module, which makes the model more difficult to train. Second, with the optimization on S au , L B can provide more direct hints about important channels in F A to the verb to verify than [L A , F A ]. Fifth, LPCA also outperforms FiLM <ref type="bibr" target="#b70">(Perez et al. 2018</ref>) by significant margins in both DT and KO modes.This is because the feature-wise affine transformation in <ref type="bibr" target="#b70">(Perez et al. 2018</ref>) is directly conditioned on the language prior; therefore, it can be affected by the semantic misalignments between visual features and word embeddings. In comparison, LPCA can better address the discrepancy because it produces channel attention scores conditioned on the correlation between language priors and visual features. Moreover, FiLM <ref type="bibr" target="#b70">(Perez et al. 2018</ref>) is only supervised by the final classification loss of the model while LPCA is also optimized by an auxiliary supervision on S au .</p><p>In comparison, our proposed LPCA achieves the best performance for the following reasons. First, it adopts language priors to provide hints regarding the channel-wise importance of F A for each HOI category. Second, it imposes direct supervision to the attention module, which helps to more effectively optimize the model parameters. Third, it refines the attention vector obtained from the language priors using a plain CA module, which enhances the quality of the channel attention vectors. The above experimental results and analysis demonstrate the effectiveness of LPCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Comparisons with Variants of Verb Classifiers</head><p>To further demonstrate the advantages of clustering-based object-specific (CSP) classifiers, we compare their performance with that of object-SHared (SH) and object-SPecific (SP) verb classifiers. To facilitate fair comparison, other settings of PD-Net remain unchanged. Experimental results are tabulated in <ref type="table" target="#tab_8">Table 6</ref>. It is shown that SH classifiers outperform SP classifiers by 1.42% (2.06%) mAP in DT (KO) mode for rare HOI categories. This is because SH classifiers enable these rare HOI categories to share verb classifiers with other HOI categories that have sufficient training data. By comparison, SP classifiers are better able to relieve the verb polysemy problem for the HOI categories that have sufficient training data. Therefore, the SP classifiers outperform SH classifiers by 0.23% (0.41%) mAP in DT (KO) mode for non-rare HOI categories.</p><p>In comparison, CSP classifiers achieve superior performance on both rare and non-rare HOI categories. This is due to the same verb classifiers being assigned to semantically similar HOI categories, enabling HOI categories with few training samples to share verb classifiers with those HOI categories that have sufficient training data. Moreover, different verb classifiers are adopted for semantically different HOI categories, which is helpful to overcome the verb polysemy problem. Overall, CSP classifiers outperform SH and SP classifiers by 1.31% (2.03%) and 1.46% (2.19%) mAP in DT (KO) mode for the full HOI categories, respectively. The superior performance on rare HOI categories demonstrates the effectiveness of CSP classifiers in the few-shot learning ability. We also further justify the effectiveness of CSP classifiers in terms of zero-shot HOI detection in Section B of the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons with State-of-the-Art Methods</head><p>We compare the performance of PD-Net with state-of-theart methods on three databases, namely HICO-DET, V-COCO, and HOI-VP. Experimental results are summarized in <ref type="table" target="#tab_9">Table  7</ref>, <ref type="table" target="#tab_1">Table 8, and Table 10</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Performance comparisons on HICO-DET</head><p>As shown in <ref type="table" target="#tab_9">Table 7</ref>, PD-Net outperforms state-of-the-art methods by significant margins using both object detector backbones. It is worth noting that one most recent method, i.e. PPDM, adopts CenterNet with Hourglass-104 backbone ) as the object detector. As shown in Table 2, this object detector significantly outperforms the two Faster R-CNN object detectors utilized in our model. To facilitate fair comparison, we mainly compare PPDM with PD-Net in the KO mode, as this mode is less affected by object detection results. As shown in <ref type="table" target="#tab_9">Table 7</ref>, PD-Net outper-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PD-Net</head><p>No-Frills 0% 10% 20% 30% 40% 50% Average Precision (AP) <ref type="figure">Fig. 5</ref> The top 10 verbs that are most likely to suffer from the polysemy problem on HICO-DET.</p><p>forms PPDM in KO mode by significant margins of 2.28%, 5.05% and 1.60% mAP on the full, rare and non-rare HOI categories, respectively. Moreover, PD-Net also outperforms PPDM by 0.64% in the DT mode on the full HOI categories. Moreover, as shown in <ref type="table" target="#tab_9">Table 7</ref> and <ref type="table" target="#tab_2">Table 2</ref>, the object detector adopted by another recent work <ref type="bibr" target="#b44">(Ulutan et al. 2020)</ref> is also much stronger than ours. But PD-Net still outperforms this model by large margins of 2.57% (22.37%-19.80%), 1.56% (17.61%-16.05%), and 2.88% (23.79%-20.91%) mAP in the DT mode on the full, rare, and non-rare HOI categories, respectively.</p><p>Finally, with a similar multi-stream representation network and object detector backbone (ResNet-50-FPN), PD-Net outperforms one very recent model 2D-RN <ref type="bibr" target="#b26">(Li et al. 2020a</ref>) by 3.03% (25.59%-22.56%) and 0.78% (20.76%-19.98%) in mAP on the full HOI categories in the KO and DT modes, respectively. Another advantage of PD-Net compared with 2D-RN is that PD-Net requires no extra human <ref type="table">Table 8</ref> Performance comparisons on V-COCO <ref type="bibr" target="#b11">(Gupta and Malik 2015)</ref>. • denotes methods that we reproduce. † denotes methods that adopt human part features as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detector Backbone AProle</head><p>Gupta et al. <ref type="bibr" target="#b11">(Gupta and Malik 2015;</ref><ref type="bibr" target="#b9">Gkioxari et al. 2018)</ref> ResNet-50-FPN 31.8 InteractNet  ResNet-50-FPN 40.0 GPNN <ref type="bibr" target="#b51">(Xu et al. 2019;</ref><ref type="bibr" target="#b41">Qi et al. 2018)</ref> ResNet-152 44.0 iCAN <ref type="bibr" target="#b5">(Gao et al. 2018)</ref> ResNet-50-FPN 45.3 iHOI <ref type="bibr" target="#b52">(Xu et al. 2020)</ref> ResNet-50-FPN 45.8 Xu et al. <ref type="bibr" target="#b51">(Xu et al. 2019)</ref> ResNet-50-FPN 45.9 No-Frills • <ref type="bibr" target="#b12">(Gupta et al. 2019)</ref> ResNet-152 46.7 Wang et al.  ResNet-50-FPN 47.3 RPNN † <ref type="bibr" target="#b62">(Zhou and Chi 2019)</ref> ResNet-50 (Mask R-CNN) 47.5 TIN (RPDCD) <ref type="bibr" target="#b25">(Li et al. 2019b)</ref> ResNet-50-FPN 47.8 C-HOI <ref type="bibr" target="#b63">(Zhou et al. 2020)</ref> ResNet-50 48.3 IP-Net  ResNet-50-FPN 51.0 VSGNet <ref type="bibr" target="#b44">(Ulutan et al. 2020)</ref> NASNet <ref type="bibr" target="#b68">(Zoph et al. 2018</ref> annotation. Besides, 3D human pose and 3D object locations are also utilized to improve 2D-RN during inference in <ref type="bibr" target="#b26">(Li et al. 2020a</ref>). To facilitate fair comparisons, we only compare the performance of PD-Net with methods that utilize 2D human pose and 2D object locations during inference.</p><p>To further illustrate the advantage of PD-Net in deciphering the verb polysemy problem, we present the top 10 verbs (from the total 117 verbs in HICO-DET) ranked by the number of HOI categories in which each verb is included in <ref type="figure">Fig. 5</ref>. The largest number of HOI categories associated with the same verb ("hold") is 61. As these verbs are more likely to be affected by the visual polysemy problem, we therefore compare the performance of PD-Net with one state-of-theart method <ref type="bibr" target="#b12">(Gupta et al. 2019</ref>) on these verbs. This method is chosen as it is very similar to our baseline. Results show that PD-Net achieves superior performance on all of these top 10 verbs.  <ref type="bibr" target="#b25">(Li et al. 2019b)</ref> ResNet-50 60.66 No-Frills <ref type="bibr" target="#b12">(Gupta et al. 2019)</ref> ResNet-152 61.05 <ref type="bibr" target="#b40">Peyre et al. (Peyre et al. 2019)</ref> ResNet-50-FPN 61.46 PMFNet <ref type="bibr" target="#b45">(Wan et al. 2019)</ref> ResNet-50-FPN 62.30</p><p>Our baseline (SH) ResNet-50 61.18</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PD-Net</head><p>ResNet-50 63.11 Our baseline (SH)</p><p>ResNet-50-FPN 61.10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PD-Net</head><p>ResNet-50-FPN 63.66 Our baseline (SH)</p><p>ResNet-152 60.69</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PD-Net</head><p>ResNet-152 63.60</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Performance comparisons on V-COCO</head><p>To boost the performance on V-COCO, we add another appearance feature stream to both our baseline and PD-Net, following <ref type="bibr" target="#b45">(Wan et al. 2019)</ref>. There are consequently a total of five feature streams for experiments on V-COCO. This new stream extracts appearance features from union boxes composed of human-object pairs. We further apply LPCA to this feature stream in PD-Net. As shown in <ref type="table">Table 8</ref>, PD-Net outperforms state-of-the-art methods by clear margins with both object detectors. In particular, PD-Net outperforms one of the most recently developed methods, i.e. VSGNet <ref type="bibr" target="#b44">(Ulutan et al. 2020)</ref>. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the object detector utilized by VSGNet is much stronger <ref type="bibr" target="#b16">(Huang et al. 2017</ref>) than ours; nevertheless, PD-Net still outperforms VSGNet by clear margins, as indicated in <ref type="table">Table 8</ref>. Moreover, PD-Net outperforms another particularly strong model, named PMFNet <ref type="bibr" target="#b45">(Wan et al. 2019</ref>) by 0.3% (52.3%-52.0%) in mAP. The excellent performance of PMFNet may benefit from the use of human part features. Therefore, we adopt the same five feature streams that include the human part features in PMFNet as input for PD-Net; this model is denoted as PD-Net † in <ref type="table">Table 8</ref>. The contributions in this paper remain unchanged. PD-Net † outperforms PMFNet by a large margin of 1.3% (53.3%-52.0%) in mAP. Moreover, as shown in <ref type="table">Table 9</ref>, we compare the performance between PD-Net † and PMFNet on each of the 24 verbs in V-COCO. Here, our method demonstrates superior performance on the vast majority of verb classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Performance comparisons on HOI-VP</head><p>We next compare the performance of PD-Net with some recent open-source methods, i.e. iCAN <ref type="bibr" target="#b5">(Gao et al. 2018)</ref>, TIN <ref type="bibr" target="#b25">(Li et al. 2019b</ref><ref type="bibr">), No-Frills (Gupta et al. 2019</ref>, and PMFNet <ref type="bibr" target="#b45">(Wan et al. 2019</ref>) on the new HOI-VP database. We also reproduce the method presented in <ref type="bibr" target="#b40">(Peyre et al. 2019</ref>) that achieves high performance on the HICO-DET database. To facilitate fair comparison, we compare the performance of PD-Net with each of these methods using the same feature extraction backbone, respectively. As shown in <ref type="table" target="#tab_1">Table 10</ref>, PD-Net consistently achieves the best performance out of all compared methods. In particular, PD-Net outperforms one recent powerful model PMFNet by a clear margin of 1.36% (63.66%-62.30%) in mAP. As the verbs (predicates) in the HOI-VP database are very common and polysemic in real world scenarios, experimental results on this database demonstrate the superiority of PD-Net to overcome the verb polysemy problem. <ref type="figure" target="#fig_2">Fig. 6</ref> illustrates attention scores produced by PAMF for four types of features. HOI categories in this figure share the verb "ride", but differ dramatically in semantic meanings. The "person" proposal in <ref type="figure" target="#fig_2">Fig. 6(a)</ref> is very small and severely occluded while the "airplane" proposal is very large; therefore, object appearance feature is much more important for verb classification than the human appearance feature. In <ref type="figure" target="#fig_2">Fig. 6(b)</ref>, both the spatial feature and the object appearance feature play important roles in determining the verb. Attention scores for <ref type="figure" target="#fig_2">Fig. 6</ref>(c) and (d) are similar, as &lt;person ride horse&gt; and &lt;person ride elephant&gt; are indeed close in semantics. <ref type="figure">Fig. 7, Fig. 8</ref>, and <ref type="figure">Fig. 9</ref> provide more examples that demonstrate PD-Net's advantages in deciphering the verb polysemy problem on HICO-DET, V-COCO, and HOI-VP, respectively. The performance gain by PD-Net compared with our baseline reaches 10.6%, 3.33%, and 48.1% in AP for the "open microwave", "carry backpack", and "play drum" category on the three datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Visualization Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The verb polysemy problem is relatively underexplored and is sometimes even ignored in existing works for HOI detection. Accordingly, in this paper, we propose a novel model named PD-Net, which significantly mitigates the challenging verb polysemy problem. PD-Net includes four novel components: Language Prior-guided Channel Attention, Language Prior-based Feature Augmentation, Polysemy-Aware Modal Fusion, and Clustering-based Object Specific classifiers. Language Prior-guided Channel Attention and Language Priorbased Feature Augmentation are introduced to generate polysemyaware visual features. Polysemy-Aware Modal Fusion highlights important feature types for each HOI category. The Clustering-based Object Specific classifiers not only relieve the verb polysemy problem, but also is capable of handling the zero-or few-shot learning problems. Exhaustive ablation studies are performed to demonstrate the effectiveness of these components. We further develop and present a new dataset, named HOI-VP, that is specifically designed to expedite the research on the verb polysemy problem for HOI detection. Finally, by decoding the verb polysemy, we achieve state-of-the-art methods on the three HOI detection benchmarks. In the future, we will study the verb polysemy problem in related tasks to HOI detection, e.g., visual relationship detection and action recognition.  <ref type="figure">Fig. 9</ref> Visualization of PD-Net's advantage in deciphering the verb polysemy problem on HOI-VP. We randomly select three verbs affected by the polysemy problem: "in" (top row), "play" (middle row), and "use" (bottom row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This supplementary material includes four sections. Section A visualizes the clustering results for several polysemic verbs and shows the statistics for polysemic verbs in the HICO-DET database. Section B shows the experimental results of Clustering-based object-SPecific (CSP) verb classifiers in terms of zero-shot HOI detection. Section C provides more randomly selected samples to show the attention scores predicted by Polysemy-Aware Modal Fusion (PAMF). Section D compares three types of verb classifiers, i.e. object-shared (SH), object-specific (SP), and CSP verb classifiers, based on our baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Visualization of Clustering Results</head><p>We visualize the clustering results for multiple polysemic verbs in <ref type="figure" target="#fig_0">Fig. 10</ref>. We observe that the more object categories one verb associates, the more polysemic it presents visually. Therefore, we propose to cluster semantically similar HOI categories for each verb and set the cluster number as the rounded square root of the number of associated objects to a verb. It is also clear that objects in the same cluster are usually of similar functions or properties, which demonstrates the effectiveness of our clustering strategy in terms of distinguishing different semantic meanings for a verb.</p><p>For example, as illustrated in <ref type="figure" target="#fig_0">Fig. 1(c)</ref> and (d) of the main paper, "hold book" and "hold elephant" present different visual characteristics for the verb "hold", we can see that "book" and "elephant" (highlighted in circle) are in different clusters in the visualization results for the verb "hold".</p><p>In <ref type="table" target="#tab_1">Table 11</ref>, we show the statistics for polysemic verbs in the HICO-DET database <ref type="bibr" target="#b1">(Chao et al. 2018</ref>). According to our clustering strategy, there are 16 verbs that contain over 3 semantic meanings and involves nearly 58.5% HOI categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Performance Comparisons on Zero-Shot HOI Detection</head><p>In this section, we compare the performance of PD-Net with previous works , <ref type="bibr" target="#b69">(Bansal et al. 2020</ref>) in both seen-object and unseen-object settings that were introduced in <ref type="bibr" target="#b69">(Bansal et al. 2020)</ref> . The experimental results for  the two settings are summarized in <ref type="table" target="#tab_1">Table 12</ref> and <ref type="table" target="#tab_1">Table 13</ref>, respectively.</p><p>In the seen-object setting, we follow <ref type="bibr" target="#b69">(Bansal et al. 2020</ref>) to select 5 random sets of 120 unseen HOI categories from the total 600 HOI categories, and ensure that each object involved in these 120 HOI categories occurs at least one time in the remaining 480 ones. During training, we only use samples of the 480 seen HOI categories to train our model. We report the mean performance over these 5 random sets of 120 unseen HOI categories in <ref type="table" target="#tab_1">Table 12</ref>. Moreover, <ref type="bibr" target="#b69">(Bansal et al. 2020)</ref> only utilizes human appearance and humanobject spatial feature as input. To facilitate fair comparisons, we also only use these two types of features for PD-Net. We can observe that PD-Net outperforms <ref type="bibr" target="#b69">(Bansal et al. 2020)</ref> by significant margins, which verifies its effectiveness for zero-shot HOI detection.</p><p>In the unseen-object setting, as <ref type="bibr" target="#b69">(Bansal et al. 2020</ref>) did not provide the selected unseen objects, we randomly select 12 objects from the total 80 object classes as unseen objects. Accordingly, there are 100 HOI categories associated with these 12 objects, which are set as unseen HOIs. Following <ref type="bibr" target="#b69">(Bansal et al. 2020)</ref>, we only use the samples of the remaining 500 HOI categories to train our model. As there are no training samples for the unseen objects, during inference, we adopt the same generic object detector as that in <ref type="bibr" target="#b69">(Bansal et al. 2020)</ref> to generate object proposals for the unseen objects. The experimental results are provided in <ref type="table" target="#tab_1">Table 13</ref>. We can see that PD-Net significantly outperforms <ref type="bibr" target="#b69">(Bansal et al. 2020)</ref> in terms of zero-shot HOI detection. We also replace CSP classifiers with object-shared (SH) classifiers in PD-Net, which is denoted as PD-Net (SH) in <ref type="table">Table D</ref>. SH classifiers share training samples for HOI categories with the same verb; therefore, it has zero-shot learning ability for unseen HOI categories. PD-Net achieves better performance than PD-Net (SH) for unseen HOI categories, which further demonstrates the effectiveness of CSP classifiers in terms of zero-shot HOI detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Visualization Results</head><p>In <ref type="figure" target="#fig_0">Fig. 11</ref>, we provide more randomly selected samples to show the attention scores predicted by Polysemy-Aware Modal Fusion (PAMF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Performance Comparisons between Three Verb Classifiers</head><p>In this section, we compare the performance of the three verb classifiers, i.e. object-shared (SH), object-specific (SP), and clustering-based object-specific (CSP) verb classifiers.</p><p>In particular, we replace the SH classifiers in our baseline with SP classifiers and CSP classifiers, respectively. The experiment results are summarized in <ref type="table" target="#tab_1">Table 14</ref>. We can see that our proposed CSP classifier consistently outperforms the other two types of classifiers, which further demonstrate its effectiveness.  <ref type="figure" target="#fig_0">Fig. 11</ref> Visualizations of attention scores produced by PAMF on four types of features. H, O, S, and P denote human appearance, object appearance, spatial feature, and human pose feature, respectively. The top 1 and top 2 important feature types and their attention scores for one HOI category are highlighted using red and blue, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>and (b). Here, the "feet" are the arXiv:2008.02918v3 [cs.CV] 24 Mar 2021 Examples reflecting the verb polysemy problem in HOI detection. In terms of describing the HOIs, (a) and (b) present HOI examples of "play". "feet" are more important in (a) while "hands" are more important in (b). (c) and (d) illustrate HOI examples of "hold". The human-object pairs in (c) and (d) are characterized by dramatically different human-object spatial features, i.e. the relative location between two bounding boxes. (e) and (f) illustrate HOI examples of "fly". The "person" in (e) exhibits discriminative pose features while the "person" in (f) does not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6</head><label>6</label><figDesc>Attention scores produced by PAMF on four types of features. HOI categories in this figure share the same verb, i.e. "ride". H, O, S, and P denote human appearance, object appearance, spatial feature, and human pose feature respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10</head><label>10</label><figDesc>Visualization of clustering results for polysemic verbs via t-SNE<ref type="bibr" target="#b83">(Maaten et al. 2008)</ref>. C v denotes the number of clusters for verb v. Objects in the same cluster are tagged with the same colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Object Appearance Stream Human Appearance Stream Pose Feature Stream Spatial Feature Stream Feature Extraction Language Priors PAMF Human Appearance Pose Feature Hold book Hold book Hold？ Hold book √ Input ImageHold book Hold book PAMF Object Appearance Spatial Feature Hold book</head><label></label><figDesc></figDesc><table><row><cell>Language Prior-guided Channel Attention</cell><cell>H Block</cell></row><row><cell>Language Prior-guided Channel Attention</cell><cell>O Block</cell></row><row><cell>Language Prior-based Feature Augmentation</cell><cell>S Block</cell></row><row><cell>Language Prior-based Feature Augmentation</cell><cell>P Block</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>The number of associated objects and instances for each verb in the HOI-VP dataset.<ref type="bibr" target="#b29">Lin et al. 2014</ref>) and contains 2,533, 2,867, and 4,946 images used for training, validation and testing, respectively. There are 24 verb categories and 259 HOI categories in total. Each verb is included in 10 HOI categories on average.</figDesc><table><row><cell>Verbs</cell><cell cols="2"># Objects # Instances</cell></row><row><cell>carry</cell><cell>49</cell><cell>1585</cell></row><row><cell>cross</cell><cell>8</cell><cell>611</cell></row><row><cell>fix</cell><cell>4</cell><cell>2063</cell></row><row><cell>hold</cell><cell>229</cell><cell>13369</cell></row><row><cell>in</cell><cell>218</cell><cell>6123</cell></row><row><cell>on</cell><cell>196</cell><cell>11752</cell></row><row><cell>make</cell><cell>8</cell><cell>147</cell></row><row><cell>open</cell><cell>7</cell><cell>181</cell></row><row><cell>operate</cell><cell>4</cell><cell>46</cell></row><row><cell>play</cell><cell>22</cell><cell>1119</cell></row><row><cell>push</cell><cell>10</cell><cell>202</cell></row><row><cell>ride</cell><cell>29</cell><cell>1734</cell></row><row><cell>swing</cell><cell>5</cell><cell>1116</cell></row><row><cell>touch</cell><cell>18</cell><cell>154</cell></row><row><cell>use</cell><cell>29</cell><cell>3333</cell></row><row><cell cols="3">mental section below, we demonstrate that INet slightly pro-</cell></row><row><cell cols="2">motes the performance of PD-Net.</cell><cell></cell></row><row><cell>4 Experimental Setup</cell><cell></cell><cell></cell></row><row><cell>4.1 Datasets</cell><cell></cell><cell></cell></row></table><note>HICO-DET (Chao et al. 2018) is a large-scale dataset for HOI detection, containing a total of 47,776 images; of these, 38,118 images are assigned to the training set, while the re- maining 9,568 images are used as the testing set. There are 117 verb categories, 80 object categories, and 600 common HOI categories overall; moreover, these 600 HOI categories are divided into 138 rare and 462 non-rare categories. Each rare HOI category contains less than 10 training samples. Each verb is included in an average of five HOI categories.V-COCO (Gupta and Malik 2015) is a subset of MS- COCO (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Performance comparisons between common object detectors on COCO<ref type="bibr" target="#b29">(Lin et al. 2014</ref>).</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell></row><row><cell>Faster R-CNN (Ren et al. 2015)</cell><cell>ResNet-152</cell><cell>36.7 (Chen and Gupta 2017)</cell></row><row><cell>Faster R-CNN</cell><cell>ResNet-50-FPN</cell><cell>36.8 (Massa and Girshick 2018)</cell></row><row><cell>Mask R-CNN (He et al. 2017)</cell><cell>ResNet-50</cell><cell>36.9 (Girshick et al. 2018)</cell></row><row><cell>CenterNet (Zhou et al. 2019b)</cell><cell>Hourglass-104 (Newell et al. 2016)</cell><cell>40.3 (Zhou et al. 2019b)</cell></row><row><cell>Faster R-CNN</cell><cell>NASNet (Zoph et al. 2018)</cell><cell>43.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Ablation studies on each component of PD-Net. Full refers to evaluation on all 600 HOI categories in HICO-DET.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Components</cell><cell></cell><cell></cell><cell>Full</cell></row><row><cell>Methods</cell><cell cols="6">SH PAMF LPFA LPCA CSP INet</cell><cell>DT</cell><cell>KO</cell></row><row><cell>Our Baseline</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>17.57 23.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>18.86 24.43</cell></row><row><cell>Incremental</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>--</cell><cell>--</cell><cell>19.38 24.64 20.71 24.85</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>21.77 26.98</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>20.14 25.65</cell></row><row><cell>Drop-one-out</cell><cell>--</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>--</cell><cell>21.37 26.15 19.32 24.30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>20.71 24.85</cell></row><row><cell>PD-Net</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>22.37 26.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Comparisons with one variant of the language prior.</figDesc><table><row><cell cols="3">Language Prior DT (Full) KO (Full)</cell></row><row><cell>Verb Only</cell><cell>19.98</cell><cell>24.99</cell></row><row><cell>Verb + Object</cell><cell>22.37</cell><cell>26.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Performance comparisons with variants for LPCA. [·] represents feature concatenation operation.</figDesc><table><row><cell>Methods</cell><cell cols="2">DT (Full) KO (Full)</cell></row><row><cell>w/o LPCA</cell><cell>19.82</cell><cell>23.94</cell></row><row><cell>w/o Sau</cell><cell>19.41</cell><cell>23.32</cell></row><row><cell>Plain CA</cell><cell>19.97</cell><cell>24.11</cell></row><row><cell>D([LA, FA])</cell><cell>20.62</cell><cell>24.45</cell></row><row><cell>FiLM (Perez et al. 2018)</cell><cell>20.90</cell><cell>25.29</cell></row><row><cell>w/o Catt</cell><cell>21.99</cell><cell>26.34</cell></row><row><cell>LPCA</cell><cell>22.37</cell><cell>26.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>obtainF A . 'w/o S au ' involves removing the extra supervision signal S au from LPCA, while 'w/o C att ' means that we directly use L B in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Performance comparisons between SH, SP, and CSP verb classifiers.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DT Mode</cell><cell></cell><cell cols="2">KO Mode</cell></row><row><cell>Methods</cell><cell>Full</cell><cell>Rare</cell><cell>Non-Rare</cell><cell>Full</cell><cell>Rare</cell><cell>Non-Rare</cell></row><row><cell>SH</cell><cell cols="2">21.06 16.45</cell><cell>22.43</cell><cell cols="2">24.83 19.89</cell><cell>26.30</cell></row><row><cell>SP</cell><cell cols="2">20.91 15.03</cell><cell>22.66</cell><cell cols="2">24.67 17.83</cell><cell>26.71</cell></row><row><cell>CSP</cell><cell cols="2">22.37 17.61</cell><cell>23.79</cell><cell cols="2">26.86 21.70</cell><cell>28.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc>Performance Comparisons on HICO-DET. The best performance on different backbones are bolded in different colours.</figDesc><table><row><cell>DT Mode</cell><cell>KO Mode</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc>Performance Comparisons on HOI-VP.</figDesc><table><row><cell>Method</cell><cell>Feature Extraction Backbone</cell><cell>mAP</cell></row><row><cell>iCAN (Gao et al. 2018)</cell><cell>ResNet-50</cell><cell>58.32</cell></row><row><cell>TIN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Visualization of PD-Net's advantage in deciphering the verb polysemy problem on HICO-DET. We randomly select three verbs affected by the polysemy problem: "hold" (top row), "ride" (middle row), and "open" (bottom row). The green and red numbers denote the AP of our baseline and PD-Net respectively for the same HOI category. Visualization of PD-Net's advantage in deciphering the verb polysemy problem on V-COCO. We randomly select three verbs affected by the polysemy problem: "carry" (top row), "ride" (middle row), and "hold" (bottom row).</figDesc><table><row><cell>hold-bird in-bus</cell><cell>carry-umbrella in-hat</cell><cell>carry-surfboard in-crowd</cell><cell>hold-chair carry-handbag</cell><cell>hold-elephant carry-suitcase</cell></row><row><cell>carry-backpack</cell><cell>hold-knife</cell><cell>hold-backpack</cell><cell>in-mirror</cell><cell>in-dress</cell></row><row><cell cols="2">19.2% 21.5 % ride-bicycle open-scissors 10.0% 14.3% 51.9% 55.3% 6.61% 8.23% 25.87% 26.54% 1.72% 2.18% ride-horse hold-book play-frisbee use-computer 47.2% 59.9% 49.3% 62.2% Fig. 8 2.2% 15.7% 16.6% 3.8% 4.8% ride-airplane open-umbrella 39.5% 40.2% Fig. 7 6.02% 9.35% 9.61% 10.39% 4.27% 5.11% ride-elephant hold-frisbee 69.9% 73.1% 0.8% 14.4% play-soccer_ball use-camera 3.6% 53.7% 60.3%</cell><cell>ride-skateboard open-refrigerator 35.8% 37.3% 51.5% 53.4% 7.7% 12.7% 17.38% 20.03% 26.11% 27.44% 2.44% 4.22% ride-motorcycle hold-surfboard play-drum use-bat 38.0% 57.4% 51.5% 99.6% 48.8% 59.8%</cell><cell>ride-horse open-microwave 13.0% 23.6% 68.0% 71.1% 8.1% 10.6% 5.04% 7.32% 4.12% 6.44% 4.74% 5.47% ride-boat hold-hot dog play-violin use-skateboard 14.7% 22.0% 40.0% 84.8% 83.7% 87.4%</cell><cell>33.4% 33.6% 35.6% 38.8% open-book 22.2% 31.9% ride-skis 6.76% 6.96% 8.73% 9.32% 9.57% 10.17% ride-bicycle hold-tennis racket 72.8% 76.7% 53.0% 70.0% use-fork play-tennis 78.4% 96.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11</head><label>11</label><figDesc>Statistics of Polysemic Verbs on HICO-DET.</figDesc><table><row><cell cols="3"># Associated Object Catgories # Verb (Ratio) # HOI category (Ratio)</cell></row><row><cell>≥9</cell><cell>16 (13.7%)</cell><cell>351 (58.5%)</cell></row><row><cell>≥4</cell><cell>38 (32.5%)</cell><cell>471 (78.5%)</cell></row><row><cell>≥2</cell><cell>78 (66.7%)</cell><cell>561 (93.5%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12</head><label>12</label><figDesc>Performance comparisons on zero-shot HOI detection in the seen-object setting. Full means all categories including both Seen and Unseen objects.</figDesc><table><row><cell>Method</cell><cell>Unseen</cell><cell>Seen</cell><cell>Full</cell></row><row><cell>Shen et al. (Shen et al. 2018)</cell><cell>5.62</cell><cell>-</cell><cell>6.26</cell></row><row><cell>Bansal et al. (Bansal et al. 2020)</cell><cell>11.31</cell><cell cols="2">12.74 12.45</cell></row><row><cell>PD-Net (SH)</cell><cell>15.28</cell><cell cols="2">17.41 16.98</cell></row><row><cell>PD-Net</cell><cell>15.95</cell><cell cols="2">18.80 18.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13</head><label>13</label><figDesc>Performance comparisons on zero-shot HOI detection in the unseen-object setting. Full means all categories including both Seen and Unseen objects.</figDesc><table><row><cell>Method</cell><cell>Unseen</cell><cell>Seen</cell><cell>Full</cell></row><row><cell>Bansal et al. (Bansal et al. 2020)</cell><cell>11.22</cell><cell cols="2">14.36 13.84</cell></row><row><cell>PD-Net (SH)</cell><cell>14.85</cell><cell cols="2">16.89 16.55</cell></row><row><cell>PD-Net</cell><cell>15.49</cell><cell cols="2">17.07 16.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14</head><label>14</label><figDesc>Performance comparisons between three types of verb classifiers. Full refers to evaluation on all 600 HOI categories in HICO-DET.</figDesc><table><row><cell>Methods</cell><cell cols="2">DT (Full) KO (Full)</cell></row><row><cell>SH</cell><cell>17.57</cell><cell>23.07</cell></row><row><cell>SP</cell><cell>17.51</cell><cell>22.77</cell></row><row><cell>CSP</cell><cell>18.30</cell><cell>23.75</cell></row><row><cell>SH + INet</cell><cell>18.43</cell><cell>23.04</cell></row><row><cell>SP + INet</cell><cell>17.92</cell><cell>22.65</cell></row><row><cell>CSP + INet</cell><cell>19.02</cell><cell>23.60</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatial attention deep net with partial pso for hierarchical hybrid hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An implementation of faster rcnn with study for region sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02138</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Dynamic Fusion With Intra-and Inter-Modality Attention Flow for Visual Question Answering. In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1969" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">No-frills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9677" to="9685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="7310" to="7311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action genome: Actions as compositions of spatio-temporal scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10236" to="10247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Compression artifact removal with stacked multi-context channel-wise attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3601" to="3605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3585" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10166" to="10175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pastanet: Toward human activity knowledge engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gps-net: Graph property sensing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3746" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Processing of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ok-vqa: A visual question answering benchmark requiring external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3195" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interpretable spatio-temporal attention for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1513" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adaptive feature recombination and recalibration for semantic segmentation with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amorim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2914" to="2925" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1981" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning humanobject interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
	<note>In ECCV</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scaling Human-Object Interaction Recognition Through Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13617" to="13626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pose-aware multilevel feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9469" to="9478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep contextual attention for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5694" to="5702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4116" to="4125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exploring context and visual pattern of relationship for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic Selection Network for Image Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1784" to="1798" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2019" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Interact as you intend: Intention-driven human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1423" to="1432" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03918</idno>
		<title level="m">Hierarchy parsing for image captioning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5532" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal Action Detection with Structured Segment Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="95" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scene Understanding by Reasoning Stability and Safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="238" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Unified Vision-Language Pre-Training for Image Captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="843" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cascaded humanobject interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4263" to="4272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Attention couplenet: Fully convolutional attention coupling network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Glance and Gaze: Inferring Action-aware Points for One-Stage Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avd</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09892</idno>
		<title level="m">Care about you: towards large-scale human-centric visual relationship detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Detecting Human-Object Interactions via Functional Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10460" to="10469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">FiLM: Visual Reasoning with a General Conditioning Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maria Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Multi-task learning with coarse priors for robust part-aware person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Addressing the Polysemy Problem in Language Modeling with Attentional Multi-Sense Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8129" to="8133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Polysemy detection in distributed representation of word sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Umemura</surname></persName>
		</author>
		<editor>KST</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Is a single vector enough? exploring node polysemy for network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="932" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
