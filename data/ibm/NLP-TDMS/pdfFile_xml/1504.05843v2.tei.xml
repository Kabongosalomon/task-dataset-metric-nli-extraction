<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploit Bounding Box Annotations for Multi-label Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SCE</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
							<email>zhouty@ihpc.a-star.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">IHPC, A*STAR</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<email>zhangyu@bii.a-star.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">Bioinformatics Institute, A*STAR</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin-Bin</forename><surname>Gao</surname></persName>
							<email>gaobb@lamda.nju.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SCE</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploit Bounding Box Annotations for Multi-label Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) have shown great performance as general feature representations for object recognition applications. However, for multi-label images that contain multiple objects from different categories, scales and locations, global CNN features are not optimal. In this paper, we incorporate local information to enhance the feature discriminative power. In particular, we first extract object proposals from each image. With each image treated as a bag and object proposals extracted from it treated as instances, we transform the multi-label recognition problem into a multi-class multi-instance learning problem. Then, in addition to extracting the typical CNN feature representation from each proposal, we propose to make use of ground-truth bounding box annotations (strong labels) to add another level of local information by using nearest-neighbor relationships of local regions to form a multi-view pipeline. The proposed multi-view multiinstance framework utilizes both weak and strong labels effectively, and more importantly it has the generalization ability to even boost the performance of unseen categories by partial strong labels from other categories. Our framework is extensively compared with state-of-the-art handcrafted feature based methods and CNN based methods on two multi-label benchmark datasets. The experimental results validate the discriminative power and the generalization ability of the proposed framework. With strong labels, our framework is able to achieve state-of-the-art results in both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the availability of large amount of labeled data has greatly boosted the development of feature learning <ref type="figure">Figure 1</ref>. An example of a typical multi-label image, which contains several cows in different locations as well as a person. methods for classification. In particular, convolutional neural networks (CNNs) achieve great success in visual recognition/classification tasks. Features extracted from CNNs can provide powerful global representations for the single object recognition problem <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. However, conventional CNN features may not generalize well for images containing multiple objects as the objects can be in different locations, scales, occlusions and categories. <ref type="figure">Fig. 1</ref> shows an example of such images. Since multi-label recognition task is more general and practical in real world applications, many CNN related methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> have been proposed to address the problem.</p><p>A well known fact is that image-level labels can be utilized to fine-tune a pre-trained CNN model and produce good global representations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19]</ref>. However, due to the diversity and the complexity of multi-label images, classifiers trained from such global representations might not be optimal. For example, if we use images similar to <ref type="figure">Fig. 1</ref> to train a classifier for "person", the classifier will have to account for not only hundreds of different variations arXiv:1504.05843v2 [cs.CV] 3 Jun 2016 of "person" but also other objects contained in the images. The complexity of multi-label images adds an extra level of difficulty for training appropriate classifiers with the global image representations. Furthermore, due to the large intraclass variations of multi-label images, the global features extracted from training images are likely to be unevenly distributed in the feature space. A classifier trained with such features can be successful at regions that are densely populated with training instances, but may fail in poorly sampled areas of the feature space <ref type="bibr" target="#b30">[31]</ref>.</p><p>To address the problems with global CNN representations, following the recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref>, we incorporate local information via extracting object proposals using general object detection techniques such as selective search <ref type="bibr" target="#b24">[25]</ref> for multi-label object recognition. By decomposing an image into local regions that could potentially contain objects, we avoid the complex process of directly recognizing multiple objects in the whole image. Instead, we only need to identify whether there exist target objects in the local regions. However, as the local regions are noisy and of large variations (see <ref type="figure" target="#fig_0">Fig. 3</ref>), the usual CNN representations might not be good enough for discrimination. Therefore, we add another level of locality by incorporating local nearest-neighbor relationships of these local regions. In this way, the resulting features will be more evenly distributed in the feature space. However, such relationships are not easy to obtain only through weak supervision, i.e., image-level labels. Fortunately, for many multi-label applications, we can exploit the strong supervision information, i.e. groundtruth bounding boxes, which can be considered as local regions with strong labels. Then, we can exploit the relationships between object proposals and ground-truth bounding boxes (e.g., nearest neighbor relationships) to help multilabel recognition.</p><p>We would like to point out that ground-truth bounding boxes have been utilized in two proposal-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12]</ref> for multi-label object recognition. In particular, <ref type="bibr" target="#b11">[12]</ref> makes use of ground-truth bounding boxes to train category-specific classifiers to classify object proposals. However, it requires ground-truth bounding boxes for each category of objects, which might not be available in practice. In contrast, for our proposed method, even with only partial strong labels (e.g., bounding boxes and labels for 10 classes in the 20 classes of Pascal VOC), the proposed local relationships generalize well and can help recognize all classes (e.g., improving recognition of the 20 classes in VOC). <ref type="bibr" target="#b17">[18]</ref> directly uses ground-truth bounding boxes to fine tune the CNN model, but its performance is not better than other proposal-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref> that do not utilize ground-truth bounding boxes. In other words, an effective way to exploit ground-truth bounding boxes for multi-label object recognition is still missing, which is what we aim to provide in this paper. <ref type="figure">Fig. 2</ref> gives an overview of our proposed framework. We utilize both strong and weak labels as two views, and propose a multi-view multi-instance framework to tackle the multi-label object recognition task. In particular, for any image, we first extract object proposals using general object detection techniques. The global image and its accompanying weak (image) label is used to fine-tune a standard CNN to generate a feature view representation for each proposal. Using the ground truth bounding boxes and their strong labels, we design a large margin nearest neighbor (LMNN) CNN architecture to learn a low-dimensional feature so that we could extract nearest neighbor relationship between local regions and a candidate pool formed by ground truth objects. These local NN features are used as the label view. When combining both views, we can achieve a balance between global semantic abstraction and local similarity, hence enhancing the discriminative power of our framework. More importantly, as the strong labels are indirectly utilized through LMNN to encode local neighborhood relationships among labelled local regions, the proposed framework can generalize well to the whole local region space, even with only partial strong labels for part of the object classes, making our framework more practical.</p><p>The main contribution of this research lies in the proposed multi-view multi-instance framework, which utilizes bounding box annotations (strong labels) to encode the label view and combine it with the typical CNN feature representation (feature view) for multi-label object recognition. Another novelty of our work is the proposed LMNN CNN which effectively extracts local information from the strong labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Our paper mainly relates to the topics of CNN based multi-label object recognition, multi-view and multiinstance learning and local and metric learning.</p><p>CNN based multi-label object recognition. Recently, CNN models have been adopted to solve the multi-label object recognition problem. Many works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> have demonstrated that CNN models pre-trained on a large dataset such as ILSVRC can be used to extract features for other applications without enough training data. A typical way ( <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b4">[5]</ref>) is to directly apply a pre-trained CNN model to extract an off-the-shelf global feature for each image from a multi-label dataset, and use these features for classification. However, different from singleobject images from the ImageNet, multi-label images usually have multiple objects in different locations, scales and occlusions, and thus global representations are not optimal for solving the problem <ref type="bibr" target="#b25">[26]</ref>. More recently, two proposalbased methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12]</ref> were propose for multi-label recognition and detection tasks with the help of ground truth bounding boxes. These methods achieve significant im-  <ref type="figure">Figure 2</ref>. Overview of the proposed multi-view multi-instance framework. We transform the multi-label object recognition problem into a multi-class multi-instance learning problem by first extracting object proposals from each image using selective search. Two types of features are then extracted for each proposal. One is a low-dimensional feature from a large-margin nearest neighbor (LMNN) CNN, which is used to generate the label view by encoding the label information of k-NN from the candidate pool (containing ground truth objects). The other is a standard CNN feature as the feature view. These two views are fused and then used to encode a Fisher vector for each image.</p><p>provement over single global representations. On the other hand, <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b25">[26]</ref> handle the problem in a weakly supervised manner by max-pooling image scores from the proposal scores. Moreover, <ref type="bibr" target="#b23">[24]</ref> employs a very deep CNN, aggregates multiple features from different scales of the image and achieves state-of-the-art results.</p><p>Multi-view and multi-instance learning. Multi-view learning deals with data from multiple sources or feature sets. The goal of multi-view learning is to exploit the relationship between views to improve the performance or reduce model complexity. Multi-view learning is well studied in conjunction with semi-supervised learning or active learning. To combine information from multi-views for supervised learning, fusion techniques at feature level or classifier level can be employed <ref type="bibr" target="#b31">[32]</ref>. Multi-instance learning aims at separating bags containing multiple instances. Over the years, many multi-instance learning algorithms have been proposed, including miBoosting <ref type="bibr" target="#b28">[29]</ref>, miSVM <ref type="bibr" target="#b0">[1]</ref>, MILES <ref type="bibr" target="#b6">[7]</ref> and miGraph <ref type="bibr" target="#b32">[33]</ref>. Several works also studied the combination of multi-view and multi-instance learning and its application to computer vision tasks.</p><p>Local and metric learning. Existing local learning methods mainly vary in the way that they utilize the labelled instances nearest to a test instance. One way is to only use a fixed number of nearest neighbors to the test point to train a model using neural network, SVM or just voting. The other way is to learn a transformation of the feature space (e.g., Linear Discriminant Analysis). In either way, the learned model can be better tailored for the test instance's neighborhood property <ref type="bibr" target="#b12">[13]</ref>. Metric learning is closely related to local learning as a good distance metric is crucial for the success of local learning. Generally, metric learning methods optimize a distance metric to best satisfy known similarity constraints between training data <ref type="bibr" target="#b2">[3]</ref>. Some metric learning methods learn a single global metric <ref type="bibr" target="#b26">[27]</ref>. Others learn local metrics that vary in different regions of the fea-ture space <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Label as Multi-Instance</head><p>In this section, we introduce the first level of locality by formulating the multi-label object recognition problem as a multi-instance learning (MIL) problem. To be specific, given a set of n training images {X i } n i=1 , we extract n i object proposals {x ij , j = 1, . . . , n i } from each image X i using general object detection techniques. By decomposing images into object proposals, each image X i becomes a bag containing several positive instances, i.e., proposals with the target objects, and negative instances, i.e., proposals with background or other objects. The problem of classifying X i is thus transformed from a multi-label classification problem to a multi-class MIL problem. The merit of such a transformation is that we do not need to deal with the complex process of directly recognizing multiple objects in multiple scales, locations and categories in a single image. Instead, we only need to identify whether there exist target objects in the proposals, which has been proven to be the forte of CNN features <ref type="bibr" target="#b11">[12]</ref>.</p><p>MIL problems assume that every positive bag contains at least one positive instance. As extensively compared and evaluated in <ref type="bibr" target="#b13">[14]</ref>, state-of-the-art general object detection methods like BING <ref type="bibr" target="#b7">[8]</ref>, selective search <ref type="bibr" target="#b24">[25]</ref>, MCG <ref type="bibr" target="#b1">[2]</ref> and EdgeBoxes <ref type="bibr" target="#b33">[34]</ref> can reach reasonably good recall rates with several hundreds of proposals. Therefore, if we sample enough proposals from each image, we can safely assume that these proposals can cover all objects (or at least all object categories) in an image, thus fulfilling the assumption of multi-instance learning.</p><p>In particular, we employ the unsupervised selective search method <ref type="bibr" target="#b24">[25]</ref> for object proposal generation. Selective search has proven to be able to achieve a balance between effectiveness and efficiency <ref type="bibr" target="#b13">[14]</ref>. More importantly, as it is unsupervised, no extra training data or ground truth bounding boxes are needed in this stage. Example of proposals extracted by selective search can be found in <ref type="figure" target="#fig_0">Fig. 3</ref>.</p><p>Traditionally, MIL is formulated as a max-margin classification problem with latent parameters optimized using alternating optimization. Typical examples include miSVM <ref type="bibr" target="#b0">[1]</ref> and Latent SVM <ref type="bibr" target="#b10">[11]</ref>. However, although these methods can achieve satisfactory accuracies, their limitations in scalability hinder their applicability to current large scale image classification tasks. For large scale MIL problem, <ref type="bibr" target="#b27">[28]</ref> shows that Fisher vector <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> (FV) can be used as an efficient and effective holistic representation for a bag. Thus, we choose to represent each bag X i as an FV.</p><p>Assume we have a K-component Gaussian Mixture Model (GMM) with parameters θ = {ω k , µ k , Σ k , k = 1, . . . , K}, where ω k , µ k and Σ k are the mixture weight, mean vector and covariance matrix of the k-th Gaussian, respectively. The covariance matrices Σ k are assumed to be diagonal, where the corresponding standard deviations of the diagonal entries form a vector σ k . We have <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_0">f Xi µ k = 1 √ ω k ni j=1 γ j (k) x ij − µ k σ k ,<label>(1)</label></formula><formula xml:id="formula_1">f Xi σ k = 1 √ ω k ni j=1 γ j (k) 1 √ 2 (x ij − µ k ) 2 σ 2 k − 1 ,<label>(2)</label></formula><p>where γ j (k) is the soft assignment weight, which is also the probability for x ij to be generated by the k-th Gaussian:</p><formula xml:id="formula_2">γ j (k) = p(k|x ij , θ).<label>(3)</label></formula><p>We map all the proposals {x ij , j = 1, . . . , n i } in an image X i to an FV by concatenating f Xi µ k and f Xi σ k for all k = 1, . . . , K, and denoting it as F Xi . F Xi will be used as the final feature to train the one-vs-all linear classifiers. Note that for simplicity, we abuse the notation of x ij for both proposal j in image i and its corresponding feature representation. In the next section, we will describe how to generate the feature representation x ij for each proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">From Global Representation to Local Similarity</head><p>Once we obtain object proposals for each image, we can naturally use CNN features to represent these proposals. Following general practices in the literature <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, each proposal is fed into a pre-trained CNN, and the output of the second last fully connected layer (e.g. layer 7 in AlexNet <ref type="bibr" target="#b14">[15]</ref>) is used as the feature representation of that particular proposal. We call this kind of representation as the feature view f ij for proposal x ij from image X i . With the proposals represented by CNN features, one baseline is to encode each image (bag) at the feature view by Fisher vector as discussed in Sec. 3. Such baseline is able to get reasonably good results by utilizing the Fisher vector generated only from the feature view.</p><p>However, since the proposals contain different objects as well as random background, there exist large variances and imbalanced distributions. As a consequence, the global representation might not be accurate enough. Inspired by the idea of local learning, which solves the data density and intra-class variation problems by focusing on a subset of the data that are more relevant to a particular instance <ref type="bibr" target="#b3">[4]</ref>, we propose the second level of locality by adding local spatial configuration information as the label view (cf. <ref type="figure">Fig. 2</ref>) to enhance the discriminative power of the feature.</p><p>To effectively encode the local spatial configuration of a proposal, we need to solve two key problems: how to form a good candidate pool for local learning and how to determine which candidates are relevant to a particular new proposal. For the former problem, since we have some ground truth object bounding boxes from the strong labels, we could use them as the candidate pool assuming all of the ground truth objects are useful. For the latter one, we follow the common assumption that the most relevant candidates are the nearest neighbors. In this way, the problem becomes how to define "nearest". Many studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> have shown that the distance metric is critical to the performances of local learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CNN as Metric Learning</head><p>Metric learning studies the problem of learning a discriminative distance metric. Conventional Mahalanobis metric learning methods optimize the parameters of a distance function in order to best satisfy known similarity or dissimilarity constraints between training instances <ref type="bibr" target="#b2">[3]</ref>. To be specific, given a set of n labelled training instances {x i , y i } n i=1 , the goal of metric learning is to learn a square matrix M such that the distance mapped between training data, represented as</p><formula xml:id="formula_3">D M (x i , x j ) = (x i − x j ) T M(x i − x j ) ,<label>(4)</label></formula><p>satisfies certain constraints. Since M is symmetric and positive semi-definite, it can be decomposed as M = W T W, and D M (x i , x j ) can be rewritten as:</p><formula xml:id="formula_4">D(x i , x j ) = W(x i − x j ) 2 .<label>(5)</label></formula><p>We can see that learning a distance metric is equivalent to learning linear projection W that maps the data from input space to a transformed space. In this sense, the extraction of CNN features from the original raw pixel space can also be viewed as a form of metric learning, while only the process is highly nonlinear. However, the goal of CNN is usually to minimize the classification error using loss functions such as the logistic loss, which may not be suitable for local encoding.</p><p>Our desired metric should be discriminative such that all categories are well separated, as well as compact so that we can find more accurate nearest neighbors. Specifically, we want the pairwise distance between instances from the same class to be smaller than that between instances from different classes. In order to achieve such a goal, <ref type="bibr" target="#b26">[27]</ref> proposed the large-margin distance to minimize the following objective function:</p><formula xml:id="formula_5">i,j η ij D(x i , x j )+ α i,j,l η ij (1 − y il ) 1 + D(x i , x j ) − D(x i , x l ) + . (6)</formula><p>Here η encodes target nearest neighbor information, where η ij = 1 if x j is one of thek positive nearest neighbors of x i ; otherwise η ij = 0. y is the label information where y il = 1 if x i and x l are in the same class; otherwise y il = 0.</p><p>[·] + = max(·, 0) is the hinge loss function. α is the tradeoff parameter. The first term in Eq. 6 penalizes large distances between instances and target neighbors, and the second term penalizes small distances between each instance and all other instances that do not share the same label. By employing such an objective function, we can ensure that thek-nearest neighbors of an instance belong to the same class, while instances from different classes are separated by a large margin.</p><p>In order to learn a discriminative metric, we propose to learn a large-margin nearest neighbor (LMNN) CNN. Specifically, we replace the logistic loss with the large margin nearest neighbor loss and train a network with lowdimension output utilizing the strong labels. Details of training and fine-tuning the LMNN CNN can be found in Section 4.3. The output of the proposed LMNN network is a low-dimensional feature that shares the good semantic abstraction power of conventional CNN feature and the good neighborhood property of large-margin metric learning. We then build the candidate pool with the LMNN CNN features extracted from ground truth objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Encoding Local Spatial Label Distribution as Label View</head><p>To effectively incorporate local label information around a local region, we encode its neighborhood as the label view. Specifically, we extract features from each proposal x ij using the LMNN CNN, then find k nearest neighbors of x ij in the candidate feature pool as nn ij = {nn 1 ij , . . . , nn k ij } and record their labels l ij = l 1 ij . . . l k ij . The label information (e.g. l k ij ) of a neighbor (e.g. nn k ij ) is encoded as a C-dimensional binary vector, which corresponds to C categories. The d-th dimension l k ij (d) = 1, (d = 1, . . . , C) if the object is annotated as class d; otherwise l k ij (d) = 0. Therefore, l ij is a 1 × kC vector and it will be used as the feature for the label view.</p><p>The merit of such indirectly utilizing ground truth bounding boxes as the label view is the good generalization ability. As the label view is a form of local structure representation, even for unseen categories, i.e. no same-category strong labels, the encoding process can naturally exploit existing semantically or visually close categories to build local support. For example, suppose we do not have the bounding box annotations for "cat" and "train", a proposal containing "cat" might have nearest neighbors of "dog", "tiger" or other related animals, and a proposal containing "train" might have nearest neighbors of "car", "truck" or other vehicles. Although lacking the exact annotations of certain category, the label view is still able to encode the local structure with semantically or visually similar objects. In this way, our framework can make use of existing strong supervision information to boost the overall performance. The experimental results shown in Section 5.3 validate this argument.</p><p>We directly concatenate the feature view and the label view to form the final representation of each proposal x ij as f ij λl ij , where λ is the trade-off parameter between the feature view and the label view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network Configurations and Implementation Details</head><p>Our framework consists of two networks, a large-margin nearest neighbor (LMNN) CNN and a standard CNN. Both networks' architectures are similar to <ref type="bibr" target="#b4">[5]</ref> with 5 convolutional layers and 3 fully-connected layers, and the dimension of the layer-7 output is set to 2048. The main differences of these two networks lie in the loss function and the fine-tuning process. For LMNN CNN, its layer-8 output is a 128-dimensional feature, based on which we measure the pair-wise distance for kNN, while the output of the standard CNN is a C-dimensional score vector, corresponding to the C categories.</p><p>Data pre-processing and pre-training. We use the ILSVRC 2012 dataset to pre-train both networks. Given an image, we resize the short side to 224 with bilinear interpolation and perform a center crop to generate the standard 224 × 224 input. Each of these inputs is then pre-processed by subtracting the mean of ILSVRC images.</p><p>Fine-tuning. To better adopt the pre-trained network for specific applications, we also fine-tune these networks using task relevant data. Unlike <ref type="bibr" target="#b4">[5]</ref>, currently our implementation does not involve any data augmentation in the fine-tuning stage.</p><p>For the standard CNN used for feature view, we only fine-tune the network with weak labels on the whole image. As our task is multi-label recognition, following <ref type="bibr" target="#b25">[26]</ref>, we use square loss instead of the logistic loss. To be specific, suppose we have a label vector y i = [y i1 , y i2 , . . . , y iC ] for the i-th image. y ij = 1 (j = 1, . . . , C) if the image is annotated with class j; otherwise y ij = 0. The ground truth probability vector of the i-th image is defined as p i = y i / y i 1 and the predicted probability vector iŝ p i = [p i1 ,p i2 , . . . ,p iC ]. Then, the cost function to be minimized is defined as</p><formula xml:id="formula_6">1 n n i=1 C j=1 (p ij −p ij ) 2 .<label>(7)</label></formula><p>During the fine-tuning, the parameters of the first seven layers of the network are initialized with the pre-trained parameters. The parameters of the last fully connected layer is initialized with a Gaussian distribution. We tune the network for 10 epochs in total. For the large-margin NN (LMNN) CNN used for label view, we execute a three-step fine-tuning. The first step is the image level fine-tuning similar to the process we have described above. The second step is ground truth objects fine-tuning, where we fine-tune the network using ground truth objects with the logistic loss. The final step is the large-margin nearest neighbor fine-tuning, where we finetune the network with the loss function of Eq. 6 described in Section 4.1. To accelerate the process, in this final step, we fix all parameters of the first seven layers and only finetune the parameters of the last fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we present the experimental results of the proposed multi-view multi-instance framework on multilabel object recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Baselines</head><p>We evaluate our method on the PASCAL Visual Object Classes Challenge (VOC) datasets <ref type="bibr" target="#b9">[10]</ref>, which are widely used as benchmark datasets for the multi-label object recognition task. In particular, we use the VOC 2007 and VOC 2012 datasets. The details of these datasets can be found in <ref type="table" target="#tab_1">Table 1</ref>. These two datasets have a pre-defined split of TRAIN, VAL and TEST sets. We use TRAIN and VAL for training and TEST for testing. The evaluation method is average precision (AP) and mean average precision (mAP). We compare the proposed framework with the following state-of-the-art approaches:</p><p>• CNN-SVM <ref type="bibr" target="#b22">[23]</ref>. This method employed OverFeat <ref type="bibr" target="#b21">[22]</ref>, which is pre-trained on ImageNet, to get CNN activations as the off-the-shelf features. Specifically, CNN-SVM employs the 4096-d feature extracted from the 22nd layer of OverFeat and uses these features to train a linear SVM for the classification task. • PRE <ref type="bibr" target="#b17">[18]</ref>. <ref type="bibr" target="#b17">[18]</ref> proposed to transfer image representations learned with CNN on ImageNet to other visual recognition tasks with limited training data. The network has exactly the same architecture as that in <ref type="bibr" target="#b14">[15]</ref>. The network is first pre-trained on ImageNet. The parameters of the first seven layers of CNN are then fixed and the last fully-connected layer is replaced by two adaptation layers. Finally, the adaptation layers are trained with images from the target dataset. • HCP <ref type="bibr" target="#b25">[26]</ref>. HCP proposed to solve the multi-label object recognition task by extracting object proposals from the images. Specifically, HCP has three main steps. The first step is to pre-train a CNN on ImageNet data. The second step is image-level fine-tuning that uses image labels and square loss to fine-tune the pre-trained CNN. The final step is to employ BING <ref type="bibr" target="#b7">[8]</ref> to extract object proposals and fine-tune the network with these proposals. The image-level scores are obtained by maxpooling from the scores of the proposals. • <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b18">[19]</ref> also handled the problem in a weakly supervised manner. Particularly, multiple windows are extracted from different scales of the images in the dense sampling fashion. The scores of these windows are combined with max-pooling from the same scale then sum-pooling across different scales. • VeryDeep <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr" target="#b23">[24]</ref> densely extracts multiple CNN features across multi-scales of the image with very-deep networks (16-layer and 19-layer). The features from the same scale are concatenated by sum-pooling and features from different scale are aggregated by stacking or sum-pooling. <ref type="bibr" target="#b23">[24]</ref> also augments the test set by horizontal flipping of the images.</p><p>• Hand-crafted Features. <ref type="bibr" target="#b5">[6]</ref> presented an Ambiguity guided Mixture Model (AMM) to integrate external context features and object features, and then used the contextualized SVM to iteratively boost the performance of object classification and detection tasks. <ref type="bibr" target="#b8">[9]</ref> proposed an Ambiguity Guided Subcategory (AGS) mining approach to improve both detection and classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Our Setup and Parameters</head><p>It is difficult to make a completely fair comparison among different CNN based methods as the CNN configurations, the data augmentation and the pre-training could substantially influence the results. All CNN based methods can benefit from extra training data and more powerful networks as shown in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref>. To fairly evaluate our proposed framework, we develop our system based on the common 8-layer CNN pretrained on ILSVRC 2012 dataset with 1000 categories. The details of the fine tuning process has been elaborated in Section 4.3. Once the LMNN CNN and the standard CNN (see <ref type="figure">Fig. 2</ref>) are trained, the system is applied to map each training image into a final FV feature. Finally, TopPush <ref type="bibr" target="#b15">[16]</ref> is chose to learn linear one-vs-all classifiers for each category, which produces the scores for each binary sub-problem of the multi-label datasets. The scores are then evaluated with standard VOC evaluation package. All the experiments are run on a computer with Intel i7-3930K CPU, 32G main memory and an nVIDIA Tesla K40 card.</p><p>For the proposal extraction, we employ selective search <ref type="bibr" target="#b24">[25]</ref>, which typically generates around 1500 proposals on average from every image in the PASCAL VOC 2007 dataset using the parameters suggested in <ref type="bibr" target="#b24">[25]</ref>. Considering the computational time and the hardware limitation, we random sample around 400 proposals per image for training and testing.</p><p>For the parameters of Fisher vector, we follow <ref type="bibr" target="#b19">[20]</ref> to first employ PCA to reduce the dimension of the original features to preserve around 90% energy. For VOC 2007 and 2012 datasets, after PCA, the standard CNN features is reduced to around 450-d. After PCA, we generate 128 GMM codewords and encode each image with IFV similar to <ref type="bibr" target="#b19">[20]</ref>.</p><p>For fine-tuning the LMNN CNN, we set the trade-off parameter α = 1 (see <ref type="bibr" target="#b5">(6)</ref>) and the nearest neighbor number k = 10 (for training). For combining the feature view and the label view features, we select the trade-off parameter λ (specified at the end of Section 4.2) from {1, 0.5, 0.25} by cross-validation. For the nearest neighbor number k used in testing, generally bigger k leads to better accuracy, but we observe there is no performance gain for k &gt; 50. Thus, we set k = 50 for testing. For faster NN search, we employ FLANN <ref type="bibr" target="#b16">[17]</ref> with "autotuned" parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image Classification Results</head><p>Image Classification on VOC 2007: <ref type="table" target="#tab_2">Table 2</ref> reports our experimental results compared with state-of-the-art methods on VOC 2007. In the upper part of the table we compare with the hand-crafted feature based methods and the CNN based methods pre-trained on ILSVRC 2012 using 8-layer network. To demonstrate the effectiveness of individual components, we consider three variations of our proposed framework: 'FeV', 'FeV+LV-10' and 'FeV+LV-20', where 'FeV' uses only the feature view (i.e. without the label view features), 'FeV+LV-10' uses both the feature view and the label view with 10 categories of ground-truth bounding boxes of the training set, and 'FeV+LV-20' is the one with 20 categories of ground-truth bounding boxes.</p><p>From the upper part of <ref type="table" target="#tab_2">Table 2</ref>, we can see that using just feature view ('FeV'), we already outperform the state-of-the-art proposal-based method ('HCP-1000C') by 2.2%, which suggests that Fisher vector as a holistic representation for bags is superior than max-pooling. With all 20 categories of ground-truth bounding boxes of the training set, our multi-view framework ('FeV+LV-20') achieves a further 2.5% performance gain. This significant performance gain validates the effectiveness of the label view. Our framework shows good performance especially for difficult categories such as BOTTLE, COW <ref type="table">, TABLE, MOTOR and</ref> PLANT.</p><p>If we just use the ground-truth bounding boxes from the first 10 categories (PLANE to COW), our framework ('FeV+LV-10') still outperforms single feature view ('FeV') by a margin of 1.3%. As expected, using the bounding boxes of the categories from PLANE to COW can boost the performance of these categories as shown in the table. However, it is interesting to see that the label view also improves the accuracies of unseen categories such as HORSE, PERSON and TV. This is mainly because the proposed label view encoding is a form of local similarity representation, which can generalize quite well to unseen categories.</p><p>In the lower part of <ref type="table" target="#tab_2">Table 2</ref>, we list the results of 'HCP-2000C' <ref type="bibr" target="#b25">[26]</ref>, which uses additional 1000 categories from ImageNet that are semantically close to VOC 2007 categories for CNN pre-training, and 'VeryDeep' <ref type="bibr" target="#b23">[24]</ref>, which densely extracts multiple CNN features from 5 scales and combines two very-deep CNN models (16-layer an 19layer). Our framework ('FeV+LV-20') can still outperform 'HCP-2000C', but is inferior to 'VeryDeep' since our framework is based on the common 8-layer CNN.</p><p>To demonstrate the potential of our framework, we replace the 8-layer CNNs in our framework by the 16-layer CNN model in <ref type="bibr" target="#b23">[24]</ref>, which is denoted as 'FeV+LV-20-VD'. Unlike <ref type="bibr" target="#b23">[24]</ref>, we do not use any data augmentation or multiscale dense sampling in the feature extraction stage. Our 'FeV+LV-20-VD' outperforms 'VeryDeep' by nearly 1%. By further averaging the scores of 'VeryDeep' <ref type="bibr" target="#b23">[24]</ref> and  'FeV+LV-20-VD' (denoted as 'Fusion'), we achieve stateof-the-art mAP of 92.0%. This suggests that our proposalbased framework and the multi-scale CNN extracted from the whole image are complement to each other. Image Classification on VOC 2012: <ref type="table" target="#tab_3">Table 3</ref> reports our experimental results compared with those of the state-ofthe-art methods on VOC 2012. Similar to <ref type="table" target="#tab_2">Table 2</ref>, we compare with the hand-crafted feature based methods and the CNN based methods pre-trained on ILSVRC 2012 using 8layer CNN model in the upper part and the methods trained with additional data or very-deep CNN models in the lower part.</p><p>The results are consistent with those on VOC 2007. Our framework that uses only the feature view ('FeV') already outperforms the state-of-the-art hand-crafted feature method ('NUS-PSL') by 1.8% and the state-of-the-art proposal-based CNN method (HCP-1000C) by 2.3%. With the aid of the label view, our 'FeV+LV-20' obtains an additional 2% performance again, even outperforming the two proposal-based methods pre-trained on additional 512 or 1000 categories of image data ('PRE-1512C' and 'HCP-2000C') and comparable to <ref type="bibr" target="#b18">[19]</ref>. By employing just 10 categories of bounding boxes, the mAP performance of our 'FeV+LV-10' does not degrade much.</p><p>When employed with the very-deep 16-layer CNN model <ref type="bibr" target="#b23">[24]</ref>, our framework ('FeV+LV-20-VD') achieves similar performance as 'VeryDeep'. When we averagely fuse the scores of <ref type="bibr" target="#b23">[24]</ref> and our proposal-based representation, our method ('Fusion') achieves state-of-the-art result, outperforming <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed a multi-view multiinstance framework for solving the multi-label classification problem. Compared with existing works, our framework makes use of the strong labels to provide another view of local information (label view) and combines it with the typical feature view information to boost the discriminative power of feature extraction for multi-label images. The experimental results validates the discriminative power and the generalization ability of the proposed framework.</p><p>For future directions, there are several possibilities to explore. First of all, we can improve the scalability and possibly also the performance of the framework by establishing a proposal selection criteria to filter out noisy proposals. Secondly, we may build a suitable candidate pool directly from the extracted proposals to eliminate the need for strong labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>An example of object proposals generated by selective search. We demonstrate 30 randomly sampled proposals from the full 218 proposals, which clearly cover two of the main objects: person and bike.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Dataset information.</figDesc><table><row><cell>Dataset</cell><cell>#TrainVal</cell><cell cols="2">#Test #Classes</cell></row><row><cell>VOC 2007</cell><cell>5011</cell><cell>4952</cell><cell>20</cell></row><row><cell>VOC 2012</cell><cell cols="2">11540 10991</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of the classification results (in %) of state-of-the-art approaches on VOC 2007 (TRAINVAL/TEST). The upper part shows the results of the hand-crafted feature based methods and the CNN based methods trained with 8-layer CNN and ILSVRC 2012 dataset. The lower part shows the results of the methods trained with very-deep CNN or with additional training data. PLANE BIKE BIRD BOAT BOTTLE BUS CAR CAT CHAIR COW TABLE DOG HORSE MOTOR PERSON PLANT SHEEP SOFA TRAIN TV MAP AGS [9] 82.2 83.0 58.4 76.1 56.4 77.5 88.8 69.1 62.2 61.8 64.2 51.3 85.4 80.2 91.1 48.1 61.7 67.7 86.3 70.9 71.1 AMM [6] 84.5 81.5 65.0 71.4 52.2 76.2 87.2 68.5 63.8 55.8 65.8 55.6 84.8 77.0 91.1 55.2 60.0 69.7 83.6 77.0 71.3 CNN-SVM [23] 88.5 81.0 83.5 82.0 42.0 72.5 85.3 81.6 59.9 58.5 66.3 77.8 81.8</figDesc><table><row><cell>78.8</cell><cell>90.2</cell><cell>54.8 71.1 62.6 87.4 71.8 73.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons of the classification results (in %) of state-of-the-art approaches on VOC 2012 (TRAINVAL/TEST). The upper part shows the results of the hand-crafted feature based methods and the CNN based methods trained with 8-layer CNN and ILSVRC 2012 dataset. The lower part shows the results of the methods trained with very-deep CNN or with additional training data. PLANE BIKE BIRD BOAT BOTTLE BUS CAR CAT CHAIR COWTABLE DOG HORSE MOTOR PERSON PLANT SHEEP SOFA TRAIN TV MAP 88.8 92.0 87.4 64.7 91.1 87.4 94.4 74.9 89.2 76.3 93.7 95.2 91.1 97.6 66.2 91.2 70.0 94.5 83.7 86.3 VeryDeep [24] 99.0 89.1 96.0 94.1 74.1 92.2 85.3 97.9 79.9 92.0 83.7 97.5 96.5 94.7 97.1 63.7 93.6 75.2 97.4 87.8 89.3 NUS-HCP-AGS [26] 99.0 91.8 94.8 92.4 72.6 95.0 91.8 97.4 85.2 92.9 83.1 96.0 96.6 96.1 94.9 68.4 92.0 79.6 97.3 88.5 90.3 FeV+LV-20-VD 98.4 92.8 93.4 90.7 74.9 93.2 90.2 96.1 78.2 89.8 80.6 95.7 96.1 95.3 97.5 73.1 91.2 75.4 97.0 88.2 89.4 Fusion 98.9 93.1 96.0 94.1 76.4 93.5 90.8 97.9 80.2 92.1 82.4 97.2 96.8 95.7 98.1 73.9 93.6 76.8 97.5 89.0 90.7</figDesc><table><row><cell>NUS-PSL [26]</cell><cell>97.3 84.2 80.8 85.3 60.8 89.9 86.8 89.3 75.4 77.8 75.1 83.0 87.5</cell><cell>90.1</cell><cell>95.0</cell><cell>57.8 79.2 73.4 94.5 80.7 82.2</cell></row><row><cell>PRE-1000C [18]</cell><cell>93.5 78.4 87.7 80.9 57.3 85.0 81.6 89.4 66.9 73.8 62.0 89.5 83.2</cell><cell>87.6</cell><cell>95.8</cell><cell>61.4 79.0 54.3 88.0 78.3 78.7</cell></row><row><cell>HCP-1000C [26]</cell><cell>97.7 83.0 93.2 87.2 59.6 88.2 81.9 94.7 66.9 81.6 68.0 93.0 88.2</cell><cell>87.7</cell><cell>92.7</cell><cell>59.0 85.1 55.4 93.0 77.2 81.7</cell></row><row><cell>FeV</cell><cell>96.8 87.8 88.7 87.2 63.8 92.3 86.2 92.3 72.4 82.0 76.0 91.9 90.3</cell><cell>90.3</cell><cell>95.2</cell><cell>61.2 82.6 65.6 92.8 84.4 84.0</cell></row><row><cell>FeV-LV-10</cell><cell>97.3 89.1 91.5 88.5 66.7 92.2 87.2 94.0 74.0 82.7 77.8 91.6 91.1</cell><cell>92.7</cell><cell>95.7</cell><cell>66.5 85.4 69.4 95.6 85.4 85.7</cell></row><row><cell>FeV-LV-20</cell><cell>97.4 88.9 91.2 87.4 64.2 92.2 86.4 95.0 75.1 84.6 78.7 93.1 91.9</cell><cell>93.1</cell><cell>96.6</cell><cell>67.3 86.2 69.4 95.3 85.8 86.0</cell></row><row><cell>PRE-1512C [18]</cell><cell>94.6 82.9 88.2 84.1 60.3 89.0 84.4 90.7 72.1 86.8 69.0 92.1 93.4</cell><cell>88.6</cell><cell>96.1</cell><cell>64.3 86.6 62.3 91.1 79.8 82.8</cell></row><row><cell>HCP-2000C [26]</cell><cell>97.5 84.3 93.0 89.4 62.5 90.2 84.6 94.8 69.7 90.2 74.1 93.4 93.7</cell><cell>88.8</cell><cell>93.3</cell><cell>59.7 90.3 61.8 94.4 78.0 84.2</cell></row><row><cell>[19]</cell><cell>96.7</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research is supported by Singapore MoE AcRF Tier-1 Grant RG138/14 and also partially supported by the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University, Singapore. The Tesla K40 used for this research was donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 15</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey on metric learning for feature vectors and structured data. CoRR, abs/1306</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6709</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="888" to="900" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contextualizing object detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPMAI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="27" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Miles: Multiple-instance learning via embedded instance selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1931" to="1947" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Subcategory-aware object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="827" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminant adaptive nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="607" to="616" />
			<date type="published" when="1996-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How good are detection proposals, really?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 25</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Top rank optimization in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 27</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1502" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable nearest neighbor algorithms for high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised object recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno>HAL-01015140</idno>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6229</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">CNN: single-label to multi-label. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">5726</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scalable multiinstance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Xiu-Shen Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Logistic regression and boosting for labeled bags of instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="272" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An efficient algorithm for local distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="543" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting useful neighborhoods for lazy local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 27</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1916" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Can visual recognition benefit from auxiliary information in training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-instance learning by treating instances as non-i.i.d. samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
