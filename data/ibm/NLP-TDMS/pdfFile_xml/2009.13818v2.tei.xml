<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
							<email>1dishen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<addrLine>365 AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhi</forename><surname>Zheng</surname></persName>
							<email>mizheng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<addrLine>365 AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
							<email>yanruqu2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<addrLine>365 AI</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<addrLine>365 AI</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial training has been shown effective at endowing the learned representations with stronger generalization ability. However, it typically requires expensive computation to determine the direction of the injected perturbations. In this paper, we introduce a set of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>simple yet efficient data augmentation strategies dubbed cutoff, where part of the information within an input sentence is erased to yield its restricted views (during the fine-tuning stage). Notably, this process relies merely on stochastic sampling and thus adds little computational overhead. A Jensen-Shannon Divergence consistency loss is further utilized to incorporate these augmented samples into the training objective in a principled manner. To verify the effectiveness of the proposed strategies, we apply cutoff to both natural language understanding and generation problems. On the GLUE benchmark, it is demonstrated that cutoff, in spite of its simplicity, performs on par or better than several competitive adversarial-based approaches. We further extend cutoff to machine translation and observe significant gains in BLEU scores (based upon the Transformer Base model). Moreover, cutoff consistently outperforms adversarial training and achieves state-of-the-art results on the IWSLT2014 German-English dataset. The source code can be obtained from: https: //github.com/dinghanshen/Cutoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale language models (LMs) pre-trained with massive unlabeled text corpora, in a selfsupervised manner, has brought impressive performance gains across a wide range of natural language processing tasks <ref type="bibr" target="#b12">(Devlin et al., 2019;</ref><ref type="bibr" target="#b43">Yang et al., 2019;</ref><ref type="bibr" target="#b9">Clark et al., 2019;</ref><ref type="bibr" target="#b23">Lewis et al., 2020;</ref><ref type="bibr" target="#b4">Bao et al., 2020;</ref><ref type="bibr" target="#b15">He et al., 2020)</ref>. Significant research efforts have focused on exploring various pre-training recipes to yield more informative LMs. However, given the imbalanced nature between the huge number of model parameters and limited task-specific data, how to leverage and unlock the knowledge from large-scale LMs (during the finetuning stage) remains a challenging issue. It has been observed that the representations from pretrained models, after being fine-tuned on specific downstream tasks, tend to degrade and become less generalizable <ref type="bibr" target="#b18">Jiang et al., 2019;</ref><ref type="bibr" target="#b0">Aghajanyan et al., 2020)</ref>.</p><p>To alleviate this issue, adversarial training objectives have been proposed to regularize the learned representations during the fine-tuning stage <ref type="bibr" target="#b25">Liu et al., 2020b;</ref><ref type="bibr" target="#b18">Jiang et al., 2019)</ref>. Specifically, label-preserving perturbations are performed on the word embedding layer, and the model is encouraged to make consistent predictions regardless of these noises. Although the model's robustness can be improved with these perturbed examples, adversarial-based methods typically require additional backward passes to decide the direction of the inject perturbations. As a result, these methods give rise to significantly more computational and memory overhead (relative to standard SGD training).</p><p>In this paper, we introduce a set of simple yet efficient data augmentation strategies. They are inspired by the consensus principle in multi-view learning <ref type="bibr" target="#b5">(Blum and Mitchell, 1998;</ref><ref type="bibr" target="#b42">Xu et al., 2013;</ref><ref type="bibr" target="#b10">Clark et al., 2018)</ref>, which states that maximizing the agreement/consensus between two different views of data can lead to lower error rate. Specifically, we propose to erase/remove part of the information within a training instance to produce multiple perturbed samples. To ensure that the model cannot utilize the information from the removed input at all, the erasing process happens at the input embeddings layer. In contrast to Dropout, which converts individual elements within the word em-bedding matrix to 0, we propose to erase the vectors along each dimension entirely. As a result, either multiple tokens or embedding dimensions are converted to vectors of all zeros, yielding partial views of the input matrix in a structured manner.</p><p>To make the augmented samples more challenging, inspired by , we further introduce an approach to derive restricted views by removing a contiguous span within an input sequence. The model is fine-tuned with the constraint of making consistent predictions on these augmented data (even with partial views of the original input). Intuitively, the resulting representations tend to have a stronger ability of fully abstracting various semantic features from a sentence, since the model can not merely utilize the most salient ones (which may not be available in partial views) to make the corresponding predictions.</p><p>To capture the intrinsic relationship among these stochastic and diverse augmented examples, we propose a specially-designed consistency regularization objective. Particularly, in addition to the cross-entropy loss typically employed in data augmentation, a Jensen-Shannon Divergence consistency loss is further introduced to match the predictions between different partial views of a given input. One advantage is that this loss is able to naturally maximize the consensus between multiple (more than 2) views in a more principled and stable manner.</p><p>We evaluate the effectiveness of the proposed data augmentation strategies on a wide range of natural language understanding (NLU) tasks from the GLUE benchmark. RoBERTa  is employed as the testbed model in our experiments. However, the augmentation methods proposed here can be easily extended to other largescale pretrained models. Despite its simplicity, our method consistently gives rises to significant performance gains. More importantly, cutoff outperforms several competitive adversarial-based approaches, while being much more computationally efficient. We further extend cutoff to the text generation scenario and verify it on a machine translation task. The proposed methods greatly outperform adversarial training on both WMT2014 English-to-German and IWSLT2014 German-to-English tasks. In addition, while combining cutoff with a Transformer base model, we achieved state-of-the-art test result on the IWSLT2014 German-to-English dataset, with a BLEU score of 37.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adversarial Training Adversarial training was originally proposed to attack neural-network-based models by applying small perturbations to the input <ref type="bibr" target="#b36">(Szegedy et al., 2013)</ref>. Thereafter, several adversarial-based approaches, including adversarial examples <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref>, PGD <ref type="bibr" target="#b28">(Madry et al., 2017)</ref>, etc, have been introduced. It has been demonstrated that these methods can improve the robustness and generalization ability of a model by augmenting the perturbed examples into the original training instances. Recently, adversarial-based approaches emerged as a popular research trend in NLP, which have been successfully applied to a wide variety of NLU tasks, including sentence classification, machine reading comprehension (MRC) and natural language inference (NLI) tasks, etc. Despite its success, computational overhead is typically required to calculate the perturbation directions. Several research efforts have been devoted to accelerate adversarial training <ref type="bibr" target="#b32">(Shafahi et al., 2019;</ref>. However, additional forward-backward passes are still needed for adversarial training. Our proposed cutoff methods are much more computationally efficient from this perspective. Besides, the connection between adversarial training and data-augmentation-based approaches has not previously been well-established. Our work bridges this gap by unifying the two types of methods under the consistency training framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-view Learning</head><p>The main idea of multiview learning is to produce distinct subsets (views) of features corresponding to the same data, and the predictions by the model according to different views are repelled to be consistent <ref type="bibr" target="#b42">(Xu et al., 2013)</ref>. Our approach is slightly different from such algorithms, e.g., co-training <ref type="bibr" target="#b5">(Blum and Mitchell, 1998)</ref> and co-regularization <ref type="bibr" target="#b33">(Sindhwani et al., 2005)</ref>, in the sense that the multiple views from cutoff have certain overlaps, rather than being entirely independent.</p><p>The intuition of our method bears resemblance to cross-view training (CVT) <ref type="bibr" target="#b10">(Clark et al., 2018)</ref>, which also proposes to improve sentence representations by encouraging consistent predictions across different views of the input. However, there are several key differences that make our work unique (except that CVT focuses on a semisupervised setting, rather than a supervised one as in our case): i) CVT generates partial views on top of latent representations, while cutoff operates at the input embedding layer. As a result, our method is more generic and model-agnostic; ii) CVT adds an auxiliary prediction module during the training stage, while span cutoff requires no changes to the original model at all; iii) we leverage Jensen-Shannon Divergence consistency loss to match the predictions with various views, which maximize their consensus in a more natural and stable manner (also more efficient than the multiple KL divergence terms used in CVT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In this section, we first discuss the motivation behind the cutoff data augmentation strategies, which leverage restricted views of a training instance. Then, we propose three simple but effective ways of obtaining partial views and highlight the advantages of each. A novel consistency loss is introduced to naturally integrate multiple cutoff samples into the training framework. Finally, we further extend the cutoff approach to the text generation scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>In the context of fine-tuning large pre-trained models, our hypothesis is that data augmentation could endow the original data with stronger ability to extract useful semantic information. Let f denote the transformation to convert an input sample x into its augmented examples. An ideal f should be label-preserving, i.e., the label of f (x) should be the same as x. Besides, f (x) should also be diverse and different enough from x, so that it could help to enrich the empirical observations and thus better cover the data space.</p><p>Different choices of f to introduce slight modifications on the original training instances have been proposed previously, such as adding Gaussian noise, adversarial training <ref type="bibr" target="#b25">(Liu et al., 2020b;</ref> and back-translation <ref type="bibr" target="#b44">(Yu et al., 2018;</ref><ref type="bibr" target="#b41">Xie et al., 2019)</ref>. Concretely, adversarial training performs perturbations on the word embedding layer to improve the model robustness. However, it takes additional backward passes to estimate the optimal perturbation direction and thus gives rise to additional computational and memory overhead. As to back translation, it first translates an existing example to another language, and then translate it back to obtain an augmented sample. Although effective, the quality of augmented data is usually sensitive to the mistakes made by the initial trans-lation models <ref type="bibr" target="#b6">(Chapelle et al., 2009;</ref><ref type="bibr" target="#b39">Wang et al., 2018)</ref>. Motivated by these observations, we aim to propose data augmentation schemes that are computationally efficient, and yet do not rely on any additional models or external data sources (e.g., paired translation data).</p><p>Taking inspiration from multi-view learning, where the connection between the consensus of predictions on two views and their error rates has been established <ref type="bibr" target="#b11">(Dasgupta et al., 2002)</ref>. Suppose we have two views x 1 and x 2 obtained with the same example, and let p 1 and p 2 denote a model's prediction on these views, respectively. The following holds true <ref type="bibr" target="#b11">(Dasgupta et al., 2002)</ref>:</p><formula xml:id="formula_0">P (p 1 = p 2 ) ≥ max{P err (p 1 ), P err (p 2 )} (1)</formula><p>The inequality above states that the error rates of the two hypotheses are both upper bounded by the probability of a disagreement between them. In other words, the accuracy of each prediction can be improved by minimizing their disagreement. Thus, encouraging consistent predictions among different views of a sentence, which contains only part of its entire information, could improve the generalization ability of the resulting models and reduce their error rates accordingly. In the next section, we will discuss how these views may be produced in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing Partial Views</head><p>To obtain partial views of a given sentence, <ref type="bibr" target="#b10">(Clark et al., 2018)</ref> proposed to carefully select hidden representations at the top of a Bi-LSTM sentence encoder. However, this strategy is not generic enough since it relies on the unidirectional nature of LSTM. For transformer-based architectures that are widely adopted nowadays, each output hidden unit has access to the information of all the input tokens (given the property of self-attention networks). In this regard, we argue that collecting restricted views at the input embedding space could be a more modelagnostic solution.</p><p>Given a text sequence x = [x 1 , ..., x L ], whose input embedding matrix is denoted by W ∈ R L×d . Note that w i,j represents the j-th dimension of the embedding vector corresponding to the i-th token, and d is the dimension of the input embeddings. We suppose that partial views may be obtained by cutting vectors along either dimensions off, hence the proposed approach is dubbed Cutoff. Cutoff removes the information from the input embedding matrix in a more structured manner, as opposed Sentence as a × matrix (a) Token Cutoff (c) Span Cutoff (b) Feature Cutoff <ref type="figure">Figure 1</ref>: Schematic illustration of the proposed cutoff augmentation strategies, including token cutoff, feature cutoff and span cutoff, respectively. Blue area indicates that the corresponding elements within the sentence's input embedding matrix are removed and converted to 0. Notably, this is distinct from Dropout, which randomly transforms elements to 0 (without considering any underlying structure of the matrix).</p><p>to Dropout, which randomly sets elements within the matrix to 0. Specifically, either the entire embedding of an individual word or one embedding dimension of every word within the sequence are converted to a vector of zeros (see <ref type="figure">Figure 1</ref>).</p><p>In the context of pre-trained transformer models, such as BERT or RoBERTa, the input embedding matrix consists of tokens, segments and positional embeddings. To make sure that no information corresponding to the removed tokens is left, all three types of embeddings, in the case of token cutoff, are converted to 0. Moreover, all embedding types are considered while sampling the feature dimensions to be erased. Intuitively, with augmented data, the learned model is encouraged to be robust enough so that it can produce consistent predictions with a few words removed from the sentence. For feature cutoff, since each input embedding dimension contains certain semantic information, the model is impelled to encapsulate rich and meaningful features w.r.t. each word given that it needs to make the correct predictions with a certain number of features erased entirely.</p><p>Span Cutoff Moreover,  advocated that predicting spans, relative to predicting individual tokens, provides a more challenging objective for self supervision tasks. Thus, we conjecture that easing a contiguous span of text may also lead to harder augmented examples, which can benefit the model during the fine-tuning stage to a larger extent. Therefore, we propose an additional strategy to obtain partial views of the input. First, a preset coefficient α is defined, which indicates the ratio between the length of removed span to that of the original sequence. Then, to obtain a span with the length of l = α × L ( · denotes the floor function), the starting index s for the span is first randomly sampled as: s ∈ {0, 1, ..., L − l}. Afterwards, the embeddings w.r.t. the tokens between the s-th and (s + l − 1)-th positions are all converted to vectors of zeros.</p><p>As illustrated in <ref type="figure">Figure 2</ref>, span cutoff removes a continuous chunk of texts away, and the remaining sentences preserve the same label (e.g. sentiment) as the original example. Since certain semantic information within a sentence has been removed, these augmented samples could better encourage the model to fully leverage different features that may be helpful for predicting the sentiment of the original input (rather than merely relying on a small set of salient ones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Incorporating Augmented Samples</head><p>Suppose there are N cutoff samples constructed from the same original input x (with a label of y), which are denoted by x 1 cutoff , x 2 cutoff , ..., x N cutoff , respectively. Since their semantic meanings are approximately preserved with the cutoff operation, we may incorporate them into the training objective by encouraging the model to make similar predictions across different samples. The training objective can be written as:</p><formula xml:id="formula_1">L = L ce (x, y) + α N i=1 L ce (x i cutoff , y) + βL divergence (x, x 1 cutoff , x 2 cutoff , ..., x N cutoff , y) ,<label>(2)</label></formula><p>where L ce denotes the cross-entropy loss, which Original input: Some actors have so much charisma that you 'd be happy to listen to them reading the phone book .</p><p>Aug sample 1: Some actors have __ __ __ that you 'd be happy to listen to them reading the phone book .</p><p>Aug sample 2: Some actors have so much charisma that you 'd be __ __ __ to them reading the phone book .</p><p>Aug sample 3: Some actors have so much charisma that you 'd be happy to listen to them __ __ __ book . <ref type="figure">Figure 2</ref>: Illustration of the proposed span cutoff method with one specific example (from the SST-2 dataset). In this case, the model is supposed to produce consistent predictions (i.e., sentiments) for all three augmented samples (with various spans of tokens removed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Same Sentiment</head><p>are applied to both original and augmented samples. Furthermore, to explicitly minimize the gap between the predictions w.r.t all the sentences, L divergence is utilized to measure the consensus between all the predictions. KL-divergence has been widely adopted as the divergence metric in previous works <ref type="bibr">(Miyato et al., 2017</ref><ref type="bibr" target="#b30">(Miyato et al., , 2018</ref><ref type="bibr" target="#b10">Clark et al., 2018;</ref><ref type="bibr" target="#b41">Xie et al., 2019)</ref>. However, since there are multiple cutoff samples' predictions, calculating KL divergence in a pair-wise manner will lead to 2 N +1 terms, and thus can be quite computationally intensive. To this end, we propose to leverage the Jensen-Shannon (JS) divergence consistency loss as L divergence . Concretely, the L divergence term can be obtained as follows:</p><formula xml:id="formula_2">p avg = 1 N + 1 N i=0 p(y|x i cutoff ) L divergence = 1 N + 1 N i=0 KL[p(y|x i cutoff )||p avg ]<label>(3)</label></formula><p>To be more specific, the average over all the predictions are first calculated, which is then employed to match with each individual predictions. with such a scheme, the consensus between multiple augmented data along with the original sample is measured in an efficient way. Moreover, it has been shown that the JS divergence loss can endow the model with more stability and consistency across a diverse set of inputs <ref type="bibr" target="#b2">(Bachman et al., 2014;</ref><ref type="bibr" target="#b46">Zheng et al., 2016;</ref><ref type="bibr" target="#b20">Kannan et al., 2018;</ref><ref type="bibr" target="#b16">Hendrycks et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extension to Language Generation</head><p>The various types of cutoff strategies proposed above can be naturally extended to conditional text generation scenario as well. Given a sentence pair (x input , x output ), the cutoff operation can be per-formed on both sentences to synthesize an augmented training pair. Intuitively, the model has access to a restricted view of the input, while being asked to predict part of the output sequence. It has been shown that neural text generation systems are highly sensitive to input noise <ref type="bibr" target="#b22">(Lee et al., 2018)</ref>, and we suppose that adding such augmented examples could improve the model's generalization ability. We evaluate this hypothesis on the machine translation task empirically (see Sec 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Computational Complexity</head><p>We now compare the asymptotic complexity of cutoff to adversarial-based approaches. FreeLB  and SMART <ref type="bibr" target="#b18">(Jiang et al., 2019)</ref> are two representative adversarial training methods applied to NLP domain. Both require additional ascent steps to determine the perturbation directions. Let T denote the number of ascent steps needed for adversarial training, where we have T ≥ 1. The numbers of forward and backward passes for FreeLB and SMART are both 1 + T . On the other hand, cutoff requires no extra backward passes (and thus has a backward pass number of 1). As to the forward pass, given that the size of augmented samples is the same as that of original training instances, the number of forward passes is doubled to 2, which is smaller relative to adversarial training. Overall, the cutoff approach takes less computational overhead compared to standard SGD-based training.</p><p>4 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the effectiveness of the proposed Cutoff approach on both natural language understanding and generation tasks. To facilitate comparisons with other baseline methods, we employ the GLUE benchmark, which consists of a wide vari-  ety of natural language understanding tasks: natural language inference (MNLI, QNLI), textual entailment (RTE), paraphrase identification (MRPC, QQP), sentiment analysis (SST), textual similarity (STS) and linguistic acceptability (CoLA). In terms of the evaluation metrics, accuracy is used for most of the datasets, except for STS and CoLA, where Spearman correlation and Matthews correlation are utilized, respectively. We use Roberta  as the testbed for our data augmentation strategies, including both the base and large models. Besides the natural language understanding benchmark, we further evaluate the effectiveness of proposed cutoff approach on the neural machine translation (NMT) task. Specifically, WMT14 German-to-English and English-to-German datasets are used as the testbeds, where a Transformer-Base model with 6-layer encoder and 6-layer decoder is employed as the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>We finetune the pre-trained models using Adam (Kingma and Ba, 2014), with the learning rate selected from {5e-6, 8e-6, 1e-5, 2e-5} for all parameters. The same learning rate decay scheme as  is employed, with a warmup ratio of 0.06 and a linear decay schedule. We also apply a weight decay of 0.1 during training. The max number of epochs is set as either 5 or 10. The batch size is chosen as 16 for all model variants. The coefficients α (corresponding to the cross-entropy loss on the cutoff samples) and β (associated with the L divergence term) are both selected from {0.1, 0.3, 1, 3} on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We consider several strong baselines to compare with the proposed methods, which can be approximately divided into two categories: i) approaches based on adversarial training, including PGD <ref type="bibr" target="#b27">(Madry et al., 2018)</ref>, FreeAT <ref type="bibr" target="#b32">(Shafahi et al., 2019)</ref>, FreeLB , ALUM <ref type="bibr" target="#b25">(Liu et al., 2020b)</ref>. Notably, these methods are more computationally intensive than Cutoff ; ii) other data augmentation strategies for natural language. Back translation is evaluated and compared with our methods given its wide adoption. Consistency training objective is utilized for back translation in our implementation to ensure fair comparison. Although back translation, similar to Cutoff, serves as a label-preserving transformation on original training instances, it requires additional data (i.e., language pairs) and translation model pre-training. From this perspective, the Cutoff approach is easier to use as a drop-in replacement to standard training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We experimented three different Cutoff variants in terms of the strategy to construct partial views, i.e., token cutoff, feature cutoff and span cutoff.</p><p>They are evaluated and compared on the GLUE benchmark. Detailed analysis and ablation studies regarding the cutoff approach are further conducted, where the advantage of utilizing the JS divergence framework is demonstrated. Besides, we also investigate the effectiveness of the Cutoff approach on German-to-English and English-to-German machine translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GLUE Benchmark Evaluation</head><p>The empirical results of proposed Cutoff strategies (relative to other strong baselines) are presented in <ref type="table" target="#tab_1">Table 1</ref>. It can be observed that the different Cutoff methods consistently outperform ALUM on top of the RoBERTa-base model, while being much more computationally efficient (see Section 3.5). Moreover, span cutoff delivers the strongest numbers on most datasets, which aligns with our assumption that easing a span from the input sequence could lead to more challenging and thus more useful augmented samples.</p><p>As to the case where RoBERTa-large is employed as the baseline, the cutoff data augmentation strategies again consistently exhibit competitive or better performance compared with several adversarial-based approaches. It is worth noting that the Cutoff approaches are related to adversarial-based training in the sense that they both try to produce additional samples with certain perturbations around the original input. However, adversarial-based methods require additional computations to determine the perturbation directions, whereas Cutoff simply remove one slice of information from the input embedding matrix (which could be at the token, feature or span level). This leverages the prior knowledge that the information is organized in a structured manner within the input embeddings, and thus a model with strong generalization ability should be able to make consistent predictions while only partial views are available.</p><p>Moreover, compared with back translation, the Cutoff approaches also demonstrate the same or stronger results on 6 out of 8 NLU tasks considered here. This further verifies the effectiveness of Cutoff as a data augmentation strategy despite its simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Application to Machine Translation</head><p>To investigate the effectiveness of cutoff on text generation problems, we further apply it to the neural machine translation tasks. Specifically, we leverage the 6-layer Transformer Base architecture <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> as the baseline. Cutoff is applied to both the input and output sequences to produce their partial views, which are used as augmented translation pairs for training purpose. To ensure fair comparison, the same beam decoding configuration with <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> is utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>BLEU score Transformer Base <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> 27.3 Admin <ref type="bibr" target="#b24">(Liu et al., 2020a)</ref> 27.9 Transformer Base 1 <ref type="bibr" target="#b34">(So et al., 2019)</ref> 28.2 Evolved Transformer <ref type="bibr" target="#b34">(So et al., 2019)</ref> 28.4 Weighted Transformer <ref type="bibr" target="#b1">(Ahmed et al., 2017)</ref> 28.4 Adversarial Training  28.4 Transformer <ref type="bibr">Base &amp; Cutoff (w/o JS loss)</ref> 28.9 Transformer Base &amp; Cutoff (w/ JS loss) 29.1 <ref type="table">Table 2</ref>: BLEU scores of the proposed cutoff method on the WMT2014 English-to-German machine translation task, compared with adversarial-based baselines. All methods are built on top of 6-layer Transformer Base model <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>.</p><p>In the initial experiments, we found that token cutoff performs the best on machine translation tasks. This may be attributed to the fact that removing spans from both the source and target sentences would result in large information mismatch between the input and output, and thus the resulting pairs may be too challenging. The results on the WMT2014 English-to-German dataset are presented in <ref type="table">Table 2</ref>. Relative to several competitive baseline methods that are based upon 6-layer Transformer Base model, our token cutoff approach exhibits the best BLEU score. More importantly, cutoff outperforms the adversarial training approach introduced by . Concretely, they proposed to inject adversarial perturbations on the output word embeddings (in the softmax layer). Notably, their adversarial training strategy requires updating the model parameters and adversarial perturbation alternately. Thus it is more complicated and computationally expensive than our approach. Besides, it is observed that the JS divergence objective leads to further gains, demonstrating its complementary nature with the standard cross-entropy objective. In addition, on the IWSLT2014 Germanto-English dataset, Cutoff again consistently exhibits significant gains over the Transformer Base model. Along with the JS loss term introduced, our model achieves a BLEU score of 37.6, greatly outperforming the adversarial-based method. As shown in <ref type="table" target="#tab_3">Table 3</ref>, by simply employing Cutoff on top of a 6-layer Transformer model, our approach Model BLEU score Actor-critic <ref type="bibr" target="#b3">(Bahdanau et al., 2016)</ref> 28.5 Transformer Base <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> 34.4 Adversarial training  35.2 Data Diversification <ref type="bibr" target="#b31">(Nguyen et al., 2019)</ref> 37.2 MAT  36.2 Mixed Representations  36.4 MAT+Knee <ref type="bibr" target="#b17">(Iyer et al., 2020)</ref> 36  outperforms all the previous works, further demonstrating its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study 5.3.1 The effect of JS divergence loss</head><p>To investigate the importance of incorporating the Jensen-Shannon (JS) divergence consistency loss, we select different values of β (ranging from 0.0 to 3.0) and explore how the dev set results (on the MNLI dataset) would change accordingly. The coefficient w.r.t. the cross-entropy (CE) loss term is set as 1 for all the ablation settings, and β controls the relative weight of the (JS) divergence consistency loss term. As shown in <ref type="table">Table 4</ref>, leveraging the JS loss term consistently improves the empirical performance (relative to only using the CE loss term), and a β value of 1 gives rise to the best empirical result on the MNLI dataset.  <ref type="table">Table 4</ref>: Ablation study for the span cutoff augmentation strategy with different choices of β, the coefficient w.r.t. the JS divergence loss term (the cross-entropy coefficient α is set as 1). The performance is measured with accuracy on the dev set of the MNLI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">The effect of Cutoff ratios</head><p>One important hyperparameter with the Cutoff approach is the ratio of elements to be removed, where the elements can be tokens, features or a span (depending on the specific Cutoff variant). The ratio can be regarded as the magnitude of perturbations applied to the input sentence. As shown in <ref type="figure">Figure 3</ref>, we applied various cutoff ratios to the different Cutoff variants, including 0.05, 0. Accuracy (%) Feature Cutoff Token Cutoff Span Cutoff <ref type="figure">Figure 3</ref>: The MNLI dev set performance with different Cutoff strategies across different choices of cutoff ratios, which refers to the ratios corresponding to the number of removed tokens, the number of removed feature dimensions or the erased span length, respectively.</p><p>that determining a sweet point of the ratio is critical to the generalization ability of the resulting model. Specifically, token cutoff shows the best performance with a ratio of 0.15, whereas feature cutoff gives rise to the strongest number at a ratio of 0.2. Span cutoff, on the other hand, performs the best with a ratio of 0.1 (the length of the removed span w.r.t the entire sentence). Using a ratio that is too large tends to result in smaller improvements. This may be attributed to the fact that the assumption that the label of the original data is preserved does not hold true (with larger perturbations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduced cutoff, a set of data augmentation strategies that can serve as a drop-in replacement to enrich original training data. The augmented samples are produced stochastically by obtaining partial views of an input sentence. Notably, this process requires no additional computational overhead, and is thus more efficient than adversarial-training-based approaches (which involves additional backward operation to determine the perturbation directions). With extensive experiments on natural language understanding and machine translation tasks, cutoff gave rise to significant gains, and performed on par or stronger than several competitive baselines based upon adversarial training (while taking a fraction of training time). It is worth noting that cutoff, combined with the proposed JS divergence loss, achieved state-ofthe-art result on the IWSLT2014 German-English dataset, with a test BLEU score of 37.6.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Accuracy 88.21 88.27 88.32 88.36 88.12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The empirical results on the dev sets of the GLUE benchmark, with both Roberta-base and Roberta-large models as the corresponding baselines. Notably, the proposed Cutoff strategies deliver competitive numbers while being more computationally efficient.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: BLEU scores of the proposed cutoff method</cell></row><row><cell>on the IWSLT2014 German-to-English machine trans-</cell></row><row><cell>lation task, relative to adversarial-based baseline and</cell></row><row><cell>other state-of-the-art models.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1, 0.15, 0.2, 0.3 and 0.4. It can be observed</figDesc><table><row><cell>88.6</cell></row><row><cell>88.4</cell></row><row><cell>88.2</cell></row><row><cell>88.0</cell></row><row><cell>87.8</cell></row><row><cell>87.6</cell></row><row><cell>0.1 Ratio of Erased Tokens/Dimensions 0.2 0.3 0.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This number is reported in<ref type="bibr" target="#b34">(So et al., 2019)</ref> for the Transformer Base model. The same evaluation settings are used for our cutoff method, i.e., case-sensitive tokenization and the compound splitting are both used.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Sandra Sajeev for her help to edit and refine the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03156</idno>
		<title level="m">Better fine-tuning by reducing representational collapse</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weighted transformer network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1711.02132</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12804</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT&apos; 98</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised learning (chapelle, o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<editor>. et al.</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pac generalization bounds for cotraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Michael L Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10270</idno>
		<title level="m">Multi-branch attentive transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<title level="m">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wide-minima density hypothesis and the exploreexploit learning rate schedule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nipun</forename><surname>Thejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramachandran</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthian</forename><surname>Ramjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sivathanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03977</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03437</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harini Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<title level="m">Adversarial logit pairing. NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Fannjiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<title level="m">Hallucinations in neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>SCL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/2004.08249</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08994</idno>
		<title level="m">Adversarial training for large neural language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<title level="m">Towards deep learning models resistant to adversarial attacks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
		<title level="m">Adversarial training methods for semi-supervised text classification. arXiv: Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Data diversification: A simple strategy for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Kui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Aw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial training for free!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Amin Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3358" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A co-regularization approach to semisupervised learning with multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML workshop on learning with multiple views</title>
		<meeting>ICML workshop on learning with multiple views</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1901.11117</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Ernie: Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving neural language modeling via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Switchout: an efficient data augmentation algorithm for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sequence generation with mixed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on multi-view learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1304.5634</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast and accurate reading comprehension by combining self-attention and convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">You only propagate once: Painless adversarial training using maximal principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00877</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Zhanxing Zhu, and Bin Dong</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4480" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
