<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Speech Emotion Recognition and Ambiguity Resolution Gaurav Sahu</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
							<email>gaurav.sahu@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo Ontario</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Speech Emotion Recognition and Ambiguity Resolution Gaurav Sahu</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-multimodal speech emotion recogni- tion</term>
					<term>machine learning</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself. In this work, we adopt a featureengineering based approach to tackle the task of speech emotion recognition. Formalizing our problem as a multi-class classification problem, we compare the performance of two categories of models. For both, we extract eight hand-crafted features from the audio signal. In the first approach, the extracted features are used to train six traditional machine learning classifiers, whereas the second approach is based on deep learning wherein a baseline feed-forward neural network and an LSTM-based classifier are trained over the same features. In order to resolve ambiguity in communication, we also include features from the text domain. We report accuracy, f-score, precision and recall for the different experiment settings we evaluated our models in. Overall, we show that lighter machine learning based models trained over a few hand-crafted features are able to achieve performance comparable to the current deep learning based stateof-the-art method for emotion recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Communication is the key to human existence and more often than not, we have to deal with ambiguous situations. For instance, the phrase "This is awesome" could be said under either happy or sad settings. Humans are able to resolve ambiguity in most cases because we can efficiently comprehend information from multiple domains (henceforth, referred to as modalities), namely, speech, text and visual. With the rise of deep learning algorithms, there have been multiple attempts to tackle the task of Speech Emotion Recognition (SER) as in <ref type="bibr" target="#b0">[1]</ref> [2] and <ref type="bibr" target="#b2">[3]</ref>. However, this rise has made practitioners rely more on the power of the deep learning models as opposed to using domain knowledge to construct meaningful features and building models that perform well as well as are interpretable. In this work, we explore the implication of hand-crafted features for SER and compare the performance of lighter machine learning models with the heavily data-reliant deep learning models. Furthermore, we also combine features from the textual modality to understand the correlation between different modalities and aid ambiguity resolution. More formally, we pose our task as a multi-class classification problem and employ the two classes of models to solve that. For both the approaches, we first extract handcrafted features from the time domain of the audio signal and train the respective models.</p><p>In the first approach, we train traditional machine learning classifiers, namely, Random Forests, Gradient Boosting, Support Vector Machines, Naive-Bayes and Logistic Regression. In the second approach, we build a Multi-Layer Perceptron and an LSTM <ref type="bibr" target="#b3">[4]</ref> classifier to recognize emotion given a speech signal. The models are evaluated on the IEMOCAP <ref type="bibr" target="#b4">[5]</ref> dataset under different settings, namely, Audio-only, Text-only and Audio + Text 1 .</p><p>The rest of the paper is organized as follows: Section II describes existing methods in the literature for the task of speech emotion recognition; Section III gives an overview of the dataset used in this work and the pre-processing steps applied before feature extraction; Section IV describes the proposed models and implementation details; Results are reported in Section V, followed by the conclusion and future scope of this work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LITERATURE REVIEW</head><p>In this section, we review some of the work that has been done in the field of speech emotion recognition (SER). The task of SER is not new and has been studied for quite some time in literature. A majority of the early approaches ( [6] <ref type="bibr" target="#b6">[7]</ref>) used Hidden Markov Models (HMMs) <ref type="bibr" target="#b7">[8]</ref> for identifying emotion from speech. Recent introduction of deep neural networks to the domain has also significantly improved the state-of-the-art performance. For instance, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b8">[9]</ref> use recurrent autoencoders to solve the task. Recently, methods have also been proposed to efficiently combine features from multiple domains, such as, Tensor Fusion Networks <ref type="bibr" target="#b9">[10]</ref> and Low-Rank Matrix Multiplication <ref type="bibr" target="#b10">[11]</ref>, instead of trivial concatenation.</p><p>This work aims to provide a comparative study between 1) deep learning based models that are trained end-to-end, and 2) lighter machine learning and deep learning based models trained over handcrafted features. We also investigate the information residing in multiple modalities and how their combination affects the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET</head><p>In this work, we use the IEMOCAP <ref type="bibr" target="#b4">[5]</ref> released in 2008 by researchers at the University of Southern California (USC). It contains five recorded sessions of conversations from ten speakers and amounts to nearly 12 hours of audio-visual information along with transcriptions. It is annotated with eight categorical emotion labels, namely, anger, happiness, sadness, neutral, surprise, fear, frustration and excited. It also contains dimensional labels such as values of the activation and valence from 1 to 5; however, they are not used in this work.</p><p>The dataset is already split into multiple utterances for each session and we further split each utterance file to obtain wav files for each sentence. This was done using the start timestamp and end timestamp provided for the transcribed sentences. This results in a total of ∼10K audio files which are then used to extract features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>This section describes the data pre-processing steps followed by a detailed description of the features extracted and the two models applied to the classification problem.</p><p>A. Data Pre-processing a) Audio: A preliminary frequency analysis revealed that the dataset is not balanced. The emotions "fear" and "surprise" were under-represented and use upsampling techniques to alleviate the issue. We then merged examples from "happy" and "excited" classes as "happy" was under-represented and the two emotions closely resemble each other. In addition to that, we discard examples classified as "others"; they corresponded to examples that were labeled ambiguous even for a human. Applying the aforementioned operations resulted in 7837 examples in total. Final sample distribution for each of the emotions is shown in <ref type="table" target="#tab_0">Table I</ref>. b) Text: The available transcriptions were first normalized to lowercase and any special symbols were removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>We now describe the handcrafted features used to train both, the ML-and the DL-based models.</p><p>1) Audio Features: a) Pitch: Pitch is important because waveforms produced by our vocal cords change depending on our emotion. Many algorithms for estimating the pitch signal exist. We use the most common method based on autocorrelation of center-clipped frames <ref type="bibr" target="#b11">[12]</ref>. Formally, the input signal y[n] is center-clipped to give a resultant signal, y clipped [n]:</p><formula xml:id="formula_0">y clipped [n] =      y[n] − C l , if y[n] ≥ C l 0, if |y[n]| &lt; C l y[n] + C l , if y[n] ≤ C l (1)</formula><p>Typically, C l is nearly half the mean of the input signal and [·] denotes the discrete nature of the input signal. Now, autocorrelation is calculated for the obtained signal y clipped , which is further normalized and the peak values associated with the pitch of the given input y[n]. It was found that centerclipping the input signal resulted in more distinct autocorrelation peaks. b) Harmonics: In the emotional state of anger or for stressed speech, there are additional excitation signals other than pitch ( <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>). This additional excitation is apparent in the spectrum as harmonics (see <ref type="figure" target="#fig_0">Figure 1</ref>) and cross-harmonics. We calculate harmonics using a median-based filter as described in <ref type="bibr" target="#b14">[15]</ref>. First, the median filter is created for a given window size l, given by:</p><formula xml:id="formula_1">y[n] = median(x[n−k : n+k]|k = (l−1)/2) (2)</formula><p>where l is odd. For cases when l is even, the median is obtained as the mean of two values in the middle of the sorted list. This filter is then applied to S h , the h−th frequency slice of a given spectrogram S, to get harmonic-enhanced spectrogram frequency slice H h as:</p><formula xml:id="formula_2">H i = M (S h , l harm )<label>(3)</label></formula><p>Here M is the median filter, i is the i−th time step and l harm is the length of the harmonic filter. c) Speech Energy: Since the energy of a speech signal can be related to its loudness, we can use it to detect certain emotions. <ref type="figure">Figure 2</ref> shows the difference in energy levels of an "angry" signal v/s that of a "sad" signal. We use standard Root Mean Square Energy (RMSE) to represent speech energy using the equation:</p><formula xml:id="formula_3">E = 1 n n i=1 y[i] 2<label>(4)</label></formula><p>RMSE is calculated frame by frame and we take both, the average and standard deviation as features. d) Pause: We use this feature to represent the "silent" portion in the audio signal. This quantity is directly related to our emotions; for instance, we tend to speak very fast when excited (say, angry or happy, resulting in a low Pause value). The feature value is given by:</p><formula xml:id="formula_4">P ause = P r(y[n] &lt; t)<label>(5)</label></formula><p>where t represents a carefully-chosen threshold of ≈ 0.4 * E, E being the RMSE. e) Central moments: Finally, we use the mean and standard deviation of the amplitude of the signal to incorporate a "summarized" information of the input. 2) Text Features:: a) Term Frequency-Inverse Document Frequency (TFIDF): TFIDF is a numerical statistic that shows the correlation between a word and a document in a collection or corpus. It consists of two parts:</p><p>• Term Frequency: It denotes how many times a word/token occurs in a document. The simplest choice is to use raw count of a token in a document (sentences, in our case).</p><p>• Inverse Document Frequency: This term is introduced to lessen the bias due to frequently occurring words in language such "the", "a" and "an". Usually, idf for a term t and a document D is defined as:</p><formula xml:id="formula_5">idf (t, D) = log N |d ∈ D : t ∈ d|<label>(6)</label></formula><p>The denominator shows the frequency of documents containing the term t and N is the total number of documents.</p><p>Finally, TFIDF value for a term is calculated by taking the product of TF and IDF values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Machine Learning Models:</head><p>This section describes the various ML-based classifiers considered in this work, namely, Random Forests, Gradient Boosting, Support Vector Machines, Naive-Bayes, and Logistic Regression a) Random Forest (RF): Random forests are ensemble learners that operate by constructing multiple decision trees at training time and outputting the class that is the mode of the classes (classification) of the individual trees. It has two base working principles:</p><p>• Each decision tree predicts using a random subset of features <ref type="bibr" target="#b15">[16]</ref> • Each decision tree is trained with only a subset of training samples. This is known as bootstrap aggregating <ref type="bibr" target="#b16">[17]</ref> Finally, a majority vote of all the decision trees is taken to predict the class of a given input. b) Gradient Boosting (XGB): XGB refers to eXtreme Gradient Boosting. It is an implementation of boosting that supports training the model in a fast and parallelized way. Boosting is another ensemble classifier combining a number of weak learners, typically decision trees. They are trained in a sequential manner, unlike RFs, using forward stagewise additive modeling. During the early iterations, the decision trees learned are simple. As training progresses, the classifier becomes more powerful because it is made to focus on the instances where the previous learners made errors. At the end of training, the final prediction is a weighted linear combination of the output from the individual learners <ref type="bibr" target="#b17">[18]</ref>. c) Support Vector Machines (SVMs): SVMs are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. An SVM training algorithm essentially builds a non-probabilistic binary linear classifier (although methods such as Platt scaling <ref type="bibr" target="#b18">[19]</ref> exist to use SVM in a probabilistic classification setting). It represents each training example as a point in space, mapped such that the examples of the separate categories are divided by a clear gap that is as wide as possible (this is usually achieved by minimizing the hinge loss). New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. SVMs were originally introduced to perform linear classification; however, they can efficiently perform a non-linear classification using the kernel trick <ref type="bibr" target="#b19">[20]</ref>, implicitly mapping their inputs into high-dimensional feature spaces.</p><p>d) Multinomial Naive Bayes (MNB): Naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Under multinomial settings, the feature vectors represent the frequencies with which certain events have been generated by a multinomial (p 1 , . . . , p n ) where p i is the probability that event i occurs. MNB is very popular for document classification task in text <ref type="bibr" target="#b20">[21]</ref> which too essentially is a multi-class classification problem. e) Logistic Regression (LR): LR is typically used for binary classification problems <ref type="bibr" target="#b21">[22]</ref>, that is, when we have only two labels. In this work, LR is implemented in a one-vs-rest manner; six classifiers have been trained for each class and finally, we consider the class that is predicted with the highest probability.</p><p>Having trained the above classifiers, we take ensemble of the best performing classifiers and use it for comparison with the current state-of-the-art for emotion recognition on the IEMOCAP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deep Learning Models</head><p>In this section, we describe the deep learning models used. Typically, Deep Neural Networks (DNNs) are trained in an end-to-end fashion and they are expected to "figure out" features completely on their own. However, training such a model can take a lot of time as well as computational resources. In order to minimize the computational overhead, we directly feed the handcrafted features as input to these models and compare their performance with the traditional end-to-end trained counterparts. In this work, we implement two types of models: a) Multi-Layer Perceptron (MLP): MLP belongs to a class of feed-forward neural network. It consists of at least three nodes: an input, a hidden and an output layer. All the nodes are interleaved with a non-linear activation function to stabilize the network during training time. Their expressive power increases as we increase the number of hidden layers upto a certain extent. Their non-linear nature allows them to distinguish data that is not linearly separable.</p><p>b) Long Short Term Memory (LSTM): LSTMs <ref type="bibr" target="#b3">[4]</ref> were introduced for long-range context capturing in sequences. Unlike MLP, it has feedback connections that allow it to decide what information is important and what is not. It consists of a gating mechanism and there are three types of gates: input, forget and output. Their equations are mentioned below:</p><formula xml:id="formula_6">f t = σ g (W f x t + U f h t−1 + b f ) (7) i t = σ g (W i x t + U i h t−1 + b i ) (8) o t = σ g (W o x t + U o g h t − 1 + b o ) (9) c t = f · c t−1 + i t · σ c (W c x t + U c h t−1 + b c ) (10) h t = o t · σ h (c t )<label>(11)</label></formula><p>where initial values are c 0 = 0 and h 0 = 0 and · denotes the element-wise product, t denotes the time step (each element in a sequence belongs to one time step), x t refers to the input vector to the LSTM unit, f t is the forget gate's activation vector, i t refers to the input gate's activation vector, o t refers to the output gate's activation vector, h t is the hidden state vector (which is typically used to map a vector from the feature space to a lowerdimensional latent space,) c t is the cell state vector and W, U and b are weight and bias matrices which need to be learned during training. From <ref type="figure" target="#fig_1">figure 3</ref>, we see that an LSTM cell is able to keep track of hidden states at all time steps through the feedback mechanism.  <ref type="figure">Figure 4</ref> shows the network implemented in this work. We feed the feature vectors as input to the network and finally pass the output of the LSTM network through a softmax layer to get probability scores for each of the six emotion classes. Since we are using feature vectors as input, we do not need another decoder network to transform it back from hidden to output space thereby reducing network size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments</head><p>Here, we describe the three different settings we conducted our experiments in:</p><p>• Audio-only: In this setting, we train all the classifiers using only the audio feature vectors described earlier. • Text-only: In this setting, we train all the classifiers using only the text feature vectors (TFIDF vectors) • Audio+Text: In this setting, we fuse the feature vectors from the two modalities. There have been some methods proposed to fuse vectors efficiently from multiple modalities but we simply concatenate the feature vectors from audio and text to obtain the combined feature vectors. Through this experiment, we would be able to infer how much information is contained in each of the modalities and how does fusion influence the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details</head><p>In this section, we describe the implementation details adopted in this work.</p><p>• We use librosa <ref type="bibr" target="#b22">[23]</ref>, a Python library, to process the audio files and extract features from them. • We use scikit-learn and xgboost <ref type="bibr" target="#b23">[24]</ref> [25], the machine learning libraries for Python, to implement all the ML classifiers (RF, XGB, SVM, MNB, and LR) and the MLP. • We use PyTorch <ref type="bibr" target="#b25">[26]</ref> to implement the LSTM classifiers described earlier. • In order to regularize the hidden space of the LSTM classifiers, we use a shut-off mechanism, called dropout <ref type="bibr" target="#b26">[27]</ref>, where a fraction of neurons are not used for final prediction. This is shown to increase the robustness of the network and prevent overfitting. We randomly split our dataset into a train (80%) and test (20%) set. The same split is used for all the experiments to ensure a fair comparison. The LSTM classifiers were trained on an NVIDIA Titan X GPU for faster processing. We stop the training when we do not see any improvement in validation performance for &gt;10 epochs. Here, one epoch refers to one iteration over all the training samples. Different batch sizes were used for different models.</p><p>Hyperparameters for the all the models under the three experiment settings could be found in the released repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Evaluation Metrics:</head><p>In this section, we first describe the various evaluation metrics used and report results for the three experiment settings. a) Accuracy: This refers to the percentage of test samples that are classified correctly. b) Precision: This measure tells us out of all predictions, how many are actually present in the ground truth (a.k.a. labels). It is calculated using the formula:</p><formula xml:id="formula_7">P recision = tp tp + f p<label>(12)</label></formula><p>c) Recall: This measure tells us how many correct labels are present in the predicted output. It is calculated using the formula:</p><formula xml:id="formula_8">P recision = tp tp + f n<label>(13)</label></formula><p>Here, tp, f p, and f n stand for true positive, false positive and false negative respectively. We can compute these values from the confusion matrix.</p><p>d) F-score: It is defined as the harmonic mean of precision and recall. This measure was included as accuracy is not a complete measure of a model's predictive power but F-score is since it is more normalized.</p><p>We compare our best performing models with the current state-of-the-art as mentioned in <ref type="bibr" target="#b1">[2]</ref>. They employ three types of recurrent encoders, namely, ARE, TRE and MDRE denoting Audio-, Text-and Multimodal Dual-Recurrent Encoders respectively. It is important to mention that <ref type="bibr" target="#b1">[2]</ref> only considers four emotions for classification, namely, angry, happy, sad and neutral as opposed to six in our case. In order to present a fair comparison of our method with theirs, we also run the experiments for the four classes (models with code 4-class in <ref type="figure" target="#fig_2">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In this section, we discuss the performance of models described in Section IV.</p><p>From <ref type="figure" target="#fig_2">Figure 5</ref>, we can see that our simpler and lighter ML models either outperform or are comparable to the much heavier current state-of-theart on this dataset. A more detailed analysis follows: a) Audio-only results: Results are especially interesting for this setting. Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight-dimensional features achieves very low accuracy as compared to the end-to-end trained ARE. However, neither of them are able to beat the lighter E1 model (Ensemble of RF, XGB and MLP) which was trained on the eightdimensional audio feature vectors. A look at the confusion matrix <ref type="figure">(Fig. 6a</ref>) reveals that detecting "neutral" or distinguishing between "angry", "happy" and "sad" is the most difficult for the model. b) Text-only results: We observe that the performance of all the models for this setting is similar. This could be attributed to the richness of TFIDF vectors known to capture word-sentence correlation. We see from the confusion matrix ( <ref type="figure">Fig. 6b)</ref> that our text-based models are able to distinguish the six emotions fairly well along with the end-to-end trained TRE. We observe that "sad" is the toughest for textual features to identify very clearly. c) Audio+Text results: We see that combining audio and text features gives us a boost of ∼14% for all the metrics. This is clear evidence of the strong correlation between text and speech features. Also, this is the only case when the recurrent encoders seem to perform slightly better in terms of accuracy but at the cost of precision. The lower performance of E1 maybe be attributed to the trivial fusion method (concatenation) we use as simple concatenation for an ML model would still contain a lot of modality-specific connections instead of the desired inter-modal connections. The promising result here is that combining features from both the modalities indeed helped to resolve the ambiguity observed for modality-specific models as shown in <ref type="figure">Fig. 6c</ref>. We can say that the textual features helped in correct classification of "angry" and "happy" classes whereas the audio features enabled the model to detect "sad" better.</p><p>Overall, we can conclude that our simple ML methods are very robust to have achieved comparable performance even though they are modeled to predict six-classes as opposed to four in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Most Important Features:</head><p>In this section, we investigate which features contribute the most during prediction in this classification task. We chose the XGB model for this study and rank the eight audio features. We see that Harmonic, which is directly related to the excitation in signals, contributes the most. It is interesting to see that "silence" attributing to Pause, is almost as significant as standard deviation of the autocorrelated signal (related to pitch). The low contribution of central moments is expected as a signal is very diverse and an global/coarse feature would be unable to identify the nuances present in it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this work, we tackle the task of speech emotion recognition and study the contribution of different modalities towards ambiguity resolution on the IEMOCAP dataset. We compare, both, ML-and DL-based models and show that even lighter and more interpretable ML models can achieve performance close to DL-based models. We show that ensembling multiple ML models also lead to some improvement in the performance. We only extract a handful of time-domain features from audio signals. The audio feature-space could be made even richer if we could include some frequency-domain features too such as Mel-Frequency Cepstral Coefficients (MFCC) <ref type="bibr" target="#b27">[28]</ref>, Spectral Roll-off and additional timedomain features such as Zero Crossing Rate (ZCR) <ref type="bibr" target="#b28">[29]</ref>. Also, better fusion methods such as TFN <ref type="bibr" target="#b9">[10]</ref> and LMF <ref type="bibr" target="#b10">[11]</ref> could be employed for combining speech and text vectors more effectively. It would also be interesting to see the scaling in the performance of ML models v/s DL models if include more data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Harmonics of angry (red) and sad (blue) audio signals Fig. 2: RMSE plots of angry (red) and sad (blue) audio signals</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of an LSTM cell Fig. 4: LSTM classifier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Audio-only setting (b) Text-only setting (c) Audio+Text setting Performance of different models; E1: Ensemble (RF + XGB + MLP); E2: Ensemble (RF + XGB + MLP + MNB + LR) (a) E1, Audio-only setting (b) E2, Text-only setting (c) E2, Audio+Text setting Fig. 6: Confusion Matrices of the our ensemble models; E1: Ensemble (RF + XGB + MLP); E2: Ensemble (RF + XGB + MLP + MNB + LR)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Most important Audio features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Number of examples for each emotion</figDesc><table><row><cell>Class</cell><cell>Count</cell></row><row><cell>Angry</cell><cell>860</cell></row><row><cell>Happy</cell><cell>1309</cell></row><row><cell>Sad</cell><cell>2327</cell></row><row><cell>Fear</cell><cell>1007</cell></row><row><cell>Surprise</cell><cell>949</cell></row><row><cell>Neutral</cell><cell>1385</cell></row><row><cell>Total</cell><cell>7837</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code for all the experiments is available at http://tinyurl.com/y55dlc3m</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="801" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal speech emotion recognition using audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="112" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using deep neural network and extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">335</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nogueiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Mariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hidden markov model-based speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2003 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;03).. II-1</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ieee assp magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for robust feature generation in audiovisual emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3687" to="3691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00064</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">New methods of pitch extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sondhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio and electroacoustics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="266" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Evidence for nonlinear sound production mechanisms in the vocal tract,&quot; in Speech production and speech modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Teager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="241" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear feature based classification of speech under stress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="216" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Harmonic/percussive separation using median filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint induction of shape features and tree classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1300" to="1305" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in large margin classifiers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multinomial naive bayes for text categorization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Kibriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="488" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Logistic regression in rare events data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="163" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-10" />
			<biblScope unit="page" from="2825" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scalable, portable and distributed gradient boosting (gbdt, gbrt or gbm) library, for python, r, java, scala, c++ and more. runs on single machine, hadoop, spark, flink and dataflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Facebook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the use of zero-crossing rate for an application of classification of percussive sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delerue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COST G-6 conference on Digital Audio Effects (DAFX-00)</title>
		<meeting>the COST G-6 conference on Digital Audio Effects (DAFX-00)<address><addrLine>Verona, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
