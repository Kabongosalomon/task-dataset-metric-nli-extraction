<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Gat</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many recent datasets contain a variety of different data modalities, for instance, image, question, and answer data in visual question answering (VQA). When training deep net classifiers on those multi-modal datasets, the modalities get exploited at different scales, i.e., some modalities can more easily contribute to the classification results than others. This is suboptimal because the classifier is inherently biased towards a subset of the modalities. To alleviate this shortcoming, we propose a novel regularization term based on the functional entropy. Intuitively, this term encourages to balance the contribution of each modality to the classification result. However, regularization with the functional entropy is challenging. To address this, we develop a method based on the log-Sobolev inequality, which bounds the functional entropy with the functional-Fisher-information. Intuitively, this maximizes the amount of information that the modalities contribute. On the two challenging multi-modal datasets VQA-CPv2 and SocialIQ, we obtain state-of-the-art results while more uniformly exploiting the modalities. In addition, we demonstrate the efficacy of our method on Colored MNIST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal data is ubiquitous and commonly used in many real-world applications. For instance, discriminative visual question answering systems take into account the question, the image and a variety of answers. In general, we treat data as multi-modal if it can be partitioned into semantic features, e.g., color and shape can be treated as multi-modal data.</p><p>Training of discriminative classifiers on multi-modal datasets like discriminative visual question answering almost always follows the classical machine learning paradigm: use a common loss function like cross-entropy and employ a standard 2 -norm regularizer (a.k.a. weight decay). The regularizer favors 'simple' classifiers over more complex ones. These classical regularizers are suitable in traditional machine learning settings that predominantly use a single data modality. Unfortunately, because they favor 'simple' models, their use is detrimental when learning from multi-modal data. Simplicity encourages use of information from a single modality, which often ends up biasing the learner. For instance, visual question answering models end up being driven by a language prior rather than visual understanding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. E.g., answering 'how many...?' questions with '2' regardless of the question. Another popular example consists of colored images whose label is correlated with their color modality and their shape modality. In these cases, standard learners often focus on the 'simple' color modality and largely ignore the shape modality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>To address this issue, we develop a novel regularization term based on the functional entropy. Intuitively, this term encourages to balance the contribution of each modality to classification. To address the computational challenges of computing the functional entropy we develop a method based on the log-Sobolev inequality which bounds the functional entropy with the functional Fisher information.  <ref type="figure">Figure 1</ref>: We illustrate our approach. In the visual question answering task, we are given a question about an image. Thus, we can partition our input into two modalities: a textual modality, and a visual modality. We measure the modalities' functional Fisher information by evaluating the sensitivity of the prediction by perturbing each modality. We maximize the functional Fisher information by incorporating it into our loss as a regularization term. Our results show that our regularization permits higher utilization of the visual modality.</p><p>We illustrate the efficacy of the proposed approach on the three challenging multi-modal datasets Colored MNIST, VQA-CPv2, and SocialIQ. We find that our regularization maximizes the utilization of essential information. We verify this empirically on the synthetic dataset Colored MNIST. We also evaluate on popular benchmarks, finding that our method permits a state-of-the-art performance on two datasets: SocialIQ (68.53% vs. 64.82%) and VQA-CPv2 (54.55% vs. 52.05%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-modal datasets. Over the years, the amount and variety of data that has been used across tasks has grown significantly. Unsurprisingly, present-day tasks are increasingly sophisticated and combine multiple data modalities like vision, text, and audio. In particular, in the past few years, many large-scale multi-modal datasets have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Subsequently, multiple works developed strong models to address these datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. However, recent work also suggests that many of these advanced models predict by leveraging one of the modalities more than the others, e.g., utilizing question type to determine the answer in VQA problems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. This property is undesirable since multi-modal tasks consider all data essential to solve the challenge without overfitting to the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias in datasets.</head><p>Recently, datasets were proposed to study whether a model can generalize and solve the task or whether it uses a single modalities' features. Usually, this evaluation is performed by partitioning data into train and test sets using different distributions. For example, VQA-CP <ref type="bibr" target="#b0">[1]</ref> is a reshuffle of the VQA <ref type="bibr" target="#b2">[3]</ref> dataset ensuring that question-type distributions differ between train and test splits. Another well-known dataset is Colored MNIST <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30]</ref>. In this dataset, each digit class is colored differently in the train set, while samples in the test set remain gray-scale. Different approaches were proposed to deal with such problems: Arjovsky et al. <ref type="bibr" target="#b29">[30]</ref> propose to improve generalization by ensuring that the optimal classifier equals all training distributions. Wang et al. <ref type="bibr" target="#b30">[31]</ref> suggest to regularize the overfitting behavior to different modalities. Methods like REPAIR <ref type="bibr" target="#b4">[5]</ref> prevent a model from exploiting dataset biases by re-sampling the training data. Kim et al. <ref type="bibr" target="#b5">[6]</ref> use an adversarial approach to learn unbiased feature representations. Clark et al. <ref type="bibr" target="#b31">[32]</ref> and Cadene et al. <ref type="bibr" target="#b32">[33]</ref> suggest methods to overcome language priors using a bias-only model in VQA tasks.</p><p>Entropy and information in deep nets. Entropy plays a pivotal role in machine learning and has been extensively used in losses and for regularization <ref type="bibr" target="#b33">[34]</ref>. However, its use is confined to probability distributions while we use functional entropy, which has a different form and is defined for any nonnegative function. More broadly, other components of information theory have been studied in deep nets, for example, the information bottleneck criteria <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. Other works use information theory to overcome generalization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. For instance, Krishna et al. <ref type="bibr" target="#b36">[37]</ref> propose to maximize the mutual information of the modalities by regularizing differences between modality representations. Fisher information is also used in various machine learning and deep learning settings, e.g., monitoring of the learning process <ref type="bibr" target="#b38">[39]</ref>. In contrast, our work considers the functional Fisher information of a non-negative function that represents a multi-modal learner, while Fisher information is defined over probability density functions. Also, we use the log-Sobolev inequality between the functional entropy and the functional Fisher information, which does not hold for entropy and Fisher information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Discriminative learning constructs mapping between data-instance x ∈ X and labels y ∈ Y given training data S. We are particularly interested in multi-modal data, where each data-instance x is composed of multiple modalities. For example, a monochrome image x is composed of two data modalities x = (x c , x s ) where x c ∈ R 3 is the monochromatic color tone and x s ∈ R d×d is the d × d intensity map of the image capturing the shape. Similarly, in discriminative visual question answering, x is composed of a visual modality, a question modality and an answer modality, i.e.,</p><formula xml:id="formula_0">x = (x v , x q , x a ) with x v ∈ R dv , x q ∈ R dq and x a ∈ R da respectively.</formula><p>Generally, x may have n modalities, i.e., x = (x 1 , . . . , x n ), each residing in Euclidean space, i.e., x i ∈ R di .</p><p>Discriminative learning searches for the parameters w of a function which assign a score to each label y given data x. In this work we focus on the softmax function p w (ŷ|x). Its goodness of fit is measured by a loss function, often the cross-entropy loss CE(1[· = y], p w (·|x)) = − ŷ 1[ŷ = y] log p w (ŷ|x), where 1 refers to the indicator function. More generally, the cross-entropy loss between two</p><formula xml:id="formula_1">distributions p w (ŷ|x), q(ŷ) is CE(q, p w ) = − ŷ q(ŷ) log p w (ŷ|x).<label>(1)</label></formula><p>Beyond the loss, a typical learning process employs a regularization term which encourages use of the 'simplest' function. Various regularization terms that favor 'simple' functions pose a considerable difficulty for multi-modal problems: deep learners easily find simple functions that ignore one of the modalities. For example, a simple discriminator for Colored MNIST, which consists of monochromatic images whose colors correlate with their labels, focuses almost exclusively on the color vector to predict the label rather than also assessing the shape of the image. Formally, if the monochromatic images are represented by their color and shape modalities x = (x c , x s ) then the simplest discriminator will only consider the 3-dimensional color x c . In this setting, the learned function p w (ŷ|x) avoids all important information within the shape modality x s .</p><p>In the following we describe the notion of functional entropy in Section 3.1. In Section 3.2 we present the log-Sobolev inequality, which bounds the functional entropy of a non-negative function by the functional Fisher information. We conclude with the notion of tensorization, which decomposes these components according to their multi-modal spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Functional entropy</head><p>In this work we consider the functional entropy that is encapsulated in multi-modal problems. Functional entropies are defined over a continuous random variable, i.e., a function f (z) over the Euclidean space z ∈ R d with a probability measure µ. Here and throughout we use z to refer to a stochastic variable, which we integrate over. The functional entropy of a non-negative function</p><formula xml:id="formula_2">f (z) ≥ 0 is Ent µ (f ) R d f (z) log f (z)dµ(z) − R d f (z)dµ(z) log R d f (z)dµ(z) .<label>(2)</label></formula><p>The functional entropy is non-negative, namely Ent µ (f ) ≥ 0 and equals zero only if f (z) is a constant. This is in contrast to differential entropy of a continuous random variable with probability density function q(z): h(q) = − R d q(z) log q(z)dz, which is defined for q(z) ≥ 0 with R d q(z)dz = 1 and may be negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Functional Fisher information</head><p>Unfortunately, the functional entropy is hard to estimate empirically, since it involves the term log( R d f (z)dµ(z)). Since the integral can only be estimated by sampling, the logarithm of its estimate is hard to compute in practice. Instead of estimating the functional entropy directly, we use the log-Sobolev inequality for Gaussian measures (cf. <ref type="bibr" target="#b39">[40]</ref>, Section 5.1.1). This permits to bound the functional entropy with the functional Fisher information. Specifically, for any non-negative function f (z) ≥ 0 we obtain</p><formula xml:id="formula_3">Ent µ (f ) ≤ 1 2 R d ∇f (z) 2 f (z) dµ(z).<label>(3)</label></formula><p>Hereby, ∇f (z) is the 2 norm of the gradient of f . The functional Fisher information is nonnegative, since it is defined for non-negative functions. It is a natural extension of the Fisher information, which is defined for probability density functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tensorization and multi-modal data</head><p>Functional entropy naturally fits into multi-modal settings that correspond to product probability spaces. For example, when considering discriminative visual-question answering, a data point x = (x v , x q , x a ) resides in the Euclidean product space of the visual modality x v , the question modality x q and the answer modality x a . This product space property is called tensorization and informally relates the functional entropy of each modality to the overall functional entropy of the system. Generally, consider the product spaceẑ = (ẑ 1 , . . . ,ẑ n ), where each modality resides in the</p><formula xml:id="formula_4">d i -dimensional Euclidean spaceẑ i ∈ R di . Consider the product measure µ = µ 1 ⊗ · · · ⊗ µ n and let f i (z i ) = f (ẑ 1 , . . . ,ẑ i−1 , z i ,ẑ i+1 , . . . ,ẑ n ).</formula><p>(4) The tensorization of the functional entropy amounts to</p><formula xml:id="formula_5">Ent µ (f ) ≤ n i=1 R d Ent µi (f i )dµ(ẑ),<label>(5)</label></formula><p>Here the dimension d is the dimension ofẑ = (ẑ 1 , . . . ,ẑ n ), namelyẑ ∈ R d and d = n i=1 d i . Tensorization is well-suited for multi-modal settings, as it provides the means to bound the overall functional entropy of the system using the functional entropies of its modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Regularization by Maximizing Functional Entropies</head><p>Functional entropy requires a probability measure. In the following we differentiate between multimodal training points x = (x 1 , . . . , x n ) and general multi-modal points in the probability measure space, which we denote by z = (z 1 , . . . , z n ). We use the training points x = (x 1 , . . . , x n ) to determine the measure and we denote by z = (z 1 , . . . , z n ) the variable of the integrands. In our work we consider a Gaussian product. Given a training point x ∈ S that resides in the multi-modal space x = (x 1 , . . . , x n ) we define the measure µ x i for the i-th modality to be the Gaussian distribution with mean x i and variance σ 2 xi , where x i is the i-th modality of the training point x and σ 2 xi is the variance of the coordinate of x i :</p><formula xml:id="formula_6">µ x i N (x i , σ 2 xi ).<label>(6)</label></formula><p>The measure µ x is the product measure over the different modalities µ x µ x 1 ⊗ · · · ⊗ µ x n . For example, given a monochromatic image x = (x c , x s ) in the training data, the distribution employed by the functional entropy in Eq.</p><formula xml:id="formula_7">(2) is µ x = N (x c , σ 2 xc ) ⊗ N (x s , σ 2 xs )</formula><p>. For each training data point x ∈ S, we define the functional entropy over the deep net softmax function p w (·|x) as f x (z 1 , . . . , z n ) CE(p w (·|z), p w (·|x)).</p><p>(7) This function measures the sensitivity of the softmax prediction to Gaussian perturbations z of the input, since the random perturbation z is sampled from a Gaussian with an expected value x, as described in Eq. (6).</p><p>The cross-entropy function is a non-negative function, therefore, it is natural to apply the log-Sobolev inequality for Gaussian measures to bound the functional entropy using the functional Fisher information, in Eq. <ref type="formula" target="#formula_3">(3)</ref>:</p><formula xml:id="formula_8">Ent µ x (CE(p w (·|z), p w (·|x))) ≤ R d ∇ z CE(p w (·|z), p w (·|x)) 2 CE(p w (·|z), p w (·|x)) dµ x (z).<label>(8)</label></formula><p>We use the functional Fisher information bound in Eq. (3) to regularize the training process, in order to implicitly encourage to maximize the information of each modality, while minimizing the training loss. In order to account for both the loss minimization and the information maximization, we take the inverse information. Given multi-modal training data S, our learning objective is</p><formula xml:id="formula_9">(x,y)∈S CE(1[· = y], p w (·|x)) + λ (x,y)∈S R d ∇ z x CE(p w (·|z x ), p w (·|x)) 2 CE(p w (·|z x ), p w (·|x)) dµ x (z) −1 .<label>(9)</label></formula><p>The hyperparameter λ balances between the training loss and the inverse information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tensorization</head><p>The tensorization argument in Section 3.3 determines a bound on the functional entropy by its functional entropy over each modality. The tensorization argument is favorable since it permits to consider the functional entropy of each modality separately in the integral of Eq. (5), given a point z = (ẑ 1 , . . . ,ẑ n ). The tensorization also permits to efficiently approximate the functional entropy, given a training point x:</p><formula xml:id="formula_10">Let z x i (x 1 , . . . , x i−1 , z i , x i+1 , . . . , x n ) and setf x i (z i ) f x (z x i )</formula><p>. Given this definition, the tensorization in Eq. (5) reduces to</p><formula xml:id="formula_11">Ent µ x (f x ) ≤ n i=1 R d Ent µ x i (f x i )dµ(ẑ) ≈ n i=1 Ent µ x i (f x i ).<label>(10)</label></formula><p>We combine this approximation with the log-Sobolev inequality to measure the amount of the functional Fisher information added by each modality, for a given multi-modal training point x = (x 1 , . . . , x n ):</p><formula xml:id="formula_12">n i=1 Ent µ x i (CE(p w (·|z x i ), p w (·|x))) ≤ n i=1 R d i ∇ z x i CE(p w (·|z x i ), p w (·|x)) 2 CE(p w (·|z x i ), p w (·|x)) dµ x i (z i ).<label>(11)</label></formula><p>We recall that z x i (x 1 , . . . , x i−1 , z i , x i+1 , . . . , x n ) and z i ∈ R di is the variable that is being integrated while all other modalities remain fixed to the training point input modality.</p><p>Similarly to Eq. (9), we may use the tensorized functional Fisher information bound in Eq. (11) to regularize the training process. Given multi-modal training data S, our tensorized learning objective is</p><formula xml:id="formula_13">(x,y)∈S CE(1[· = y], p w (·|x)) + λ (x,y)∈S n i=1 R d i ∇ z x i CE(p w (·|z x i ), p w (·|x)) 2 CE(p w (·|z x i ), p w (·|x)) dµ x i (z i ) −1 .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Connection Between Functional Entropy and Variance</head><p>Rothaus <ref type="bibr" target="#b40">[41]</ref> has shown a connection between the functional entropy of a non-negative function and its variance.</p><formula xml:id="formula_14">Var µ (f ) R d f 2 (z)dµ(z) − R d f (z)dµ(z) 2 .<label>(13)</label></formula><p>Particularly, when the values of the non-negative function f (z) are small, one can expand the Taylor series of 1 + f (z) to show that</p><formula xml:id="formula_15">Ent µ (1 + f ) = Var µ (f ) + o( f 2 ∞ ),<label>(14)</label></formula><p>where the residual function o(t) is non-negative and approaches zero faster than t approaches zero, i.e., lim t→0 o(t) t = 0. Interestingly, a similar bound to the log-Sobolev inequality (Eq. (3)) exists for the variance of continuous random variables f (z), which is widely known as the Poincaré inequality:</p><formula xml:id="formula_16">Var µ (f ) ≤ R d ∇f (z) 2 dµ(z).<label>(15)</label></formula><p>The relation between the functional entropy and the variance, expressed in Eq. <ref type="bibr" target="#b13">(14)</ref>, suggests that these bounds should behave similarly in practice. To fit the variance into multi-modal settings we In ColoredMNIST, we observe that training a model with our regularization, the prediction is based on both the shape and the color. Unlike, a model trained without our regularization which makes predictions based on the color only.</p><p>need to show tensorization (as in Sec. 3.3). In the variance case, this property is called the Efron-Stein theorem (cf. <ref type="bibr" target="#b41">[42]</ref>, Proposition 2.2),</p><formula xml:id="formula_17">Var µ (f ) ≤ n i=1 R d Var µi (f i )dµ(ẑ).<label>(16)</label></formula><p>Next, we present a similar regularization term to the one described in Sec. 4. This time we use variance and Poincaré inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Regularization using Variance</head><p>In Sec. 4, we were interested in bounding the functional entropy (Eq. (8)), for each training point x ∈ S, of CE(p w (·|z), p w (·|x)). Similarly, we want to bound the variance of CE(p w (·|z), p w (·|x)).</p><p>For this purpose, we can use the Poincaré inequality, described in Eq. <ref type="formula" target="#formula_1">(15)</ref>,</p><formula xml:id="formula_18">Var µ x (CE(p w (·|z), p w (·|x))) ≤ R d ∇ z CE(p w (·|z), p w (·|x)) 2 dµ x (z).<label>(17)</label></formula><p>We use the above inequality to regularize the training process. To consider both the loss minimization and the regularization term we formulate the learning objective,</p><formula xml:id="formula_19">(x,y)∈S CE(1[· = y], p w (·|x)) + λ (x,y)∈S R d ∇ z x CE(p w (·|z x ), p w (·|x)) 2 dµ x (z) −1 .<label>(18)</label></formula><p>To fit our multi-modal settings, we need to follow the tensorization process as illustrated in Sec. 4.1.</p><p>The same tensorization process can be applied to the variance using the Poincaré bound given in Eq. <ref type="bibr" target="#b14">(15)</ref>. For tensorized Poincaré bound leads to the learning objective</p><formula xml:id="formula_20">(x,y)∈S CE(1[· = y], p w (·|x)) + λ (x,y)∈S n i=1 R d i ∇ z x i CE(p w (·|z x i ), p w (·|x)) 2 dµ x i (z i ) −1 .<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In the following, we evaluate our proposed regularization on four different datasets. One of the datasets is a synthetic dataset (Colored MNIST), which permits to study whether a classifier leverages the wrong features. We show that adding the discussed regularization improves the generalization of a given classifier. We briefly describe each dataset and discuss the results of the proposed method.  <ref type="bibr" target="#b6">[7]</ref> and Dogs &amp; Cats <ref type="bibr" target="#b5">[6]</ref> datasets. We report maximum accuracy observed and accuracy after convergence of the model (Convg). We compare the 4 regularizers specified by the equation numbers. We underline the highest maximum accuracy and bold the highest results after convergence. Using functional Fisher information regularization (Eq. (12)) leads to a smaller difference between the maximum accuracy and accuracy after convergence. * refers to results we achieve without using our proposed regularization. ** denotes training with weight-decay ( 2 regularization). Accuracy (%) <ref type="figure">Figure 3</ref>: Training process with and without regularization. We note that generalization significantly improves when using our proposed regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Colored MNIST</head><p>Dataset: Colored MNIST <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> is a synthetic dataset based on MNIST <ref type="bibr" target="#b44">[45]</ref>. The train and validation set consist of 60,000 and 10,000 samples, respectively. Each sample is biased with a color that correlates with its digit. The biasing process assigns to each digit an RGB vector which represents a mean color. Then, each sample receives its color, sampled from a normal distribution with a fixed variance around the digit's mean color. This process results in a monochromatic image and high correlation between the digit's color and its label. To introduce a bias in a multi-modal approach, we split each sample x into a color modality x c and a shape (gray-scale representation of the image) modality x s . For humans it is evident that a digit should be classified based on its shape and not its color. For a learner this fact is not as clear. To minimize the loss, it is much easier for a classifier to leverage the color modality, which correlates very well with the label. In its nature, Colored MNIST evaluates the generalization of a model since it has a test set that assesses whether a classifier relies solely on color or both the color and the shape.</p><p>Baseline: A simple deep net achieves high accuracy on both colored train and colored validation set. However, on the gray-scale validation set, the network fails drastically, achieving only a 41.11% accuracy when using the model from the last training epoch. We note that the more we train the more the baseline relies on color rather than shape. We also compute an upper-bound by training the deep net on a gray-scale version. The upper-bound accuracy on the gray-scale validation set is 98.47%.</p><p>Results: Adding our proposed regularization encourages to exploit information from both shape and color modalities. We provide results in Tab. 1. <ref type="figure" target="#fig_0">Fig. 2</ref> shows that without entropy regularization, the Fisher information value of the shape is almost zero while adding the regularization results in a higher shape information value than the color. This fact complements the classifier's performance on the gray-scale validation set shown in <ref type="figure">Fig. 3</ref>. Using functional Fisher information based regularization outperforms the same classifier trained without regularization by almost 55%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">VQA-CPv2</head><p>Dataset: VQA-CPv2 <ref type="bibr" target="#b0">[1]</ref> is a re-shuffle of the VQAv2 <ref type="bibr" target="#b45">[46]</ref> dataset. Visual question answering (VQA) requires to answer a given question-image pair. <ref type="bibr" target="#b0">[1]</ref> observed that the original split of the VQAv2 dataset permits to leverage language priors. To challenge models to not use these priors, the question type distributions of the train and validation set were changed to differ from one another. VQA-CPv2 consist of 438,183 samples in the train set and 219,928 samples in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>We evaluated our method by adding functional Fisher information regularization to the current state-of-the-art <ref type="bibr" target="#b31">[32]</ref>. In doing so, the result improves by 2.5%, achieving 54.55% accuracy. We provide a comparison with recent state-of-the-art methods in Tab. 2.</p><p>The authors of <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> raise the concern that new regularization methods mainly boost the performance of yes/no questions. Investigating the improvements due to our result shows that this is not the case. The accuracy difference to the previous state-of-the-art on the different answer types is: yes/no +1.5%, number +18%, and other -1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">SocialIQ</head><p>Dataset: The SocialIQ dataset is designed to develop models for understanding of social situations in videos. Each sample consists of a video clip, a question, and an answer. The task is to predict whether the answer is correct or not given this tuple. The dataset is split into 37,191 training samples, and 5,320 validation set samples. Note that an inherent bias exists in this dataset: specifically the sentiment of the answer provides a good cue.</p><p>Baseline: A simple classifier based on only the answer modality performs significantly better than chance level accuracy (using our settings~6% more). Such biases in the train set lead to a classic case of overfitting.</p><p>Results: As seen in <ref type="figure">Fig. 3</ref>, training without functional Fisher information regularization leads tõ 80% accuracy on the train set and~64% accuracy on the validation set. Although, functional Fisher information regularization results in 70% accuracy on the train set, it improves validation set accuracy to 67.93% accuracy.</p><p>We further investigate the information values during the training phase with and without functional Fisher information regularization. In <ref type="figure" target="#fig_0">Fig. 2</ref> we observe that without our regularization, the answer modality has the highest information value while the question modality is almost entirely ignored. Adding the proposed regularization balances the information between modalities, the desired behavior in multi-modal learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Dogs and Cats</head><p>Dataset: Following the settings of Kim et al. <ref type="bibr" target="#b5">[6]</ref>, we evaluate our models on the biased "Dogs and Cats" dataset. This dataset comes in two splits: The TB1 set consists of bright dogs and dark cats and contains 10,047 samples. The TB2 set consist of dark dogs and bright cats and contains 6,738 samples. We use the image as a single-modality.</p><p>Baseline: The authors show that training of ResNet-18 <ref type="bibr" target="#b48">[49]</ref> on TB1 and testing on TB2 results in a poor performance of 74.98%. The authors also show that using TB2 as the train set and TB1 as the test set results in even worse accuracy of 66.45%.</p><p>Functional Fisher information regularization training on TB1 and testing on TB2 with λ (see Eq. <ref type="formula" target="#formula_1">(12)</ref>) set to equal 3e-10 results in 94.71% accuracy, exceeding [6] by 3.5%. Training on TB2 while testing on TB1 achieves an accuracy of 88.11%, 1% higher than <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Classical regularizers applied on multi-modal datasets lead to models which may ignore one or more of the modalities. This is sub-optimal as we expect all modalities to contribute to classification. To alleviate this concern we study regularization via the functional entropy. It encourages the model to more uniformly exploit the available modalities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>We study functional entropy based regularizers which enable classifiers to more uniformly benefit from available dataset modalities in multi-modal tasks. We think the proposed method will help to reduce biases that present-day classifiers exploit when being trained on data which contains modalities, some of which are easier to leverage than others.</p><p>We think this research will have positive societal implications. With machine learning being used more widely, bias from various modalities has become ubiquitous. Minority groups are disadvantaged by present-day AI algorithms, which work very well for the average person but are not suitable for other groups. We provide two examples next:</p><p>1. It is widely believed that criminal risk scores are biased against minorities 1 , and mathematical methods that reduce the bias in machine learning are desperately needed. In our work we show how our regularization allows to reduce the color modality effect in colored MNIST, which hopefully facilitates to reduce bias in deep nets.</p><p>2. Consider virtual assistants as another example: if pronunciation is not mainstream, replies of AI systems are less helpful. Consequently, current AI ignores parts of society.</p><p>To conclude, we think the proposed research is a first step towards machine learning becoming more inclusive.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Proportions of the Fisher information values during training for SocialIQ, Colored MNIST, VQA-CPv2 and Dogs&amp;Cats. Using our proposed regularization brings the modalities Fisher information value closer than training without our regularization, a desired property in multi-modal learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Acknowledgements:</head><label></label><figDesc>This work is supported in part by NSF under Grant # 1718221, 2008387, NIFA award 2020-67021-32799 and, BSF under Grant# 2019783.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison between our proposed regularization terms on the Colored MNIST (multi-modal settings, gray-scale test set), SocialIQ</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison between the state-of-the-art on the VQA-CPv2 test set. The best results for each category are in bold. * denotes models that make use of external data. Eq. (19) 54.01 ± 0.27 73.02 ± 1.21 43.15 ± 1.01 47.02 ± 0.28 LMH +Ours Eq. (12) 54.55 ± 0.29 74.03 ± 1.13 49.16 ± 1.22 45.82 ± 0.37</figDesc><table><row><cell>Model</cell><cell>Overall</cell><cell></cell><cell>Answer type</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Yes/No</cell><cell>Number</cell><cell>Other</cell></row><row><cell>BUTD [15]</cell><cell>39.34</cell><cell>42.13</cell><cell>12.29</cell><cell>45.29</cell></row><row><cell>AdvReg [38]</cell><cell>41.17</cell><cell>65.49</cell><cell>15.48</cell><cell>35.48</cell></row><row><cell>HINT [43]*</cell><cell>46.73</cell><cell>67.27</cell><cell>10.61</cell><cell>45.88</cell></row><row><cell>RUBi [33]</cell><cell>47.11</cell><cell>68.65</cell><cell>20.28</cell><cell>43.18</cell></row><row><cell>SCR [44]*</cell><cell>49.45</cell><cell>72.36</cell><cell>10.93</cell><cell>48.02</cell></row><row><cell>LMH [32]</cell><cell>52.05</cell><cell>72.58</cell><cell>31.12</cell><cell>46.97</cell></row><row><cell>LMH +Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.propublica.org/article/bias-in-criminal-risk-scores-ismathematically-inevitable-researchers-say</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analyzing the behavior of visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Repair: Removing representation bias by dataset resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning not to learn: Training deep neural networks with biased data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social-iq: A question answering benchmark for artificial social intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-order attention models for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilinear Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factor graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV, 2019. * equal contribution</title>
		<meeting>ICCV, 2019. * equal contribution</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Creativity: Generating Diverse Questions using Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2017. * equal contribution</title>
		<meeting>CVPR, 2017. * equal contribution</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two can play this Game: Visual Dialog with Discriminative Question Generation and Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diverse and Coherent Paragraph Generation from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TAB-VCR: Tags and Attributes based VCR Baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple baseline for audio-visual scene-aware dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL (Short Papers)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<editor>CoNLL</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1907.02893" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What makes training multi-modal classification networks hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rubi: Reducing unimodal biases for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An introduction to variational methods for graphical models. Learning in Graphical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravid</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ziv</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.00810" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Information maximizing visual question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overcoming language priors in visual question answering with adversarial regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Sainandan Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Approximate fisher information matrix to characterize the training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Analysis and geometry of Markov diffusion operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Bakry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gentil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>SBM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analytic inequalities, isoperimetric inequalities and logarithmic sobolev inequalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Os Rothaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JFA</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Concentration of Measure Phenomenon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>AMS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Taking a hint: Leveraging explanations to make vision and language models more grounded</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-critical reasoning for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A negative case analysis of visual grounding methods for vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robik</forename><surname>Kushal Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.05704" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
