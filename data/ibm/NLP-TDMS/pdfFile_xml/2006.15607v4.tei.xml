<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localization Uncertainty Estimation for Anchor-Free Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
							<email>yw.lee@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Won</forename><surname>Hwang</surname></persName>
							<email>jwhwang@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung-Il</forename><surname>Kim</surname></persName>
							<email>hikim@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Yun</surname></persName>
							<email>kimin.yun@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjin</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Localization Uncertainty Estimation for Anchor-Free Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since many safety-critical systems, such as surgical robots and autonomous driving cars, are in unstable environments with sensor noise and incomplete data, it is desirable for object detectors to take into account the confidence of localization prediction. There are three limitations of the prior uncertainty estimation methods for anchor-based object detection. 1) They model the uncertainty based on object properties having different characteristics, such as location (center point) and scale (width, height). 2) they model a box offset and ground-truth as Gaussian distribution and Dirac delta distribution, which leads to the model misspecification problem. Because the Dirac delta distribution is not exactly represented as Gaussian, i.e., for any µ and Σ. 3) Since anchor-based methods are sensitive to hyper-parameters of anchor, the localization uncertainty modeling is also sensitive to these parameters. Therefore, we propose a new localization uncertainty estimation method called Gaussian-FCOS for anchor-free object detection. Our method captures the uncertainty based on four directions of box offsets (left, right, top, bottom)  that have similar properties, which enables to capture which direction is uncertain and provide a quantitative value in range [0, 1]. To this end, we design a new uncertainty loss, negative power log-likelihood loss, to measure uncertainty by weighting IoU to the likelihood loss, which alleviates the model misspecification problem. Experiments on COCO datasets demonstrate that our Gaussian-FCOS reduces false positives and finds more missing-objects by mitigating over-confidence scores with the estimated uncertainty. We hope Gaussian-FCOS serves as a crucial component for the reliability-required task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection based on CNN is widely used in many automated systems such as autonomous vehicles and surgical robots <ref type="bibr" target="#b23">[24]</ref>. In such a safety-related system, it is very important to know how reliable the estimated output <ref type="figure">Figure 1</ref>: Examples of 4-directions uncertainty for anchor-free object detection. C L, C R, C T, and C B denote the estimated certainty in [0, 1] value with respect to left, right, top, and bottom. For example, Gaussian-FCOS estimates lower top-direction certainty due to its ambiguous head boundary of the cat wearing a hat. This demonstrates that our method makes it possible to quantify which direction is uncertain due to unclear or obvious objects. is as well as good performance. Object detection is a task that combines object localization and classification, however, most of the state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> provide the reliability of their algorithm as a single value (e.g., confidence) for each bounding box. That is, they only use the classification score as the detection quality without the localization uncertainty. As a consequence, these methods produce mislocalized detection boxes with overconfidence <ref type="bibr" target="#b8">[9]</ref>. For example, as shown in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>, the mislocalized detection with the confidence score of 50% (green box) is not removed because its classification confidence score is higher than the threshold. Therefore, in addition to the classification confidence score, the confidence of the bounding box's localization is also necessary for the detection certainty.</p><p>Recently, efforts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref> to estimate uncertainty for . The colors of bounding boxes are chosen by its detection score (a) and certainty score (b), (c). Specifically, the higher score (up to 1.0) is, the box color is to be red otherwise blue, where 'conf th' denotes confidence threshold for the visualization. 'C' in (b), (c) denotes the estimated certainty score. Gaussian-FCOS captures localization uncertainty for each bounding box which means how certain box location is. For example, each detected person has a higher certainty score over 90% whereas the false positive (cyan box) due to the overlap between persons gets a lower certainty score (45%). Unlike FCOS (a), Gaussian-FCOS (c) filters out the false positive by decaying the detection score with the estimated certainty score. compared to FCOS, Gaussian-FCOS localizes the person with a tennis racket more accurately (tightly).</p><p>object detection have been attempted. All of these efforts model the uncertainty of location (center point) and scale (width, height) as Gaussian distribution in the anchor-based methods by adding four channels in the regression output. However, since center point, width, and height have semantically different characteristics <ref type="bibr" target="#b12">[13]</ref>, this approach considering each value equally is inadequate for modeling localization uncertainty. For example, the estimated distributions of center point and scale shows different shapes in <ref type="bibr" target="#b12">[13]</ref>. Besides, since anchor-based methods are sensitive to hyperparameters of anchor, the localization uncertainty modeling on the anchor-based methods is also sensitive to these parameters.</p><p>Recently, anchor-free methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26]</ref> that do not need the heuristic anchor-box tuning (e.g., scale, aspect ratio) surpassed conventional anchor-box based methods such as Faster R-CNN <ref type="bibr" target="#b22">[23]</ref>, RetinaNet <ref type="bibr" target="#b18">[19]</ref>, and their variants <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref>. As a representative anchor-free method, FCOS <ref type="bibr" target="#b25">[26]</ref> adopts the concept of centerness to filter out false positive boxes. That is, the centerness can be interpreted as the implicit localization uncertainty of a proposal box. However, FCOS <ref type="bibr" target="#b25">[26]</ref> heuristically measures the localization uncertainty by how well the predicted box fits the center, which does not reflect full information for localization uncertainty of box (e.g., scale).</p><p>In terms of the loss function for uncertainty modeling, the conventional methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref> use the negative loglikelihood loss to regress output as Gaussian distribution. He et al. <ref type="bibr" target="#b8">[9]</ref> introduce KL divergence loss for Gaussian dis-tribution of box prediction and Dirac delta function for a ground-truth box. In the perspective of cross-entropy, however, these methods face the model misspecification problem <ref type="bibr" target="#b9">[10]</ref> in that the Dirac delta function is not exactly represented as Gaussian distribution, i.e., for any µ and Σ, δ(x) = N (x|µ, Σ).</p><p>To deal with these limitations, in this paper, we propose a method, called Gaussian-FCOS, that estimates explicitly localization uncertainty for anchor-free method, FCOS <ref type="bibr" target="#b25">[26]</ref>. Unlike centerness in FCOS, we model the uncertainty for each of the four box offsets (left, right, top, bottom) from the center of the box to fully describe the localization uncertainty. Moreover, unlike conventional anchor-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref> for localization uncertainty, our method estimates the uncertainty of the four box offsets having a similar semantic characteristic. It makes possible to inform which direction of a box boundary is uncertain as a quantitative value in [0,1] independently from the overall box uncertainty as shown in <ref type="figure">Fig. 1.</ref> (more examples are illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>.) To do this, we model the box offset and its uncertainty of FCOS through Gaussian distribution with the newly desgined uncertainty loss by adding the uncertainty branch.</p><p>To resolve the model misspecification <ref type="bibr" target="#b9">[10]</ref> between Dirac delta and Gaussian distribution, we design a novel uncertainty loss, negative power log-likelihood loss, inspired by Power likelihood <ref type="bibr" target="#b9">[10]</ref> (NPLL), to enable the uncertainty branch to learn to estimate localization uncertainty by weighing IoU to the log-likelihood loss. In particular, this new loss creates a synergy with the existing box regression loss, which tells the difference between the ground truth and the predicted box offset, enabling more accurate box prediction. For example, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>(a) and (c), we can see that Gaussian-FCOS localizes objects more accurately (tightly) than FCOS by comparing the detected box of the person with a tennis racket. Furthermore, we calibrate the detection score through the localization uncertainty. In detail, we compute certainty from the estimated uncertainty and then multiply it to the classification score to get the final detection score. <ref type="figure" target="#fig_0">Fig. 2(b</ref>)-(c) shows the effectiveness of uncertainty calibration. Unlike the detection result from FCOS in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>, Gaussian-FCOS calibrates (or penalizes) the detection score with the certainty, which can filter out the mislocalized box (cyan box) between two persons.</p><p>The main contributions are summarized as below:</p><p>• We propose a simple and effective four-directions localization uncertainty estimation for anchor-free object detection that can serve as a detection quality measure and provide which direction is uncertain as a quantitative value in [0, 1].</p><p>• We newly design the uncertainty loss function, inspired by power likelihood, that has IoUs as weights of negative log-likelihood loss that resolves the model misspecification problem.</p><p>• We analyze the influence of uncertainty on object localization and confirm that it improves mislocalization and reduces missing objects on challenging COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Anchor-Free Object Detection</head><p>Recently, anchor-free object detectors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26]</ref> have attracted attention beyond anchor-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b29">30]</ref> that need to tune sensitive hyper-parameters related to anchor box (e.g., scale, aspect ratio, etc). Cor-nerNet <ref type="bibr" target="#b14">[15]</ref> predicts an object location as a pair of keypoints (top-left and bottom-right). CenterNet <ref type="bibr" target="#b2">[3]</ref> extends CornerNet as a triplet instead of a pair of key points to boost performance. ExtremeNet <ref type="bibr" target="#b31">[32]</ref> locates four extreme points (top, bottom, left, right) and one center point to generate the object box. Zhu et al. <ref type="bibr" target="#b30">[31]</ref> utilizes keypoint estimation to predict center point objects and regresses to other attributes including size, orientation, pose, and 3D location. FCOS <ref type="bibr" target="#b25">[26]</ref> views all points inside the ground-truth box as positive samples and regresses four distances (left, right, top, bottom) from the object boundary. We propose to endow FCOS with localization uncertainty due to its simplicity and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Uncertainty Estimation</head><p>Uncertainty in deep neural networks can be estimated in two types <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>: epistemic (sampling-based) and aleatoric (sampling-free) uncertainty. Epistemic uncertainty measures the model uncertainty in the models' parameters through Bayesian neural networks <ref type="bibr" target="#b24">[25]</ref>, Monte Carlo dropout <ref type="bibr" target="#b4">[5]</ref>, and Bootstrap Ensemble <ref type="bibr" target="#b13">[14]</ref>. As they need to be re-evaluated several times and store several sets of weights for each network, it is hard to apply them for realtime applications. Aleatoric uncertainty is data and problem inherent such as sensor noise and ambiguities in data. It can be estimated by explicitly modeling it as model output.</p><p>Recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref> have adopted uncertainty estimation for object detection. Lakshminarayanan et al. <ref type="bibr" target="#b13">[14]</ref> and Harakeh et al. <ref type="bibr" target="#b5">[6]</ref> use Monte Carlo dropout in Epistemic based methods. As described above, since epistemic uncertainty needs to inference several times, it is not suitable for real-time object detection. Le et al. <ref type="bibr" target="#b15">[16]</ref> and Choi et al. <ref type="bibr" target="#b1">[2]</ref> are aleatoric based methods and jointly estimate the uncertainties of four parameters of bounding box from SSD <ref type="bibr" target="#b20">[21]</ref> and YOLOv3 <ref type="bibr" target="#b21">[22]</ref>. He <ref type="bibr" target="#b8">[9]</ref> estimates the uncertainty of bounding box by minimizing the KLdivergence loss for Gaussian distribution of predicted box and Dirac delta distribution of ground-truth box on the Faster R-CNN <ref type="bibr" target="#b22">[23]</ref> (anchor-based method). From the crossentropy perspective, however, Dirac delta distribution cannot be represented Gaussian distribution, which results in a misspecification problem <ref type="bibr" target="#b9">[10]</ref>. To overcome this problem, we adopt the power likelihood concept <ref type="bibr" target="#b9">[10]</ref> to the Gaussian log-likelihood loss. The latest concurrent work is Generalized Focal loss (GFocal) <ref type="bibr" target="#b17">[18]</ref> that represents jointly localization quality and classification and model bounding box as arbitrary distribution. The distinct difference from GFocal <ref type="bibr" target="#b17">[18]</ref> is that our method estimates 4-directions uncertainties as quantitative values in the range [0, 1] thus these estimated values can be used as an informative cue for decisionmaking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>For the uncertainty estimation for object detector, we choose anchor-free detector, FCOS <ref type="bibr" target="#b25">[26]</ref> based on two reasons: 1) Simplicity. FCOS directly regresses the target bounding boxes in a pixel-wise prediction manner without heuristic anchor tuning (aspect ratio, scales, etc  values sharing semantic meanings that have similar properties. Furthermore, it enables to notify which direction of a box boundary is uncertain separately from the overall box uncertainty.</p><p>In this section, we first introduce the localization step of FCOS <ref type="bibr" target="#b25">[26]</ref>. Then, we present uncertainty loss for modeling the uncertainty of the object coordinates in FCOS as the Gaussian parameters (i.e., the mean and variance). Next, we introduce the new uncertainty branch that estimates the localization uncertainty in addition to FCOS branches. Finally, we show how the estimated localization uncertainty is applied to provide localization confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">FCOS detector</head><p>In FCOS <ref type="bibr" target="#b25">[26]</ref>, if the location belongs to the ground-truth box area, it is regarded as a positive sample and as a negative sample otherwise. On each location (x, y), the box offsets are regressed as 4D vector B x,y = [l, r, t, b] that is the distances from the location to four sides of the bounding box (i.e., left, right, top, and bottom). The regression targets B g</p><p>x,y = [l g , r g , t g , b g ] are computed as,</p><formula xml:id="formula_0">l g = x−x g lt , r g = x g rb −x, t g = y−y g lt , b g = y g rb −y,<label>(1)</label></formula><p>where (x g lt , y g lt ) and (x g rb , y g rb ) denote the coordinates of the left-top and right-bottom corners of the ground-truth box, respectively. Then, for all locations of positive samples, the IoU loss <ref type="bibr" target="#b27">[28]</ref> is measured between the predicted B x,y and the ground truth B g x,y for the regression loss. FCOS also adopts centerness to suppress low quality detected boxes in the inference stage. The main concept of centerness is to estimate not only the position of the object but also how well it fits with the center. That is, it provides confidence that the box predicted in the test step fits the center of the object well. Also, this centerness value is used as uncertainty to penalize the detection score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gaussian-FCOS</head><p>FCOS <ref type="bibr" target="#b25">[26]</ref> predicts the class score, box offsets B=(l, r, t, b), and centerness. In FCOS, centerness can be regarded as implicit uncertainty because centerness is used to filter predicted boxes, but centerness alone is insufficient to measure localization uncertainty. In other words, centerness is a single value that simply shows the localization uncertainty as to how well the center of the box fits, but for accurate modeling of localization uncertainty, it is necessary to consider the four positions that make up the box. Therefore, we propose Gaussian-FCOS that estimates localization uncertainty of the box, based on the regressed box offsets (l, r, t, b). To predict the uncertainties of four box offsets, we model the box offsets through Gaussian distribution and train the network to estimate its uncertainty (standard deviation). Assuming each instance of box offsets is independent, we use multivariate Gaussian distribution of output B * with diagonal covariance matrix Σ B to model each box offset B:</p><formula xml:id="formula_1">P Θ (B * |B) = N (B * ; µ B , Σ B ),<label>(2)</label></formula><p>where Θ is the learnable network parameters, and d is the</p><formula xml:id="formula_2">dimension of B (i.e., d = 4). µ B = [µ l , µ r , µ t , µ b ] and Σ B = diag(σ 2 l , σ 2 r , σ 2 t , σ 2 b</formula><p>) denote the predicted box offset and its uncertainty, respectively.</p><p>Power likelihood. Prior works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref> also model box offset and ground-truth box as Gaussian distribution and Dirac delta distribution. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref> adopt negative loglikelihood loss (NLL) and <ref type="bibr" target="#b8">[9]</ref> use KL-divergence loss (KL-Loss). In cross-entropy perspective, minimizing NLL and KL-loss is equivalent as below:</p><formula xml:id="formula_3">L = − 1 N P D (x) · logP Θ (x),<label>(3)</label></formula><p>where P D and P Θ are Dirac delta function and Gaussian probability density function, respectively. When the box offset is located in a ground-truth box, the P D is 1 then Eq. 3 becomes negative log-likelihood loss. However, there is a significant problem that Dirac delta distribution does not belong to the family of Gaussian distributions, called the model misspecification problem <ref type="bibr" target="#b9">[10]</ref>. In a number of statistical literature, to estimate parameters of interest in a robust way when the model is misspecified, the Power likelihood (P Θ (·) w ), which raises the likelihood (P Θ (·)) to a power (w) that controls how influential the data is, has been proposed <ref type="bibr" target="#b9">[10]</ref>. Thus, to fill the gaps between Dirac delta distribution and Gaussian distribution, inspired by Power likelihood, we introduce a novel uncertainty loss, negative power log-likelihood loss (NPLL), that exploits Intersection-over-Union (IoU) as the power since the offset that has higher IoU should be more influential. By multiplying IoU term to the log-likelihood, the new uncertainty loss is defined as :</p><formula xml:id="formula_4">L u = − λ N pos i k IoU i · logP Θ B g i,k |µ k , σ 2 k (4) = λ N pos i IoU i × k (B g i,k − µ k ) 2 2σ 2 k + 1 2 logσ 2 k + 2log2π ,<label>(5)</label></formula><p>where N pos denotes the number of positive samples, λ (λ = 0.05 in this paper) is the balance weight for L u , IoU i is the intersection-over-union between the predicted box and the ground-truth box at location i and k is in {l, r, t, b}. The summation is calculated over all positive locations on the feature maps. From this uncertainty loss, when the predicted coordinate µ k from the regression branch is inaccurate, the network is trained to estimate larger uncertainty σ k . For the rest of the losses, following FCOS <ref type="bibr" target="#b25">[26]</ref>, we use focal loss <ref type="bibr" target="#b18">[19]</ref> for classification (L c ), binary cross-entropy loss for centerness (L ct ), and IoU loss <ref type="bibr" target="#b27">[28]</ref> for regression (L b ). The total loss is defined as:</p><formula xml:id="formula_5">L = L c + L ct + L b + L u .<label>(6)</label></formula><p>It is noted that unlike centerness, our network is trained to directly estimate four localization uncertainties (σ l , σ r , σ t , σ b ) of each box offsets. Also, it can be estimated which direction of a box boundary is uncertain separately apart from the overall box uncertainty.</p><p>Uncertainty branch. To implement our idea, we redesign the FCOS <ref type="bibr" target="#b25">[26]</ref> network structure by adding the uncertainty branch as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.35">FCOS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian-FCOS</head><p>Step Regress Loss <ref type="figure">Figure 4</ref>: Comparison of the regression Loss. Regression loss of Gaussian-FCOS tends to be lower than that of FCOS. In other words, training with the proposed uncertainty loss further reduces the regression loss and helps the trained network estimate better object position.</p><p>The regression branch and the uncertainty branch shares same feature (4 conv layers) as their inputs to estimate two kinds of statistic parameters (i.e. µ k , σ k ).</p><p>Uncertainty calibration. With the uncertainty branch, Gaussian-FCOS can obtain the localization uncertainty σ k and utilize it to infer the box confidence. Concretely, box confidence is interpreted as the certainty that is defined as <ref type="figure">(1 − σ)</ref>, where the σ is obtained by averaging σ k for all k ∈ {l, r, t, b}. In the post-processing step, Non-Maximum-Supression (NMS) is applied for removing overlapped box proposals. The class confidence score is widely adopted to decide the boxes to be removed, but this does not reflect localization uncertainty. Thus, for NMS, we calibrate the detection score by multiplying the class score by the our box confidence (1 − σ) that considers the localization uncertainty. <ref type="figure" target="#fig_0">Fig. 2(b)</ref> shows that the false positive (cyan box) has lower box confidence (45%) than other true positives (over 90%), which means Gaussian-FCOS estimates localization uncertainty well. As shown in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>, Gaussian-FCOS removes the false positive through the calibrated (penalized) the detection score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the effectiveness of Gaussian-FCOS on the challenging COCO <ref type="bibr" target="#b19">[20]</ref> dataset which has 80 object categories. We use the COCO train2017 set (80k images) for training and val2017 set (5k images) for ablation studies. Final results are evaluated on test-dev2017 in the evaluation server for the comparison with state-of-the-arts. There are two key metrics for object detection evaluation, the one is average precision (AP) and the other is average recall (AR how proposed box are correctly classified. AR means how many objects our detector have detected without missing. Thus, AR is a crucial metric for safety-critical applications such as autonomous cars and surgical robots where missing can cause serious problem. AP and AR are averaged over IoU thresholds (.5 : .95), and the higher IoU is, the more accurate localization needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We train Gaussian-FCOS by using Stochastic Gradient Descent (SGD) for 270k iterations (3× schedule <ref type="bibr" target="#b6">[7]</ref>) with a mini-batch of 16 images. An initial learning rate is 0.01, and it is decreased by a factor of 10 at 210K and 250K iterations, respectively. Unless specified, the scale-jitter <ref type="bibr" target="#b6">[7]</ref> augmentation is applied where the shorter image side is randomly sampled from [640, 800] pixels. As a backbone network, we use ResNet-50 with ImageNet pre-trained weights in the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>Uncertainty loss with power likelihood. We first validate the effectiveness of the proposed uncertainty loss compared to the baseline, Negative log-Likelihood Loss (NLL), in Table 1. We can find that adopting IoU power term in the NLL improves performance AP as well as AR, suggesting that power likelihood with IoU alleviates effectively the misspecification problem. <ref type="figure">Fig. 4</ref> also shows that the regression loss of Gaussian-FCOS tends to be lower than FCOS. It means that uncertainty loss helps the regression branch learn to reduce the error with the ground-truth location. As a result, both AP and AR are improved from FCOS <ref type="bibr" target="#b25">[26]</ref>. Besides, uncertainty calibration also boosts the performance, which means the detection score calibrated by localization uncertainty alleviates the over-confidence problem.</p><p>Uncertainty calibration. <ref type="table" target="#tab_3">Table 2</ref> shows the effect according to the order of the usage of NMS and uncertainty calibration. The first row 'w/o Calibration' is the same as 'FCOS + Uncertainty Loss'. We find that the AR gain of 'Calibration before NMS' is bigger than that of 'Calibration after NMS'. It means that the uncertainty calibration prevents more well-localized objects from being filtered out by NMS. In addition, the AP of 'Calibration before NMS' is more improved that of 'Calibration after NMS'. In other   <ref type="table">Table 3</ref>: Comparison of different backbones on Gaussian-FCOS.</p><p>words, the NMS may remove the well-localized box that has low confidence, while the proposed method can preserve this box by score calibration. This means that the score calibration before NMS makes the incorrectly located box with over-confidence removed in the NMS step.</p><p>Backbone network. We validate Gaussian-FCOS with various backbone networks such as ResNet <ref type="bibr" target="#b7">[8]</ref> and VoVNet <ref type="bibr" target="#b16">[17]</ref>. <ref type="table">Table 3</ref> shows that Gaussian-FCOS achieves the consistent performance gains of both AP and AR on various backbone networks. It is noted that Gaussian-FCOS obtains a large improvement of AR, which demonstrates our certainty estimation helps to prevent objects from being missed. Also, through the difference in operation time is very insignificant, (1∼2 ms), our uncertainty branch and calibration efficiently model the localization uncertainty without computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Localization analysis</head><p>We investigate how Gaussian-FCOS improves the object localization (AP) and preserves objects without missing (AR). In particular, we compare Gaussian-FCOS with not only FCOS <ref type="bibr" target="#b25">[26]</ref> but also Faster R-CNN <ref type="bibr" target="#b22">[23]</ref> and Reti-naNet <ref type="bibr" target="#b18">[19]</ref> in that both are representative baselines in object detection field. First, we analyze Average Precision (AP) at different IoU thresholds and object scales in <ref type="table" target="#tab_5">Table 4</ref>. At soft-metric with 0.5 and 0.6 IoU thresholds, Faster R-CNN achieves better AP than the others because Region Proposal Network (RPN) helps Faster R-CNN to remove more false positives. On the other hand, due to RPN, Faster R-CNN tends to be a lower recall rate than others as shown in <ref type="table" target="#tab_6">Table 5</ref>. From strict metrics with 0.7 over IoU thresholds, Gaussian-FCOS shows better performance than the others.    This is because the estimated uncertainty enables the network to detect more accurate localized objects by calibrating the detection score. In particular, Gaussian-FCOS obtains bigger AP gain on large objects. We conjecture that this is because mislocalization occurs more frequently in larger objects than in smaller objects. We also explore the influence of Gaussian-FCOS on preventing objects from being missed (AR). <ref type="table" target="#tab_6">Table 5</ref> shows anchor-free methods (FCOS <ref type="bibr" target="#b25">[26]</ref> and Gaussian-FCOS) tend to achieve better Average Recall (AR) than anchor-based methods (Faster-RCNN <ref type="bibr" target="#b22">[23]</ref> and RetinaNet <ref type="bibr" target="#b18">[19]</ref>) at overall IoUs and scales. This is because anchor-free methods uses more positive samples than anchor-based methods, which increases the recall rate. we can also find that Gaussian-FCOS outperforms FCOS at all metrics. We speculate that the network can re-order the calibrated scores by reflecting localization uncertainty, which keeps well-localized objects from the NMS. For small objects, which are more likely to be missed, Gaussian-FCOS can preserve more small objects compared to FCOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with other methods.</head><p>We also validate Gaussian-FCOS with other methods. For a fair comparison, we adopt 1x learning schedule for   12 epochs without multi-scale training. <ref type="table" target="#tab_7">Table 6</ref> and <ref type="table" target="#tab_9">Table 7</ref> show the comparison results in terms of box quality estimation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref> and the target box representation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18]</ref>, respectively. In <ref type="table" target="#tab_7">Table 6</ref>, our method (c) (d) achieves better performance than other methods. Compared to center-point <ref type="bibr" target="#b25">[26]</ref> and IoU <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref> that are confined to only overall quality, our 4-directions uncertainty values can reflect more degrees (l, r, t, b) of box uality. <ref type="table" target="#tab_9">Table 7</ref> show that our method outperforms other box representation methods including non-parametric General distribution <ref type="bibr" target="#b17">[18]</ref>. Our Gaussian-FCOS exploits the explicit uncertainty loss with the proposed negative power log-likelihood loss (NPLL) that helps better understand the underlying distribution.</p><p>Lastly, we evaluate Gaussian-FCOS on COCO <ref type="bibr" target="#b19">[20]</ref> test-dev2017 dataset for other detection methods. Table 8 summarizes the results. Compared to the state-of-theart methods, Gaussian-FCOS with VoVNet-99 achieves the best performance. In case of same ResNet-101 backbone, GFL shows the best performance because GFL uses the better baseline (i.e., ATSS) that our baseline (i.e., FCOS). It is expected that applying our method for the anchor-based ATSS <ref type="bibr" target="#b28">[29]</ref> would boost performance but it is out of our research scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed Gaussian-FCOS that estimates 4directions uncertainty for anchor-free object detector. To this end, We design the new uncertainty loss, negative Power log-likelihood loss (NPLL) to train the network that produces the localization uncertainty and enables accurate localization. Gaussian-FCOS captures not only the quality of the detected box but also which direction is uncertain by quantified value [0,1]. This localization uncertainty is also utilized as box confidence with which the detection score is calibrated, boosting localization quality and preventing objects from being missed. Experiments on challenging COCO dataset demonstrate that our Gaussian-FCOS improves the overall performance and especially improves the average recall by reducing the missing objects. We can expect the proposed Gaussian-FCOS can serve as a component providing an important cue for safety-critical application or decision-making system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example of over-confidence problem resolved by Gaussian-FCOS (proposed)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of Gaussian-FCOS. Different from FCOS [26], Gaussian-FCOS estimates localization uncertainty from uncertainty branch that outputs four uncertainties of box offsets (left, right, top, and bottom.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Our network predicts a probability distribution instead of only box coordinates. The mean values µ k of each box offsets are predicted from the regression branch in FCOS. The new uncertainty branch with sigmoid function outputs four uncertainty values σ k in [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Estimated uncertainty examples of the proposed Gaussian-FCOS. Since there is no supervision of uncertainty, we analyze the estimated uncertainty qualitatively. Gaussian-FCOS captures lower certainties on unclear or occluded sides. For example, the left-directional certainty of the bird in the top-left image diminishes (66%) since it is occluded by a branch of a tree. Both the surfboard and the person in the top-center image have much lower bottom-directional certainties (5% and 33%) since their shapes are quite unclear due to the water. The leftmost giraffe in the bottom-left image, occluded by another girraffe. also has lower right-and bottom-directional certainties (41% and 8%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). 2) Semantic symmetry of regression. anchor-based methods regress center point (x,y), width, and height based on each anchor box, while FCOS directly regresses four boundaries (left, right, top, bottom) of a bounding box at each location. Although the center, width, and height from anchorbased methods have different characteristics, the distances between four boundaries and each location are semantically symmetric. In terms of modeling, it is easier to model the</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Classification</cell></row><row><cell></cell><cell>P7</cell><cell>Head</cell><cell>x4</cell><cell>H x W x C</cell></row><row><cell></cell><cell>P6</cell><cell>Head</cell><cell>H x W x 256</cell></row><row><cell>C5</cell><cell>P5</cell><cell>Head</cell><cell></cell><cell>Center-ness H x W x 1</cell></row><row><cell>C4</cell><cell>P4</cell><cell>Head</cell><cell></cell></row><row><cell>C3</cell><cell>P3</cell><cell>Head</cell><cell></cell><cell>Regression H x W x 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>x4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>H x W x 256</cell><cell>Uncertainty</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>H x W x 4</cell></row><row><cell>Backbone</cell><cell>Feature pyramid</cell><cell></cell><cell>Detection head</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>5+0.3 45.4+1.0 60.2+1.2 66.6+3.4 +L u w/ IoU power 42.0+0.8 46.2+1.8 60.8+1.8 67.0+3.8 Effectiveness of power likelihood. L u denotes negativelog-likelihoodloss(NLL).Theproposeduncertainty loss with IoU power term (NPLL) improves the baseline.</figDesc><table><row><cell>). AP reflects</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Calib. before NMS 42.0+0.3 46.2+1.1 60.8+1.4 67.0+3.3</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>AP 75</cell><cell>AR</cell><cell>AR 75</cell></row><row><cell>w/o Calibration</cell><cell>41.7</cell><cell>45.1</cell><cell>59.4</cell><cell>63.7</cell></row><row><cell>Calib. after NMS</cell><cell>41.5</cell><cell>45.2</cell><cell>59.4</cell><cell>63.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Uncertainty calibration with NMS. Calibrating the detection score before NMS prevents well-localized boxes from being filtered out while inaccurately located boxes with over-confidence are removed in NMS step.</figDesc><table><row><cell>Backbone</cell><cell cols="2">Uncertainty AP</cell><cell>AP 75 AR</cell><cell>AR 75 Time</cell></row><row><cell>ResNet-50</cell><cell>√</cell><cell cols="3">41.2 44.4 59.0 63.2 42.0 46.2 60.8 67.0</cell><cell>0.041 0.042</cell></row><row><cell>ResNet-101</cell><cell>√</cell><cell cols="3">43.1 46.7 60.6 65.4 43.7 47.8 61.9 67.8</cell><cell>0.054 0.055</cell></row><row><cell>VoVNet-39</cell><cell>√</cell><cell cols="3">43.5 47.2 61.4 66.2 44.3 48.8 62.8 69.1</cell><cell>0.042 0.044</cell></row><row><cell>VoVNet-57</cell><cell>√</cell><cell cols="3">44.4 47.6 61.6 66.2 45.2 49.7 63.2 69.7</cell><cell>0.048 0.049</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Gaussian-FCOS 42.0+0.8 58.7-0.3 55.2-0.5 50.3+0.9 40.4+2.3 20.7+2.2 26.3+0.8 45.8+1.0 53.9+1.9</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 60</cell><cell>AP 70</cell><cell>AP 80</cell><cell>AP 90</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell cols="2">Faster R-CNN [23] 40.2</cell><cell>61.0</cell><cell>56.6</cell><cell>49.1</cell><cell>36.7</cell><cell>13.7</cell><cell>24.2</cell><cell>43.5</cell><cell>52.0</cell></row><row><cell>RetinaNet [19]</cell><cell>38.7</cell><cell>58.0</cell><cell>53.6</cell><cell>46.4</cell><cell>34.7</cell><cell>15.7</cell><cell>23.3</cell><cell>42.3</cell><cell>50.3</cell></row><row><cell>FCOS [26]</cell><cell>41.2</cell><cell>60.0</cell><cell>55.7</cell><cell>49.4</cell><cell>38.1</cell><cell>18.5</cell><cell>25.7</cell><cell>44.8</cell><cell>52.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of Average Precision (AP) at different IoUs and object scales. Note that for fair comparison, all models are trained using same training protocol<ref type="bibr" target="#b6">[7]</ref> (e.g., 3× schedule, scale jitter) and same backbone (ResNet-50).</figDesc><table><row><cell>Method</cell><cell>AR</cell><cell>AR 50</cell><cell>AR 60</cell><cell>AR 70</cell><cell>AR 80</cell><cell>AR 90</cell><cell>AR S</cell><cell>AR M</cell><cell>AR L</cell></row><row><cell cols="2">Faster R-CNN [23] 54.0</cell><cell>78.1</cell><cell>73.6</cell><cell>64.6</cell><cell>50.3</cell><cell>23.9</cell><cell>35.9</cell><cell>57.4</cell><cell>67.8</cell></row><row><cell>RetinaNet [19]</cell><cell>55.4</cell><cell>80.1</cell><cell>75.5</cell><cell>65.6</cell><cell>49.7</cell><cell>26.2</cell><cell>37.2</cell><cell>58.9</cell><cell>70.5</cell></row><row><cell>FCOS [26]</cell><cell>59.0</cell><cell>81.9</cell><cell>77.7</cell><cell>70.1</cell><cell>55.0</cell><cell>31.2</cell><cell>40.4</cell><cell>62.8</cell><cell>74.1</cell></row><row><cell>Gaussian-FCOS</cell><cell cols="9">60.8+1.8 82.3+0.4 78.5+0.8 72.2+2.1 59.2+4.2 32.5+1.3 42.8+2.4 64.9+2.1 76.1+2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of Average Recall (AR) at different IoUs and object scales. These results demonstrate how well Gaussian-FCOS preserves objects without missing. Due to uncertainty calibration, Gaussian-FCOS keeps well-localized objects from being filtered out.</figDesc><table><row><cell>FCOS</cell><cell>AP</cell><cell>AP 75 AP S AP M AP L</cell></row><row><cell cols="3">+ centerness-branch [26] 38.5 41.6 22.4 42.4 49.1</cell></row><row><cell>+ IoU-branch [11, 27]</cell><cell cols="2">38.7 42.0 21.6 43.0 50.3</cell></row><row><cell>+ QFL [18]</cell><cell cols="2">39.0 41.9 22.0 43.1 51.0</cell></row><row><cell>+ ours</cell><cell cols="2">39.2 43.2 21.9 43.2 51.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison between box quality estimation methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Gaussian with NPLL (ours) 39.2 43.2 21.9 43.2 51.0</figDesc><table><row><cell>Distribution</cell><cell>AP</cell><cell>AP 75 AP S AP M AP L</cell></row><row><cell>Dirac delta [26]</cell><cell cols="2">38.5 41.6 22.4 42.4 49.1</cell></row><row><cell>Gaussian [2, 13, 9]</cell><cell cols="2">38.6 41.6 21.7 42.5 50.0</cell></row><row><cell>General w/ DFL [18]</cell><cell cols="2">39.0 42.3 22.6 43.0 50.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison with representations of box location. NPLL denotes the proposed negative power loglikelihood loss.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP AP 75 AP S AP M AP L</cell></row><row><cell>anchor-based:</cell><cell></cell><cell></cell></row><row><cell>RetinaNet [19]</cell><cell>ResNet-101</cell><cell>39.1 42.3 21.8 42.7 50.2</cell></row><row><cell>ATSS [29]</cell><cell>ResNet-101</cell><cell>43.6 47.4 26.1 47.0 53.6</cell></row><row><cell>GFL [18]</cell><cell>ResNet-101</cell><cell>45.0 48.9 27.2 48.8 54.5</cell></row><row><cell>anchor-free:</cell><cell></cell><cell></cell></row><row><cell cols="3">ExtremeNet [32] Hourglass-104 40.2 43.2 20.4 43.2 53.1</cell></row><row><cell>CornerNet [15]</cell><cell cols="2">Hourglass-104 40.5 43.1 19.4 42.7 53.9</cell></row><row><cell>CenterNet [3]</cell><cell cols="2">Hourglass-104 44.9 49.0 26.6 48.6 57.5</cell></row><row><cell>FCOS [26]</cell><cell>ResNet-101</cell><cell>41.5 45.0 24.4 44.8 51.6</cell></row><row><cell>FCOS [26]</cell><cell>ResNeXt-101</cell><cell>44.7 48.4 27.6 47.5 55.6</cell></row><row><cell>ours:</cell><cell></cell><cell></cell></row><row><cell cols="2">Gaussian-FCOS ResNet-101</cell><cell>43.8 48.1 25.6 46.9 54.5</cell></row><row><cell cols="2">Gaussian-FCOS ResNeXt-101</cell><cell>45.5 49.5 28.2 48.5 56.1</cell></row><row><cell cols="2">Gaussian-FCOS VoVNet-99</cell><cell>46.0 50.6 28.4 49.0 56.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparison to state-of-the-art methods on COCO test-dev2017. These results are tested without multi-scale testing.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayoung</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuk-Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bayesod: A bayesian approach for uncertainty estimation in deep object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03838</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Assigning a value to a power likelihood in a general bayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="497" to="503" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uncertainty estimation in one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uncertainty estimation for deep neural object detectors in safety-critical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Michael Truong Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ITSC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detection and localization of robotic tools in robot-assisted surgery videos using deep neural networks for region proposal and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurshid A</forename><surname>Guru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Laumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02731</idno>
		<title level="m">A comprehensive guide to bayesian convolutional neural network with variational inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Iou-aware single-stage object detector for accurate localization. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengkai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
