<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFFICIENT DOMAIN GENERALIZATION VIA COMMON-SPECIFIC LOW-RANK DECOMPOSITION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-08">April 8, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
							<affiliation key="aff0">
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
							<affiliation key="aff1">
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IIT</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EFFICIENT DOMAIN GENERALIZATION VIA COMMON-SPECIFIC LOW-RANK DECOMPOSITION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-08">April 8, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain generalization refers to the task of training a model which generalizes to new domains that are not seen during training. We present CSD (Common Specific Decomposition), for this setting, which jointly learns a common component (which generalizes to new domains) and a domain specific component (which overfits on training domains). The domain specific components are discarded after training and only the common component is retained. The algorithm is extremely simple and involves only modifying the final linear classification layer of any given neural network architecture. We present a principled analysis to understand existing approaches, provide identifiability results of CSD, and study effect of low-rank on domain generalization. We show that CSD either matches or beats state of the art approaches for domain generalization based on domain erasure, domain perturbed data augmentation, and meta-learning. Further diagnostics on rotated MNIST, where domains are interpretable, confirm the hypothesis that CSD successfully disentangles common and domain specific components and hence leads to better domain generalization. The code and datasets can be found at the following URL: https://github.com/vihari/CSD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the domain generalization (DG) task we are given domain-demarcated data from multiple domains during training, and our goal is to create a model that will generalize to instances from new domains during testing. Unlike in the more popular domain adaptation task <ref type="bibr" target="#b12">Mansour et al. (2009);</ref><ref type="bibr" target="#b2">Ben-David et al. (2006)</ref>; <ref type="bibr" target="#b4">DauméIII et al. (2010)</ref> that explicitly adapts to a fixed target domain, DG requires zero-shot generalization to individual instances from multiple new domains. Domain generalization is of particular significance in large-scale deep learning networks because large training sets often entail aggregation from multiple distinct domains, on which today's high-capacity networks easily overfit. Standard methods of regularization only address generalization to unseen instances sampled from the distribution of training domains, and have been shown to perform poorly on instances from unseen domains. Domain Generalization approaches have a rich history. Earlier methods were simpler and either sought to learn feature representations that were invariant across domains <ref type="bibr" target="#b13">Motiian et al. (2017)</ref>; <ref type="bibr" target="#b14">Muandet et al. (2013)</ref>; <ref type="bibr" target="#b7">Ghifary et al. (2015)</ref>; <ref type="bibr" target="#b17">Wang et al. (2019)</ref>, or decomposed parameters into shared and domain-specific components <ref type="bibr" target="#b9">Khosla et al. (2012)</ref>; <ref type="bibr" target="#b10">Li et al. (2017)</ref>. Of late however the methods proposed for DG are significantly more complicated and expensive. A recent favorite is gradient-based meta-learning that trains with sampled domain pairs to minimize either loss on one domain while updating parameters on another domain <ref type="bibr" target="#b1">Balaji et al. (2018)</ref>; <ref type="bibr" target="#b11">Li et al. (2018a)</ref>, or minimizes the divergence between their representations <ref type="bibr" target="#b5">Dou et al. (2019)</ref>. Another approach is to augment training data with domain adversarial perturbations <ref type="bibr" target="#b15">Shankar et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>Our paper starts with analyzing the domain generalization problem in a simple and natural generative setting. We use this setting to provide a principled understanding of existing DG approaches and improve upon prior decomposition methods. We design an algorithm CSD that decomposes only the last softmax parameters into a common component and a low-rank domain-specific component with a regularizer to promote orthogonality of the two parts. We prove identifiability of the shared parameters, which was missing in earlier decomposition-based approaches. We analytically <ref type="bibr">arXiv:2003.12815v2 [cs.</ref>LG] 7 Apr 2020 study the effect of rank in trading off domain-specific noise suppression and domain generalization, which in earlier work was largely heuristics-driven.</p><p>We show that our method is almost an order of magnitude faster than state-of-the-art meta-learning based methods <ref type="bibr" target="#b5">Dou et al. (2019)</ref>, and provides higher accuracy than existing approaches, particularly when the number of domains is large. Our experiments span both image and speech datasets and a large range of training domains (5 to 1000), in contrast to recent DG approaches evaluated only on a few domains. We provide empirical insights on the working of CSD on the rotated MNIST datasets where the domains are interpretable, and show that CSD indeed manages to separate out the generalizable shared parameters while training with simple domain-specific losses. We present an ablation study to evaluate the importance of the different terms in our training objective that led to improvements with regard to earlier decomposition approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The work on Domain Generalization is broadly characterized by four major themes:</p><p>Domain Erasure Many early approaches attempted to repair the feature representations so as to reduce divergence between representations of different training domains. <ref type="bibr" target="#b14">Muandet et al. (2013)</ref> learns a kernel-based domain-invariant representation. <ref type="bibr" target="#b7">Ghifary et al. (2015)</ref> estimates shared features by jointly learning multiple data-reconstruction tasks. <ref type="bibr" target="#b12">Li et al. (2018b)</ref> uses MMD to maximize the match in the feature distribution of two different domains. The idea of domain erasure is further specialized in <ref type="bibr" target="#b17">Wang et al. (2019)</ref> by trying to project superficial (say textural) features out using image specific kernels. Domain erasure is also the founding idea behind many domain adaptation approaches, example <ref type="bibr" target="#b2">Ben-David et al. (2006);</ref><ref type="bibr" target="#b8">Hoffman et al. (2018)</ref>; <ref type="bibr" target="#b6">Ganin et al. (2016)</ref> to name a few.</p><p>Augmentation The idea behind these approaches is to train the classifier with instances obtained by domains hallucinated from the training domains, and thus make the network 'ready' for these neighboring domains. <ref type="bibr" target="#b15">Shankar et al. (2018)</ref> proposes to augment training data with instances perturbed along directions of domain change. A second classifier is trained in parallel to capture directions of domain change. <ref type="bibr" target="#b16">Volpi et al. (2018)</ref> applies such augmentation on single domain data. Another type of augmentation is to simultaneously solve for an auxiliary task. For example, <ref type="bibr" target="#b3">Carlucci et al. (2019)</ref> achieves domain generalization for images by solving an auxiliary unsupervised jig-saw puzzle on the side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Learning/Meta-Training</head><p>A recent popular approach is to pose the problem as a meta-learning task, whereby we update parameters using meta-train loss but simultaneously minimizing meta-test loss <ref type="bibr" target="#b11">Li et al. (2018a)</ref>, <ref type="bibr" target="#b1">Balaji et al. (2018)</ref> or learn discriminative features that will allow for semantic coherence across meta-train and meta-test domains <ref type="bibr" target="#b5">Dou et al. (2019)</ref>. More recently, this problem is being pursued in the spirit of estimating an invariant optimizer across different domains and solved by a form of meta-learning in <ref type="bibr" target="#b0">Arjovsky et al. (2019)</ref>. Meta-learning approaches are complicated to implement, and slow to train.</p><p>Decomposition In these approaches the parameters of the network are expressed as the sum of a common parameter and domain-specific parameters during training. <ref type="bibr">DauméIII (2007)</ref> first applied this idea for domain adaptation. <ref type="bibr" target="#b9">Khosla et al. (2012)</ref> applied decomposition to DG by retaining only the common parameter for inference. <ref type="bibr" target="#b10">Li et al. (2017)</ref> extended this work to CNNs where each layer of the network was decomposed into common and specific low-rank components. Our work provides a principled understanding of when and why these methods might work and uses this understanding to design an improved algorithm CSD. Three key differences are: CSD decomposes only the last layer, imposes loss on both the common and domain-specific parameters, and constrains the two parts to be orthogonal. We show that orthogonality is required for theoretically proving identifiability. As a result, this newer avatar of an old decomposition-based approach surpasses recent, more involved augmentation and meta-learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Our approach is guided by the following assumption about domain generalization settings.</p><p>Assumption: There are common features in the data whose correlation with label is consistent across domains and domain specific features whose correlation with label varies (from positive to negative) across domains. Classifiers that rely on common features generalize to new unseen domains far better than those that rely on domain specific features.</p><p>Note that we make no assumptions about a) the domain predictive power of common features and b) the net correlation between domain specific features and the label. Let us consider the following simple example which illustrates these points. There are D training domains and examples (x, y) from domain i ∈ [D] are generated as follows:</p><formula xml:id="formula_0">x = y(e c + β i e s ) + N (0, Σ i ) ∈ R m , ∀ i ∈ [D]</formula><p>(1)</p><p>where y = ±1 with equal probability, m is the dimension of the training examples, e c ∈ R m is a common feature whose correlation with the label is constant across domains and e s ⊥ e c ∈ R m is a domain specific feature whose correlation with the label, given by the coefficients β i , varies from domain to domain. In particular, for each domain i, <ref type="bibr">2]</ref>. Note that though β i vary from positive to negative across various domains, there is a net positive correlation between e s and the label y. N (0, Σ i ) denotes a standard normal random variable with mean zero and covariance matrix Σ i . Since Σ i varies across domains, every feature captures some domain information. We note that the assumption e s ⊥ e c is indeed restrictive -we use it here only to make the discussion and expressions simpler. We relax this assumption later in this section when we discuss the identifiability of domain generalizing classifier.</p><formula xml:id="formula_1">suppose β i ∼ Unif [−1,</formula><p>Our assumption at the beginning of this section (which envisages the possibility of seeing β i / ∈ [−1, 2] at test time) implies that the only classifier that generalizes to new domains is one that depends solely on e c 1 . Consider training a linear classifier on this dataset. We will describe the issues faced by existing domain generalization methods.</p><p>• Empirical risk minimization (ERM): When we empirically train a linear classifier using ERM with cross entropy loss on all of the training data, the resulting classifier puts significant nonzero weight on the domain specific component e s . The reason for this is that there is a bias in the training data which gives an overall positive correlation between e s and the label.</p><p>• Domain erasure <ref type="bibr" target="#b6">Ganin et al. (2016)</ref>: Domain erasure methods seek to extract features that have the same distribution across different domains and construct a classifier using those features. The difference in noise variance in (1) across domains means that all features have domain signal. In fact, linear classifiers on any feature can obtain nontrivial domain classification accuracy. The premise of domain erasure methods, that there exist features which have high prediction power of the label but do not capture domain information, does not apply in this setting and domain erasure methods do not perform well.</p><p>• Domain adversarial perturbations <ref type="bibr" target="#b15">Shankar et al. (2018)</ref>: Domain adversarial perturbations seek to augment the training dataset with adversarial examples obtained using domain classification loss, and train a classifier on the resulting augmented dataset. Since the common component e c has domain signal, the adversarial examples will induce variation in this component and so the resulting classifier puts less weight on the common component.</p><p>• Meta-learning: Meta-learning based DG approaches such as <ref type="bibr" target="#b5">Dou et al. (2019)</ref> work with pairs of domains. Parameters updated using gradients on loss of one domain, when applied on samples of both domains in the pair should lead to similar class distributions. If the method used to detect similarity is robust to domain-specific noise, meta-learning methods could work well in this setting. But meta-learning methods require second order gradient updates, and/or are generally considered expensive to implement.</p><p>Decomposition based approaches <ref type="bibr" target="#b9">Khosla et al. (2012)</ref>; <ref type="bibr" target="#b10">Li et al. (2017)</ref>: Decomposition based approaches rely on the observation that for problems like (1), there exist good domain specific classifiers w i , one for each domain i, such that:w</p><formula xml:id="formula_2">i = e c + γ i e s ,<label>(2)</label></formula><p>where γ i is a function of β i . Note that all these domain specific classifiers share the common component e c which is the domain generalizing classifier that we are looking for! If we are able to find domain specific classifiers of the form (2), we can extract e c from them. This idea can be extended to a generalized version of (1), where the latent dimension of the domain space is k i.e., say</p><formula xml:id="formula_3">x = y(e c + k j=1 β i,j e sj ) + N (0, Σ i ).<label>(3)</label></formula><p>e sj ⊥ e c ∈ R m , and e sj ⊥ e s for j, ∈ [k], j = are domain specific features whose correlation with the label, given by the coefficients β i,j , varies from domain to domain. In this setting, there exist good domain specific classifiersw i such that:w i = e c + E s γ i 1 Note that this last statement relies on the assumption that ec ⊥ es. If this is not the case, the correct domain generalizing classifier is the component of ec that is orthogonal to es i.e., ec − ec,es es 2 · es. See <ref type="formula" target="#formula_8">(6)</ref>.</p><p>where e c ∈ R m is a domain generalizing classifier, E s = [e s1 e s2 · · · e s k ] ∈ R m×k consists of domain specific components and γ i ∈ R k is a domain specific combination of the domain specific components that depends on β i,j for j = 1, · · · , k. With this observation, the algorithm is simple to state: train domain specific classifiersw i that can be represented asw</p><formula xml:id="formula_4">i = w c + W s γ i ∈ R m .<label>(4)</label></formula><p>Here the training variables are w c ∈ R m , W s ∈ R m×k and γ i ∈ R k . After training, discard all the domain specific components W s and γ i and return the common classifier w c . Note that (4) can equivalently be written as</p><formula xml:id="formula_5">W = w c 1 + W s Γ ,<label>(5)</label></formula><p>where W := [w 1w2 · · ·w D ], 1 ∈ R D is the all ones vector and Γ :</p><formula xml:id="formula_6">= [γ 1 γ 2 · · · γ D ].</formula><p>This framing of the decomposition approach, in the context of simple and concrete examples as in <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_3">(3)</ref>, lets us understand the three main aspects that are not properly addressed by prior works in this space: 1) identifiability of w c , 2) choice of low rank and 3) extension to non-linear models such as neural networks.</p><p>Identifiability of the common component w c : None of the prior decomposition based approaches investigate identifiability of w c . In fact, given a general matrix W which can be written as w c 1 + W s Γ , there are multiple ways of decomposing W into this form, so w c cannot be uniquely determined by this decomposition alone. For example, given a decomposition <ref type="formula" target="#formula_5">(5)</ref>, for any (k + 1)</p><formula xml:id="formula_7">× (k + 1) invertible matrix R, we can write W = [w c W s ] R −1 R[1 Γ] .</formula><p>As long as the first row of R is equal to [1 0 · · · 0], the structure of the decomposition <ref type="formula" target="#formula_5">(5)</ref> is preserved while w c might no longer be the same. Out of all the different w c that can be obtained this way, which one is the correct domain generalizing classifier? In the setting of <ref type="formula" target="#formula_3">(3)</ref>, where e c ⊥ E s , we proposed that the correct domain generalizing classifier is w c = e c . In the setting where e c ⊥ E s , we propose that the correct domain generalizing classifier is the projection of e c onto the space orthogonal to Span (E s ) i.e.,</p><formula xml:id="formula_8">w c = e c − P Es e c ,<label>(6)</label></formula><p>where P Es is the projection matrix onto the span of the domain specific vectors e s . The following lemma characterizes this condition in terms of the decomposition <ref type="formula" target="#formula_5">(5)</ref>. Proof.</p><formula xml:id="formula_9">Lemma 1. Suppose W := e c 1 + E sΓ = w c 1 + W s Γ is a rank-(k + 1) matrix, where E s ∈ R m×k ,Γ ∈ R D×k , W s ∈ R m×k</formula><formula xml:id="formula_10">If direction: Suppose w c ⊥ Span (W s ). Then, W w c = e c , w c · 1 +Γ · E s w c = w c 2 · 1.</formula><p>Since W is a rank-(k + 1) matrix, we know that 1 / ∈ Span Γ and so it has to be the case that e c , w c = w c 2 and E s w c = 0.</p><p>Both of these together imply that w c is the projection of e c onto the space orthogonal to E s i.e., w c = e c − P Es e c .</p><p>Only if direction: Let w c = e c − P Es e c . Then e c 1 − w c 1 + E sΓ = P Es e c 1 + E sΓ is a rank-k matrix and can be written as</p><formula xml:id="formula_11">W s Γ with Span (W s ) = Span (E s ). Since w c ⊥ Span (E s ), we also have w c ⊥ Span (W s ).</formula><p>So we train for classifiers (5) satisfying w c ⊥ Span (W s ).</p><p>Why low rank?: An important choice in the decomposition approaches is the low rank of the decomposition <ref type="formula" target="#formula_5">(5)</ref>, which in prior works was justified heuristically, by appealing to number of parameters. We prove the following result, which gives us a more principled reason for the choice of low rank parameter k.</p><formula xml:id="formula_12">Theorem 1. Given any matrix W ∈ R m×D , the minimizers of the function f (w c , W s , Γ) = W − w c 1 − W s Γ 2 F , where W s ∈ R m×k and w c ⊥ Span (W s )</formula><p>can be computed by the following steps:</p><formula xml:id="formula_13">• w c ← 1 D W · 1. • W s , Γ ← Top-k SVD W − w c 1 . • w new c ← 1 (wc1 +WsΓ ) + 1 2 w c 1 + W s Γ + 1. • W new s Γ new ← w c 1 + W s Γ − w new c 1 . • Output w new c , W new s , Γ new</formula><p>The proof of this theorem is similar to that of the classical low rank approximation theorem of Eckart-Young-Mirsky, and is presented in the supplementary material. As special cases of the above result, we see that for k = 0, we just obtain the average classifier over all domains</p><formula xml:id="formula_14">w c = 1 D W · 1, while for k = D − 1, we obtain w c = W + 1/ W + 1 2 . When W = w c 1 + W s Γ + N ,</formula><p>where N is a noise matrix (for example due to finite samples), both extremes k = 0 and k = D − 1 have different advantages/drawbacks:</p><p>• k = 0: The averaging effectively reduces the noise component N but ends up retaining some domain specific components if there is net correlation with the label in the training data.</p><p>• k = D − 1: The pseudoinverse effectively removes domain specific components and retains only the common component (by Theorem 1). However, the pseudoinverse does not reduce noise to the same extent as a plain averaging would (since empirical mean is often asymptotically the best estimator for mean).</p><p>In general, the sweet spot for k lies between 0 and D − 1 and its precise value depends on the dimension and magnitude of the domain specific components as well as the magnitude of noise. In our implementation, we perform cross validation to choose a good value for k but also note that the performance of our algorithm is relatively stable with respect to this choice in Section 4.3.</p><p>Extension to neural networks: Finally, prior works extend this approach to non-linear models such as neural networks by imposing decomposition of the form <ref type="formula" target="#formula_5">(5)</ref> for parameters in all layers separately. This increases the size of the model significantly and leads to worse generalization performance. Further, it is not clear whether any of the insights we gained above for linear models continue to hold when we include non-linearities and stack them together. So, we propose the following two simple modifications instead:</p><p>• enforcing the structure (5) only in the final linear layer, as opposed to the standard single softmax layer, and</p><p>• including a loss term for predictions of common component, in addition to the domain specific losses, both of which encourage learning of features with common-specific structure.</p><p>Our experiments (Section 4.2) show that these modifications (orthogonality, changing only the final linear layer and including common loss) are instrumental in making decomposition methods state of the art for domain generalization. Our overall training algorithm below details the steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm CSD</head><p>Our method of training neural networks for domain generalization appears as Algorithm 1 and is called CSD for Common-specific Decomposition.The analysis above was for the binary setting, but we present the algorithm for the multi-class case with C = # classes. The only extra parameters that CSD requires, beyond normal feature parameters θ and softmax parameters w c ∈ R C×m , are the domain-specific low-rank parameters W s ∈ R C×m×k and γ</p><formula xml:id="formula_15">i ∈ R k , for i ∈ [D].</formula><p>Here m is the representation size in the penultimate layer. Thus, Γ = [γ 1 , . . . , γ D ] can be viewed as a domain-specific embedding matrix of size k × D. Note that unlike a standard mixture of softmax, the γ i values are not required to be on the simplex. Each training instance consists of an input x, true label y, and a domain identifier i from 1 to D. Its domain-specific softmax parameter is computed by</p><formula xml:id="formula_16">w i = w c + W s γ i .</formula><p>Instead of first computing the full-rank parameters and then performing SVD, we directly compute the low-rank decomposition along with training the network parameters θ. For this we add a weighted combination of these three terms in our training objective:</p><p>(1) Orthonormality regularizers to make w c [y] orthogonal to domain-specific W s [y] softmax parameters for each label y and to avoid degeneracy by controlling the norm of each softmax parameter to be close to 1.</p><p>(2) A cross-entropy loss between y and distribution computed from the w i parameters to train both the common and low-rank domain-specific parameters.</p><p>(3) A cross-entropy loss between y and distribution computed from the w c parameters. This loss might appear like normal ERM loss but when coupled with the orthogonality regularizer above it achieves domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synthetic setting: comparing CSD with ERM</head><p>We use the data model proposed in Equation 1 to simulate multi-domain training data with D = 10 domains and m = 2 features. For each domain, we sample β i uniformly from -1, 2 and σ ij uniformly from 0, 1. We set e c = [1, 0] and e s = [0, 1]. We sample 100 data points for each domain using its corresponding values: β i , Σ i . We then fit the </p><formula xml:id="formula_17">w i ← w c + W s γ i 9:</formula><p>loss += L(G θ (x), y; w i ) + λL(G θ (x), y; w c ) 10: end for 11: Optimize loss+κR wrt θ, w c , W s , γ i 12: Return θ, w c for inference parameters of a linear classifier with log loss using either standard expected risk minimization (ERM) estimator or CSD .</p><p>The scaled solution obtained using ERM is [1, 0.2] and [1, 0.03] using CSD with high probability from ten runs. As expected, the solution of ERM has a positive but small coefficient on the second component due to the net positive correlation on this component. CSD on the other hand correctly decomposed the common component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare our method with three existing domain generalization methods: (1) MASF Dou et al. <ref type="formula" target="#formula_2">(2019)</ref> is a recently proposed meta-learning based strategy to learn domain-invariant features.</p><p>(2) CG: As a representative of methods that augment data for domain generalization we compare with <ref type="bibr" target="#b15">Shankar et al. (2018)</ref>, and (3) LRD: the low-rank decomposition approach of <ref type="bibr" target="#b10">Li et al. (2017)</ref> but only at the last softmax layer. Our baseline is standard expected risk minimization (ERM) using cross-entropy loss that ignores domain boundaries altogether.</p><p>We evaluate on five different datasets spanning image and speech data types and varying number of training domains. We assess quality of domain generalization as accuracy on a set of test domains that are disjoint from the set of training domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment setup details</head><p>We use ResNet-18 to evaluate on rotated image tasks, LeNet for Handwritten Character datasets, and a multi-layer convolution network similar to what was used for Speech tasks in <ref type="bibr" target="#b15">Shankar et al. (2018)</ref>. We added a layer normalization just before the final layer in all these networks since it helped generalization error on all methods, including the baseline. CSD is relatively stable to hyper-parameter choice, we set the default rank to 1, and parameters of weighted loss to λ = 1 and κ = 1. These hyper-parameters along with learning rates of all other methods as well as number of meta-train/meta-test domains for MASF and step size of perturbation in CG are all picked using a task-specific development set. Further, we scale Γ using sigmoid activation.</p><p>Handwritten character datasets: In these datasets we have characters written by many different people, where the person writing serves as domain and generalizing to new writers is a natural requirement. Handwriting datasets are challenging since it is difficult to disentangle a person's writing style from the character (label), and methods that attempt to erase domains are unlikely to succeed. We have two such datasets.</p><p>(1) The LipitK dataset 2 earlier used in <ref type="bibr" target="#b15">Shankar et al. (2018)</ref> is a Devanagari Character dataset which has classification over 111 characters (label) collected from 106 people (domain). We train three different models on each of 25, 50, and 76 domains, and test on a disjoint set of 20 domains while using 10 domains for validation.</p><p>(2) Nepali Hand Written Character Dataset (NepaliC) 3 contains data collected from 41 different people on consonants as the character set which has 36 classes. Since the number of available domains is small, in this case we create a fixed split of 27 domains for training, 5 for validation and remaining 9 for testing.</p><p>We use LeNet as the base classifier on both the datasets.</p><p>In  <ref type="bibr">LRD Li et al. (2017)</ref> 76.2 (0.7) 83.2 (0.4) 84.4 (0.2) 82.5 (0.5) CG <ref type="bibr" target="#b15">Shankar et al. (2018)</ref>   baseline (ERM), and all three existing methods LRD, CG and MASF. The gap between prior decomposition-based approach (LRD) and ours, establishes the importance of our orthogonality regularizer and common loss term. MASF is better than CSD only for 25 domains and as the number of domains increases to 76, CSD's accuracy is 87.3 whereas MASF's is 85.9.</p><p>In terms of training time MASF is 5-10 times slower than CSD, and CG is 3-4 times slower than CSD. In contrast CSD is just 1.1 times slower than ERM. Thus, the increased generalization of CSD incurs little additional overheads in terms of training time compared to existing methods. Speech utterances dataset We use the utterance data released by Google which was also used in <ref type="bibr" target="#b15">Shankar et al. (2018)</ref> and is collected from a large number of subjects 4 . The base classifier and the preprocessing pipeline for the utterances are borrowed from the implementation provided in the Tensorflow examples 5 . We used the default ten (of the 30 total) classes for classification similar to <ref type="bibr" target="#b15">Shankar et al. (2018)</ref>. We use ten percent of total number of domains for each of validation and test.</p><p>The accuracy comparison for each of the methods on varying number of training domains is shown in <ref type="table">Table 2</ref>. We could not compare with MASF since their implementation is only made available for image tasks. Also, we skip comparison with LRD since earlier experiments established that it can be worse than even the baseline. <ref type="table">Table 2</ref> shows that CSD is better than both the baseline and CG on all domain settings. When the number of domains is very large (for example, 1000 in the table), even standard training can suffice since the training domains could 'cover' the variations in the test domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head><p>Fashion-MNIST in-domain out-domain in-domain out-domain ERM 98.3 (0.0) 93.6 (0.7) 89.5 (0.1) 76.5 (0.7) MASF 98.2 (0.1) 93.2 (0.2) 86.9 (0.3) 72.4 (2.9) CSD 98.4 (0.0) 94.7 (0.2) 89.7 (0.2) 78.9 (0.7)  Rotated MNIST and Fashion-MNIST: Rotated MNIST is a popular benchmark for evaluating domain generalization where the angle by which images are rotated is the proxy for domain. We randomly select 6 a subset of 2000 images for MNIST and 10,000 images for Fashion MNIST, the original set of images is considered to have rotated by 0 • and is denoted as M 0 . Each of the images in the data split when rotated by θ degrees is denoted M θ . The training data is union of all images rotated by 15 • through 75 • in intervals of 15 • , creating a total of 5 domains. We evaluate on M 0 , M 90 . In that sense only in this artificially created domains, are we truly sure of the test domains being outside the span of train domains. Further, we employ batch augmentations such as flip left-right and random crop since they significantly improve generalization error and are commonly used in practice. We train using the ResNet-18 architecture. <ref type="table" target="#tab_4">Table 3</ref> compares the baseline, MASF, and CSD on MNIST and Fashion-MNIST. We show accuracy on test set from the same domains as training (in-domain) and test set from 0 • and 90 • that are outside the training domains. Note how the CSD's improvement on in-domain accuracy is insignificant, while gaining substantially on out of domain data. This shows that CSD specifically targets domain generalization. Surprisingly MASF does not perform well at all, and is significantly worse than even the baseline. One possibility could be that the domain-invariance loss introduced by MASF conflicts with the standard data augmentations used on this dataset. To test this, we compared all the methods without such augmentation. We observe that although all numbers have dropped 1-4%, now MASF is showing sane improvements over baseline, but CSD is better than MASF even in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">How does CSD work?</head><p>(a) Beta fit on estimated probabilities of correct class using common common component.</p><p>(b) Beta fit on estimated probabilities of correct class using specialized component. <ref type="figure">Figure 1</ref>: Distribution of probability assigned to the correct class using common or specialized components alone.</p><p>We provide empirical evidence in this section that CSD effectively decomposes common and low-rank specialized components. Consider the rotated MNIST task trained on ResNet-18 as discussed in Section 4. Since each domain differs only in the amount of rotation, we expect W s to be of rank 1 and so we chose k = 1 giving us one common and one specialized component. We are interested in finding out if the common component is agnostic to the domains and see how the specialized component varies across domains.</p><p>We look at the probability assigned to the correct class for all the train instances using only common component w c and using only specialized component W s . For probabilities assigned to examples in each domain using each component, we fit a Beta distribution. Shown in <ref type="figure">Figure 1(a)</ref> is fitted beta distribution on probability assigned using w c and <ref type="figure">Figure 1(b)</ref> for w s . Note how in <ref type="figure">Figure 1(a)</ref>, the colors are distinguishable, yet are largely overlapping. However in <ref type="figure">Figure 1(b)</ref>, notice how modes corresponding to each domain are widely spaced, moreover the order of modes and spacing between them cleanly reflects the underlying degree of rotation from 15 • to 75 • .</p><p>These observations support our claims on utility of CSD for low-rank decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head><p>In this section we study the importance of each of the three terms in CSD's final loss: common loss computed from w c (L c ), specialized loss (L s ) computed from w i that sums common (w c ) and domain-specific parameters (W s , Γ), orthonormal loss (R) that makes w c orthogonal to domain specific softmax (Refer: Algorithm 1). In <ref type="table">Table 5</ref>, we demonstrate the contribution of each term to CSD loss by comparing accuracy on LipitK with 76 domains.</p><p>Common Specialized Orthonormality Accuracy <ref type="table">Table 5</ref>: Ablation analysis on CSD loss using LipitK <ref type="formula" target="#formula_8">(76)</ref> The first row is the baseline with only the common loss. The second row shows prior decomposition methods that imposed only the specialized loss without any orthogonality or a separate common loss. This is worse than even the baseline. This can be attributed to decomposition without identifiability guarantees thereby losing part of w c when the specialized W s is discarded. Using orthogonal constraint, third row, fixes this ill-posed decomposition however, understandably, just fixing the last layer does not gain big over baseline. Using both common and specialized loss even without orthogonal constraint showed some merit, perhaps because feature sharing from common loss covered up for bad decomposition. Finally, fixing this bad decomposition with orthogonality constraint and using both common and specialized loss constitutes our CSD algorithm and is significantly better than any other variant.</p><formula xml:id="formula_18">loss L c loss L s regularizer R Y N N 85.5 (.7) N Y N 84.4 (.2) N Y Y 85.3 (.1) Y N Y 85.7 (.4) Y Y N 85.8 (.6) Y Y Y 87.3 (.3)</formula><p>This empirical study goes on to show that both L c and R are important. Imposing L c with w c does not help feature sharing if it is not devoid of specialized components from bad decomposition. A good decomposition on final layer without L c does not help generalize much. <ref type="table">Table 6</ref> shows accuracy on various speech tasks with increasing k controlling the rank of the domain-specific component. Rank-0 corresponds to the baseline ERM without any domain-specific part. We observe that accuracy drops with increasing rank beyond 1 and the best is with k = 1 when number of domains D ≤ 100. As we increase D to 200 domains, a higher rank (4) becomes optimal and the results stay stable for a large range of rank values. This matches our analytical understanding resulting from Theorem 1 that we will be able to successfully disentangle only those domain specific components which have been observed in the training domains, and using a higher rank will increase noise in the estimation of w c . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Importance of Low-Rank</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We considered a natural multi-domain setting and looked at how standard classifier could overfit on domain signals and delved on efficacy of several other existing solutions to the domain generalization problem. Motivated by this simple setting, we developed a new algorithm called CSD that effectively decomposes classifier parameters into a common part and a low-rank domain-specific part. We presented a principled analysis to provide identifiability results of CSD while delineating the underlying assumptions. We analytically studied the effect of rank in trading off domain-specific noise suppression and domain generalization, which in earlier work was largely heuristics-driven.</p><p>We empirically evaluated CSD against four existing algorithms on five datasets spanning speech and images and a large range of domains. We show that CSD generalizes better and is considerably faster than existing algorithms, while being very simple to implement. In future, we plan to investigate algorithms that combine data augmentation with parameter decomposition to gain even higher accuracy on test domains that are related to training domains. We present comparisons in <ref type="table" target="#tab_8">Table 7</ref> of our method with <ref type="bibr" target="#b3">Carlucci et al. (2019)</ref> and baseline while using their implementation for a fair comparison. The baseline network is ResNet-18. We perform almost the same or slightly better than image specific JiGen <ref type="bibr" target="#b3">Carlucci et al. (2019)</ref>. We stay away from comparing with <ref type="bibr" target="#b5">Dou et al. (2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient Domain Generalization via</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Proof of Theorem 1</head><p>Proof of Theorem 1. The high level outline of the proof is to show that the first two steps obtain the best rank-(k + 1) approximation of W such that the row space contains 1. The last two steps retain this low rank approximation while ensuring that w new c ⊥ Span (W new s ). The proof that the first two steps obtain the best rank-(k + 1) approximation follows that of the classical low rank approximation theorem of Eckart-Young-Mirsky. We first note that the minimization problem of f under the given constraints, can be equivalently written as:</p><formula xml:id="formula_19">min W W − W 2 F s.t. rank( W ) ≤ k + 1 and 1 ∈ Span W .<label>(7)</label></formula><p>Let w := 1 D W 1 and W − w1 = U Σ V be the SVD of W − w1 . Since W − w1 1 = 0, we have that 1 ⊥ Span V . We will first show that W * : Optimality in operator norm: Fix any W satisfying the conditions in (7). Since 1 ∈ Span W and rank W = k + 1, there is a unit vector v ∈ Span V k+1 such that v ⊥ Span W . Let v = V k+1 x. Since v = 1 and V is an orthonormal matrix, we have i x 2 i = 1. We have:</p><formula xml:id="formula_20">= w1 + U k Σ k V k , where U k Σ k V k is the top-k component of U Σ V ,</formula><formula xml:id="formula_21">W − W 2 2 ≥ (W − W ) v 2 = W v 2 = W V k+1 x 2 = U k+1 Σ k+1 x 2 = k+1 i=1 σ 2 i · x 2 i ≥σ 2 k+1 = W − w1 2 .</formula><p>This proves the optimality in operator norm.</p><p>Optimality in Frobenius norm: Fix again any W satisfying the conditions in <ref type="formula" target="#formula_19">(7)</ref>. Let σ i (A) denote the i th largest singular value of A and A i denote the best rank-i approximation to A. Let W := W − W . We have that:</p><formula xml:id="formula_22">σ i (W ) = W − W i−1 = W − W i−1 + W − W ≤ W + W − W i−1 − W = W − W i−1 − W ≥ min W W − W ,</formula><p>where the minimum is taken over all W such that rank W ≤ i + k, 1 ∈ Span W . Picking W = w1 + U k+i−1 Σ k+i−1 V k+i−1 , we see that σ i (W − W ) ≥ σ k+i . It follows from this that W − W</p><formula xml:id="formula_23">F ≥ W − w1 − U k Σ k V k F .</formula><p>For the last two steps, note that they preserve the matrix w c 1 + W s Γ . If w c 1 + W s Γ is the unique way to write w c 1 + W s Γ such that w c ⊥ Span W s , then we see that w c w c 1 + W s Γ = w c 2 1 , meaning that w c = w c 2 w c 1 + W s Γ + 1. This proves the theorem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and Γ ∈ R D×k are all rank-k matrices with k &lt; m, D. Then, w c = e c − P Es e c if and only if w c ⊥ Span (W s ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>75.3 (0.5) 83.8 (0.3) 85.5 (0.3) 82.6 (0.5) MASF Dou et al. (2019) 78.5 (0.5) 84.3 (0.3) 85.9 (0.3) 83.3 (1.6) CSD (Ours) 77.6 (0.4) 85.1 (0.6) 87.3 (0.4) 84.1 (0.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>W satisfying the conditions in (7). Let σ i denote the i th largest singular value of U Σ V .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Common-Specific Low-Rank Decomposition (CSD ) 1: Given: D, m, k, C, λ, κ,train-data 2: Initialize params w c ∈ R C×m , W s ∈ R C×m×k 3: Initialize γ i ∈ R k : i ∈ [D] 4: Initialize params θ of feature network G θ : X → R m</figDesc><table><row><cell>5:Ŵ = [w T c , W T s ] T 6: R ← C y=1 I k+1 −Ŵ [y] TŴ [y] 2 F</cell><cell>Orthonormality constraint</cell></row><row><cell>7: for (x, y, i) ∈ train-data do</cell><cell></cell></row><row><cell>8:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>LipitK</cell><cell>NepaliC</cell></row></table><note>we show the accuracy using different methods for different number of training domains on the LipitK dataset, and on the Nepali dataset. We observe that across all four models CSD provides significant gains in accuracy over the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our method on two handwritting datasets: LipitK and NepaliC. For LipitK since number of available training domains is large we also report results with increasing number of domains. The numbers are average (and standard deviation) from three runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Accuracy comparison on speech utterance data with varying number of training domains. The numbers are average (and standard deviation) from three runs.</figDesc><table><row><cell>Method</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>1000</cell></row><row><cell>ERM</cell><cell cols="4">72.6 (.1) 80.0 (.1) 86.8 (.3) 90.8 (.2)</cell></row><row><cell>CG</cell><cell cols="4">73.3 (.1) 80.4 (.0) 86.9 (.4) 91.2 (.2)</cell></row><row><cell>CSD</cell><cell cols="4">73.7 (.1) 81.4 (.4) 87.5 (.1) 91.3 (.2)</cell></row><row><cell>Table 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">MNIST</cell></row><row><cell></cell><cell cols="2">in-domain out-domain</cell></row><row><cell>ERM</cell><cell>97.7 (0.)</cell><cell>89.0 (.8)</cell></row><row><cell>MASF</cell><cell>97.8 (0.)</cell><cell>89.5 (.6)</cell></row><row><cell>CSD</cell><cell>97.8 (0.)</cell><cell>90.8 (.3)</cell></row><row><cell>: Performance comparison on rotated MNIST and rotated</cell><cell></cell><cell></cell></row><row><cell>Fashion-MNIST, shown are the in-domain and out-domain accu-</cell><cell></cell><cell></cell></row><row><cell>racies averaged over three runs along with standard deviation in</cell><cell></cell><cell></cell></row><row><cell>the brackets.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: In-domain and out-domain accuracies</cell></row><row><cell>on rotated MNIST without batch augmentations.</cell></row><row><cell>Shown are average and standard deviation from</cell></row><row><cell>three runs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Effect of rank constraint (k) on test accuracy for Speech task with varying number of train domains.</figDesc><table><row><cell>Rank k</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>0</cell><cell cols="3">72.6 (.1) 80.0 (.1) 86.8 (.3)</cell></row><row><cell>1</cell><cell cols="3">74.1 (.3) 81.4 (.4) 87.3 (.5)</cell></row><row><cell>4</cell><cell cols="3">73.7 (.1) 80.6 (.7) 87.5 (.1)</cell></row><row><cell>9</cell><cell cols="3">73.0 (.6) 80.1 (.5) 87.5 (.2)</cell></row><row><cell>24</cell><cell cols="3">72.3 (.2) 80.5 (.4) 87.4 (.3)</cell></row><row><cell>Table 6:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>PACS 7 is a popular domain generalization benchmark. The dataset in aggregate contains around 10,000 images from seven object categories collected from 4 different sources: Photo, Art, Cartoon and Sketch. Evaluation using this dataset trains on three of four sources and tests on left out domain. This setting is challenging since it tests generalization to a radically different target domain. Despite being a very popular dataset, evaluations using this dataset is laced with several inconsistent or unfair comparisons.<ref type="bibr" target="#b3">Carlucci et al. (2019)</ref> uses ten percent of train data for validation (albeit different from the official split), while Dou et al. (2019), Balaji et al. (2018) do not use any validation split. Carlucci et al. (2019), Dou et al. (2019) use data augmentation techniques while Balaji et al. (2018)do not. Also, since the dataset is small and target domain is very far from source domains, the results are sensitive to optimization parameters such as learning rate, optimizer, learning rate schedule. As a result, the comparisons using this dataset across different implementations are rendered useless.Table 1of<ref type="bibr" target="#b3">Carlucci et al. (2019)</ref> highlights these differences even in baseline across different implementations; With such widely differing baseline numbers it is hard to compare across different methods. For this reason, we relegate evaluation of CSD on this dataset to supplementary material accompanied by this word of caution.</figDesc><table><row><cell>Common-Specific Low-Rank Decomposition</cell></row><row><cell>(Appendix)</cell></row><row><cell>6 Evaluation on PACS dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>since their reported numbers from different implementation, which is unavailable for ResNet-18, could have different baseline number. Comparison between JiGen Carlucci et al. (2019) and CSD (ours) using PACS datset with ResNet-18 architecture. The header of each column identifies the target domain.</figDesc><table><row><cell cols="2">Method Photo</cell><cell cols="4">Art Cartoon Sketch Average</cell></row><row><cell>ERM</cell><cell cols="2">95.73 77.85</cell><cell>74.86</cell><cell>67.74</cell><cell>79.05</cell></row><row><cell>JiGen</cell><cell cols="2">96.03 79.42</cell><cell>75.25</cell><cell>71.35</cell><cell>80.41</cell></row><row><cell>CSD</cell><cell cols="2">95.45 79.79</cell><cell>75.04</cell><cell>72.46</cell><cell>80.69</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://lipitk.sourceforge.net/datasets/dvngchardata.htm 3 https://www.kaggle.com/ashokpant/devanagari-character-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html 5 https://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/examples/speech_ commands</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The earlier work on this dataset however lacks standardization of splits, train sizes, and baseline network across the various papers<ref type="bibr" target="#b15">Shankar et al. (2018)</ref> <ref type="bibr" target="#b17">Wang et al. (2019)</ref>. Hence we rerun experiments using different methods on our split and baseline network.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://domaingeneralization.github.io/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">D. Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metareg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2976456.2976474" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Neural Information Processing Systems, NIPS&apos;06</title>
		<meeting>the 19th International Conference on Neural Information Processing Systems, NIPS&apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
	<note>DauméIII, H. Frustratingly easy domain adaptation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-regularization based semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dauméiii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6447" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms and theory for multiple-source adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8246" to="8256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
	<note>Domain adaptation with multiple sources</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5715" to="5725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Dx7fbCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06256</idno>
		<title level="m">Learning robust representations by projecting superficial statistics out</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
