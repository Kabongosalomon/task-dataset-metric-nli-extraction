<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ClusterFit: Improving Generalization of Visual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ClusterFit: Improving Generalization of Visual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-training convolutional neural networks with weaklysupervised and self-supervised strategies is becoming increasingly popular for several computer vision tasks. However, due to the lack of strong discriminative signals, these learned representations may overfit to the pre-training objective (e.g., hashtag prediction) and not generalize well to downstream tasks. In this work, we present a simple strategy -ClusterFit (CF) to improve the robustness of the visual representations learned during pre-training. Given a dataset, we (a) cluster its features extracted from a pretrained network using k-means and (b) re-train a new network from scratch on this dataset using cluster assignments as pseudo-labels. We empirically show that clustering helps reduce the pre-training task-specific information from the extracted features thereby minimizing overfitting to the same. Our approach is extensible to different pretraining frameworks -weak-and self-supervised, modalities -images and videos, and pre-training tasks -object and action classification. Through extensive transfer learning experiments on 11 different target datasets of varied vocabularies and granularities, we show that CF significantly improves the representation quality compared to the state-ofthe-art large-scale (millions / billions) weakly-supervised image and video models and self-supervised image models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Weak and self-supervised pre-training approaches offer scalability by exploiting free annotation. But there is no free lunch -these methods often first optimize a proxy objective function, for example, predicting image hashtags <ref type="bibr" target="#b30">[31]</ref> or color from grayscale images <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b62">63]</ref>. Similar to supervised pre-training, the underlying assumption (hope) is that this proxy objective function is fairly well aligned with the subsequent transfer tasks, thus optimizing this function could potentially yield suitable pre-trained visual representations. While this assumption holds mostly true in case of fully-supervised pre-training, it may not extend to weak and self-supervision. In the latter pre-training cases, the lack * † Equal Contribution Pre-training method (N pre ) ∆ of CF (N cf ) on transfer Fully-supervised Images §3.2, <ref type="figure">Figure 3b</ref> +2.1% on ImageNet-9K <ref type="bibr" target="#b9">[10]</ref> ResNet-50, ImageNet-1K, 1K labels Weakly-supervised Images §4.1.1, <ref type="table" target="#tab_3">Table 4</ref> +4.6% on ImageNet-9K <ref type="bibr" target="#b9">[10]</ref> ResNet-50, 1B Images, 1.5K hashtags <ref type="bibr" target="#b37">[38]</ref> +5.8% on iNaturalist <ref type="bibr" target="#b54">[55]</ref> Weakly-supervised Videos §4.1.2, <ref type="table" target="#tab_5">Table 5</ref> +3.2% on Kinetics <ref type="bibr" target="#b58">[59]</ref> R(2+1)D- <ref type="bibr">34, 19M</ref> videos, 438 hashtags <ref type="bibr" target="#b19">[20]</ref> +4.3% on Sports1M <ref type="bibr" target="#b31">[32]</ref> Self-supervised Images §4.2, <ref type="table">Tables 6, 8</ref> +7-9% on ImageNet-1K <ref type="bibr" target="#b46">[47]</ref> ResNet-50, 1M images +3-7% mAP on VOC07 <ref type="bibr" target="#b14">[15]</ref> Jigsaw <ref type="bibr" target="#b41">[42]</ref> and RotNet <ref type="bibr" target="#b20">[21]</ref>, Multi-task (appendix E) +3-5% on Places205 <ref type="bibr" target="#b64">[65]</ref> Table 1: A summary of results: We show that ClusterFit (CF) can be applied to a variety of different pre-training methods, modalities, and architectures. We report absolute gains in top-1 accuracy (except for VOC07 where we report mAP). In each setting, CF provides improvements with the same model architecture and without additional data or supervision. of strong discriminative signals may result in an undesirable scenario where the visual representations overfit to the idiosyncrasies of the pre-training task and dataset instead, thereby rendering them unsuitable for transfer tasks. For instance, it was noted in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51]</ref> that factors such as label noise, polysemy (apple the fruit vs. Apple Inc.), linguistic ambiguity, lack of 'visual'ness of tags (e.g. #love) significantly hampered the pre-training proxy objective from being well-aligned with the transfer tasks. Further, the authors of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b63">64]</ref> studied multiple self-supervised methods and observed that, compared to earlier layers, features from the last layer are more "aligned" with the proxy objective, and thus generalize poorly to target tasks.</p><p>In this work, we ask a simple question -is there a way to avoid such overfitting to the proxy objective during weakand self-supervised pre-training? Can we overcome the 'artifacts' of proxy objectives so that the representation is generic and transferable? Our key insight is that smoothing the feature space learned via proxy objectives should help us remove these artifacts and avoid overfitting to the the proxy objective. But how do we smoothen the feature space? Should it be done while optimizing the proxy objective or in a post-hoc manner?</p><p>To this end, we propose a surprisingly simple yet effective framework called ClusterFit (CF). Specifically, given a pre-trained network trained using a proxy objective and a new dataset, we first use the learned feature space to cluster that dataset. Next, we train a new network from scratch on this new dataset using the cluster memberships as pseudo labels <ref type="figure" target="#fig_8">(Figure 1</ref>). We demonstrate that clustering </p><formula xml:id="formula_0">t O Q o M o V i T A i 0 N + z K A K Y d Y D H F F B s G I z T R A W V G e F e I I E w k q 3 V Z i X 0 M p Q / / r y X 9 K r V u x a p X Z V L b X P l n X k w S E 4 A i f A B g 3 Q B p e g A 7 o A g z v w A J 7 A s 3 F v P B o v x u t i N G c s d w 7</formula><p>A D x h v n 3 w V l E w = &lt; / l a t e x i t &gt; <ref type="figure" target="#fig_8">Figure 1</ref>: ClusterFit (CF): We start with a pre-trained network (Npre) that is trained on some pre-training task (not shown). We use this network to extract features and cluster a new dataset D cf using k-means clustering. We show that training a new network N cf from scratch on these cluster assignments as labels results in a more transferable feature representation. of the features helps retain only the essential invariances in them and eliminates proxy objective's artifacts (essentially smoothing the feature space). Re-training on the cluster memberships yields a visually coherent pre-training feature space for downstream tasks. Our approach of feature space smoothing is guided through unsupervised k-means clustering, making it scalable to millions (billions) of videos and images in both weak-and self-supervised pre-training frameworks.</p><p>We take inspiration from recent work in self-supervised learning which aims to learn a smooth visual feature space via clustering and trains representations on the clusters as classes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44]</ref>. While <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> use clustering as the training objective itself, in our work, we investigate the value of post-hoc smoothing. ClusterFit can also be viewed as a variant of knowledge distillation <ref type="bibr" target="#b27">[28]</ref> that distills via 'lossy' clustering, as opposed to the standard setup of using soft targets in original label space.</p><p>ClusterFit demonstrates significant performance gains on a total of 11 public, challenging image and video benchmark datasets. As summarized in <ref type="table">Table 1</ref>, our approach, while extremely simple, consistently improves performance across different pre-training methods, input modalities, network architectures, and benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly Supervised Learning: Training ConvNets on very large, weakly supervised images by defining the proxy tasks using the associated meta-data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b55">56]</ref> has shown tremendous benefits. Proxy tasks include hashtags predictions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b55">56]</ref>, GPS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b57">58]</ref>, search queries prediction <ref type="bibr" target="#b50">[51]</ref>, and word or n-grams predictions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref>. Our approach builds upon these works and shows that even better representations can be trained by leveraging the features from such pre-training frameworks for clustering to mitigate the effect of noise. Yalniz et al. <ref type="bibr" target="#b61">[62]</ref> propose a target task specific noise re-"Standard" Pre-train and Transfer    moval framework by ranking images for each class by their softmax values and retaining only top-K images for retraining. However, their method is specific to a particular target task and discards most of the data during re-training. By contrast, our approach does not adhere to a particular target task and leverages all the data, since, they may contain complementary visual information beyond hashtags. Self-Supervised Learning: Self-supervised approaches typically learn a feature representation by defining a 'pretext' task on the visual domain. These pre-text tasks can either be domain agnostic <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref> or exploit domain-specific information like spatial structure in images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>, color <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>, illumination <ref type="bibr" target="#b13">[14]</ref>, temporal structure <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> or a co-occurring modality like sound <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46]</ref>. In this work, we use two diverse image-based self-supervision approaches -Jigsaw <ref type="bibr" target="#b41">[42]</ref> and RotNet <ref type="bibr" target="#b20">[21]</ref> that have shown competitive performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. Since the difference between pretext tasks and semantic transfer learning tasks is huge, our method shows much larger improvement for self-supervised methods ( §4.2). Our work builds upon <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, who use clustering and pseudo-labels for self-supervised learning and <ref type="bibr" target="#b43">[44]</ref>, who distill predictions from different self-supervised models to a common architecture. Compared to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, ClusterFit does not require any alternate optimization and thus is more stable and computationally efficient. As we show in §4, this property makes ClusterFit easily scalable to different modalities and large-scale data. Compared to <ref type="bibr" target="#b43">[44]</ref>, our focus is not distilling information to a common architecture, but instead to remove the pre-training task biases. This makes ClusterFit applicable broadly to any kind of pretrained models -fully supervised or use noisy supervision ( §3.2), weakly supervised from billions of images or millions of videos ( §4.1), and self-supervised models ( §4.2). Model Distillation: Model distillation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref> typically involves transferring knowledge from a 'teacher' model to a 'student' model by training the student on predictions of the teacher in addition to task labels. These Dataset Label Type # classes Train/Eval Metric Weakly-supervised Images §4.1.1 ImageNet-1K <ref type="bibr" target="#b46">[47]</ref> multi-class object 1000 1.3M/50K top-1 acc ImageNet-9K <ref type="bibr" target="#b9">[10]</ref> multi-class object 9000 10.5M/450K top-1 acc Places365 <ref type="bibr" target="#b64">[65]</ref> multi-class scene 365 1.8M/36.5K top-1 acc iNaturalist 2018 <ref type="bibr" target="#b54">[55]</ref> multi-class object 8142 438K/24K top-1 acc Weakly-supervised Videos §4.1.2 Kinetics <ref type="bibr" target="#b58">[59]</ref> multi-class action 400 246K/20K top-1 acc Sports1M <ref type="bibr" target="#b31">[32]</ref> multi-class action 487 882K/204K top-1 acc Something-Something V1 <ref type="bibr" target="#b22">[23]</ref> multi-class action 174 86K/11.5K top-1 acc Self-supervised Images §4.2 VOC07 <ref type="bibr" target="#b14">[15]</ref> multi-label object 20 5K/5K mAP ImageNet-1K <ref type="bibr" target="#b46">[47]</ref> multi-class object 1000 1.3M/50K top-1 acc Places205 <ref type="bibr" target="#b64">[65]</ref> multi-class scene 205 2.4M/21K top-1 acc iNaturalist 2018 <ref type="bibr" target="#b54">[55]</ref> multi-class object 8142 438K/24K top-1 acc methods are designed to transfer knowledge (not contained in the labels) about the task from the teacher to the student network. Since distillation retains more knowledge about the original task, it performs poorly in the case of weak-supervision ( §4.1). Interestingly, the failure of standard knowledge distillation approaches in the context of self-supervised learning has also been shown in <ref type="bibr" target="#b43">[44]</ref>.</p><formula xml:id="formula_1">Dataset D pre &lt; l a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t e x i t s h a 1 _ b a s e 6 4 = " Q f H s t X F I u A K L 4 2 t A g K O e i h E V p X g = " &gt; A A A B + 3 i c b V A 9 T 8 M w F H w p X 6 V 8 h T K y W L R I T F X S B c Z K M D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>h d + 1 G h 5 V 1 V O E U z u A C X L i E D t x C F 3 p A 4 A m e 4 R X e r N x 6 s d 6 t j + V q x S o 9 J / A H 1 u c P C E e U i w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q f H s t X F I u A K L 4 2 t A g K O e i h E V p X g = " &gt; A A A B + 3 i c b V A 9 T 8 M w F H w p X 6 V 8 h T K y W L R I T F X S B c Z K M D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>h d + 1 G h 5 V 1 V O E U z u A C X L i E D t x C F 3 p A 4 A m e 4 R X e r N x 6 s d 6 t j + V q x S o 9 J / A H 1 u c P C E e U i w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q f H s t X F I u A K L 4 2 t A g K O e i h E V p X g = " &gt; A A A B + 3 i c b V A 9 T 8 M w F H w p X 6 V 8 h T K y W L R I T F X S B c Z K M D</head><formula xml:id="formula_2">v W s A d W g H K 0 v 4 8 = " &gt; A A A B + n i c b V D L S s N A F J 3 U V 6 2 v V J d u B l v B V U l a 7 G N X 1 I X L C r Y V 2 h A m 0 0 k 7 d C Y J M x O l x H y K G x e K u P V L 3 P k 3 T t o i v g 4 M H M 6 5 l 3 v m e B G j U l n W h 5 F b W V 1 b 3 8 h v F r a 2 d 3 b 3 z O J + T 4 a x w K S L Q x a K G w 9 J w m h A u o o q R m 4 i Q R D 3 G O l 7 0 / P M 7 9 8 S I W k Y X K t Z R B y O x g H 1 K U Z K S 6 5 Z L F + 4 y Z A j N R E 8 w X 6 a l l 2 z Z F U s 2 7 J q p 1 A T q 9 V q 1 u F C a T S h r Z U M J b B E x z X f h 6 M Q x 5 w E C j M k 5 c C 2 I u U k S C i K G U k L w 1 i S C O E p G p O B p g H i R D r J P H o K j 7 U y g n 4 o 9 A s U n K v f N x L E p Z x x T 0 9 m I e V v L x P / 8 w a x 8 p t O Q o M o V i T A i 0 N + z K A K Y d Y D H F F B s G I z T R A W V G e F e I I E w k q 3 V Z i X 0 M p Q / / r y X 9 K r V u x a p X Z V L b X P l n X k w S E 4 A i f A B g 3 Q B p e g A 7 o A g z v w A J 7 A s 3 F v P B o v x u t i N G c s d w 7 A D x h v n 3 w V l E w = &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to learn a generalizable feature space for a variety of target tasks that does not overfit to the pre-training proxy objective. We first describe the framework of Clus-terFit (CF) in §3.1. Next, we report a control experiment on the ImageNet-1K dataset that sheds light on how CF combats the 'bias' introduced due to the proxy objective ( §3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ClusterFit Framework</head><p>Our method starts with a ConvNet N pre that is pretrained on a dataset D pre and labels L pre . First, we use the penultimate layer of N pre to extract features from each datapoint belonging to another dataset D cf . Next, we cluster these features using k-means into K groups and treat these cluster assignments as the new categorical 'labels' (L cf ) for D cf . Finally, we fit a different network N cf (initialized from scratch) on D cf that minimizes a cross-entropy objective on L cf . We illustrate these steps in <ref type="figure" target="#fig_8">Figure 1</ref>. We highlight that re-learning N cf from scratch on D cf is completely unsupervised and thus allows leveraging large-scale datasets. Intuition: We hypothesize that ClusterFit (CF) leverages the underlying visual smoothness in the feature space to create visually coherent clusters. We believe that "cluster" followed by "fit" weakens the underlying pre-training objective-specific bias. One may view ClusterFit from an information bottleneck <ref type="bibr" target="#b53">[54]</ref> perspective wherein the 'lossy' clustering step introduces a bottleneck and removes any pretraining proxy objective bias. How to evaluate CF? As in prior efforts <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref>, we use transfer learning performance on downstream tasks to understand whether CF improves generalization of the feature representations. Specifically, to evaluate N pre and N cf , we train linear classifiers on fixed feature representations from the networks on the downstream task and report final performance on held-out data (see <ref type="table" target="#tab_0">Table 2</ref>). <ref type="figure" target="#fig_4">Figure 2</ref> illustrates ClusterFit's setup. We stress that ClusterFit is simple to implement and makes minimal assumptions about input modalities, architectures etc. but provides a powerful way to improve the generalization of the feature space. We explore various design choices ClusterFit offers such as relative properties of N pre , N cf , D pre , and D cf in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Control Experiment using Synthetic Noise</head><p>Here, our goal is to study the extent of generalization of features learned from a 'proxy' pre-training objective in a controlled setup. We start with a supervised pre-training dataset ImageNet-1K <ref type="bibr" target="#b46">[47]</ref>, and add synthetic label noise to it. Our motive behind this setup is to intentionally misalign the pre-training objective with downstream tasks. We acknowledge that the synthetic noise simulated in this experiment is an over simplification of the complex noise present in real world data. Nevertheless, it provides several key insights into ClusterFit as we show next. Control Experiment Setup: To isolate the effect of CF, in this experiment, we fix D pre = D cf = ImageNet-1K and the network architectures N pre and N cf to ResNet-50 <ref type="bibr" target="#b26">[27]</ref>. We start by adding varying amounts (p%) of uniform random label noise 1 to D pre . Next, we train a separate N pre for each fraction p of the noisy labels. We then apply CF (with different values of K in k-means) to each N pre to obtain a corresponding N cf . Finally, we evaluate the representations by training linear classifiers on fixed res5 features on three target image classification datasets -ImageNet-1K, ImageNet-9K, and iNaturalist. We use model distillation <ref type="bibr" target="#b27">[28]</ref> as a baseline to better understand the behavior of ClusterFit.</p><p>Our motivation behind this setup is the following: when p = 0, N pre denotes the true, noise-free supervised task; as p increases, the proxy objective becomes a poorer approximation of the original pre-training objective and allows us to closely inspect ClusterFit. Results and Observations: We report the transfer learning performance of N pre (i.e., before CF) and N cf (i.e., after CF) in <ref type="figure">Figure 3</ref> for different values of label noise p. Let us first consider p = 0, i.e., a setting without any label noise. In this case, N pre is trained on clean labels. On the target dataset ImageNet-1K, N pre performs significantly better than N cf for all values of K ( <ref type="figure">Fig. 3 (a)</ref>). This is expected, since when D pre = D tar = ImageNet-1K, the pre-training and transfer tasks are exactly aligned. However, N cf performs comparably or better than N pre for other target da -ImageNet-9K and iNaturalist at higher values of K. This suggests that CF can improve even fully-supervised rep- Top-1 accuracy on transfer</p><formula xml:id="formula_3">Transfer: T tar = iNaturalist Npre Ncf (K=500) Ncf (K=1500) Ncf (K=5000) Ncf (K=15000)</formula><p>Distill.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Control Experiment:</head><p>We inject uniform label noise in the labels Lpre from Dpre = ImageNet-1K and train a separate ResNet-50 model (Npre) on these noisy labels. We apply ClusterFit on each of these pre-trained models (Npre) and vary the number of clusters K to train N cf . We then study the transfer learning performance of the representations by training a linear classifier on fixed features from Npre or N cf on three target tasks -a noise free ImageNet-1K, ImageNet-9K, and iNaturalist. ClusterFit is able to learn more transferable features despite high amounts of label noise in pre-training. For finer-grained target tasks like ImageNet-9K, ClusterFit can even improve a fully supervised ResNet-50 model (p = 0).</p><p>resentations for more fine-grained downstream tasks. We note that model distillation also provides an improvement over N pre on ImageNet-9K but is worse on iNaturalist.</p><p>Let us now consider scenarios where p &gt; 0. <ref type="figure">Figure 3</ref> indicates that increased label noise (p) in D pre translates to poor performance across all three target tasks. We highlight that the drop in the performance is more drastic for N pre (i.e., before CF), than for N cf (i.e., after CF). More importantly, the performance gap between N cf and N pre continues to increase with p. From <ref type="figure">Fig. 3 (b)</ref> and (c), we observe that N cf consistently outperforms N pre on two target tasks ImageNet-9K and iNaturalist. Notably, for D tar = ImageNet-1K <ref type="figure">(Figure 3 (a)</ref>), when p ≥ 50, N cf outperform N pre , which is pre-trained on noisy ImageNet-1K. Model distillation provides some gains over N pre but is consistently outperformed by ClusterFit.</p><p>These results suggest that as p increases, the proxy objective gets further away from the 'true' pre-training objective, and makes features from N pre less transferable. In those very cases, CF captures useful visual invariances in the feature representations, thereby providing more noiseresilient pseudo-labels for learning transferable representations. Finally, we also note that larger number of clusters K generally leads to better transfer learning performance. The gains are larger for more challenging and fine-grained datasets like ImageNet-9K and iNaturalist. We study the effect of this hyper-parameter K in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now examine the broad applicability of ClusterFit in three different pre-training scenarios for N pre : (a) weaklysupervised pre-training for images ( §4.1.1), (b) weaklysupervised pre-training for videos ( §4.1.2), and (c) selfsupervised pre-training for images ( §4.2). Common CF Setting: Throughout this section, we set D pre = D cf and N pre = N cf (architecture-wise). We train N pre on D pre , N cf on D cf for equal number of epochs. <ref type="table" target="#tab_2">Table 3</ref> summarizes these settings. By keeping the data, architecture, and training schedule constant, we hope to measure the difference in performance between N cf and N pre solely due to ClusterFit. Evaluation: As mentioned in §3.1, we evaluate Cluster-Fit via transfer learning on target tasks. Specifically, we train linear classifiers on the fixed features obtained from the penultimate layer of N pre or N cf on target datasets. The transfer learning tasks are summarized in <ref type="table" target="#tab_0">Table 2</ref>. Baselines: We use the following baselines:</p><p>• N pre : We use features from N pre for transfer learning.</p><p>Since ClusterFit (CF) is applied on N pre to get N cf , this baseline serves to show improvements through CF. • Distillation: To empirically understand the importance of the clustering step in CF, we compare with model distillation <ref type="bibr" target="#b27">[28]</ref>. Unlike CF, distillation transfers knowledge from N pre without clustering, thus retaining more information about the learned features. We train a distilled model using a weighted average of 2 loss functions: (a) cross-entropy with soft targets computed using N pre and temperature T and (b) cross-entropy with image/video labels in weakly-supervised setup. We also experimented with training a network to directly regress the features from N pre but found consistently worse results. • Prototype: ClusterFit uses unsupervised k-means to create pseudo-labels. To understand the effect of this unsupervised step, we add a baseline that uses semantic information during clustering. Under this prototype alignment <ref type="bibr" target="#b48">[49]</ref> baseline, unlike random cluster initialization as done in k-means, we use label information in D cf to initialize cluster centers. Specifically, we first set K equal to the number of 'classes' in D cf . Here, each cluster corresponds to a 'prototype' of that class. We then compute K prototypes by averaging image embeddings of all images belonging to each class. Finally, pseudo-labels are assigned to each data point by finding its nearest 'prototype' cluster center. Since this method uses explicit label   information present in D cf , it requires more 'supervision' than ClusterFit. We also note that this baseline is not applicable to self-supervised methods (suppl. material). • Longer pre-training: Since N cf is trained for the same number of epochs as N pre , we also compare against a network trained on the pre-train task for 2× longer (denoted by N pre 2×). Specifically, N pre 2× is trained for a combined number of epochs as N pre and N cf . By comparing N cf against this baseline, we hope to isolate improvements due to longer pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Weakly-supervised pre-training</head><p>In this section, we study weakly-supervised pre-training on noisy web images and videos. These approaches predict the noisy hashtags associated with images/videos and thus minimize a proxy objective during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Weakly-supervised image pre-training</head><p>Data and Model: As in <ref type="bibr" target="#b37">[38]</ref>, we collect IG-ImageNet-1B dataset of 1B public images associated with hashtags from a social media website. To construct this dataset, we consider images tagged with at least one hashtag that maps to any of the ImageNet-1K synsets. The architecture of N pre and N cf network is fixed to a ResNet-50 <ref type="bibr" target="#b26">[27]</ref>, while D pre = D cf = IG-ImageNet-1B. ClusterFit Details: We extract features from the 2048 dimensional res5 layer from N pre for clustering. N cf is trained from scratch on D cf = IG-ImageNet-1B on the cluster assignments as pseudo-labels. Details on the hyper parameters during pre-training and ClusterFit are provided in the supplementary material. We report results in <ref type="table" target="#tab_3">Table 4</ref>, which we discuss next. Effect of longer pre-training: N pre pre-trained on D pre =IG-ImageNet-1B already exhibits very strong performance on all target datasets. By construction, the label space of the target dataset ImageNet-1K matches with that of D pre . As noted in <ref type="bibr" target="#b37">[38]</ref>, this translates to N pre yielding an impressive top-1 accuracy of 78% on ImageNet-1K.  Features from longer pre-training (N pre 2×) show improvements on ImageNet-1K, ImageNet-9K, and iNaturalist but not on Places365. As noted in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref>, Places365 is not well-aligned with ImageNet-1K (and by extension with IG-ImageNet-1B). Thus, (longer) pre-training yields no benefit. By contrast, the target dataset ImageNet-9K is wellaligned with D pre = IG-ImageNet-1B, thus achieving improvements from longer pre-training.</p><p>Comparison with Model Distillation: Training a student network via distillation, i.e., soft targets provided by the teacher (N pre ) and hashtags, performs worse than N pre itself. In our case, the student and teacher network are of the same capacity (ResNet-50). We believe that the noisy label setting combined with the same capacity student and teacher networks are not ideal for model distillation.</p><p>Comparison with Prototype: Except on ImageNet-1K, the prototype baseline shows improvement over both N pre and N pre 2×. This shows that pseudo-labels derived based on label information can provide a better training objective than hashtags used for pre-training N pre . However, similar to CF, prototype shows a reduction in performance on ImageNet-1K which we explain next. Gains of ClusterFit: N cf achieves substantial gains over the strong N pre model especially on fine-grained datasets like ImageNet-9K (4.6 points) and iNaturalist (5.8 points), at higher values of K. This may be because N cf captures a more diverse and finer-grained visual feature space that benefits fine-grained transfer tasks. We observe a small decrease in the performance on ImageNet-1K (1.5 points) which can be attributed again to the hand-crafted label alignment of the IG-ImageNet-1B with ImageNet-1K. This result is inline with observations from <ref type="bibr" target="#b37">[38]</ref>. We believe the performance decrease of 'prototype' on ImageNet-1K is also due to this reason. N cf shows improved performance than 'prototype,' yet does not use any additional supervision while generating pseudo-labels. Finally, we note that finding an optimal number of clusters K for each transfer learning task is procedurally easier than finding a pre-training task (or label space) that aligns with the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Weakly-supervised video pre-training</head><p>Data and Model: Following <ref type="bibr" target="#b19">[20]</ref>, we collect IG-Verb-19M, a dataset of 19M public videos with hashtags from a social media website. We consider videos tagged with at least one of the 438 verbs from Kinetics <ref type="bibr" target="#b58">[59]</ref> and Verb-Net <ref type="bibr" target="#b56">[57]</ref>. We set D pre = D cf = IG-Verb-19M. We use the clip-based R(2+1)D-34 <ref type="bibr" target="#b7">[8]</ref> architecture for N pre and N cf . Each video clip is generated by scaling its shortest edge to 128 followed by cropping a random patch of size 112×112. We use 32 consecutive frames per video clip, with temporal jittering applied to the input. ClusterFit details: We uniformly sample 6 clips of 32 consecutive frames per video, extract video features per clip, and average pool them. We use the 512 dimensional res5 layer from N pre . We direct the reader to the supplementary material for hyper-parameter details.</p><p>Observations: We present the transfer learning results in <ref type="table" target="#tab_5">Table 5</ref>. Once again, the baseline N pre exhibits strong performance on all target datasets. Longer pretraining (N pre 2×) provides limited benefit on Kinetics and Sports1M, and loses performance compared to N pre on Sth-Sth V1. As observed in §4.1.1, model distillation performs worse than N pre on all target datasets. We observe that CF (N cf ) provides significant improvements of 3.2 -4.3% across all the datasets over N pre . The optimal number of clusters K vary depending on each dataset, but is typically an order of magnitude higher than the size of the original label space (i.e., 438 verbs in IG-Verb-19M). For example, performance does not saturate for Kinetics even at K = 6400. We study the effect of K in §5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Self-Supervised pre-training for Images</head><p>We now apply ClusterFit framework to self-supervised methods. We study two popular and diverse self-supervised methods -Jigsaw <ref type="bibr" target="#b41">[42]</ref> and RotNet <ref type="bibr" target="#b20">[21]</ref>. These methods do not use semantic labels and instead create pre-training labels using a 'pre-text' task such as rotation. As mentioned in §2 and <ref type="bibr" target="#b43">[44]</ref>, distillation is not a valid baseline for these self-supervised methods (more in supplementary material). Also, as these methods do not use semantic label information, 'prototype' is also not a valid baseline. Data and Model: We fix the network architectures of N pre and N cf to ResNet-50. We also fix D pre = D cf = ImageNet-1K to pre-train Jigsaw and RotNet models (N pre ). We discard the semantic labels and use only images from both tasks. We use the models released by <ref type="bibr" target="#b21">[22]</ref> for Jigsaw and train RotNet models following the approach in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. ClusterFit Details: We set K = 16, 000. N cf is trained for the same number of epochs as the pre-trained selfsupervised network N pre . We strictly follow the training hyper parameters and the transfer learning setup outlined in Goyal et al. <ref type="bibr" target="#b21">[22]</ref>. We report additional results for different values of K in the supplemental material. Layer-wise transfer: In <ref type="figure">Figure 4</ref>, we report the transfer learning performance of each layer of N pre and compare with N cf after applying ClusterFit. We see that for the pretrained network N pre , res5 features transfer poorly com-  <ref type="table">Table 6</ref>: Self-supervised methods: We apply ClusterFit to selfsupervised methods and evaluate them following the setup in <ref type="bibr" target="#b21">[22]</ref> on four datasets by training a linear classifier on fixed features. All methods use the ResNet-50 architecture for Npre and N cf . We report the performance of the best performing layer for each method and use the mean Average Precision (mAP) metric for the VOC07 dataset and top-1 accuracy for all other datasets.</p><p>pared to res4 features. For example, on VOC07 dataset, linear classifiers trained on res4 perform ∼ 3-10 points better than those trained on res5 for both Jigsaw and RotNet networks. As noted in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b63">64]</ref>, this is because the final layer features overfit to the pre-training ('pre-text') task.</p><p>After applying ClusterFit, we see that features of N cf transfer better across all the layers except for conv1-an improvement of 7 to 9 points on ImageNet-1K-for both Jigsaw and RotNet methods. On VOC07, res5 features transfer better than res4: for N pre the gap is −9 points while for N cf it is about +1 points. On ImageNet-1K and Places205, the performance gap of N cf when using res4 vs. res5 features is considerably reduced. This strongly suggests that ClusterFit reduces the overfitting of res5 features to the pre-text task, thus making them generalize better. Results: We show additional transfer learning results in Table 6. Longer pre-training (N pre 2×) shows mixed resultsa small drop in performance for Jigsaw and a small increase in performance for RotNet. ClusterFit provides consistent improvements on both Jigsaw and RotNet tasks, across all pre-training and target tasks. We achieve significant boosts of 3-5 points on Places205 and 5-8 points on iNaturalist. Easy multi-task Learning using ClusterFit: In appendix E, we show that ClusterFit can be easily applied to combine multiple different self-supervised methods and provides impressive gains of more than 8 points on ImageNet-1K in top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary:</head><p>We demonstrate that the misalignment between pre-training and transfer tasks due to the high levels of noise in the web data or the non-semantic nature of the selfsupervised pretext tasks leads to a less-generalizable feature space. Through extensive experiments, we show that ClusterFit consistently combats this issue across different modalities and pre-training settings. D cf , size and granularity of the pre-training label space, and so on. In this section, we study the effect of these design choices on the transfer learning performance with videos as an example use case ( <ref type="table" target="#tab_0">Table 2)</ref>.  Ncf with unsupervised clustering Ncf with per-label clustering as follows: (a) N pre = N cf = R(2+1)D-18; (b) N pre &gt; N cf , where N pre = R(2+1)D-34 model (64M parameters) and thus higher capacity than N cf (33M parameters). From <ref type="figure">Figure 5</ref>, we observe a consistent improvement of 2% − 3% across different values of K when a higher capacity model was used as N pre . This result is intuitive and indicates that a higher capacity N pre yields richer visual features for clustering and thus improves the transfer learning performance. We note that in the aforementioned case (b), our framework can be viewed to be distilling knowledge from a higher capacity teacher model (N pre ) to a lowercapacity student model (N cf ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analyzing ClusterFit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Unsupervised vs. Per-Label Clustering</head><p>As noted before, the clustering step in ClusterFit is 'unsupervised' because it discards the labels associated with D cf and operates purely on the feature representations. But is there any advantage of using the semantic information of labels in D cf for clustering? To address this question, we formulate a per-label clustering setup. Specifically, given each label l, we cluster videos belonging to it into k l clusters. We treat K = {k l : ∀l} as pseudo-labels to train N cf . Each k l is defined to be proportional to √ n l 2 where n l denotes the number of videos associated with the label l. <ref type="figure" target="#fig_9">Figure 6</ref> compares the two clustering approaches on Kinetics and Sports1M. We observe that on both datasets, unsupervised clustering consistently outperforms per-label clustering across all values of K. We believe that by operating purely on video features, the unsupervised approach effectively captures the visual coherence in D cf . Consequently, factors around label noise such as wrong / missing labels and lexical ambiguity are being automatically addressed in the unsupervised framework, leading to superior performance over per-label clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Properties of D pre</head><p>In this section, we address the following question: what constitutes a valuable pre-training label space (D pre ) and how to construct one? Towards this end, we study two properties of D pre : the nature of it's labels and their cardinality. We refer the readers to the supplementary material for discussion on the nature of labels. Number of labels in D pre : We now study how varying the number of labels in D pre effects ClusterFit. To study this, we fix the total number of unique videos in D pre to 19M and vary the number of pre-training labels. First, we consider IG-Verb-62M and rank it's 438 weak verb labels by their frequency of occurrence. Next, we construct 4 different D pre datasets by considering unique 19M videos tagged with top-m verbs, where m = {10, 30, 100, 438}. Note that for a fixed number of videos in D pre , reducing the number of labels implies reduced content diversity.</p><p>From <ref type="figure" target="#fig_10">Figure 7</ref>, we observe that the transfer learning per-formance increases log-linearly with the number of pretraining labels in D pre . When we use just the top-10 verbs (m = 10), accuracy drops by around 9% compared to m = 438. This indicates that label space diversity is essential to generate good quality clusters. However, when m = 100, N cf is within 2% of the accuracy obtained when using all 438 verbs, and it outperform its weakly supervised pre-trained counterpart which uses 62M videos and all 438 verbs. This experiment clearly demonstrates the utility of our approach in designing a generic pre-training label space with minimal effort. Contrary to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38]</ref> which propose careful, manual label engineering, ClusterFit offers an easy way to construct a powerful, generalizable pre-training label space. Increasing the label space granularity is as simple as increasing the number of clusters in ClusterFit and requires no additional manual effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this work, we presented ClusterFit, a simple approach to significantly improve the generalizability of features learnt in weakly-supervised and self-supervised frameworks for images and videos. While models trained in these frameworks are prone to overfit to the pre-training objective, ClusterFit combats this issue by first clustering the original feature space and re-learning a new model on cluster assignments. Clustering in CF may be viewed as a lossy compression scheme that effectively captures the essential visual invariances in the feature space. Thus, predicting the cluster labels gives the 're-learned' network an opportunity to learn features that are less sensitive to the original pretraining objective, making them more transferable.</p><p>While the clustering step in ClusterFit is unsupervised, in practice, domain knowledge from downstream target tasks can be used to guide clustering and possibly improve the transfer learning performance. Additionally, we found that in its current unsupervised form, iterative application of CF provides little improvements; incorporating domain knowledge could be a potential solution.</p><p>ClusterFit is a universal framework -it is scalable and imposes no restrictions on model architectures, modalities of data, and forms of supervision. Future research should take advantage of its flexibility and combine different types of pre-trained models for learning cluster assignments in a multi-task manner. Using evidence accumulation methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref> for clustering is another worthwhile direction to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details on baseline methods</head><p>Details on distillation The distillation loss function is a convex combination of two individual losses -(1) a loss that tries to match the 'soft' target outputs by a teacher model, i.e., a probability distribution computed by applying a softmax function on the logits of a teacher model using a temperature T ; (2) a cross-entropy loss that tries to match the predictions with the ground truth labels for each datapoint. The two losses are combined with a convex combination (α used for the first loss and 1−α used for the second loss). We performed a grid search to find the optimal values for temperature T and weight α. We set these values as T = 20 and α = 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details on prototype baseline</head><p>Prototype method is a simplified version of k-means with only one iteration and controlled initialization. Similar to k-means, it consists of two steps: (1) cluster center initialization and (2) cluster label reassignment. In step 1, we compute an average of the visual features of all datapoints (videos or images) belonging to each label. These visual embeddings per label are chosen as cluster centers. Then, in step 2, data points are re-assigned to their nearest cluster centers (computed from step 1). Finally, the newly assigned cluster id is used as the label in training N cf . Prototype method is limited to (weakly) supervised setting as it requires label information in step 1. Also, it yields the same number of clusters as the cardinality of the input label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weakly supervised Images</head><p>We provide details for Section 4.1.1 of the main paper.</p><p>Details on the IG-ImageNet dataset We get 1500(&gt; 1000) total hashtags because multiple hashtags may map to the same ImageNet synset.</p><p>Details on pre-training We follow the implementation from <ref type="bibr" target="#b37">[38]</ref>. All models are trained using Synchronized stochastic gradient descent (SGD) on 128 GPUs across 16 machines. Each GPU processes 32 images at a time. All the pre-training runs process 2B images in total. An initial learning rate of 1.6 is used and decreased by a factor of 2 at 13 equally spaced steps.</p><p>Details on ClusterFit k-means: Since number of images are very large, we randomly sub-sample 200M images and perform 30 iterations of k-means clustering on them. We use the cluster centers obtained in this step, and then perform 5 more iterations of k-means with all the 1B images.</p><p>Transfer Learning Hyperparameters We closely follow transfer learning settings in <ref type="bibr" target="#b37">[38]</ref> and apply parameter sweep on learning rate for different target datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Weakly supervised Videos</head><p>We provide details for Section 4.1.2 of the main paper.</p><p>Details on the IG-Verb dataset We strictly follow <ref type="bibr" target="#b19">[20]</ref> to construct large-scale weakly supervised video datasets. We borrow label space from public datasets and crawl videos that contain matching hashtags from a social website. Given the great amount of noise in web data, labels with at least 50 matching videos are finally retained. IG-Verb-62M dataset consists of 438 verbs, a union of Kinetics and VerbNet <ref type="bibr" target="#b56">[57]</ref> verbs, and 62M videos with at least one matching hashtag. If multiple hashtags are attached to one video, one hashtag/verb is randomly picked as label. We also follow the tail-preserving strategy in <ref type="bibr" target="#b19">[20]</ref> to construct IG-verb-19M, which is a subset of IG-Verb-62M.</p><p>Details on the IG-Kinetics and IG-Noun dataset Following the same rules as above, we construct IG-Kinetics-19M comprising 359 labels from Kinetics label space, and IG-Noun-19M comprising 1438 labels from ImageNet synsets. These two datasets are used later in studying the effect of nature of labels in D pre .</p><p>Details on pre-training Training for both N pre and N cf follow the same setting. 128 GPUs across 16 machines are used. Each GPU processes 6 videos at a time and batch normalization <ref type="bibr" target="#b29">[30]</ref> is applied to all convolutional layers on each GPU. All the pre-training experiments process 490M videos in total across all epochs. We closely follow the training hyper-parameters mentioned in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Transfer Learning Hyperparameters We closely follow transfer learning settings in <ref type="bibr" target="#b19">[20]</ref> and apply parameter sweep on learning rate for different target datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Self-supervised Images</head><p>We provide details for Section 4.2 of the main paper.</p><p>Self-supervised Pre-training (N pre ) The Jigsaw and RotNet model pre-training is based on the code release from <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. We use a standard ResNet-50 model for both methods. We use a batchsize of 32 per GPU, a total of 8 GPUs and optimize these models using mini-batch SGD for a total of 105 epochs with an initial learning rate of 0.1, decayed by a factor of 10 after every 30 epochs.</p><p>Details on Jigsaw N pre We follow <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref> to construct the 'jigsaw puzzles' from the images. We first resize the image (maintaining aspect ratio) to make its shortest side 255, and then extract a random square crop of 255 × 255 from it. This crop is divided into a 3 × 3 grid and a random crop of 64 × 64 is extracted from each of the 9 grids to get 9 patches. The patches are input individually to the network to obtain their features, and are concatenated in a random order. Finally, the concatenated features are input to a classification layer which predicts the 'class index' of the random permutation used to concatenate the features. We use 2000 permutations as used in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Details on RotNet N pre We follow <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref> and apply a random rotation from {0 • , 90 • , 180 • , 270 • } to the input image. The network is trained (4-way classification) to predict the index of the rotation applied to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details on ClusterFit</head><p>We extract features (res5 after average pooling, 2048 dimensional vector) from each of the self-supervised N pre networks on the ImageNet-1K dataset (train split of 1.28M images). We then l 2 normalize the features and use k-means to cluster these images and obtain pseudo-labels as the cluster assignments for each point.</p><p>We also trained a version of N cf by clustering the res4 features from N pre . We found that this version gave similar performance to the N cf trained on cluster assignments from the res5 layer of N pre .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Learning Hyperparameters</head><p>We train linear classifiers on fixed features. Following <ref type="bibr" target="#b21">[22]</ref> we use minibatch SGD with a batchsize of 256, learning rate of 0.01 dropped by a factor of 10 after two equally spaced intervals, momentum of 0.9, and weight decay of 5 × 10 −4 . The features from each layer (conv1, res4, res5 etc.) are average pooled to get a feature of about 9000 dimensions each. We try to keep the number of parameter updates for training the linear models are roughly constant across all the transfer datasets. Thus, the models are trained for 28 epochs on ImageNet-1K (1.28M training images), 14 epochs on Places205 (2.4M training images) and for 84 epochs on iNaturalist-2018 (437K training images). We follow <ref type="bibr" target="#b21">[22]</ref> and train linear SVMs for the VOC07 transfer task.</p><p>Transfer Learning Results In Section 4.2 of the main paper, we showed transfer learning results when D pre = D cf = ImageNet-1K and the architecture of N pre and N cf was ResNet-50. In <ref type="table">Table 7</ref> we show results for different values of the number of cluster, K, used to generate the pseudo-labels. Although the performance of N cf increases as the number of clusters K increases, we observe that ClusterFit provides significant improvements over the pretrained N pre at smaller values (K = 1000).  <ref type="table">Table 7</ref>: Self-supervised methods: We apply ClusterFit to selfsupervised methods and evaluate them following the setup in <ref type="bibr" target="#b21">[22]</ref> on four datasets by training a linear classifier on fixed features. All methods use the ResNet-50 architecture for Npre and N cf . We report the performance of the best performing layer for each method and use the mean Average Precision (mAP) metric for the VOC07 dataset and top-1 accuracy for all other datasets. We show results for different values of K used to generate the pseudo-labels.</p><p>Distillation is not a valid baseline In self-supervised methods like Jigsaw and RotNet, the predictions depend upon the image transformation (permutation of patches for Jigsaw or the rotations for RotNet) applied to the input. Thus, given an untransformed input image, the selfsupervised methods do not produce a 'distribution' over the possible set of output values. For example, in RotNet, if we only pass an untransformed image, the network predicts (with high confidence) that the image is rotated by 0 • , and thus the 'distribution' over the possible rotation values of the input is not very meaningful to use in a distillation method. For Jigsaw, since the output is the index of the permutation applied to the input patches, the distribution produced for a 'regular' input is not meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Bonus: Multi-task Self-supervised Learning</head><p>We study the generalization of ClusterFit by using it for self-supervised multi-task learning. We take a pre-trained network N pre on Jigsaw and use it to compute the pseudolabels (via clustering) on D cf . We repeat the process for another N pre trained on RotNet to get a different set of pseudolabels on D cf . We treat these two sets of pseudo-labels as two different multi-class classification problems and train a new N cf from scratch using these labels. Thus, N cf is trained with two different fully-connected layers, each of which predicts the pseudo-labels from a different N pre . We sum the losses from these two layers and optimize the network.</p><p>We follow the setup from §4.2, and use the ResNet-50 architecture for both N pre and N cf , and set D pre = D cf = ImageNet-1K. The linear evaluation results are presented in <ref type="table">Table 8</ref>. This näive way of multi-task learning using Clus-terFit still improves the performance and provides gains of 8 points on ImageNet-1K, iNaturalist in top-1 accuracy and VOC07 mAP compared to the N pre models. The multi-task models also improve over the single task N cf models. (Ours, Multi-task) <ref type="table">Table 8</ref>: Multi-task Self-supervised: We show that ClusterFit can be used for easy multi-task learning. We apply ClusterFit to self-supervised methods and evaluate them following the setup in <ref type="bibr" target="#b21">[22]</ref> on four datasets by training a linear classifier on fixed features. All methods use the ResNet-50 architecture for Npre and N cf . We report the performance of the best performing layer for each method and use the mean Average Precision (mAP) metric for the VOC07 dataset and top-1 accuracy for all other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Analysis of ClusterFit</head><p>We provide details for Section 5 of the main paper.</p><p>Details on N cf training: In Section 5, we use R(2+1)D-18 as N cf and IG-Verb-62M as D cf . Training is done with 64 GPUs across 8 machines. Each GPU processes 16 videos at a time and batch normalization <ref type="bibr" target="#b29">[30]</ref> is applied to all convolutional layers on each GPU. The training processes 250M video in total. An initial learning rate of 0.005 per GPU is applied and decreased by a factor of 2 at 13 equally spaced steps.</p><p>Effect of nature of labels in D pre : In the main paper, we have shown results where N pre is pre-trained on labels that are verbs (i.e., on IG-Verb-19M dataset). It is natural to question: how does the choice of D pre 's label space effect ClusterFit? To study this, we vary D pre by changing the properties of their labels, but keeping the volume of the data fixed. All other settings, including D cf , N pre and N cf , are fixed. Specifically, we consider three D cf datasets with 19M videos each: (a) IG-Verb-19M, (b) IG-Noun-19M and (c) IG-Kinetics-19M. Next, we pre-trained three separate N pre on these datasets. Then, we use D cf = IG-Verb-62M to apply ClusterFit on each of them, and get three N cf . Finally, transfer learning is done on three N cf . <ref type="figure" target="#fig_11">Figure 8</ref> shows the transfer learning performance on Kinetics and Sports1M, where D cf is fixed to IG-Verb-62M. For a fair comparison, we report the performance when N pre is pre-trained on IG-Verb-62M, without applying ClusterFit (dotted magenta line). We make the following observations: first, all three D pre datasets show significant improvements over the weakly-supervised upper bound upon applying ClusterFit, further reaffirming its generalizability. Second, D pre = IG-Kinetics-19M yields a slightly higher performance on Kinetics, which indicates that prior domain knowledge of the downstream target task can help design D pre for maximum benefit (as observed in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38]</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 9 G S g f J i v a t s l + a v W s A d W g H K 0 v 4 8 = " &gt; A A A B + n i c b V D L S s N A F J 3 U V 6 2 v V J d u B l v B V U l a 7 G N X 1 I X L C r Y V 2 h A m 0 0 k 7 d C Y J M x O l x H y K G x e K u P V L 3 P k 3 T t o i v g 4 M H M 6 5 l 3 v m e B G j U l n W h 5 F b W V 1 b 3 8 h v F r a 2 d 3 b 3 z O J + T 4 a x w K S L Q x a K G w 9 J w m h A u o o q R m 4 i Q R D 3 G O l 7 0 / P M 7 9 8 S I W k Y X K t Z R B y O x g H 1 K U Z K S 6 5 Z L F + 4 y Z A j N R E 8 w X 6 a l l 2 z Z F U s 2 7 J q p 1 A T q 9 V q 1 u F C a T S h r Z U M J b B E x z X f h 6 M Q x 5 w E C j M k 5 c C 2 I u U k S C i K G U k L w 1 i S C O E p G p O B p g H i R D r J P H o K j 7 U y g n 4 o 9 A s U n K v f N x L E p Z x x T 0 9 m I e V v L x P / 8 w a x 8 p t O Q o M o V i T A i 0 N + z K A K Y d Y D H F F B s G I z T R A W V G e F e I I E w k q 3 V Z i X 0 M p Q / / r y X 9 K r V u x a p X Z V L b X P l n X k w S E 4 A i f A B g 3 Q B p e g A 7 o A g z v w A J 7 A s 3 F v P B o v x u t i N G c s d w 7 A D x h v n 3 w V l E w = &lt; / l a t e x i t &gt; D cf &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 G S g f J i v a t s l + a v W s A d W g H K 0 v 4 8 = " &gt; A A A B + n i c b V D L S s N A F J 3 U V 6 2 v V J d u B l v B V U l a 7 G N X 1 I X L C r Y V 2 h A m 0 0 k 7 d C Y J M x O l x H y K G x e K u P V L 3 P k 3 T t o i v g 4 M H M 6 5 l 3 v m e B G j U l n W h 5 F b W V 1 b 3 8 h v F r a 2 d 3 b 3 z O J + T 4 a x w K S L Q x a K G w 9 J w m h A u o o q R m 4 i Q R D 3 G O l 7 0 / P M 7 9 8 S I W k Y X K t Z R B y O x g H 1 K U Z K S 6 5 Z L F + 4 y Z A j N R E 8 w X 6 a l l 2 z Z F U s 2 7 J q p 1 A T q 9 V q 1 u F C a T S h r Z U M J b B E x z X f h 6 M Q x 5 w E C j M k 5 c C 2 I u U k S C i K G U k L w 1 i S C O E p G p O B p g H i R D r J P H o K j 7 U y g n 4 o 9 A s U n K v f N x L E p Z x x T 0 9 m I e V v L x P / 8 w a x 8 p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>A W i b Z I b R Q 5 r t N a t Z P I d h B V l L / C w g B C r P w R N v 4 N T p s B W k 6 y d L r 3 T u 9 8 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + 7 j e V 3 E q C e 2 R m M f y I c C K c h b R n m a a 0 4 d E U i w C T g f B 7 L q Y D x 6 p V C y O 7 v U 8 o Z 7 A k 4 i F j G B t J N + u N 2 / 8 b C S w n k q R G W e e N 3 2 7 4 b S c B d A 6 c U v S g B J d 3 / 4 a j W O S C h p p w r F S Q 9 d J t J d h q R n h N K + N U k U T T G Z 4 Q o e G R l h Q 5 W W L 7 D k 6 N 8 o Y h b E 0 L 9 J o o f 5 2 Z F g o N R e B 2 S x S q t V Z I f 4 3 G 6 Y 6 v P I y F i W p p h F Z H g p T j n S M i i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u m i n B X f 3 y O u m 3 W 6 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A W i b Z I b R Q 5 r t N a t Z P I d h B V l L / C w g B C r P w R N v 4 N T p s B W k 6 y d L r 3 T u 9 8 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + 7 j e V 3 E q C e 2 R m M f y I c C K c h b R n m a a 0 4 d E U i w C T g f B 7 L q Y D x 6 p V C y O 7 v U 8 o Z 7 A k 4 i F j G B t J N + u N 2 / 8 b C S w n k q R G W e e N 3 2 7 4 b S c B d A 6 c U v S g B J d 3 / 4 a j W O S C h p p w r F S Q 9 d J t J d h q R n h N K + N U k U T T G Z 4 Q o e G R l h Q 5 W W L 7 D k 6 N 8 o Y h b E 0 L 9 J o o f 5 2 Z F g o N R e B 2 S x S q t V Z I f 4 3 G 6 Y 6 v P I y F i W p p h F Z H g p T j n S M i i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u m i n B X f 3 y O u m 3 W 6 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>A W i b Z I b R Q 5 r t N a t Z P I d h B V l L / C w g B C r P w R N v 4 N T p s B W k 6 y d L r 3 T u 9 8 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + 7 j e V 3 E q C e 2 R m M f y I c C K c h b R n m a a 0 4 d E U i w C T g f B 7 L q Y D x 6 p V C y O 7 v U 8 o Z 7 A k 4 i F j G B t J N + u N 2 / 8 b C S w n k q R G W e e N 3 2 7 4 b S c B d A 6 c U v S g B J d 3 / 4 a j W O S C h p p w r F S Q 9 d J t J d h q R n h N K + N U k U T T G Z 4 Q o e G R l h Q 5 W W L 7 D k 6 N 8 o Y h b E 0 L 9 J o o f 5 2 Z F g o N R e B 2 S x S q t V Z I f 4 3 G 6 Y 6 v P I y F i W p p h F Z H g p T j n S M i i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u m i n B X f 3 y O u m 3 W 6 7h d + 1 G h 5 V 1 V O E U z u A C X L i E D t x C F 3 p A 4 A m e 4 R X e r N x 6 s d 6 t j + V q x S o 9 J / A H 1 u c P C E e U i w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q f H s t X F I u A K L 4 2 t A g K O e i h E V p X g = " &gt; A A A B + 3 i c b V A 9 T 8 M w F H w p X 6 V 8 h T K y W L R I T F X S B c Z K M DA W i b Z I b R Q 5 r t N a t Z P I d h B V l L / C w g B C r P w R N v 4 N T p s B W k 6 y d L r 3 T u 9 8 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + 7 j e V 3 E q C e 2 R m M f y I c C K c h b R n m a a 0 4 d E U i w C T g f B 7 L q Y D x 6 p V C y O 7 v U 8 o Z 7 A k 4 i F j G B t J N + u N 2 / 8 b C S w n k q R G W e e N 3 2 7 4 b S c B d A 6 c U v S g B J d 3 / 4 a j W O S C h p p w r F S Q 9 d J t J d h q R n h N K + N U k U T T G Z 4 Q o e G R l h Q 5 W W L 7 D k 6 N 8 o Y h b E 0 L 9 J o o f 5 2 Z F g o N R e B 2 S x S q t V Z I f 4 3 G 6 Y 6 v P I y F i W p p h F Z H g p T j n S M i i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u m i n B X f 3 y O u m 3 W 6 7 h d + 1 G h 5 V 1 V O E U z u A C X L i E D t x C F 3 p A 4 A m e 4 R X e r N x 6 s d 6 t j + V q x S o 9 J / A H 1 u c P C E e U i w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " Q f H s t X F I u A K L 4 2 t A g K O e i h E V p X g = " &gt; A A A B + 3 i c b V A 9 T 8 M w F H w p X 6 V 8 h T K y W L R I T F X S B c Z K M D A W i b Z I b R Q 5 r t N a t Z P I d h B V l L / C w g B C r P w R N v 4 N T p s B W k 6 y d L r 3 T u 9 8 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + 7 j e V 3 E q C e 2 R m M f y I c C K c h b R n m a a 0 4 d E U i w C T g f B 7 L q Y D x 6 p V C y O 7 v U 8 o Z 7 A k 4 i F j G B t J N + u N 2 / 8 b C S w n k q R G W e e N 3 2 7 4 b S c B d A 6 c U v S g B J d 3 / 4 a j W O S C h p p w r F S Q 9 d J t J d h q R n h N K + N U k U T T G Z 4 Q o e G R l h Q 5 W W L 7 D k 6 N 8 o Y h b E 0 L 9 J o o f 5 2 Z F g o N R e B 2 S x S q t V Z I f 4 3 G 6 Y 6 v P I y F i W p p h F Z H g p T j n S M i i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u m i n B X f 3 y O u m 3 W 6 7 h d + 1 G h 5 V 1 V O E U z u A C X L i E D t x C F 3 p A 4 A m e 4 R X e r N x 6 s d 6 t j + V q x S o 9 J / A H 1 u c P C E e U i w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q f H s t X F I u A K L 4 2 t A g K O e i h E V p X g = " &gt; A A A B + 3 i c b V A 9 T 8 M w F H w p X 6 V 8 h T K y W L R I T F X S B c Z K M D A W i b Z I b R Q 5 r t N a t Z P I d h B V l L / C w g B C r P w R N v 4 N T p s B W k 6 y d L r 3 T u 9 8 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + 7 j e V 3 E q C e 2 R m M f y I c C K c h b R n m a a 0 4 d E U i w C T g f B 7 L q Y D x 6 p V C y O 7 v U 8 o Z 7 A k 4 i F j G B t J N + u N 2 / 8 b C S w n k q R G W e e N 3 2 7 4 b S c B d A 6 c U v S g B J d 3 / 4 a j W O S C h p p w r F S Q 9 d J t J d h q R n h N K + N U k U T T G Z 4 Q o e G R l h Q 5 W W L 7 D k 6 N 8 o Y h b E 0 L 9 J o o f 5 2 Z F g o N R e B 2 S x S q t V Z I f 4 3 G 6 Y 6 v P I y F i W p p h F Z H g p T j n S M i i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u m i n B X f 3 y O u m 3 W 6 7 h d + 1 G h 5 V 1 V O E U z u A C X L i E D t x C F 3 p A 4 A m e 4 R X e r N x 6 s d 6 t j + V q x S o 9 J / A H 1 u c P C E e U i w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q f H s t X F I u A K L 4 2 t A g K O e i h E V p X g = " &gt; A A A B + 3 i c b V A 9 T 8 M w F H w p X 6 V 8 h T K y W L R I T F X S B c Z K M D A W i b Z I b R Q 5 r t N a t Z P I d h B V l L / C w g B C r P w R N v 4 N T p s B W k 6 y d L r 3 T u 9 8 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + 7 j e V 3 E q C e 2 R m M f y I c C K c h b R n m a a 0 4 d E U i w C T g f B 7 L q Y D x 6 p V C y O 7 v U 8 o Z 7 A k 4 i F j G B t J N + u N 2 / 8 b C S w n k q R G W e e N 3 2 7 4 b S c B d A 6 c U v S g B J d 3 / 4 a j W O S C h p p w r F S Q 9 d J t J d h q R n h N K + N U k U T T G Z 4 Q o e G R l h Q 5 W W L 7 D k 6 N 8 o Y h b E 0 L 9 J o o f 5 2 Z F g o N R e B 2 S x S q t V Z I f 4 3 G 6 Y 6 v P I y F i W p p h F Z H g p T j n S M i i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u m i n B X f 3 y O u m 3 W 6 7 h d + 1 G h 5 V 1 V O E U z u A C X L i E D t x C F 3 p A 4 A m e 4 R X e r N x 6 s d 6 t j + V q x S o 9 J / A H 1 u c P C E e U i w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q f H s t X F I u A K L 4 2 t A g K O e i h E V p X g = " &gt; A A A B + 3 i c b V A 9 T 8 M w F H w p X 6 V 8 h T K y W L R I T F X S B c Z K M D A W i b Z I b R Q 5 r t N a t Z P I d h B V l L / C w g B C r P w R N v 4 N T p s B W k 6 y d L r 3 T u 9 8 Q c K Z 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + 7 j e V 3 E q C e 2 R m M f y I c C K c h b R n m a a 0 4 d E U i w C T g f B 7 L q Y D x 6 p V C y O 7 v U 8 o Z 7 A k 4 i F j G B t J N + u N 2 / 8 b C S w n k q R G W e e N 3 2 7 4 b S c B d A 6 c U v S g B J d 3 / 4 a j W O S C h p p w r F S Q 9 d J t J d h q R n h N K + N U k U T T G Z 4 Q o e G R l h Q 5 W W L 7 D k 6 N 8 o Y h b E 0 L 9 J o o f 5 2 Z F g o N R e B 2 S x S q t V Z I f 4 3 G 6 Y 6 v P I y F i W p p h F Z H g p T j n S M i i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u m i n B X f 3 y O u m 3 W 6 7 h d + 1 G h 5 V 1 V O E U z u A C X L i E D t x C F 3 p A 4 A m e 4 R X e r N x 6 s d 6 t j + V q x S o 9 J / A H 1 u c P C E e U i w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " 9 G S g f J i v a t s l + a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Full ClusterFit pipeline: A typical transfer learning framework involves two stages: pre-training followed by transfer learning. ClusterFit introduces a step between these stages. We evaluate all representations by training a linear classifier on fixed ConvNet weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 : 8 Figure 5 :</head><label>485</label><figDesc>ClusterFit involves several aspects such as the relative model capacities of N pre and N cf , properties of D pre and c o n v 1 re s 2 re s 3 re s 4 re s Self-supervised Images (Layerwise): We examine the layer-wise performance of self-supervised models before applying our technique (Npre) and after (N cf ). We fix Dpre = D cf = ImageNet-1K (without labels) and use the same architecture (ResNet-50) for Npre and N cf . The last layer (res5) features for Npre transfer poorly compared to the lower res4 layer. After CF, N cf shows an improved performance for all layers except for conv1 and reduces the gap in performance between res4 and res5. of N cf for different N pre capacities N pre capacity R2+1D-34-32 N pre capacity R2+1D-18-Relative Model Capacity of Npre and N cf ( §5.1): We fix N cf = R(2+1)D-18. We vary (a) Npre = N cf = R(2+1)D-18 (light green) and (b) Npre &gt; N cf , where Npre = R(2+1)D-34 (dark green). We report the transfer performance of the N cf model for cases (a) and (b) on Kinetics. A higher capacity Npre results in better transfer performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Similar to IG-Verb-19M in Sec. 4.1.2, we construct IG-Verb-62M, a weakly-supervised dataset comprising 62M videos and use it as D cf . For faster training of N cf , we consider a computationally cheaper R(2+1)D-18 [8] architecture and process 8 frames per video clip. Unless specified otherwise, D pre = IG-Verb-19M and N pre = R(2+1)D-34 [8] with 32 frames per video. All other settings are same as in Sec. 4.1.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>5. 1 .</head><label>1</label><figDesc>Relative model capacity of N pre and N cfThe relative model capacities of N pre and N cf can impact the final transfer performance of N cf . To study this behavior, we fix D pre = IG-Verb-19M and D cf = IG-Verb-62M, and N cf = R(2+1)D-18. We vary the architecture of N pre</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Unsupervised vs. Per-Label Clustering ( §5.2): In per-label clustering, we retain the semantic information of the class labels and cluster videos belonging to each label. We note that for all values of K, unsupervised clustering used in ClusterFit yields better transfer learning performance on Kinetics and Sports1M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>of N cf with different number of labels in D pre N cf on IG-Verb-62M Weakly supervised upper bound Effect of number of labels in Npre ( §5.3). We design 4 different Dpre, each with 19M videos but #labels = {10, 30, 100, 438}, K = 3200 and D cf = IG-Verb-62M. X-axis is in log-linear scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Nature of pre-training labels (appendix F):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Target tasks for Transfer Learning used for evaluating feature representations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Pre-training methodDpre = D cf Arch. of Npre &amp; N cf Weakly-Supervised Images §4.1.1 IG-ImageNet-1BResNet-50 Weakly-Supervised Videos §4.1.2 IG-Verb-19M R(2+1)D-34 Self-supervised Images §4.2</figDesc><table><row><cell>ImageNet-1k</cell><cell>ResNet-50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Data and model architectures used in §4: weakly supervised videos, weakly supervised images, and self supervised images. In each setting, we train Npre and N cf for equal number of epochs.</figDesc><table><row><cell>D tar</cell><cell cols="4">N pre N pre 2× Distill. Prototype</cell><cell>CF (N cf ), K → 1.5k 3.75k 7.5k 15k 30k</cell></row><row><cell cols="2">ImageNet-1K 78.0</cell><cell>78.8</cell><cell>73.8</cell><cell>76.9</cell><cell>75.3 76.1 76.5 76.5 76.2</cell></row><row><cell cols="2">ImageNet-9K 32.9</cell><cell>34.1</cell><cell>29.1</cell><cell>35.1</cell><cell>33.5 35.4 36.4 37.1 37.5</cell></row><row><cell>Places365</cell><cell>51.2</cell><cell>51.2</cell><cell>49.9</cell><cell>51.9</cell><cell>52.0 52.1 52.4 52.6 52.1</cell></row><row><cell>iNaturalist</cell><cell>43.9</cell><cell>45.3</cell><cell>35.9</cell><cell>49.0</cell><cell>43.8 46.4 47.9 49.7 49.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Weakly-supervised Images: Top-1 accuracy for various trans- fer learning datasets with Dpre = D cf = IG-ImageNet-1B and the same architecture (ResNet-50) for Npre and N cf .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Weakly-supervised videos: Top-1 accuracy for various transfer learning datasets with Dpre = D cf = IG-Verb-19M and the same architecture (R(2+1)D-34) for Npre and N cf .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We randomly replace a label (l) in ImageNet-1K train split with one that is obtained by uniformly sampling from ImageNet-1K labels excluding l.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also experimented with k l ≈ n l but this resulted in worse performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We would like to thank Rob Fergus, and Zhenheng Yang for feedback on the manuscript; Filip Radenovic, and Vignesh Ramanathan for feedback on the experimental setup; Laurens van der Maaten, Larry Zitnick, Armand Joulin, and Xinlei Chen for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large scale distributed neural network training through online distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ormandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03235</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning classification with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virginia R De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">User conditional hashtag prediction for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1731" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning large-scale automatic image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Data clustering using evidence accumulation. In Object recognition supported by user interaction for service robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Ana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="276" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04770</idno>
		<title level="m">Born again neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to separate object sounds by watching unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Large-scale weaklysupervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00561</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01235</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hard mixtures of experts for large scale weakly supervised vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Im2gps: estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ieee conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09005</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning visual n-grams from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Predicting deeper into the future of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Consensus clusterings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh IEEE International Conference on Data Mining (ICDM 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="607" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV, 115</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Separating self-expression and visual content in hashtag supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno>arXiv 1711.09825</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">VerbNet : A Computational Lexical Resource for Verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verbnet</surname></persName>
		</author>
		<ptr target="https://verbs.colorado.edu/verbnet/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Revisiting im2gps in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2621" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Zeki</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
