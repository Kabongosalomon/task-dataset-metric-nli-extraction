<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 19-23. 2018. 2018. August 19-23. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
							<email>leiji@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="department">School of CSSE East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<email>nanduan@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
							<email>jianyong@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of CSSE East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<addrLine>Asia 1</addrLine>
									<country>CAS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Asia Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering</title>
					</analytic>
					<monogr>
						<title level="j" type="main">KDD</title>
						<meeting> <address><addrLine>London, United Kingdom</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">18</biblScope>
							<date type="published">August 19-23. 2018. 2018. August 19-23. 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3219819.3220036</idno>
					<note>* This work was mainly performed when Pan Lu was visiting Microsoft Research. † Wei Zhang is the corresponding author. London, United Kingdom. ACM, New York, NY, USA, 10 pages. https://doi.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies → Knowledge representation and reasoning;</term>
					<term>Information systems → Question answering;</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Visual Question Answering (VQA) has emerged as one of the most significant tasks in multimodal learning as it requires understanding both visual and textual modalities. Existing methods mainly rely on extracting image and question features to learn their joint feature embedding via multimodal fusion or attention mechanism. Some recent studies utilize external VQA-independent models to detect candidate entities or attributes in images, which serve as semantic knowledge complementary to the VQA task. However, these candidate entities or attributes might be unrelated to the VQA task and have limited semantic capacities. To better utilize semantic knowledge in images, we propose a novel framework to learn visual relation facts for VQA. Specifically, we build up a Relation-VQA (R-VQA) dataset based on the Visual Genome dataset via a semantic similarity module, in which each data consists of an image, a corresponding question, a correct answer and a supporting relation fact. A well-defined relation detector is then adopted to predict visual question-related relation facts. We further propose a multi-step attention model composed of visual attention and semantic attention sequentially to extract related visual knowledge and semantic knowledge. We conduct comprehensive experiments on the two benchmark datasets, demonstrating that our model achieves state-of-the-art performance and verifying the benefit of considering visual relation facts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the great development of natural language processing, computer vision, knowledge embedding and reasoning, and multimodal representation learning, Visual Question Answering has become a popular research topic in recent years. The VQA task is required to provide the correct answer to a question with a corresponding image, which has been regarded as an important Turing test to evaluate the intelligence of a machine. The VQA problem can be easily expanded to other tasks and play a significant role in various applications, including human-machine interaction and medical assistance. However, it is difficult to address the problem, as the AI system needs to understand both language and vision content, to extract and encode necessary common sense and semantic knowledge, and then to make reasoning to obtain the final answer. Thanks to multimodal embedding methods and attention mechanisms, researchers have made remarkable progress in VQA development.</p><p>The predominant methods first extract language feature embedding by an RNN model and image feature embedding by a pretrained model, then learning their joint embedding by multimodal fusion like element-wise addition or multiplication, and finally feeding it to a sequential network to generate free-form answers or to a multi-class classifier to predict most related answers. Inspired by image captioning, some VQA approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> introduce semantic concepts such as entities and attributes from off-the-shelf CV methods, which provide various semantic information for the models. Compared with entities and attributes, relation facts have larger semantic capacities as they consist of three elements: subject entity, relation, and object entity, leading to a large number of combinations. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, given the question "what is the man doing" and the image, relation facts like (man, standing on, skateboard), (man, on, ice), (person, in, ski suit) enable providing important semantic information for question answering.</p><p>The main challenge for VQA lies in the semantic gap from language to image. To deal with the semantic gap, existing attempts come in two forms. To be specific, some methods extract high-level semantic information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, such as entities, attributes, or even retrieval results in knowledge base <ref type="bibr" target="#b15">[16]</ref>, such as DBpedia <ref type="bibr" target="#b1">[2]</ref> and Freebase <ref type="bibr" target="#b4">[5]</ref>. Other methods introduce visual attention <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> to select related image regions corresponding to salient visual information. Unfortunately, these progressions of introducing semantic knowledge are still limited in two aspects. On one hand, they use entities or attributes as high-level semantic concepts, which are individual and only cover restricted knowledge information. On the other hand, as they extract the concept based on off-the-shelf CV methods in other tasks or datasets, the candidate concepts might be irrelevant to the VQA task.</p><p>To make full use of semantic knowledge in images, we propose a novel semantic attention model for VQA. We build a large-scale Relation-VQA (R-VQA) dataset including over 335k data samples based on the Visual Genome dataset. Each data instance is composed of an image, a relevant question, and a relation fact semantically similar to the image-question pair. We then adopt a relation detector to predict the most related visual relation facts given an image and a question. We further propose a novel multi-step attention model to incorporate visual attention and semantic attention into a sequential attention framework. Our model is composed of three major components (see <ref type="figure" target="#fig_3">Figure 4</ref>). The visual attention module (Subsection 5.1) is designed to extract image feature representation. The output of the visual attention module is then fed into semantic attention (Subsection 5.2), which learns to select important relation facts generated by the relation detector (Section 4). Finally, joint knowledge learning (Subsection 5.3) is applied to simultaneously learn visual knowledge and semantic knowledge based on visual and semantic feature embeddings.</p><p>The main contributions of our work are four-fold.</p><p>• We propose a novel VQA framework which enables learning visual relation facts as semantic knowledge to help answer the questions. • We develop a multi-step semantic attention network (MSAN) which combines visual attention and semantic attention sequentially to simultaneously learn visual and semantic knowledge representations.</p><p>• To achieve that, we build up a large-scale VQA dataset accompanied by relation facts and design a fine-grained relation detector model. • We evaluate our model on two benchmark datasets and achieve state-of-the-art performance. We also conduct substantial experiments to illustrate the ability of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Visual Question Answering</head><p>As the intersection of natural language processing, knowledge representation and reasoning, and computer vision, the task of Visual Question Answering has attracted increasing interest recently in multiple research fields. A series of large-scale datasets have been constructed, including VQA <ref type="bibr" target="#b0">[1]</ref>, COCO-QA <ref type="bibr" target="#b26">[27]</ref>, and Visual Genome <ref type="bibr" target="#b14">[15]</ref> datasets. A commonly used framework is to first encode each question as a semantic vector using a long short-term memory network (LSTM) and to extract image features via a pretrained convolution neural network (CNN), then to fuse these two feature embeddings to predict the answer. In contrast to work in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> which use simple feature fusion like element-wise operation or concatenation, effective bilinear pooling methods are well studied in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Methods</head><p>Attention networks have recently shown remarkable success in many applications of knowledge mining and natural language processing, such as neural machine translation <ref type="bibr" target="#b2">[3]</ref>, recommendation systems <ref type="bibr" target="#b29">[30]</ref>, advertising <ref type="bibr" target="#b41">[42]</ref>, document classification <ref type="bibr" target="#b38">[39]</ref>, sentiment analysis <ref type="bibr" target="#b21">[22]</ref>, question answering <ref type="bibr" target="#b16">[17]</ref>, and others. Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref> introduced an attention mechanism to automatically select parts of words in a source sentence relevant to predicting a target word, which improves the performance of basic encoder-decoder architecture. Long et al. <ref type="bibr" target="#b21">[22]</ref> propose a cognition based attention (CBA) layer for neural sentiment analysis to help capture the attention of words in source sentences. Different from above works focusing on word-level attention, sentence-level <ref type="bibr" target="#b38">[39]</ref> and documentlevel attention <ref type="bibr" target="#b29">[30]</ref> pay more holistic attention to the whole textual content. An attention mechanism has also been successfully applied to computer vision tasks like image captioning <ref type="bibr" target="#b39">[40]</ref>, image retrieval <ref type="bibr" target="#b20">[21]</ref>, image classification <ref type="bibr" target="#b33">[34]</ref>, image popularity prediction <ref type="bibr" target="#b42">[43]</ref>, et al. Inspired by the great success achieved by attention mechanisms on natural language processing and computer vision, lots of VQA approaches perform attention mechanism to improve model capacity. Current attention methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref> for VQA mainly perform visual attention to learn image regions relevant to the question. Some recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> integrate effective multimodal feature embedding with visual attention to further improve VQA performance. More recently, Lu et al. <ref type="bibr" target="#b23">[24]</ref> design a novel dual attention network which introduces two types of visual features and enables learning question-releted free-form and detection-based image regions. Different from these studies, we propose a novel sequential attention mechanism to seamlessly combine visual and semantic clues for VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Facts</head><p>Relation facts, standing for relationships between two entities, play an important role in representation and reasoning in knowledge graph. The encoding and applications of relation facts have been widely studied in multiple tasks of knowledge representation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>. Visual relationship detection is an emerging task aiming to generate relation facts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>, e.g. (man, riding, bicycle) and (man, pushing, bicycle), which capture various interactions between pairs of entities in images.</p><p>Existing relevant VQA methods involve using knowledge information to either obtain retrieval results of entities and attributes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, or detect high-level concepts in the image according to the question query <ref type="bibr" target="#b30">[31]</ref>. However, it is not effective enough to exploit the complicated semantic relations between the question and image by simply treating semantic knowledge in images as entities and attributes. To the best of our knowledge, it is still rare to incorporate relation facts in VQA to provide rich semantic knowledge. In order to utilize relation facts in the VQA task, we propose effectively learning relation facts and selecting related facts via semantic attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>In this section, we first formulate the VQA problem addressed in this paper, and then clarify the predominant framework for the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given a question Q and a related image I , the VQA algorithm is designed to predict the most possible answerâ based on both the language and image content. The predominant approaches formalize VQA as a multi-class classification problem in the space of candidate answer phrases from most frequent answers in training data. This can be formulated aŝ</p><formula xml:id="formula_0">a = arg max a ∈Ω p(a|Q, I ; Θ),<label>(1)</label></formula><p>where Θ denotes the parameters of the model and Ω is the set of candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Common Framework</head><p>The common frameworks for VQA are composed of three major parts: the image embedding model, the question embedding model, and the joint feature learning model. CNN models like <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> are used in the image model to extract image feature representation. For example, a typical deep residual network ResNet-152 <ref type="bibr" target="#b9">[10]</ref> can extract the image feature map v from the last convolution layer before the pooling layer, which is given by:</p><formula xml:id="formula_1">v = CNN(I ).<label>(2)</label></formula><p>Before being fed into the CNN model, the input image is resized to be 448 × 448 pixels from the raw image. The convolution feature map extracted from the CNN model has a size of 2048 × 14 × 14, where 14 × 14 is its spatial size corresponding to different image regions, and 2048 represents the number of feature embedding dimension of each region. For the question model, recurrent neural networks like Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">[11]</ref> and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[7]</ref> are utilized to obtain the question semantic representation, which is given by:</p><formula xml:id="formula_2">q = RNN(Q).<label>(3)</label></formula><p>To be specific, given a question with T words, the embedding of each question word is sequentially fed into the RNN model. The final hidden state h T of the RNN model is taken as the question embedding.</p><p>The question and image representations are then jointly embedded into the same space through multimodal pooling, including element-wise product or sum, as well as the concatenation of these representations</p><formula xml:id="formula_3">h = Φ(v, q),<label>(4)</label></formula><p>where Φ is the multimodal pooling module. The joint representation h is then fed to a classifier which predicts the final answer. A large quantity of recent works incorporate visual attention mechanisms for more effective visual feature embedding. In general, a semantic similarity layer is introduced to calculate the relevance between the question and image regions defined as:</p><formula xml:id="formula_4">m i = sigmoid(ψ (q, v i )),<label>(5)</label></formula><p>where ψ is the module of semantic similarity, sigmoid is a sigmoidtype of function, such as softmax, to map the semantic results to the value interval [0,1], and m i is the semantic weight of one image region. Finally, the visual representation of the image is updated by the weighted sum over all image regions as:</p><formula xml:id="formula_5">v = 14×14 i=1 m i v i ,<label>(6)</label></formula><p>which is able to highlight the representations of image regions most related to the input question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATION FACT DETECTOR</head><p>In this section, we describe the process of collecting our Relation-VQA (R-VQA) dataset, as well as the data analysis in Subsection 4.1.</p><p>We then design a relation fact detector in Subsection 4.2 based on R-VQA to predict visual relation facts related to given questions and images, which is further incorporated into our VQA model in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Collection for Relation-VQA</head><p>Survey of Existing Datasets Existing VQA datasets like VQA <ref type="bibr" target="#b0">[1]</ref> and COCO-QA <ref type="bibr" target="#b26">[27]</ref> are made up of images, questions and labeled answers, not involving supporting semantic relation facts.</p><p>Although the Visual Genome Dataset <ref type="bibr" target="#b14">[15]</ref> provides semantic knowledge information such as objects, attributes, and visual relationships about parts of images, which is not aligned with their corresponding question-answer pairs. Therefore, we expand Visual Genome based on semantic similarity and build up the Relation-VQA dataset, which is composed of questions, images, answers, and aligned semantic knowledge. The dataset will be released at https://github.com/lupantech/rvqa. Data Collection We first define relation facts used in our paper as shown in <ref type="table" target="#tab_2">Table 1</ref>. The relation facts are categorized as one of three types: entity concept, entity attribute, and entities relation, based on the semantic data of concepts, attributes, and relationships  in Visual Genome, respectively. For simplicity, the attribute in an entity attribute can be an adjective, noun, or preposition phrase. For Visual Genome, most images are provided with related questionanswer pairs, and parts of images are annotated with semantic knowledge. Thus, we keep images containing both question-answer pairs and semantic knowledge, and treat these semantic knowledge as candidate facts with the form of the above templates. Response Ranking, a semantic similarity ranking method proposed in <ref type="bibr" target="#b36">[37]</ref>, is then adopted to compute the relevance between each QA pair and its candidate facts. It should be noted that as the ranking algorithm is not our work's main focus and various ranking algorithms are compatible in our framework, we simply adopt one of the state-of the-art ranking methods, such as Response Ranking. We leave the choice or design of a better ranking algorithm in future work. The relevance matching score obtained from the Response Ranking module ranges from 0 to 1, and value 0 means the candidate fact is completely unrelated to the given QA data, while value 1 means perfect correlation. In the end, after removing candidate facts below a certain threshold matching score, the fact with the largest score is chosen as the ground truth. We randomly partition the generated data into a training set (60%), a development set (20%) and a testing set (20%). <ref type="table" target="#tab_4">Table 2</ref> shows the data sizes of R-VQA with different matching score thresholds.  Human Evaluation To ensure the quality of matched facts, we employ crowdsourcing workers to label whether the generated facts are closely related to the given QA pair. For each generated dataset with a certain threshold score, we randomly sample 1,000 examples for human labeling and ask three workers to label them. The final accuracy for each dataset is the average accuracy obtained by the three workers. Additionally, the workers are encouraged to label every question-answer-fact tuple in more than three seconds. As we can see in <ref type="table" target="#tab_4">Table 2</ref>, with the increase of relevance score threshold, the R-VQA dataset has higher matched accuracy, together with a smaller data size. <ref type="figure" target="#fig_1">Figure 2</ref> shows two examples in the R-VQA dataset with a score threshold value of 0.30.</p><p>Data Analysis To balance the quality of matched facts and quantity of data sample, we compromise by choosing a matched score threshold value of 0.30, leading to a dataset of 198,889 samples with an average matched accuracy of 90.8% for all questionanswer-fact tuples. There are 5,833, 2,608, and 6,914 unique subjects, Q: What sport is the person playing? A: tennis R: (A man, playing, tennis) Q: How many animals are there? A: two R: (two horses, stand on, the grass)   relations, and objects, respectively, covering a wide range of semantic topics. In <ref type="table" target="#tab_6">Table 3</ref>, we can see the distribution of the most frequent subjects, relations, objects, and facts on the generated R-VQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relation Fact Detector</head><p>The Relation-VQA Dataset provides 198,889 image-question-answerfact with a matching score of 0.30. That is to say, for each image in the dataset, a question and a correct answer corresponding to the image content are provided, as well as a relation fact well supporting the question-answer data. As stated before, a relation fact describes semantic knowledge information, which benefits a VQA model a lot with better image understanding. For these reasons, we develop a relation fact detector to obtain a relation fact related to both the question and image semantic content. The fact detector will be further expanded in our relation fact-based VQA model, as illustrated in Section 5. Detector Modeling Given the input image and question, we formulate the fact prediction as a multi-task classification following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. For the image embedding layer, we feed the resized image to a pre-trained ResNet-152 <ref type="bibr" target="#b9">[10]</ref>, and take the output of the last convolution layer as a spatial representation of the input image content. Then we add a spatial average pooling layer to extract a dense image representation v ∈ R 2048 as v = Meanpooling(CNN(I )).</p><p>The Gated Recurrent Unit (GRU) network is adopted to encode the input question semantic feature as q ∈ R 2400 q = GRU(Q).  To encode the image and question in a shared semantic space, the feature representations v and q are fed into a linear transformation layer followed by a non-linear activate function, respectively, as the following equations,</p><formula xml:id="formula_8">f v = tanh(W v v + b v ), f q = tanh(W q q + b q ),<label>(9)</label></formula><p>where W v ,W b , b v , b q are the learnable parameters for linear transformation, and tanh is a hyperbolic tangent function. A joint semantic feature embedding is learned by combing the image and question embeddings in the common space,</p><formula xml:id="formula_9">h = tanh(W vh f v + W qh f q + b h ).<label>(10)</label></formula><p>where element-wise addition is employed for the fusion strategy of two modalities. After fusing the image and question representations, a group of linear classifiers are learned for predicting the subject, relation and object in a relation fact,</p><formula xml:id="formula_10">p sub = softmax(W hs h + b s ),<label>(11)</label></formula><formula xml:id="formula_11">p r el = softmax(W hr h + b r ),<label>(12)</label></formula><formula xml:id="formula_12">p ob j = softmax(W ho h + b o ),<label>(13)</label></formula><p>where p sub , p r el , p ob j denote the classification probabilities for subject, relation and object over pre-specific candidates, respectively. Our loss function combines the group classifiers as</p><formula xml:id="formula_13">L t = λ s L(s,ŝ) + λ r L(r,r ) + λ o L(o,ô) + λ w ∥W ∥ 2 ,<label>(14)</label></formula><p>where s, r, o are target subjects, relations, and objects, andŝ,r ,ô are the predicted results. λ s = 1.0, λ r = 0.8, λ o = 1.2 are hyperparameters obtained though grid search on the development set. L denotes the cross entropy criterion function used for multi-class classification. An L2 regularization term is added to prevent overfitting, and the regularization weight λ w is set to 1 × 10 −7 in our experiment.</p><p>Experiments Given an input image and question, the goal of the proposed relation detector is to generate a set of relation facts subject,relation,object related to semantic contents of both image and question. The possibility of a predicted fact is the sum of probabilities of the subject, relation, and object in Eqs 11-13. We conduct experiments on the training and development sets for learning, and the testing set for evaluation.</p><p>Before carrying out the experiments, some essential operations of data preprocessing are performed. It is observed that there exist some similar and synonymous elements in facts on R-VQA, which may confuse the training of fact detection. For example, "on" vs. "on the top of " vs. "is on", "tree" vs. "trees", etc. Therefore, we merge these ambiguous elements to their simplest forms based on alias concept dictionaries labeled by <ref type="bibr" target="#b14">[15]</ref>, e.g., "on the top of " and "is on" are simplified to "on". The merging results are shown in <ref type="table" target="#tab_9">Table 4</ref>. We take the most frequent subjects, relations, and objects from all   <ref type="table">Table 5</ref>: Results for the relation detector.</p><p>unique candidates in training data, which leads to 2,000 subjects, 256 relations and 2,000 objects, respectively, with more details shown in <ref type="table" target="#tab_9">Table 4</ref>. The evaluation metrics we report are recall@1, recall@5, and recall@10, similar to <ref type="bibr" target="#b22">[23]</ref>. recall@k is defined as the fraction of numbers the correct relation fact is predicted in the top k ranked predicted facts. The RMSProp learning method is adopted to train the detector, with an initial learning rate of 3 × 10 −4 , a momentum of 0.98 and a weight decay of 0.01. The batch size is set to 100, and dropout strategy is applied before every linear transformation layer.</p><p>Results <ref type="table">Table 5</ref> shows the experiment results on the R-VQA test set. The first part of <ref type="table">Table 5</ref> reports two baseline models, which fully supports that both image and question semantic information is beneficial to relation fact prediction. On the one hand, the model without question content (denoted as V only) shows a sharp drop in the accuracy of predicted facts. This phenomenon is intuitive since semantic facts and questions both come from textual modality, while images come from visual modality. In order to improve the semantic space of relation facts, we formulate fact prediction as a multi-objective classification problem, and candidate facts are combinations of three elements, namely a subject, a relation, and an object. Therefore, it is important to provide the question semantic information to reduce the space of candidate facts. On the other hand, the model without image content (denoted as Q only) suffers from limited prediction performance, indicating images also contain some useful semantic knowledge.</p><p>The second part of <ref type="table">Table 5</ref> illustrates that the model based on the merged R-VQA data (denoted as Ours -final) works much better than the model based on initial R-VQA data (denoted as Ours -no merge). Although existing methods have made good progress in visual detection achieving Rec@100 accuracy of 10-15% on Visual Genome for visual facts, these approaches are not suitable to predict question-related visual facts. In contrast with these works, our model incorporates the question feature for fact prediction, and achieves a much higher accuracy with a smaller candidate number of k, as well as a much simpler framework. In future work, it will be still meaningful to design a fine-grained model to obtain better prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUAL QUESTION ANSWERING WITH FACTS</head><p>The overall framework of our proposed multi-step attention network for VQA is demonstrated in <ref type="figure" target="#fig_3">Figure 4</ref>, which takes a semantic question and an image as inputs, and learns visual and semantic knowledge sequentially to infer the correct answer. Our proposed network consists of three major components: (A) Context-aware Visual Attention, (B) Fact-aware Semantic Attention, and (C) Joint Knowledge Embedding Learning. Context-aware visual attention is designed to select image regions associated with the input question and to obtain visual semantic representation of these regions. Factaware semantic attention aims to weigh detected relevant relation facts by the learned visual semantic representation, and to learn semantic knowledge. Finally, a joint knowledge embedding learning model is able to jointly encode visual and semantic knowledge and infer the most possible answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Context-aware Visual Attention</head><p>Similar to many previous VQA approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>, we adopt a question-aware visual attention mechanism to choose related image regions. Image Encoding We apply a ResNet-152 network <ref type="bibr" target="#b9">[10]</ref> to extract image feature embedding for an input image. The 2048×14×14 feature map from the last convolution layer is taken as the image visual feature v, which corresponds to 14 × 14 image regions with 2048 feature channels.</p><p>Question Encoding A gate recurrent unit (GRU) <ref type="bibr" target="#b6">[7]</ref> is used to encode the question embedding, which is widely adopted in NLP and multimodal tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41]</ref>. To be specific, given a question with T words Q = [q 1 , ..., q t , ..., q T ], where q t is the one hot vector of the question word at position t, we first embed them into a dense representation via a linear transformation x t = W e q t . At each time t, we feed the word embedding x t into the GRU layer sequentially, and the GRU recursively updates the current hidden state h t = GRU(h t −1 , x t ) with the input x t and previous hidden state h t −1 . Finally, we take the last hidden state h T as the question representation.</p><p>Visual Attention A visual attention mechanism is adopted to highlight image regions related to question semantic information, and to learn more effective multimodal features between textual and visual semantic information. First, we apply the multimodal lowrank bilinear pooling (MLB) method <ref type="bibr" target="#b13">[14]</ref> to merge two modalities of the question and image as</p><formula xml:id="formula_14">c = MLB(q, v).<label>(15)</label></formula><p>where context vector c contains both question and image semantic content. We map the context vector to attention weights via a linear transformation layer followed by a softmax layer,</p><formula xml:id="formula_15">m = softmax(W c c + b c ),<label>(16)</label></formula><p>where weights m has a size of 14 × 14, and the value of each dimension represents the semantic relevance between corresponding image region and the input question. The context-aware visual feature is calculated as weighted sum of representations over all image regions, which is given by:</p><formula xml:id="formula_16">v = 14×14 i=1 m(i)v(i).<label>(17)</label></formula><p>We further combine the context-aware visual feature with the question feature to obtain the final visual representation as</p><formula xml:id="formula_17">f v =ṽ • tanh(W q q + b q ),<label>(18)</label></formula><p>where • denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fact-aware Semantic Attention</head><p>Visual attention enables the mining of visual context-aware knowledge, such as object and spatial information, which is beneficial to questions mainly focusing on object detection. However, models only with visual attention may suffer from limited performance when more relation reasoning is required. Therefore, we incorporate a list of relation facts as semantic clues and propose a semantic attention model to weigh different relation facts for better answer prediction. Some existing studies mine semantic concepts or attributes as semantic knowledge to assist VQA models. Our proposed model differs from these works in two ways. On one hand, existing methods only mine concepts or attributes, while our model extracts relation facts containing concepts and attributes, obviously increasing the semantic capacity of the semantic knowledge used. On the other hand, concepts or attributes in previous works may be irrelevant to VQA, because they are extracted only considering image content and based on data or pre-trained CNN models from other tasks like caption and object recognition <ref type="bibr" target="#b40">[41]</ref>. In contrast, we build up the Relation-VQA dataset to train the relation fact detector directly focusing on both the input image and question. Fact Detection First, we incorporate the fact detector introduced previously in Section 4 into our VQA model. Given the input image and question, the fact detector is used to generate the most possible K relation facts as a candidate set T = [t 1 ; t 2 ; ...; t K ]. For a fact t i = (s i , r i , o i ), we embed each element of the fact into a common semantic space R n , and concatenate these three embeddings as the fact embedding as follows:</p><formula xml:id="formula_18">f t i = [W sh s i ,W r h r i ,W oh o i ] ∈ R 3n .<label>(19)</label></formula><p>Then we can obtain the representation of K fact candidate, denoted</p><formula xml:id="formula_19">as f T = [f t 1 ; f t 2 ; ...; f t K ] ∈ R K ×3n .</formula><p>Semantic Attention Second, we develop a semantic attention to find out important facts considering the input image and question. Concretely, we use the context-aware visual representation as a query to select significant facts in a candidate set. Similar to context-aware visual attention, given the context-aware visual embedding f v and fact embedding f T , we first obtain joint context representation c t and then calculate attention weight vector m t as follows:</p><formula xml:id="formula_20">c t = MLB(f v , f T ), (20) m t = softmax(W c t c t + b c t ).<label>(21)</label></formula><p>What is the man doing?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Fact Mining</head><p>Semantic Attention The final attended fact representation over candidate facts is calculated as</p><formula xml:id="formula_21">f s = K i=1 m t (i)f T (i),<label>(22)</label></formula><p>which serves as semantic knowledge information for answering visual questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Joint Knowledge Embedding Learning</head><p>Our proposed multi-step attention model consists of two attention components. One is visual attention which aims to select related image regions and output context-ware visual knowledge representation f v . Another is semantic attention which focuses on choosing related relation facts and output fact-ware semantic knowledge representation f s . We merge these two representations via elementwise addition with linear transformation and a non-linear activation function to jointly learn visual and semantic knowledge,</p><formula xml:id="formula_22">h = tanh(W vh f v + b v ) + tanh(W sh f s + b s ).<label>(23)</label></formula><p>As we formulate VQA as a multi-class classification task, a linear classifier is trained to infer the final answer,</p><formula xml:id="formula_23">p ans = softmax(W a h + b a ).<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS 6.1 Datasets and Evaluation Metrics</head><p>We evaluate our proposed model on two popular benchmark datasets, th VQA dataset <ref type="bibr" target="#b0">[1]</ref> and the COCO-QA dataset <ref type="bibr" target="#b26">[27]</ref>, due to large data sizes and various question types. The VQA dataset is annotated by Amazon Mechanical Turk (AMT), and contains 248,349 training instances, 121,512 validation instances and 244,302 testing instances based on a number of 123,287 unique natural images. The dataset is made up of three question categories including yes/no, number and other. For each question, ten answers are provided by different annotators. We take the top 2,000 most frequent answers following previous work <ref type="bibr" target="#b13">[14]</ref> as candidate answer outputs, which cover 90.45% of answers in training and validation sets. For testing, we train our model on the train+val set and report the testing result on the test-dev set from a VQA evaluation server maintained by <ref type="bibr" target="#b0">[1]</ref>. There are two different tasks, an open-ended task and a multi-choice task. For the open-ended task, we select the most possible answer from our candidate answer set, while for the multi-choice task, we choose the answer with the highest activation score among the given choices.</p><p>The COCO-QA dataset is another benchmark dataset, including 78,736 training questions and 38,948 testing questions. There are four question types, object, number, color, and location, which cover 70%, 7%, 17% and 6% of total question-answer pairs, respectively. All of the answers in the dataset are single words. As the COCO-QA dataset is smaller, we select all the unique answers as possible answers, which leads to a candidate set with a size of 430.</p><p>Evaluation Metric For the VQA dataset, we report the results following the evaluation metric provided by the authors of the dataset, where a predicted answer is considered correct only if more than three annotators vote for that answer, that is to say, Acc(ans) = min <ref type="bibr">(1,</ref> #humans vote for ans 3 ).</p><p>For the COCO-QA dataset, a predicted answer is regarded as correct if it is the same as the labeled answer in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation Details</head><p>For encoding question, the embedding size for each word is set to 620. For encoding facts in the VQA model, the top ten facts are generated and the size of element embedding size m is set as 900.</p><p>All other visual and textual representations are vectors of size 2400. We implement our model with the Torch computing framework, one of the most popular recent deep learning libraries. In our experiments, we utilize the RMSProp method for the training process with mini-batches of 200, an initial learning rate of 3 × 10 −4 , a momentum of 0.99, and a weight-decay of 10 −8 . The validation is performed every 10,000 iterations and early stopping is applied if the validation accuracy does not improve at the last five validations.   We use a drop strategy with a probability of 0.5 at every linear transformation layer to reduce overfitting.  <ref type="table">Table 8</ref>: Ablation study on the VQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with State-of-the-art</head><p>textual content, instead of only using the image <ref type="bibr" target="#b40">[41]</ref>. As our proposed R-VQA dataset extended from Visual Genome dataset shares similar image semantic space with current datasetes like VQA and COCO-QA, semantic knowledge learned from the fact detector can be easily transferred to the VQA task. These are the main reasons that RelAtt beats MLAN significantly. <ref type="table" target="#tab_12">Table 7</ref> compares our approach with state-of-the-arts on the COCO-QA dataset. Different from the VQA dataset, COCO-QA doesn't contain the multi-choice task, and fewer results are reported on it. Our model improves the state-of-the art QRU [18] from 62.50% to 65.15% with a growth of 2.65%. In particular, our model significantly outperforms the state-of-the-art semantic attention model AMA [33] by 3.77%, indicating the benefits of modeling semantic relation facts and learning semantic knowledge from R-VQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation Study</head><p>In this section, we conduct five ablation experiments to study the role of individual components designed in our model. <ref type="table">Table 8</ref> reports the ablation results of compared baseline models, which are trained on the training set, and evaluated on the validation set. Specifically, the ablation experiments are as follows:</p><p>• Q+I, where we only take the image and question to infer the answer, and image-question joint representation is learned by a simple fusion method of element wise addition. • Q+R, where only the question and relation facts generated by the detector are considered to predict the answer. • Q+I+Att, where we apply visual attention to learn the joint representation of the image and question. • RelAtt-Average, where the semantic attention mechanism denoted in Eqs 20 -22 is removed from our best model RelAtt. Instead, the fact representation is calculated by averaging different fact embeddings. • RelAtt-MUL, where element-wise addition is replaced by multiplication in Eq 23 to learn the joint knowledge embedding.</p><p>The results of first three ablated models indicate that visual attention provides limited visual information for question answering and relation facts can play an important role as they contain semantic information. A drop of 0.79% in accuracy for RelAtt-Average illustrates that semantic attention is essential to encode relation facts. Moreover, it is shown that the fusion method of element-wise addition might work better than multiplication when encoding joint visual-textual knowledge representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Case Study</head><p>To illustrate the capability of our model in learning relation facts as semantic knowledge, we show some examples on the VQA testing set with the image, question and predicted answer. We also list relation facts generated by the fact detector and their attention weights in the semantic attention component. For saving space, only five in ten relation facts are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. In <ref type="figure" target="#fig_4">Figures 5 (a)</ref> and (b), the fact detector mines semantic fact candidates related to both the image and the question, and semantic attention highlights the most possible facts for question answering. In <ref type="figure" target="#fig_4">Figures 5 (c)</ref> and (d), although given the same image, the fact detector can depend on the different questions to generate corresponding semantic facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we aim to learn visual relation facts from images and questions for semantic reasoning of visual question answering. We propose a novel framework by first learning a relation factor detector based on the built Relation-VQA (R-VQA) dataset. Then a multi-step attention model is developed to incorporate the detected relation facts with sequential visual and semantic attentions, enabling the effective fusion of visual and semantic knowledge for answering. Our comprehensive experiments show our method outperforms state-of-the-art approaches and demonstrate the effectiveness of considering visual semantic knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our proposed model, which learns to mine relation facts with semantic attention for visual question answering. KEYWORDS visual question answering; relation fact mining; semantic knowledge; attention network; question answering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Examples on the R-VQA dataset. For each imagequestion-answer pair, the dataset provides its aligned relation fact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Relation Fact Detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(Figure 4 :</head><label>4</label><figDesc>man, on, skateboard) (man, on, snow) (person, in, ski suit) …… (people, is, far away) Our proposed multi-step attention network for VQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Testing samples on the VQA test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>What is the man doing?</figDesc><table><row><cell></cell><cell cols="2">(man, on, skateboard)</cell></row><row><cell>Relation Fact Mining</cell><cell cols="2">(man, on, snow) (person, in, ski suit) ……</cell></row><row><cell></cell><cell cols="2">(people, is, far away)</cell></row><row><cell></cell><cell>Semantic Attention</cell><cell>QA Module</cell><cell>Skiing.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Types of relation facts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Basic statistics of the R-VQA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Top relation facts in the R-VQA dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Merging results of the R-VQA dataset for relation detector. After merging similar elements, the top element candidate in relation facts can cover more training data.</figDesc><table><row><cell></cell><cell cols="3">Element (Accuracy)</cell><cell></cell><cell>Fact (Recall)</cell><cell></cell></row><row><cell>Models</cell><cell>Sub.</cell><cell>Rel.</cell><cell>Obj.</cell><cell>R@1</cell><cell cols="2">R@5 R@10</cell></row><row><cell>V only</cell><cell>3.25</cell><cell>39.19</cell><cell>2.11</cell><cell>0.14</cell><cell>0.43</cell><cell>0.72</cell></row><row><cell>Q only</cell><cell cols="5">56.66 77.34 40.76 23.14 37.82</cell><cell>43.16</cell></row><row><cell cols="6">ours -no merge 65.98 74.79 43.61 25.23 44.25</cell><cell>51.26</cell></row><row><cell>ours -final</cell><cell cols="6">66.47 78.80 45.13 27.39 46.72 54.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>] 53.74 78.94 35.24 36.42 57.17 78.85 35.80 43.41 DPPnet [26] 57.22 80.71 37.24 41.69 62.48 80.79 38.94 52.16 FDA [12] 59.24 81.14 36.16 45.77 64.01 81.50 39.00 54.72 RelAtt (ours) 65.69 83.55 36.92 56.94 69.60 83.58 38.56 64.65</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Open-Ended</cell><cell></cell><cell cols="2">Multi-Choice</cell></row><row><cell>Method</cell><cell>All</cell><cell>Y/N</cell><cell>Num. Other</cell><cell>All</cell><cell>Y/N</cell><cell cols="2">Num. Other</cell></row><row><cell>LSTM Q+I [1DMN+ [35]</cell><cell cols="3">60.30 80.50 36.80 48.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SMem [36]</cell><cell cols="3">57.99 80.87 37.32 43.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [38]</cell><cell cols="3">58.70 79.30 36.60 46.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>QRU [18]</cell><cell cols="7">60.72 82.29 37.02 47.67 65.43 82.24 38.69 57.12</cell></row><row><cell>MRN [13]</cell><cell cols="7">61.68 82.28 38.82 49.25 66.15 82.30 40.45 58.16</cell></row><row><cell>MCB [8]</cell><cell cols="4">64.20 82.20 37.70 54.80 68.60</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MLB [14]</cell><cell cols="3">64.53 83.41 37.82 54.43</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>V2L [31]</cell><cell cols="3">57.46 78.90 36.11 40.07</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AMA [31]</cell><cell cols="3">59.17 81.01 38.42 45.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MLAN [41]</cell><cell cols="4">64.60 83.80 40.20 53.70 64.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Evaluation results for our proposed model and compared methods on the VQA dataset.</figDesc><table><row><cell>Method</cell><cell>All</cell><cell cols="4">Obj. Num. Color Loc.</cell></row><row><cell cols="6">2VIS+BLSTM [27] 55.09 58.17 44.79 49.53 47.34</cell></row><row><cell>IMG-CNN [25]</cell><cell>58.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DDPnet [26]</cell><cell>61.16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [38]</cell><cell cols="5">61.60 65.40 48.60 57.90 54.00</cell></row><row><cell>AMA [33]</cell><cell cols="5">61.38 63.92 51.83 57.29 54.84</cell></row><row><cell>QRU [18]</cell><cell cols="5">62.50 65.06 46.90 60.50 56.99</cell></row><row><cell>RelAtt (ours)</cell><cell cols="5">65.15 67.50 48.81 62.64 58.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Evaluation results for our proposed model and compared methods on the COCO QA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6</head><label>6</label><figDesc>demonstrates our proposed model for both open-ended and multi-choice tasks with state-of-the-arts on the VQA test set. Note that all listed approaches apply only one type of visual feature and the report results of a single model.The first part in the table shows models using simple multimodal joint learning without an attention mechanism. Models in the second part are based on visual attention, while models in the third part apply semantic attention to learn semantic knowledge like concepts and attributes. It's shown that our proposed multistep semantic attention network (denoted as RelAtt) improves the state-of-the-art MLAN [41] model from 64.60% to 65.69% on the open-ended task, and from 64.80% to 69.60% on the multi-choice task. To be specific, our model obtains the improvement of 2.51% in the question types Other. As the state-of-the-art model, apart form visual attention, MLAN uses semantic attention to mine important concepts based on image content. In contrast, our model RelAtt introduces relation facts instead of concepts as semantic knowledge, which obviously increase semantic capacity. Moreover, we train a relation detector to learn facts based on both visual and</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Q+I</cell><cell>53.22</cell></row><row><cell>Q+R</cell><cell>51.34</cell></row><row><cell>Q+I+Att</cell><cell>57.40</cell></row><row><cell>RelAtt-Average</cell><cell>57.84</cell></row><row><cell>RelAtt-MUL</cell><cell>58.12</cell></row><row><cell>RelAtt (final)</cell><cell>58.63</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank our anonymous reviewers for their constructive feedback and suggestions. This work was supported in part by the National Natural Science </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV &apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;14)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MU-TAN: Multimodal Tucker Fusion for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV &apos;17)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data (SIGMOD &apos;08)</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data (SIGMOD &apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Çaä §lar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Question-Guided Hybrid Convolution for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yikang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV &apos;18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems (NIPS &apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;17)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Incorporating External Knowledge to Answer Open-Domain Visual Questions with Dynamic Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00733</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Contextaware Attention Network for Interactive Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">Renqiang</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD &apos;17)</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD &apos;17)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="927" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual question answering with question representation update (qru)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems (NIPS &apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vip-cnn: Visual phrase guided convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7244" to="7253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognitio (CVPR &apos;17)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4408" to="4417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Content-based image retrieval using computational visual attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Hai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoyong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2554" to="2566" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Cognition Based Attention Model for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual Relationship Detection with Language Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV &apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7218" to="7225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Answer Questions from Image Using Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI &apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems (NIPS &apos;16)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic attention deep model for article recommendation by learning human editors&apos; demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD &apos;17)</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image Captioning and Visual Question Answering Based on Attributes and External Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4622" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML &apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring questionguided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Docchat: An information retrieval approach for chatbot engines using unstructured documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshe</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL &apos;16)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="516" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-level attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4709" to="4717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepintent: Learning attentions for online advertising with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Hao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei Mark</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD &apos;16)</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD &apos;16)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1295" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">User-guided Hierarchical Attention Network for Multi-modal Social Image Popularity Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference (WWW &apos;18</title>
		<meeting>the 2018 World Wide Web Conference (WWW &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1277" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
