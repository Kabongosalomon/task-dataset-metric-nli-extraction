<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Graph Parsing with Recurrent Neural Network DAG Grammars</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-20">20 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Fancellu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorcha</forename><surname>Gilroy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Peak AI</orgName>
								<orgName type="institution">L University of Edinburgh</orgName>
								<address>
									<settlement>Manchester, Edinburgh</settlement>
									<country>United Kingdom, United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre (SAIC)</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Graph Parsing with Recurrent Neural Network DAG Grammars</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-20">20 Oct 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic parses are directed acyclic graphs (DAGs), so semantic parsing should be modeled as graph prediction. But predicting graphs presents difficult technical challenges, so it is simpler and more common to predict the linearized graphs found in semantic parsing datasets using well-understood sequence models. The cost of this simplicity is that the predicted strings may not be wellformed graphs. We present recurrent neural network DAG grammars, a graph-aware sequence model that ensures only well-formed graphs while sidestepping many difficulties in graph prediction. We test our model on the Parallel Meaning Bank-a multilingual semantic graphbank. Our approach yields competitive results in English and establishes the first results for German, Italian and Dutch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing is the task of mapping natural language to machine interpretable meaning representations, which in turn can be expressed in many different formalisms, including lambda calculus <ref type="bibr" target="#b24">(Montague, 1973)</ref>, dependency-based compositional semantics <ref type="bibr" target="#b20">(Liang et al., 2011)</ref>, frame semantics <ref type="bibr" target="#b2">(Baker et al., 1998)</ref>, abstract meaning representations (AMR; <ref type="bibr" target="#b3">Banarescu et al. 2013</ref>), minimal recursion semantics (MRS; <ref type="bibr" target="#b8">Copestake et al. 2005)</ref>, and discourse representation theory (DRT; <ref type="bibr" target="#b16">Kamp 1981)</ref>.</p><p>Explicitly or implicitly, a representation in any of these formalisms can be expressed as a directed acyclic graph (DAG). Consider the sentence "Every ship in the dock needs a big anchor". Its meaning representation, expressed as a Discourse Representation Structure <ref type="bibr">(DRS, Kamp 1981)</ref>, is shown in <ref type="figure" target="#fig_0">Figure 1</ref> two parts: the top part lists variables for discourse referents (e.g. x 1 , e 1 ) and the bottom part can contain unary predicates expressing the type of a variable (e.g. ship, need), binary predicates specifying relationships between variables (e.g. PARTOF, TOPIC), logical operators expressing relationships between nested boxes (e.g. ⇒, ¬), or binary discourse relations (e.g., <ref type="bibr">RESULT, CONTRAST)</ref>. To express a DRS as a graph, we represent each box as a node labeled ; each variable as a node labeled by its associated unary predicate; and each binary predicate, logical operator, or discourse relation as an edge from the first argument to the second ( <ref type="figure" target="#fig_1">Figure 2</ref>). To fully realize the representation as a DAG, additional transformations are sometimes necessary: in DRS, when a box represents a presupposition, as box b 4 does, the label of the node corresponding to the presupposed variable is marked (e.g. x 2 /dock P ); and edges can be reversed (e.g. TOPIC(s 1 , x 3 ) becomes TOPICOF(s 1 , x 3 )).</p><p>Since meaning representations are graphs, semantic parsing should be modeled as graph prediction. But how do we predict graphs? A popular approach is to predict the linearized graphthat is, the string representation of the graph found in most semantic graphbanks. trates one style of linearization using PENMAN notation, in which graphs are written as wellbracketed strings which can also be interpreted as trees-note the correspondence between the treelike structure of <ref type="figure" target="#fig_1">Figure 2</ref> and the string in <ref type="bibr">Figure 3. 2</ref> Each subtree is a bracketed string starting with a node variable and its label (e.g. b 2 / ), followed by a list of relations corresponding to the outgoing edges of the node. A relation consists of the edge label prefixed with a colon (:), followed by either the subtree rooted at the target node (e.g. :DRS (x 1 /ship :PARTOF(x 2 /dock p ))), or a reference to the target node (e.g. :PIVOT x 1 ). By convention, if a node is the target of multiple edges, then the leftmost one is written as a subtree, and the remainder are written as references. Hence, every node is written as a subtree exactly once.</p><p>The advantage of predicting linearized graphs is twofold. The first advantage is that graphbank datasets usually already contain linearizations, which can be used without additional work. These linearizations are provided by annotators or algorithms and are thus likely to be very consistent in ways that are beneficial to a learning algorithm. The second advantage is that we can use simple, well-understood sequence models <ref type="bibr" target="#b13">(Gu et al., 2016;</ref><ref type="bibr" target="#b15">Jia and Liang, 2016;</ref><ref type="bibr" target="#b25">van Noord et al., 2018)</ref> to model them. But this simplicity comes with a cost: sequence models can predict strings that don't correspond to graphs-for example, strings with ill-formed bracketings or unbound variable names. While it is often possible to fix these strings with pre-or post-processing, we would prefer to model the problem in a way that does not require this.</p><p>(b 1 / :IMP 1 (b 2 / :DRS(x 1 /ship :PARTOF(x 2 /dock p ))) :IMP 2 (b 3 / :DRS(e 1 / need :PIVOT x 1 :THEME(x 3 / anchor :TOPICOF(s 1 / big))))) Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out <ref type="bibr" target="#b19">(Li et al., 2018)</ref>. <ref type="bibr" target="#b12">Groschwitz et al. (2018)</ref> model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; <ref type="bibr" target="#b23">Lyu and Titov (2018)</ref> model graphs with a conditional variant of the classic <ref type="bibr" target="#b11">Erdös and Rényi (1959)</ref> model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. <ref type="bibr" target="#b5">Buys and Blunsom (2017)</ref>, <ref type="bibr" target="#b7">Chen et al. (2018)</ref>, and <ref type="bibr" target="#b9">Damonte et al. (2017)</ref> all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable-a derivation tree or alignment-which must be accounted for via preprocessing or complex inference techniques.</p><p>Can we combine the simplicity of sequence prediction with the fidelity of graph prediction? We show that this is possible by developing a new model that predicts sequences through a simple string rewriting process, in which each rewrite corresponds to a well-defined graph fragment. Importantly, any well-formed string produced by our model has exactly one derivation, and thus no latent variables. We evaluate our model on the Parallel Meaning Bank (PMB, ), a multilingual corpus of sentences paired with DRS representations. Our model performs competitively on English, and better than sequence models in German, Italian, and Dutch.</p><p>2 Graph-aware string rewriting</p><p>We use a grammar to model the process of rewriting. Formally, our grammar is a graph grammar, specifically a restricted DAG grammar <ref type="bibr" target="#b4">(Björklund et al., 2016)</ref>, a type of contextfree graph grammar designed to model linearized DAGs. Since linearized DAGs are strings, we present it as a string-rewriting system, which can be described more compactly than a graph grammar while making the connection to sequences more explicit. The correspondence between string rewriting and graph grammars is given in the Appendix.</p><p>A grammar in our model is defined by a set Σ of terminal symbols consisting of all symbols that can appear in the final string-brackets, variable types, node labels, and edge labels; a set N of n + 1 nonterminal symbols denoted by {L, T 0 , . . . , T n }, for some maximum value n; an unbounded set V of variable references {$1, $2, . . . }; and a set of productions, which are defined below.</p><p>We say that T 0 is the start symbol, and for each symbol T i ∈ N , we say that i is its rank, and we say that L has a rank of 0. A nonterminal of rank i can be written as a function of i variable references-for example, we can write T 2 ($1, $2). By convention, we write the rank-0 nonterminals L and T 0 without brackets. Productions in our grammar take the form α → β, where α is a function of rank i over $1, . . . , $i; and β is a linearized graph in PENMAN format, with each of its subtrees replaced by either a function or a variable reference. Optionally, the variable name in β may replace one of the variable references in α. All variable references in a production must appear at least twice. Hence every variable reference in α must appear at least once in β, and variables that do not appear in α must appear at least twice in β.</p><p>To illustrate, we will use the following grammar, which can generate the string in <ref type="figure" target="#fig_2">Figure 3</ref>, assuming L can also rewrite as any node label.</p><formula xml:id="formula_0">T 0 → (b/ :IMP 1 T 1 ($1) :IMP 2 T 1 ($1)) (r 1 ) T 0 → (x/L) (r 2 ) T 0 → (s/L) (r 3 ) T 0 → (x/L :TOPICOF T 0 ) (r 4 ) T 1 ($1) → (b/ :DRS T 1 ($1)) (r 5 ) T 1 ($1) → (e/L :PIVOT $1 :THEME T 0 ) (r 6 ) T 1 (x) → (x/L :PARTOF T 0 ) (r 7 )</formula><p>Our grammar derives strings by first rewriting the start symbol T 0 , and at each subsequent step rewriting the leftmost function in the partially derived string, with special handling for variable references described below. A derivation is complete when no functions remain.</p><p>We illustrate the rewriting process in <ref type="figure">Figure 4</ref>. The start symbol T 0 at step 1 is rewritten by production r 1 in step 2, and the new b variable introduced at this step is deterministically renamed to the unique name b 1 . In step 3, the leftmost T 1 ($1) is rewritten by production r 5 , and the new b variable is likewise renamed to the unique b 2 . All productions apply in this way, simply replacing a left-hand-side function with a right-hand side expression. These rewrites are coupled with a mechanism to correctly handle multiple references to shared variables, as illustrated in Step 4 when production r 7 is applied. In this production, the left-hand-side function applies to the x variable naming the right-hand-side node. When this production applies, x is renamed to the unique x 1 as in previous steps, but because it appears in the left-hand-side, the reference $1 is bound to this new variable name throughout the partiallyderived string. In this way, the reference to x 1 is passed through the subsequent rewrites, becoming the target of a PIVOT edge at step 10. Derivations of a DAG grammar are context-free ( <ref type="figure" target="#fig_3">Figure 5</ref>). <ref type="bibr">3</ref> Our model requires an explicit grammar like the one in r 1 ...r 7 , which we obtain by converting each DAG in the training data into a sequence of productions. The conversion yields a single, unique sequence of productions via a simple linear-time algorithm that recursively decomposes a DAG into subgraphs <ref type="bibr" target="#b4">(Björklund et al., 2016)</ref>. Each subgraph consists of single node and its outgoing edges, as exemplified by the PENMAN-formatted righthand-sides of r 1 through r 7 . Each outgoing edge points to a nonterminal symbol representing a subgraph. If a subgraph does not share any nodes with its siblings, it is represented by T 0 . But if any subgraphs share a node, then a variable reference must refer for this node in the production associated with the lowest common ancestor of all its incoming edges. For example, in <ref type="figure" target="#fig_2">Figure 3</ref>, the common ancestor of the two edges targeting x 1 is the node b 1 , so production r 1 must contain two copies of variable reference $1 to account for this. A more mathematical account can be found along with a proof of correctness in <ref type="bibr" target="#b4">Björklund et al. (2016)</ref> Our implementation follows their description unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Network Realizer</head><p>We model graph parsing with an encoder-decoder architecture that takes as input a sentence w and outputs a directed acyclic graph G derived using the rewriting system of Section 2. Specifically, we model its derivation tree in top-down, left-toright order as a sequence of actions a = a 1 . . . a |a| , inspired by Recurrent Neural Network Grammars <ref type="bibr">(RNNG, Dyer et al., 2016)</ref>. As in RNNG, we use a stack to store partial derivations.</p><p>We model two types of actions: GEN-FRAG rewrites T i nonterminals, while GEN-LABEL rewrites L nonterminals, always resulting in a leaf of the derivation tree. A third REDUCE action is applied whenever a subtree of the derivation tree is complete, and since the number of subtrees is known in advance, it is applied deterministically. For example, when we predict r 1 , this determines that we must rewrite an L and then recursively rewrite two copies of T 1 ($1) and then apply RE-DUCE. Hence graph generation reduces to predicting rewrites only.</p><p>We define the probability of generating graph G conditioned of input sentence w as follows:</p><formula xml:id="formula_1">p(G|w) = p(a|w) = |a| i=1 p(a i |a &lt;i , w) (1)</formula><p>Input Encoder We represent the ith word w i of input sentence w = w 1 . . . w |w| using both learned and pre-trained word embeddings (w i and w p i respectively), lemma embedding (l i ), part-ofspeech embedding (p i ), universal semantic tag (Abzianidze and Bos, 2017) embedding (u i ), and dependency label embedding (d i ). <ref type="bibr">4</ref> An input x i is computed as the weighted concatenation of these features followed by a non-linear projection (with vectors and matrices in bold):</p><formula xml:id="formula_2">x i = tanh(W (1) [w i ; w p i ; l i ; p i ; u i ; d i ]) (2)</formula><p>Input x i is then encoded with a bidirectional LSTM, yielding contextual representation h e i . Graph decoder Since we know in advance whether the next action is GEN-FRAG or GEN-LABEL, we use different models for them.</p><p>GEN-FRAG. If step t rewrites a T nonterminal, we predict the production y t that rewrites it using context vector c t and incoming edge embedding e t . To obtain c t we use soft attention <ref type="bibr" target="#b22">(Luong et al., 2015)</ref> and weight each input hidden representation h e i to decoding hidden state h d t :</p><formula xml:id="formula_3">score(h e i , h d t ) = h e i W (2) h d t α ti = exp(score(h e i , h d t )) i ′ exp(score(h e i ′ , h d t )) c t = n i=1 α ti h e i y t = W (3) c t + W (4) e<label>(3)</label></formula><p>The contribution of c and e is weighted by matrices W (3) and W (4) , respectively. We then update the stackLSTM representation using the embedding of the non-terminal fragment y t (denoted as y e t ), as follows:</p><formula xml:id="formula_4">h d t+1 = LSTM(y e t , h d t )<label>(4)</label></formula><p>GEN-LABEL. Labels L can be rewritten to either semantic constants (e.g., 'speaker', 'now', 'hearer') or unary predicates that often corresponds to the lemmas of the input words (e.g., 'love') or. We predict the former using a model identical to the one for GEN-FRAG. For the latter, we use a selection mechanism to choose an input lemma to copy to output. We model selection following <ref type="bibr" target="#b21">Liu et al. (2018)</ref>, assigning each input lemma a score o ji that we then pass through a softmax layer to obtain a distibution:</p><formula xml:id="formula_5">o ji = h dT j W (5) h e i p copy i = SOFTMAX(o ji )<label>(5)</label></formula><p>where h i is the encoder hidden state for word w i . We allow the model to learn whether to use softattention or the selection mechanism through a binary classifier, conditioned on the decoder hidden state at time t, h d t . Similar to Equation <ref type="formula" target="#formula_4">(4)</ref>, we update the stackLSTM with the embedding of terminal predicted. 5 REDUCE. When a reduce action is applied, we use an LSTM to compose the fragments on top of 5 In the PMB, each terminal is annotated for sense (e.g. 'n.01', 's.01') and presupposition (e.g. for 'dock p ' in <ref type="figure" target="#fig_2">Figure 3</ref>) as well. We predict both the sense tag and whether a terminal is presupposed or not independently conditioned on the current stackLSTM state and the embedding of the main terminal labels but are not used to update the state of the stackLSTM.</p><p>Step Action</p><formula xml:id="formula_6">Production Result 1 START start T 0 2 GEN-FRAG r 1 (b 1 / :IMP 1 T 1 ($1) :IMP 2 T 1 ($1)) 3 GEN-FRAG r 5 (b 1 / :IMP 1 (b 2 / :DRS T 1 ($1)) :IMP 2 T 1 ($1)) 4 GEN-FRAG r 7 (b 1 / :IMP 1 (b 2 / :DRS (x 1 /L :PARTOF T 0 ) :IMP 2 T 1 (x 1 )) 5 GEN-LABEL L → ship (b 1 / :IMP 1 (b 2 / :DRS (x 1 /ship :PARTOF T 0 ) :IMP 2 T 1 (x 1 )) 6 GEN-FRAG r 2 (b 1 / :IMP 1 (b 2 / :DRS (x 1 /ship :PARTOF (x 2 /L)) :IMP 2 T 1 (x 1 )) 7 GEN-LABEL L → dock p (b 1 / :IMP 1 (b 2 / :DRS (x 1 /ship :PARTOF (x 2 /dock p ))</formula><p>:</p><formula xml:id="formula_7">IMP 2 T 1 (x 1 )) 8 REDUCE −− = 9 REDUCE −− = 8 GEN-FRAG r 5 (b 1 / :IMP 1 (b 2 / :DRS (x 1 /ship :PARTOF (x 2 /dock p )) :IMP 2 (b 3 / :DRS T 1 (x 1 ))) 9 GEN-FRAG r 6 (b 1 / :IMP 1 (b 2 / :DRS (x 1 /ship :PARTOF (x 2 /dock p )) :IMP 2 (b 3 / :DRS (e 1 /L :PIVOT x 1 :THEME T 0 ))) 10 GEN-LABEL L → need (b 1 / :IMP 1 (b 2 / :DRS (x 1 /ship :PARTOF (x 2 /dock p ))</formula><p>:IMP 2 (b 3 / :DRS (e 1 /need :PIVOT x 1 :THEME T 0 ))) <ref type="figure">Figure 4</ref>: A partial derivation of the string in <ref type="figure" target="#fig_2">Figure 3</ref>. The stack operations follow closely each step in the derivation, where GEN-FRAG and GEN-LABEL are invoked when rewriting a non-terminal T and a terminal L respectively. In the result of each step, the leftmost function is underlined, and is rewritten in the fragment in blue in the next step. On the other hand, a REDUCE operation is invoked when a generated fragment does not contain non-terminals T to expand further (in this partial derivation, this is the case of the result of production r 2 ). the stack. Using the derivation tree in <ref type="figure" target="#fig_3">Figure 5</ref> as reference, let [c 1 , ..., c n ] denote the embeddings of one or more sister nodes r i and p u the embedding of their parent node, which we refer to as children and parent fragments respectively. A reduce operation runs an LSTM over the children fragments and the parent fragment in order and then uses the final state u to update the stack LSTM as follows:</p><formula xml:id="formula_8">i = [c 1 ...c n , p u ] − → u = LST M (i t , h c t ) h d t+1 = LST M (u, h d t )</formula><p>The models are trained to minimize a cross-entropy loss objective J over the sequence of gold actions a i in the derivation:</p><formula xml:id="formula_9">J = − |a| i=1 log p(a i )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We evaluated our model on the Parallel Meaning Bank (PMB; ), a semantic bank where sentences in English, Italian, German, and Dutch have been annotated following Discourse Representation Theory <ref type="bibr" target="#b17">(Kamp and Reyle, 2013)</ref>. Lexical predicates in PMB are in English, even for non-English languages. Since this is not compatible with our copy mechanism, we revert predicates to their orignal language by substituting them with the lemmas of the tokens they are aligned to.  Statistics on the data and the grammar extracted from v.2.2.0 are reported in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Converting DRSs to Graphs</head><p>In this section we discuss how DRSs are converted to acyclic, single-rooted, and fully-instantiated graphs (i.e., how to translate <ref type="figure" target="#fig_0">Figure 1</ref> to <ref type="figure" target="#fig_1">Figure 2</ref>). In general, box structures are converted to acyclic graphs by rendering boxes and lexical predicates as nodes, while conditions, operators, and discourse relations become edge labels between these nodes. We consider main boxes (see b 2 , b 3 , and b 4 in <ref type="figure" target="#fig_0">Figure 1</ref> separately from presuppositional boxes (see b 1 ), which represent instances that are presupposed in the wider discourse context (e.g., definite expressions). Using <ref type="figure" target="#fig_1">Figure 2</ref> as an example, b 2 , b 3 and b 4 become nodes in the graph and material implication (IMP stands for ⇒) becomes an edge label. If an operator or a relation is binary, as in this case, we number the edge label so as to preserve the order of the operands.</p><p>For each node in a main box, we expand the graph by adding all relations and variables belonging to it. We identify the head of the first relation or the first referent mentioned as the head variable. These are 'ship (x 1 )' for b 2 , and 'need(e 1 )' for b 3 . We attach the head variable as a child of the box-node and follow the relations recursively to expand the subgraph. If while expanding a graph a variable in a condition is part of a presuppositional box, we introduce it as a new node and add to its label the superscript p . When expanding the DAG along the edge PartOf, since x 2 is also in the presuppositional box in <ref type="figure" target="#fig_0">Figure 1</ref>, we attach the node 'dock p '. Graphs extracted this way are mostly acyclic except for adjectival phrases and relative clauses where state variables can be them-selves root (e.g., big "a.01" s 1 ). We get rid of these extra roots by reversing the direction of the edge involved and adding an '-of' to the edge label to flag this change (see TOPIC-OF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System Comparison</head><p>We compared the performance of our graph parser (seq2graph below) with a sequence-to-sequence model (enchanced with a copy mechanism) which decodes to a string linearization of the graph similar to the one shown in <ref type="figure" target="#fig_2">Figure 3</ref> (seq2seq + copy below). We also compare against the recently proposed model of <ref type="bibr" target="#b25">van Noord et al. (2018)</ref>; they introduce a seq2seq model that generates a DRS as a concatenation of clauses, essentially a flat version of the standard box notation. The decoded string is made aware of the overall graph structure during preprocessing where variables are replaced by indices indicating when they were first introduced and their recency. In contrast, we model the graph structure explicitly. <ref type="bibr" target="#b25">van Noord et al. (2018)</ref> experimented with both word and character-based models, as well as with an ensemble of both, using word embedding features. Since all our models are word-based, we compare our results with their best word model, using word embedding features only (trained using 10-fold cross validation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Configurations</head><p>In addition to word embeddings 7 , we also report on experiments which make use of additional features. Specifically, for each word we add information about its universal PoS tag, lemma, universal semantic tag, and dependency label. <ref type="bibr">8</ref> In Section 2, we mentioned that given a production α → β, variable references in α should appear at least once in β (i.e., they should have the same rank). In all experiments so far, we did not model this constraint explicitly to investigate whether the model is able by default to predict rank correctly. However, in exploring model configurations we also report on whether adding this constrant leads to better performance . <ref type="bibr">7</ref> We used word embeddings pretrained with Glove and available at https://nlp.stanford.edu/projects/glove/.</p><p>8 Universal PoS tags, lemmas and dependency labels for all languages were obtained using pretrained UDPipe models available at http://ufal.mff.cuni.cz/udpipe#download; gold-standard universal semantic tags were extracted from the PMB release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cross-lingual Experiments</head><p>We conducted two sets of experiments: one monolingual (mono below) where we train and test on the same language and one cross-lingual (cross below), where we train a model on English and test it on one of the other three languages. The goal of our cross-lingual experiments is to examine whether we need data in a target language at all since the semantic representation itself is language agnostic and lexical predicates are dealt with via the copy mechanism. Most of the features mentioned above are cross-linguistic and therefore fit both mono and cross-lingual settings, with the exception of lemma and word embeddings, where we exclude the former and replaced the latter with multilingual word embeddings. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">System settings</head><p>For training, we used the Adam optimizer <ref type="bibr" target="#b18">(Kingma and Ba, 2014)</ref> with an initial learning rate of 0.001 and a decay rate of 0.1 every 10 epochs. Randomly initialized and pre-trained word embeddings have a dimensionality of 128 and 100 respectively, and all other features a dimensionality of 50. In all cross-lingual experiments, the pre-trained word embeddings have a dimensionality of 300. The LSTMs in the encoder and the decoder have a dimensionality of 150 and non-terminal and terminal embeddings during decoding have a dimensionality of 50. The system is trained for 30 epochs, with the best system chosen based on dev set performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Evaluation Metric</head><p>We evaluated our system by scoring the similarity between predicted and gold graphs. We used Counter <ref type="bibr" target="#b25">(van Noord et al., 2018)</ref>, an adaptation of Smatch  to Discourse Representation Structures where graphs are first transformed into a set of 'source node -edge labeltarget node' triples and the best mapping between the variables is found through an iterative hillclimbing strategy. Furthermore, Counter checks whether DRSs are well-formed in that all boxes <ref type="bibr">9</ref> We experimented with embeddings obtained with iterative procrustes (available at https://github.com/facebookresearch/MUSE) and with <ref type="bibr" target="#b14">Guo et al. (2016)</ref>'s 'robust projection' method where the embedding of non-English words is computed as the weighted average of English ones. We found the first method to perform better on cross-lingual word similarity tasks and used it in our experiments. should be connected, acyclic, with fully instantiated variables, and correctly assigned sense tags.</p><p>It is worth mentioning that there can be cases where our parser generates ill-formed graphs according to Counter; this is however not due to the model itself but to the way the graph is converted back in a format accepted by Counter.</p><p>All results shown are an averages over 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>System comparison <ref type="table" target="#tab_3">Table 2</ref> summarizes our results on the PMB gold data (v.2.1.0, test set). We compare our graph decoder against the system of van <ref type="bibr" target="#b25">Noord et al. (2018)</ref> and our implementation of a seq2seq model, enhanced with a copy mechanism. Overall, we see that our graph decoder outperforms both models. Moreover, it reduces the number of illformed representations without any specific constraints or post-processing in order to ensure the well-formedness of the semantics of the output.</p><p>The PMB (v.2.1.0) contains a large number of silver standard annotations which have been only partially manually corrected (see <ref type="table" target="#tab_1">Table 1</ref>). Following <ref type="bibr" target="#b25">van Noord et al. (2018)</ref>, we also trained our parser on both silver and gold standard data combined. As shown in <ref type="table" target="#tab_4">Table 3</ref>, increasing the training data improves performance but the difference is not as dramatic as in <ref type="bibr" target="#b25">van Noord et al. (2018)</ref>. We found that this is because our parser requires graphs that are fully instantiated -all unary predicates (e.g. ship(x)) need to be present for the graph to be fully connected, which is often not the case for silver graphs. Our model is at a disadvantage since it could exploit less training data; during grammar extraction we could not process around 20K sentences and in some cases could not reconstruct the whole graph, as shown by the conversion score. 10   We also experimented with the full gamut of additional features (+feats) as well as with ablations of individual feature classes. For comparsion, we also show the performance of a graph-to-string model (seq2seq+copy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model configurations</head><p>As can be seen, all linguistic features seem to improve performance. Restricting fragment selection by rank does not seem to improve the overall result showing that our baseline model is already able to predict fragments with the correct rank throughout the derivation. Subsequent experiments report results with this model using all linguistic features, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-lingual experiments</head><p>Results on DRT parsing for languages other than English are reported in <ref type="table" target="#tab_7">Table 5</ref>. There is no gold standard training data for non-English languages in the PMB (v.2.2.0). We therefore trained our parser on silver standard data but did use the provided gold standard data for development and testing (see <ref type="table" target="#tab_1">Table 1</ref>). We present two versions of our parser, one where we train and test on the same language (s2g mono-silver) and another one where a model is trained on English but tested on the other languages (s2g cross-silver). We also show the results of a sequence-to-sequence model enhanced with a copy mechanism.</p><p>In the monolingual setting, our graph parser outperforms the seq2seq baseline by a large margin; we hypothesize this is due to the large percentage of ill-formed semantics, mostly due to training on silver data. The difference in performance between our cross-lingual parser and the monolingual parser for all languages is small, and   in Dutch the two parsers perform on par, suggesting that English data and language independent features can be leveraged to build parsers in other languages when data is scarse or event absent. We also conducted various ablation studies to examine the contribution of individual features to crosslinguistic semantic parsing. Our experiments revealed that universal semantic tags are most useful, while the multilingual word embeddings that we have tested with are not. We refer the interested reader to the supplementary material for more detail on these experiments.</p><p>Error Analysis We further analyzed the output of our parser to gain insight as to what parts of meaning representation are still challenging. <ref type="table" target="#tab_9">Table 6</ref> shows a more detailed break-down of system output as computed by Counter, where operators (e.g., negation, implication), roles (i.e., binary relations, such as 'Theme'), concepts (i.e., unary predicates like 'ship'), and synsets (i.e., sense tags like 'n.01') are scored separately. Synsets are further broken down into into 'Nouns', 'Verbs', 'Adverbs', and 'Adjectives'. We compare our best  seq2graph models (+feats) trained on the PMB v.2.2.0, gold and gold+silver data respectively. Adding silver data helps with semantic elements (operators, roles and concepts), but does not in the case of sense prediction where the only category that benefits from additional data are is nouns. We also found that ignoring the prediction of sense tags altogether helps with the performance of both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we have introduced a novel graph parser that can leverage the power and flexibility of sequential neural models while still operating on graph structures. Heavy preprocessing tailored to a specific formalism is replaced by a flexible grammar extraction method that relies solely on the graph while yielding performance that is on par or better than string-based approaches. Future work should focus on extending our parser to other formalisms (AMR, MRS, etc.). We also plan to explore modelling alternatives, such as taking different graph generation oders into account (bottom-up vs. top-down) as well as predicting the components of a fragment (type, number of edges, edge labels) separately.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>. 1 A DRS is drawn as a box with The discourse representation structure for "Every ship in the dock needs a big anchor". For ease of reference in later figures, each box includes a variable corresponding to the box itself, at top right in gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FigureFigure 2 :</head><label>2</label><figDesc>The DRS of Figure 1 expressed as a DAG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The DAG of Figure 2 expressed as a string.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>A derivation tree corresponding to Figure 4. Solid edges rewrite T n nonterminals, while dotted rewrite L nonterminals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Data statistics of PMB v.2.2.0.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>reports on vari-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Model performance (Precision, Recall, F 1 ) on PMB data (v.2.1.0, test set); models were trained on gold standard data.</figDesc><table><row><cell></cell><cell>#sents</cell><cell>conversion score</cell><cell>F1 illformed</cell></row><row><cell cols="2">van Noord et al. (2018) 73,778</cell><cell>100%</cell><cell>82.7 1.10%</cell></row><row><cell>seq2seq+copy</cell><cell cols="3">56.694 89.58% 74.67 13.45%</cell></row><row><cell>seq2graph</cell><cell cols="3">56.694 89.58% 77.1 0.90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Model performance (Precision, Recall, F 1 ) on</cell></row><row><cell>PMB data (v.2.1.0, test set); models were trained on</cell></row><row><cell>gold and silver standard data combined.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Model performance (Precision, Recall, F 1 ) on PMB (v2.2.0, development set).</figDesc><table><row><cell>Italian</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>ill</cell></row><row><cell cols="5">s2s + copy mono-silver 61.33 72.42 66.41 14.80</cell></row><row><cell>s2g mono-silver</cell><cell cols="3">74.50 77.27 75.86</cell><cell>0.10</cell></row><row><cell>s2g cross-silver</cell><cell cols="3">70.35 71.91 71.12</cell><cell>0.00</cell></row><row><cell>German</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>ill</cell></row><row><cell cols="5">s2s + copy mono-silver 56.86 65.40 60.83 10.67</cell></row><row><cell>s2g mono-silver</cell><cell cols="3">66.44 69.34 67.86</cell><cell>0.80</cell></row><row><cell>s2g cross-silver</cell><cell cols="3">66.14 65.72 63.50</cell><cell>0.40</cell></row><row><cell>Dutch</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>ill</cell></row><row><cell cols="5">s2s + copy mono-silver 54.27 64.17 58.81 10.67</cell></row><row><cell>s2g mono-silver</cell><cell cols="3">63.50 68.37 65.84</cell><cell>0.86</cell></row><row><cell>s2g cross-silver</cell><cell cols="3">62.94 67.32 65.06</cell><cell>0.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Model performance across languages (Precision, Recall, F 1 ). Results are reported on the PMB test set (v.2.2.0) for each language.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>F 1 -scores of fine-grained evaluation on the PMB (v.2.2.0) development set; the seq2graph models trained on gold (left) and with gold and silver data combined (right) are compared.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity, our examples do not show time representations, though these are consistently present in our data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although PENMAN notation is now closely associated with AMR, it can represent quite arbitrary graphs as strings. Our actual implementation does not use PENMAN notation, but we use it here for expository purposes since it is relatively familiar; the underlying ideas are unchanged.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">However, the language of derived strings may not be context-free due to variable references.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Universal semantic tags are language neutral tags intended to characterize lexical semantics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">English data is available for both releases at https://github.com/RikVN/DRS_parsing; for the other languages, we used the officially released data available at http://pmb.let.rug.nl/data.php</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">The conversion score is computed using Counter where we consider the converted graph representation as a 'predicted' DRS and the original DRS structure as gold data. As a comparison, converting the gold DRS sturctures yields a conversion score of 99.8%.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yova Kementchedjhieva, Andreas Grivas and the three anonymous reviewers for their useful comments. This work was done while Federico Fancellu was a post-doctoral researcher at the University of Edinburgh. The views expressed are his own and do not necessarily represent the views of Samsung Research. We gratefully acknowledge the support of the European Research Council (Fancellu, Lapata; award number 681760, "Translating Multiple Modalities into Text"); and the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (Gilroy; grant EP/L016427/1) and the University of Edinburgh (Gilroy).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Abzianidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ludmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="242" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards universal semantic tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Abzianidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWCS 201712th International Conference on Computational SemanticsShort papers</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Between a rock and a hard place-uniform parsing for hyperedge replacement dag grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Björklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Drewes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ericson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language and Automata Theory and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust incremental neural semantic graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1215" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sequenceto-action: End-to-end semantic graph generation for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00773</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimal recursion semantics: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on language and computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="281" to="332" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An incremental parser for abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="536" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdös</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publicationes Mathematicae</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Amr dependency parsing with a typed semantic algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11465</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A representation learning framework for multi-source transfer parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2734" to="2740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A theory of truth and semantic representation. Formal semantics-the essential readings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="189" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">From discourse to logic: Introduction to modeltheoretic semantics of natural language, formal logic and discourse representation theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<title level="m">Learning deep generative models of graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discourse representation structure parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="429" to="439" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Amr parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05286</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The proper treatment of quantification in ordinary English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approaches to Natural Language</title>
		<editor>Hintikka, K., Moravcsik, J., and Suppes, P.</editor>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1973" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="221" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring neural methods for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Abzianidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
