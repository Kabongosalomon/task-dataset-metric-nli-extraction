<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONDITIONAL FLOW VARIATIONAL AUTOENCODERS FOR STRUCTURED SEQUENCE PREDICTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hanselmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph-Nikolas</forename><surname>Straehle</surname></persName>
						</author>
						<title level="a" type="main">CONDITIONAL FLOW VARIATIONAL AUTOENCODERS FOR STRUCTURED SEQUENCE PREDICTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prediction of future states of the environment and interacting agents is a key competence required for autonomous agents to operate successfully in the real world. Prior work for structured sequence prediction based on latent variable models imposes a uni-modal standard Gaussian prior on the latent variables. This induces a strong model bias which makes it challenging to fully capture the multi-modality of the distribution of the future states. In this work, we introduce Conditional Flow Variational Autoencoders (CF-VAE) using our novel conditional normalizing flow based prior to capture complex multi-modal conditional distributions for effective structured sequence prediction. Moreover, we propose two novel regularization schemes which stabilizes training and deals with posterior collapse for stable training and better fit to the target data distribution. Our experiments on three multi-modal structured sequence prediction datasets -MNIST Sequences, Stanford Drone and HighD -show that the proposed method obtains state of art results across different evaluation metrics. *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Anticipating future states of the environment is a key competence necessary for the success of autonomous agents. In complex real world environments, the future is highly uncertain. Therefore, structured predictions, one to many mappings <ref type="bibr" target="#b43">(Sohn et al., 2015;</ref><ref type="bibr" target="#b6">Bhattacharyya et al., 2018)</ref> of the likely future states of the world, are important. In many scenarios, these tasks can be cast as sequence prediction problems. Particularly, Conditional Variational Autoencoders (CVAE) <ref type="bibr" target="#b43">(Sohn et al., 2015)</ref> have been successful for such problems -from prediction of future pedestrians trajectories <ref type="bibr" target="#b31">(Lee et al., 2017;</ref><ref type="bibr" target="#b6">Bhattacharyya et al., 2018;</ref><ref type="bibr" target="#b35">Pajouheshgar &amp; Lampert, 2018)</ref> to outcomes of robotic actions <ref type="bibr" target="#b3">(Babaeizadeh et al., 2018)</ref>. The distribution of future sequences is diverse and highly multi-modal. CVAEs model diverse futures by factorizing the distribution of future states using a set of latent variables which are mapped to likely future states. However, CVAEs assume a standard Gaussian prior on the latent variables which induces a strong model bias <ref type="bibr" target="#b23">(Hoffman &amp; Johnson, 2016;</ref> which makes it challenging to capture multi-modal distributions. This also leads to missing modes due to posterior collapse <ref type="bibr" target="#b8">(Bowman et al., 2016;</ref><ref type="bibr" target="#b36">Razavi et al., 2019)</ref>.</p><p>Recent work <ref type="bibr" target="#b18">Gu et al., 2018)</ref> has therefore focused on more expressive Gaussian mixture based priors. However, Gaussian mixtures still have limited expressiveness and optimization suffers from complications e.g. determining the number of mixture components. In contrast, normalizing flows are more expressive and enable the modelling of complex multi-modal priors. Recent work on flow based priors <ref type="bibr" target="#b9">(Chen et al., 2017;</ref><ref type="bibr" target="#b53">Ziegler &amp; Rush, 2019)</ref>, have focused only on the unconditional (plain VAE) case. However, this not sufficient for CVAEs because in the conditional case the complexity of the distributions are highly dependent on the condition.</p><p>In this work, 1. We propose Conditional Flow Variational Autoencoders (CF-VAE) based on novel conditional normalizing flow based priors In order to model complex multi-modal conditional distributions over sequences. In <ref type="figure">Figure 1</ref>, we show example predictions of MNIST handwriting stroke of our CF-VAE. We observe that, given a starting stroke, our CF-VAE model with data dependent normalizing flow based latent prior captures the two main modes of the conditional distribution -i.e. 1 and 8 -while CVAEs with fixed uni-modal Gaussian prior predictions have limited diversity. 2. We propose a novel regularization scheme that stabilizes the optimization of the evidence lower bound Latent Prior Clustered Predictions Latent Prior</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustered Predictions</head><p>Standard Gaussian Prior -CVAE.</p><p>Conditional Flow Prior -Our CF-VAE. <ref type="figure">Figure 1</ref>: Clustered stroke predictions on MNIST sequences. Our multi-modal Conditional Normalizing Flow based prior (right) enables our regularized CF-VAE to capture the two modes of the conditional distribution, while predictions with uni-modal Gaussian prior (left) have limited diversity. Note, our 64D CF-VAE latent distribution is (approximately) projected to 2D using tSNE and KDE. and leads to better fit to the target data distribution. 3. We leverage our conditional flow prior to deal with posterior collapse which causes standard CVAEs to ignore modes in sequence prediction tasks. 4. Finally, our method outperforms the state of the art on three structured sequence prediction taskshandwriting stroke prediction on MNIST, trajectory prediction on Stanford Drone and HighD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Normalizing Flows. Normalizing flows are a powerful class of density estimation methods with exact inference. <ref type="bibr" target="#b15">(Dinh et al., 2015)</ref> introduced affine normalizing flows with triangular Jacobians. <ref type="bibr" target="#b16">(Dinh et al., 2017)</ref> extend flows with masked convolutions which allow for complex (non-autoregessive) dependence between the dimensions. In <ref type="bibr" target="#b27">(Kingma &amp; Dhariwal, 2018)</ref>, 1 × 1 convolutions were proposed for improved image generation compared to <ref type="bibr" target="#b16">(Dinh et al., 2017)</ref>. In <ref type="bibr" target="#b25">(Huang et al., 2018)</ref> normalizing flows are auto-regressive and <ref type="bibr" target="#b4">(Behrmann et al., 2019)</ref> extend it to ResNet. <ref type="bibr" target="#b33">(Lu &amp; Huang, 2019)</ref> extended normalizing flows to model conditional distributions. Here, we propose conditional normalizing flows to learn conditional priors for variational latent models. Here, we focus on the orthogonal direction of more expressive priors and the above approaches are compatible with our approach.</p><p>Recent work which focus more expressive priors include <ref type="bibr" target="#b34">(Nalisnick &amp; Smyth, 2017)</ref> which proposes a Dirichlet process prior and <ref type="bibr" target="#b17">(Goyal et al., 2017)</ref> which proposes a nested Chinese restaurant process prior. However, these methods require sophisticated learning methods. In contrast,  proposes a mixture of Gaussians based prior (with fixed number of components) which is easier to train and shows promising results on some image generation tasks. <ref type="bibr" target="#b9">(Chen et al., 2017)</ref>, proposes a inverse autoregressive flow based prior which leads to improvements in complex image generation tasks like CIFAR-10. (Ziegler &amp; Rush, 2019) proposes a prior for VAE based text generation using complex non-linear flows which allows for complex multi-modal priors. While these works focus on unconditional priors, we aim to develop more expressive conditional priors.</p><p>Posterior Collapse. Posterior collapse arises when the latent posterior does not encode useful information. Most prior work <ref type="bibr" target="#b50">(Yang et al., 2017;</ref><ref type="bibr" target="#b14">Dieng et al., 2019;</ref><ref type="bibr" target="#b22">Higgins et al., 2017)</ref> concentrate on unconditional VAEs and modify the training objective -the KL divergence term is annealed to prevent collapse to the prior. <ref type="bibr" target="#b32">Liu et al. (2019)</ref> extends KL annealing to CVAEs. However, KL annealing does not optimize a true lower bound of the ELBO for most of training. <ref type="bibr" target="#b51">Zhao et al. (2017)</ref> also modifies the objective to choose the model with the maximal rate. <ref type="bibr" target="#b36">Razavi et al. (2019)</ref> propose anti-causal sequential priors for text modelling tasks. Bowman et al. <ref type="formula" target="#formula_0">(2016)</ref>; <ref type="bibr" target="#b19">Gulrajani et al. (2017)</ref> proposes to weaken the decoder so that the latent variables cannot be ignored, however only unconditional VAEs are considered.  shows the advantage of normalizing flow based posteriors for preventing posterior collapse. In contrast, we study for the first time posterior collapse in conditional models on datasets with minor modes.</p><p>Structured Sequence Prediction. <ref type="bibr" target="#b21">Helbing &amp; Molnar (1995)</ref>; ; ; <ref type="bibr" target="#b20">Gupta et al. (2018)</ref>; <ref type="bibr" target="#b52">Zhao et al. (2019)</ref>; <ref type="bibr" target="#b42">Sadeghian et al. (2019)</ref> consider the problem of traffic participant trajectory prediction in a social context. Notably, <ref type="bibr" target="#b20">(Gupta et al., 2018;</ref><ref type="bibr" target="#b52">Zhao et al., 2019;</ref><ref type="bibr" target="#b42">Sadeghian et al., 2019)</ref> use generative adversarial networks to generate socially compliant trajectories. However, the predictions are uni-modal. <ref type="bibr" target="#b31">Lee et al. (2017)</ref>; <ref type="bibr" target="#b6">Bhattacharyya et al. (2018)</ref>; <ref type="bibr" target="#b38">Rhinehart et al. (2018)</ref>; <ref type="bibr" target="#b12">Deo &amp; Trivedi (2019)</ref>; <ref type="bibr" target="#b35">Pajouheshgar &amp; Lampert (2018)</ref> considers structured (one to many) predictions using -a CVAE, improved CVAE training, pushforward policies for vehicle ego-motion prediction, motion planning, spatio-temporal convolutional network respectively. <ref type="bibr" target="#b30">Kumar et al. (2019)</ref> proposes a normalizing flow based model for video sequence prediction, however the sequences considered have very limited diversity compared to the trajectory prediction tasks considered here.</p><p>Here, we focus on improving structured predictions using conditional normalizing flows based priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONDITIONAL FLOW VARIATIONAL AUTOENCODER (CF-VAE)</head><p>Our Conditional Flow Variational Autoencoder is based on the conditional variational autoencoder <ref type="bibr" target="#b43">(Sohn et al., 2015)</ref> which is a deep directed graphical model for modeling conditional data distributions p θ (y|x). Here, x is the sequence up to time t, x = x 1 , · · · , x t and y is the sequence to be predicted up to time T , y = y t+1 , · · · , y T . CVAEs factorize the conditional distribution using latent variables z -p θ (y|x) is factorized as p θ (y|z, x)p(z|x), where p(z|x) is the prior on the latent variables. During training, amortized variational inference is used and the posterior distribution q φ (z|x, y) is learnt using a recognition network. The ELBO is maximized, given by,</p><formula xml:id="formula_0">log(p θ (y|x)) ≥ E q φ (z|x,y) log(p θ (y|z, x)) − D KL (q φ (z|x, y)||p(z|x)).<label>(1)</label></formula><p>In practice, to simplify learning, simple unconditional standard Gaussian priors are used <ref type="bibr" target="#b43">(Sohn et al., 2015)</ref>. However, the complexity e.g. the number of modes of the target distributions p θ (y|x), is highly dependent upon the condition x. An unconditional prior demands identical latent distributions irrespective complexity of the target conditional distribution -a very strong constraint on the recognition network. Moreover, the latent variables cannot encode any conditioning information and this leaves the burden of learning the dependence on the condition completely on the decoder.</p><p>Furthermore, on complex conditional multi-modal data, Gaussian priors have been shown to induce a strong model bias <ref type="bibr" target="#b46">(Tomczak &amp; Welling, 2016;</ref><ref type="bibr" target="#b53">Ziegler &amp; Rush, 2019)</ref>. It becomes increasingly difficult to map complex multi-modal distributions to uni-modal Gaussian distributions, further complicated by the sensitivity of the RNNs encoder/decoders to subtle variations in the hidden states <ref type="bibr" target="#b8">(Bowman et al., 2016)</ref>. Moreover, the standard closed form estimate of the KL-divergence pushes the encoded latent distributions to the mean of the Gaussian leading to latent variable collapse <ref type="bibr" target="#b18">Gu et al., 2018)</ref> while discriminator based approaches <ref type="bibr" target="#b45">(Tolstikhin et al., 2017)</ref> lead to underestimates of the KL-divergence <ref type="bibr" target="#b40">(Rosca et al., 2017)</ref>.</p><p>Therefore, we propose conditional priors based on conditional normalizing flows to enable the latent variables to encode conditional information and allow for complex multi-modal latent representations. Next, we introduce our novel conditional non-linear normalizing flows followed by our novel regularized Conditional Flow Variational Autoencoder (CF-VAE) formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CONDITIONAL NORMALIZING FLOWS</head><p>Recently, normalizing flow <ref type="bibr" target="#b44">(Tabak et al., 2010;</ref><ref type="bibr" target="#b15">Dinh et al., 2015)</ref> based priors for VAEs have been proposed <ref type="bibr" target="#b9">(Chen et al., 2017;</ref><ref type="bibr" target="#b53">Ziegler &amp; Rush, 2019)</ref>. Normalizing flows allows for complex priors by transforming a simple base density e.g. standard Gaussian to a complex multi-modal density through a series of n layers of invertible transformations f i ,</p><formula xml:id="formula_1">f1 ←→ h 1 f2 ←→ h 2 · · · fn ←→ z.<label>(2)</label></formula><p>However, such flows cannot model conditional priors. In contrast to prior work, we utilize conditional normalizing flows to model complex conditional priors. Conditional normalizing flows also consists of a series of n layers of invertible transformations f i (with parameters ψ), however we modify the transformations f i such that they are dependent on the condition x,</p><formula xml:id="formula_2">|x f1|x ←→ h 1 |x f2|x ←→ h 2 |x · · · fn|x ←→ z|x.<label>(3)</label></formula><p>Further, in contrast to prior work <ref type="bibr" target="#b33">(Lu &amp; Huang, 2019;</ref><ref type="bibr" target="#b2">Atanov et al., 2019;</ref><ref type="bibr" target="#b1">Ardizzone et al., 2019)</ref> which use affine flows (f i ), we build upon <ref type="bibr" target="#b53">(Ziegler &amp; Rush, 2019)</ref> and introduce conditional nonlinear normalizing flows with split coupling. Split couplings ensure invertibility by applying a flow layer f i on only half of the dimensions at a time. To compute (5), we split the dimensions z D of the latent variable into halfs, z L = {1, · · · , D /2} and z R = { D /2, · · · , d} at each invertible layer f i . Our transformation takes the following form for each dimension z j alternatively from z L or z R ,</p><formula xml:id="formula_3">f −1 i (z j |z R , x) = j = a(z R , x) + b(z R , x) × z j + c(z R , x) 1 + (d(z R , x) × z j + g z R , x) 2 .<label>(4)</label></formula><p>where, z j ∈ z L . Details of the forward (generating) operation f i are in Appendix A. To ensure that the generated prior distribution is conditioned on x, in (4) and in the corresponding forward operation f i , the coefficients {a, b, c, d, g} ∈ R are functions of both the other half of the dimensions of z and the condition x (unlike Ziegler &amp; Rush <ref type="formula" target="#formula_0">(2019)</ref>). Finally, due to the expressive power of our conditional non-linear normalizing flows, simple spherical Gaussians base distributions were sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VARIATIONAL INFERENCE USING CONDITIONAL NORMALIZING FLOWS BASED PRIORS</head><p>Here, we derive the ELBO (1) for our novel regularized CF-VAE with our conditional flow based prior. In case of the standard CVAE with the Gaussian prior, the KL divergence term in the ELBO has a simple closed form expression. In case of our conditional flow based prior, we can use the change of variables formula to compute the KL divergence. In detail, given the base density p( |x) and the Jacobian J i of each layer i of the transformation, the log-likelihood of the latent variable z under the prior can be expressed using the change of variables formula,</p><formula xml:id="formula_4">log(p ψ (z|x)) = log(p( |x)) + n i=1 log(|det J i |).<label>(5)</label></formula><p>This change of variables allows us to evaluate the likelihood of latent variable z over the base distribution instead of the complex conditional prior and to express the KL divergence as,</p><formula xml:id="formula_5">−D KL (q φ (z|x, y)||p ψ (z|x)) = −E q φ (z|x,y) log(q φ (z|x, y)) + E q φ (z|x,y) log(p ψ (z|x)) = H(q φ ) + E q φ (z|x,y) log(p( |x)) + n i=1 log(|det J i |).<label>(6)</label></formula><p>where, H(q φ ) is the entropy of the variational distribution. Therefore, the ELBO can be expressed as,</p><formula xml:id="formula_6">log(p θ (y|x)) ≥ E q φ (z|x,y) log(p θ (y|z, x)) + H(q φ ) + E q φ (z|x,y) log(p( |x)) + n i=1 log(|det J i |) (7)</formula><p>Figure 2: CF-VAE. The decoder is regularized by removing conditioning (grey arrow) to prevent posterior collapse.</p><p>To learn complex conditional priors, we alternately optimize both the variational posterior distribution q φ (z|x, y) and the conditional prior p ψ (z|x) in <ref type="formula">(7)</ref>. This would allow the variational posterior q θ to match the conditional prior and vice-versa so that the ELBO <ref type="formula">(7)</ref> is maximized. However, in practice we observe instabilities during training and posterior collapse. Next, we introduce our novel regularization schemes to deal with both these problems.</p><p>Posterior Regularization for Stability (pR). The entropy and the log-Jacobian of the CF-VAE objective <ref type="formula">(7)</ref> are at odds with each other. The log-Jacobian favours the contraction of the base density. Therefore, log-Jacobian at the right of <ref type="formula">(7)</ref> is maximized when the conditional flow maps the base distribution ( ↔ z in <ref type="figure">Figure 2</ref>) to a low entropy conditional prior and thus a low entropy variational distribution q φ (z|x, y). Therefore, in practice we observe instabilities during training. We observe that either the entropy or the log-Jacobian term dominates and the data log-likelihood is fully or partially ignored. Therefore, we regularize the posterior q φ (z|x, y) by fixing the variance to C. This leads to a constant entropy term which in turn bounds the maximum possible amount of contraction, thus upper bounding the log-Jacobian. This encourages our model to concentrate on explaining the data and leads better fit to the target data distribution. Note that, although q φ (z|x, y) has fixed variance, this does not significantly effect expressivity as the marginal q φ (z|x) can be arbitrarily complex due to our conditional flow prior. Moreover, we observe that the LSTM based decoders employed demonstrate robust performance across a wide range of values C = [0.05, 0.25].</p><p>Condition Regularization for Posterior Collapse (cR). We observe missing modes when the target conditional data distribution has a major mode(s) and one or more minor modes (corresponding to rare events). This is because the condition x on the decoder is already enough to model the main mode(s). If the cost of ignoring the minor modes is out-weighed by the cost of encoding a more complex latent distribution reflecting all modes, the minor modes and the latent variables are ignored. We propose a novel regularization scheme by removing the additional conditioning x on the decoder, when the dataset in question has a dominating mode(s). This enabled by our novel conditional flow prior, which already ensures that conditioning information can be encoded in the latent space. This assumes a simpler factorization of the conditional distribution p θ (y|x) = p θ (y|z)p ψ (z|x). This ensures that the latent variable z cannot be ignored by the CF-VAE and thus must encode useful information. Note that this regularization scheme is only possible due to our conditional prior, the unconditional Gaussian prior of CVAE would always need to condition the decoder.</p><p>Finally, we discuss the integration of diverse sources of contextual information into the conditional prior p ψ (z|x) for even richer conditional latent distributions of our regularized CF-VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CONDITIONING PRIORS ON CONTEXTUAL INFORMATION</head><p>For prediction tasks, it is often crucial to integrate sources of contextual information e.g. past trajectories or environmental information for accurate predictions. As these sources are heterogeneous, we employ source specific networks to extract fixed length vectors from each source.</p><p>Past Trajectory. We encode the past trajectories using a LSTM to an fixed length vector x t . For efficiency we share the condition encoder between the conditional flow and the CF-VAE decoder.</p><p>Environmental Map. We use a CNN to encode environmental information to a set of region specific feature vectors. We apply attention conditioned on the past trajectory to extract a fixed length conditioning vector x m , such that x m contains information relevant to the future trajectory.</p><p>Interacting Agents. To encode information of interacting traffic participants/agents, we build on <ref type="bibr" target="#b11">Deo &amp; Trivedi (2018)</ref> and propose a fully convolutional social pooling layer. We aggregate information of interacting agents using a grid overlayed on the environment. This grid is represented using a tensor, where the past trajectory information of traffic participants are aggregated into the tensor indexed corresponding to the grid in the environment. In <ref type="bibr" target="#b11">Deo &amp; Trivedi (2018)</ref> past trajectory information is aggregated using a LSTM. We aggregate the past trajectory information into the tensor using 1 × 1 convolutions as it allows for stable learning and is computationally efficient. Finally, we apply several layers of k × k convolutions to capture interaction aware contextual features x p of traffic participants in the scene.</p><p>Due to the expressive power of our conditional non-linear normalizing flows, simple concatenation into a single vector x = {x t , x m , x t } was sufficient to learn powerful conditional priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our CF-VAE on three popular and highly multi-modal sequence prediction datasets. We begin with a description of our evaluation metrics and model architecture.</p><p>Evaluation Metrics. In line with prior work <ref type="bibr" target="#b31">(Lee et al., 2017;</ref><ref type="bibr" target="#b6">Bhattacharyya et al., 2018;</ref><ref type="bibr" target="#b35">Pajouheshgar &amp; Lampert, 2018;</ref><ref type="bibr" target="#b12">Deo &amp; Trivedi, 2019;</ref><ref type="bibr" target="#b7">Bhattacharyya et al., 2019)</ref>, we use the negative conditional log-likelihood (-CLL) and mean Euclidean distances of the oracle Top n% of N predictions. The oracle Top n% metric measures not only the coverage of all modes but also discourages random guessing for a reasonably large value of n (e.g. n = 10%). This is because, a model can only improve this metric by moving randomly guessed samples from an overestimated mode to the correct modes (detailed analysis in Appendix F). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MNIST SEQUENCES</head><p>The MNIST Sequence dataset (D. De Jong, 2016) consists of sequences of handwriting strokes of the MNIST digits. The state-of-the-art approach is the "Best-of-Many"-CVAE <ref type="bibr" target="#b6">(Bhattacharyya et al., 2018)</ref> with a Gaussian prior. We follow the evaluation protocol of <ref type="bibr" target="#b6">Bhattacharyya et al. (2018)</ref> and predict the complete stroke given the first ten steps. We also compare with, 1. A standard CVAE with uni-modal Gaussian prior; 2. A CVAE with a data dependent conditional mixture of Gaussians (MoG) prior; 3. A CF-VAE without any regularization ; 4. A CF-VAE without the conditional non-linear flow layers (CF-VAE-Affine, replaced with affine flows <ref type="bibr" target="#b33">(Lu &amp; Huang, 2019;</ref><ref type="bibr" target="#b2">Atanov et al., 2019)</ref>). We also experiment with a conditional MoG prior (see Appendix D and E). We use the same model architecture <ref type="bibr" target="#b6">(Bhattacharyya et al., 2018)</ref>   We report the results in <ref type="table" target="#tab_1">Table 1</ref>. We see that our CF-VAE with posterior regularization (pR) performs best. It has a performance advantage of over 20% against the state of the art BMS-CVAE. We see that without regularization (pR) (C = 0.2) there is a 40% drop in performance, highlighting the effectiveness of our novel regularization scheme. We further illustrate the modes captured and the learnt multi-modal conditional flow priors in <ref type="figure" target="#fig_1">Figure 3</ref>. We do not use condition regularization here (cR) as we do not observe posterior collapse. In contrast, the BMS-CVAE is unable to fully capture all modes -its predictions are pushed to the mean due to the strong model bias induced by the Gaussian prior. The results improve considerably with the multi-modal MoG prior (M = 3 components work best). We also experiment with optimizing the standard CVAE architecture. This improves performance only slightly (after increasing LSTM encoder/decoder units to 256 from 48, increasing the number of layers did not help). Moreover, our experiments with a conditional (MoG) AAE/WAE <ref type="bibr" target="#b18">(Gu et al., 2018)</ref> based baseline did not improve performance beyond the standard (MoG) CVAE, because the discriminator based KL estimate tends to be an underestimate <ref type="bibr" target="#b40">(Rosca et al., 2017)</ref>. This illustrates that in practice it is difficult to map highly multi-modal sequences to a Gaussian prior and highlights the need of a data-dependent multi-modal priors. Our CF-VAE still significantly outperforms the MoG-CVAE as normalizing flows are better at learning complex multi-modal distributions <ref type="bibr" target="#b27">(Kingma &amp; Dhariwal, 2018)</ref>. We also see that affine conditional flow based priors leads to a drop in performance (77.2 vs 74.9 CLL) illustrating the advantage of our non-linear conditional flows.   <ref type="figure">Figure 4</ref>: Randomly sampled predictions of our CF-VAE + pR model on the Stanford Drone. We observe that our prediction are highly multi-modal and is reflected by the Conditional Flow Priors. Note, our 64D CF-VAE latent distribution is (approximatly) projected to 2D using tSNE and KDE.</p><p>Figure 5: Comparison of our CF-VAE + pR (Red) and the "Shoutgun" baseline (Yellow) of (Pajouheshgar &amp; Lampert, 2018), Groundtruth (Blue). Initial conditioning trajectory in white. Our CF-VAE not only learns to capture the correct modes but also generates more fine-grained predictions.</p><p>The Stanford Drone dataset  consists of multi-model trajectories of traffic participant e.g. pedestrians, bicyclists, cars captured from a drone. Prior works follow two different evaluation protocols, 1. <ref type="bibr" target="#b31">(Lee et al., 2017;</ref><ref type="bibr" target="#b6">Bhattacharyya et al., 2018;</ref><ref type="bibr" target="#b35">Pajouheshgar &amp; Lampert, 2018)</ref> use 5 fold cross validation, 2. <ref type="bibr" target="#b41">Sadeghian et al., 2018;</ref><ref type="bibr" target="#b12">Deo &amp; Trivedi, 2019</ref>) use a single split. We evaluate using the first protocol in <ref type="table" target="#tab_3">Table 2</ref> and the second in <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mADE ↓ mFDE ↓</head><p>SocialGAN <ref type="bibr" target="#b20">(Gupta et al., 2018)</ref> 27.2 41.4 MATF GAN <ref type="bibr" target="#b52">(Zhao et al., 2019)</ref> 22.5 33.5 SoPhie <ref type="bibr" target="#b42">(Sadeghian et al., 2019)</ref> 16.2 29.3 Goal Prediction <ref type="bibr" target="#b12">(Deo &amp; Trivedi, 2019)</ref> 15.   <ref type="table" target="#tab_3">Table 2</ref>).</p><p>Additionally, <ref type="bibr" target="#b35">Pajouheshgar &amp; Lampert (2018)</ref> suggest a "Shotgun" baseline. This baseline extrapolates the trajectory from the last known position and orientation in 10 different ways -5 orientations: (0 • , ±8 • , ±15 • ) and 5 velocities: None or exponentially weighted over the past with coefficients (0, 0.3, 0.7, 1.0). This baseline obtains results at par with the state-of-the-art because it a good template which covers the most likely possible futures (modes) for traffic participant motion in this dataset. We report the results using 5 fold cross validation in <ref type="table" target="#tab_3">Table 2</ref>. We additionally compare to a mixture of Gaussians prior (Appendix D). We use the same model architecture as in <ref type="bibr" target="#b6">Bhattacharyya et al. (2018)</ref> and a CNN encoder with attention to extract features from the last observed RGB image (Appendix C). These visual features serve as additional conditioning (x m ) to our Conditional Flow model. We see that our CF-VAE model with RGB input and posterior regularization (pR) performs best -outperforming the state-of-art "Shotgun" and BMS-CVAE by over 20% (Error @ 4sec). We see that our conditional flows are able to utilize visual scene (RGB) information to improve performance (3.5 vs 3.6 Error @ 4sec). We also see that the MoG-CVAE and our CF-VAE + pR outperforms the BMS-CVAE, even without visual scene information. This again reinforces our claim that the standard Gaussian prior induces a strong model bias and data dependent multi-modal priors are needed for best performance. The performance advantage of CF-VAE over the MoG-CVAE again illustrates the advantage of normalizing flows at learning complex conditional multi-modal distributions. The performance advantage over the "Shotgun" baseline shows that our CF-VAE + pR not only learns to capture the correct modes but also generates more fine-grained predictions. The qualitative examples in <ref type="figure">Figure 5</ref> shows that our CF-VAE is better able to capture complex trajectories with sharp turns.</p><p>We report results using the single train/test split of <ref type="bibr" target="#b41">Sadeghian et al., 2018;</ref><ref type="bibr" target="#b12">Deo &amp; Trivedi, 2019)</ref> in <ref type="table" target="#tab_5">Table 3</ref>. We use the minimum Average Displacement Error (mADE) and minimum Final Displacement Error (mFDE) metrics as in <ref type="bibr" target="#b12">(Deo &amp; Trivedi, 2019)</ref>. The minimum is over as set of predictions of size N . Although this metric is less robust to random guessing compared to the Top n% metric, it avoids rewarding random guessing for a small enough value of N . We choose N = 20 as in <ref type="bibr" target="#b12">(Deo &amp; Trivedi, 2019)</ref>. Similar to the results with 5 fold cross validation, we observe 20% improvement over the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">HIGHD</head><p>The HighD dataset <ref type="bibr" target="#b29">(Krajewski et al., 2018)</ref> consists of vehicle trajectories recorded using a drone over highways. In contrast to other vehicle trajectory datasets e.g. NGSIM it contains minimal false positive trajectory collisions or physically improvable velocities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Context ADE ↓ FDE ↓ -CLL ↓   <ref type="bibr" target="#b13">Diehl et al. (2019)</ref>, which captures interactions, only narrowly outperforms the FF model. This dataset is challenging for CVAE based models as they frequently suffer from posterior collapse when a single mode dominates. This is clearly observed with our CVAE baseline in <ref type="table" target="#tab_7">Table 4</ref>. To prevent posterior collapse, we use the cyclic KL annealing scheme proposed in <ref type="bibr" target="#b32">Liu et al. (2019)</ref> (using a MoG prior did not help). This already leads to significant improvement over the deterministic FF and GAT baselines. We also observe posterior collapse with our CF-VAE model. Therefore, we regularize by removing additional conditioning (cR). Our CF-VAE + {pR,cR} with condition regularization significantly outperforms the CF-VAE + pR and CVAE baselines (with cyclic KL annealing), demonstrating the effectiveness of our condition regularization scheme (cR) in preventing posterior collapse. The addition of contextual information of interacting traffic participants using our convolutional social pooling network with 1×1 convolutions significantly improves performance (also see Appendix G), demonstrating the effectiveness of our conditional normalizing flow based priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we presented the first variational model for learning multi-modal conditional data distributions with Conditional Flow based priors -the Conditional Flow Variational Autoencoder (CF-VAE). Furthermore, we propose two novel regularization techniques -posterior regularization (pR) and condition regularization (cR) -which stabilizes training solutions and prevents posterior collapse leading to better fit to the target distribution. This techniques lead to better match to the target distribution. Our experiments on diverse sequence prediction datasets show that our CF-VAE achieves state-of-the-art results across different performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. CONDITIONAL NON-LINEAR NORMALIZING FLOWS</head><p>In Subsection 3.1 of the main paper, we describe the inverse operation f −1 i of our non-linear conditional normalizing flows. Here, we describe the forward operation. Note that while the forward operation is necessary to compute the likelihood (3) (in the main paper) during training, the forward operation is necessary to sample from the latent prior distribution of our CF-VAE. The forward operation consists of solving for the roots of the following equation (more details in <ref type="bibr" target="#b53">(Ziegler &amp; Rush, 2019)</ref>),</p><formula xml:id="formula_7">− bd 2 ( j ) 3 + ((z j − a)d 2 − 2dgb)( j ) 2 + (2dg(z j − a) − b(g 2 + 1)) j + ((z j − a)(g 2 + 1) − c) = 0<label>(8)</label></formula><p>This equation has one real root which can be found analytically (Holmes). As mentioned in the main paper, note that the coefficients {a, b, c, d, g} are also functions of the condition x (unlike <ref type="bibr" target="#b53">(Ziegler &amp; Rush, 2019)</ref>   <ref type="bibr" target="#b2">(Atanov et al., 2019;</ref><ref type="bibr" target="#b33">Lu &amp; Huang, 2019)</ref> and our conditional non-linear (Cond NL) flows. We see that the conditional affine flows cannot fully capture multi-modal distributions ("tails" between modes), while our conditional non-linear flows does not have distinctive "tails".</p><p>We compare conditional affine flows of <ref type="bibr" target="#b2">(Atanov et al., 2019;</ref><ref type="bibr" target="#b33">Lu &amp; Huang, 2019)</ref> and our conditional non-linear (Cond NL) flows in <ref type="figure" target="#fig_2">Figure 6</ref> and <ref type="figure">Figure 7</ref>. We plot the conditional distribution p(y|x) and the corresponding condition x in the second and first columns. We use 8 and 16 layers of flow in case of the densities in <ref type="figure" target="#fig_2">Figure 6</ref> and <ref type="figure">Figure 7</ref> respectively. We see that the estimated density by the conditional affine flows of <ref type="bibr" target="#b2">(Atanov et al., 2019;</ref><ref type="bibr" target="#b33">Lu &amp; Huang, 2019)</ref> contains distinctive "tails" in case of <ref type="figure" target="#fig_2">Figure 6</ref> and discontinuities in case of <ref type="figure">Figure 7</ref>. In comparison our conditional non-linear flows does not have distinctive "tails" or discontinuities and is able to complex capture the multi-modal distributions better. Note, the "ring"-like distributions in <ref type="figure">Figure 7</ref> cannot be well captured by more traditional methods like Mixture of Gaussians. We see in <ref type="figure">Figure 8</ref> that even with 64 mixture components, the learnt density is not smooth in comparison to our conditional non-linear flows. This again demonstrates the advantage of our conditional non-linear flows. Given x in, p(y|x) Cond Affine Flow Our Cond NL Flow <ref type="figure">Figure 7</ref>: Comparison between conditional affine flows of <ref type="bibr" target="#b2">(Atanov et al., 2019;</ref><ref type="bibr" target="#b33">Lu &amp; Huang, 2019)</ref> and our conditional non-linear (Cond NL) flows. We see that the conditional affine flows cannot fully capture "ring"-like conditional distributions (note the discontinuity at the top), while our conditional non-linear flows does not have such discontinuities. </p><formula xml:id="formula_8">p ξ (z|x) = M i=1 p(c i |x)N (z; µ i , σ i |x).<label>(9)</label></formula><p>We use a simple feed forward neural network that takes in the condition x (see Section 3.4 of the main paper) and predicts the parameters of the MoG, ξ = {c 1 , µ 1 , σ 1 , · · · , c M , µ M , σ M }. Note, to ensure a reasonable number of parameters, we consider spherical Gaussians. Similar to (5) in the main paper, the ELBO can be expressed as,</p><formula xml:id="formula_9">log(p θ (y|x)) ≥ E q φ (z|x,y) log(p θ (y|z, x)) + H(q φ ) + E q φ (z|x,y) log(p ξ (z|x)).<label>(10)</label></formula><p>Note that we fix the entropy of the posterior distribution q φ for stability APPENDIX E. ADDITIONAL EVALUATION ON THE MNIST SEQUENCE DATASET Here, we perform a comprehensive evaluation using the MoG prior with varying mixture components. Moreover, we experiment with a CVAE with unconditional non-linear flow based prior (NL-CVAE). We report the results in <ref type="table" target="#tab_11">Table 5</ref>.  As mentioned in the main paper, we see that the MoG-CVAE outperforms the plain CVAE. This again reinforces our claim that the standard Gaussian prior induces a strong model bias. We see that using M = 3 components with the variance of the posterior distribution fixed to C = 0.2 leads to the best performance. This is expected as 3 is the most frequent number of possible strokes in the MNIST Sequence dataset. Also note that the results with the MoG prior are also relatively robust across C = [0.05, 0.2] as we learn the variance of the prior (see the section above). Finally, our CF-VAE + pR still significantly outperforms the MoG-CVAE (74.9 vs 84.6). This is expected as normalizing flows are more powerful compared to MoG at learning complex multi-modal distributions (Kingma &amp; Dhariwal, 2018) (also see <ref type="figure">Figure 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We also see that using an unconditional non-linear flow based prior actually harms performance (107.6 vs 96.4). This is because the latent distribution is highly dependent upon the condition. Therefore, without conditioning information the non-linear conditional flow learns a global representation of the latent space which leads to out-of-distribution samples at prediction time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F. EVALUATION OF THE ROBUSTNESS OF THE TOP N% METRIC</head><p>We use two simpler uniform "Shotgun" baselines to study the robustness of the Top n% metric against random guessing. In particular, we consider the "Shotgun"-u90 • and "Shotgun"-u135 • baselines which: given a budget of N predictions, it uniformly distributes the predictions between (−90 • , 90 • ) and (−135 • , 135 • ) respectively of the original orientation and using the velocity of the last time-step.</p><p>In <ref type="table" target="#tab_12">Table 6</ref> we compare the Top 1 (best guess) to Top 10% metric with N= 50, 100, 500 predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method K</head><p>Error @ 1sec Error @ 2sec Error @ 3sec Error @ 4sec</p><p>Top 1 (Best Guess)</p><p>"Shotgun"-u90 • 50 0.9 1.9 3.1 4.4 "Shotgun"-u90 • 100 0.9 1.9 3.0 4.3 "Shotgun"-u90 • 500 0.9 1.9 3.0 4.3</p><p>Top 10%</p><p>"Shotgun"-u90 • 50 1.2 2.5 3.9 5.4 "Shotgun"-u90 • 100 1.2 2.5 3.9 5.4 "Shotgun"-u90 • 500 1.2 2.5 3.9 5.4</p><p>Top 1 (Best Guess)</p><p>"Shotgun"-u135 • 50 0.9 2.0 3.1 4.5 "Shotgun"-u135 • 100 0.9 1.9 3.0 4.3 "Shotgun"-u135 • 500 0.9 1.9 3.0 4.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top 10%</head><p>"Shotgun"-u135 • 50 1.4 2.9 4.5 6.2 "Shotgun"-u135 • 100 1.4 2.9 4.5 6.2 "Shotgun"-u135 • 500 1.4 2.9 4.5 6.2 We see that in case of both the "Shotgun"-u90 • and "Shotgun"-u135 • baselines, the Top 1 (best guess) metric improves with increasing number of guesses. This effect is even more pronounced in case of the "Shotgun"-u135 • baseline as the random guesses are distributed over a larger spatial range. In contrast, the Top 10% metric remains remarkably stable. This is because, in order to improve the Top 10% metric, random guessing is not enough -the predictions have to be on the correct modes. In other words, the only way to improve the Top 10% metric is move random predictions to any of the correct modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX G. QUALITATIVE EXAMPLES ON THE HIGHD DATASET</head><p>We show qualitative examples on the HighD dataset in <ref type="figure">Figure 9</ref>. In the left of <ref type="figure">Figure 9</ref> we show 128 random samples from the HighD test set. In the middle we show predictions on these samples by the CVAE (with cyclic Kl annealing <ref type="bibr" target="#b32">(Liu et al., 2019)</ref>). We see that even with cyclic KL annealing, we observe posterior collapse. All samples have been pushed towards the mean and the variance in the 5 samples per test set example is minimal. E.g. note the top most sample track from the test set in <ref type="figure">Figure 9</ref> (left). All CVAE sample predictions are a linear continuation of the trajectory (continuing on the same lane), while there is in fact a turn (change of lanes). In contrast, our CF-VAE + {pR,cR} sample predictions are much more diverse and cover such eventualities. This also shows that our CF-VAE + {pR,cR} does not suffer from such posterior variable collapse. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Variational</head><label></label><figDesc>Autoencoders. The original variational autoencoder (Kingma &amp; Welling, 2014) used uni-modal Gaussian prior and posterior distributions. Thereafter, two lines of work have focused on developing either more expressive prior or posterior distributions. Rezende &amp; Mohamed (2015) propose normalizing flows to model complex posterior distributions. Kingma et al. (2016); Tomczak &amp; Welling (2016); Berg et al. (2018) present more complex inverse autoregessive flows, householder and Sylvester normalizing flow based posteriors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Random samples clustered using k-means. The number of clusters is set manually to the number of expected digits. The corresponding priors of our CF-VAE + pR on the right. Note, our 64D CF-VAE latent distribution is (approximately) projected to 2D using tSNE and KDE.Conditional Flow Model Architecture. Our conditional flow prior consists of 16 layers of conditional non-linear flows with split coupling. Increasing the number of conditional non-linear flows generally led to "over-fitting" on the training latent distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Comparison between conditional affine flows of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Evaluation on MNIST Sequences.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MethodVisual Error @ 1sec Error @ 2sec Error @ 3sec Error @ 4sec -CLL ↓</figDesc><table><row><cell cols="2">"Shotgun" (Top 10%) (Pajouheshgar &amp; Lampert, 2018) None</cell><cell>0.7</cell><cell>1.7</cell><cell>3.0</cell><cell>4.5</cell><cell>91.6</cell></row><row><cell>DESIRE-SI-IT4 (Top 10%) (Lee et al., 2017)</cell><cell>RGB</cell><cell>1.2</cell><cell>2.3</cell><cell>3.4</cell><cell>5.3</cell><cell>x</cell></row><row><cell>STCNN (Top 10%) (Pajouheshgar &amp; Lampert, 2018)</cell><cell>RGB</cell><cell>1.2</cell><cell>2.1</cell><cell>3.3</cell><cell>4.6</cell><cell>x</cell></row><row><cell>BMS-CVAE (Top 10%) (Bhattacharyya et al., 2018)</cell><cell>RGB</cell><cell>0.8</cell><cell>1.7</cell><cell>3.1</cell><cell>4.6</cell><cell>126.6</cell></row><row><cell>MoG-CVAE, M = 3 (Top 10%)</cell><cell>None</cell><cell>0.8</cell><cell>1.7</cell><cell>2.7</cell><cell>3.9</cell><cell>86.1</cell></row><row><cell>CF-VAE -no regularization (Ours, Top 10%)</cell><cell>None</cell><cell>0.9</cell><cell>1.9</cell><cell>3.3</cell><cell>4.7</cell><cell>96.2</cell></row><row><cell>CF-VAE + pR, C = 0.2 (Ours, Top 10%)</cell><cell>None</cell><cell>0.7</cell><cell>1.5</cell><cell>2.5</cell><cell>3.6</cell><cell>84.6</cell></row><row><cell>CF-VAE + pR, C = 0.2 (Ours, Top 10%)</cell><cell>RGB</cell><cell>0.7</cell><cell>1.5</cell><cell>2.4</cell><cell>3.5</cell><cell>84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Five fold cross validation on the Stanford Drone dataset. Euclidean error at ( 1 /5) resolution.</figDesc><table><row><cell>4.2 STANFORD DRONE</cell></row><row><cell>Sampled Predictions Latent Prior Sampled Predictions Latent Prior Sampled Predictions Latent Prior</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on the Stanford Drone dataset on a single split (see also</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Evaluation on the HighD dataset.</figDesc><table><row><cell>The HighD dataset is challenging because</cell></row><row><cell>lane changes or interactions are rare ∼ 10%</cell></row><row><cell>of all trajectories. The distribution of future</cell></row><row><cell>trajectories contain a single main mode (lin-</cell></row><row><cell>ear continuations) along with several minor</cell></row><row><cell>modes. Thus, approaches which predict a</cell></row><row><cell>single mean trajectory (targeting the main</cell></row><row><cell>mode) are challenging to outperform. In Ta-</cell></row><row><cell>ble 4, we see that the simple Feed Forward</cell></row><row><cell>(FF) model performs well and the Graph</cell></row><row><cell>Convolutional GAT model of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>MoG) model. We see that even with 64 mixture components, the learnt density is not smooth in comparison to our conditional non-linear flows.APPENDIX C. ADDITIONAL DETAILS OF OUR MODEL ARCHITECTURESHighD. We use the same model architecture with both the CVAE and CF-VAE models. As in the Stanford drone dataset, we use LSTM condition encoder on the input sequence x and the decoder LSTM network with 64 hidden neurons each and the LSTM recognition network q θ with 128 hidden neurons. The contextual information of interacting traffic participants are encoded into a spatial grid tensor of size 13×3 (see Section 3.2 of the main paper). We use a CNN with 5 layers of sizes 64, 128, 256, 256 and 256 to extract contextual features.APPENDIX D. DETAILS OF THE MIXTURE OF GAUSSIANS (MOG) BASELINEIn the main paper, we include results on the MNIST Sequence and Stanford Drone dataset with a Mixture of Gaussians (MoG) prior. In detail, instead of a normalizing flow, we set the prior to a MoG form,</figDesc><table><row><cell>p(y|x)</cell><cell>MoG, M = 4</cell><cell>MoG, M = 8</cell><cell>MoG, M = 32</cell><cell>MoG, M = 64</cell><cell>Our Cond NL Flow</cell></row><row><cell cols="6">Figure 8: Comparison between our conditional non-linear (Cond NL) flows and a Mixture of</cell></row><row><cell>Gaussians (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Evaluation on MNIST Sequences (CLL: lower is better).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Five fold cross validation on the Stanford Drone dataset. Euclidean error at ( 1 /5) resolution.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Predictions on the HighD dataset. Left: 128 random samples from the HighD test set (in yellow). Middle: CVAE predictions (5 samples per test set example). Right: Our CV-VAE + {pR,cR} predictions (5 samples per test set example). While the predictions by the CVAE are linear continuations, our CF-VAE sample predictions are much more diverse and cover events like lane changes e.g. top most sample track from the test set.</figDesc><table><row><cell>Groundtruth samples</cell><cell>CVAE samples</cell><cell>Our CF-VAE + {pR,cR} samples</cell></row><row><cell>Figure 9:</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Here, we provide details of the model architectures used across the three datasets used in the main paper.MNIST Sequences. We use the same model architecture as in<ref type="bibr" target="#b6">Bhattacharyya et al. (2018)</ref>. The LSTM condition encoder on the input sequence x, the LSTM recognition network q θ and the decoder LSTM network has 48 hidden neurons each. Also as in<ref type="bibr" target="#b6">Bhattacharyya et al. (2018)</ref>, we use a 64 dimensional latent space.Stanford Drone. Again, we use the same model architecture as in<ref type="bibr" target="#b6">Bhattacharyya et al. (2018)</ref> except for the CNN encoder. The LSTM condition encoder on the input sequence x and the decoder LSTM network has 64 hidden neurons each. The LSTM recognition network q θ has 128 hidden neurons. Also as in<ref type="bibr" target="#b6">Bhattacharyya et al. (2018)</ref>, we use a 64 dimensional latent space. Our CNN encoder has 6 convolutional layers of size 32, 64, 128, 256, 512 and 512. We predict the attention weights on the final feature vectors using the encoding of the LSTM condition encoder. The attention weighted feature vectors are passed through a final fully connected layer to obtain the final CNN encoding. Furthermore, we found it helpful to additionally encode the past trajectory as an image (as in<ref type="bibr" target="#b35">(Pajouheshgar &amp; Lampert, 2018)</ref>) as provide this as an additional channel to the CNN encoder.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing inverse problems with invertible neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Lynton Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wirkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Rahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><forename type="middle">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Klessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Köthe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semiconditional normalizing flows for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Atanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Volokhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sylvester normalizing flows for variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate and diverse sampling of sequences based on a best of many sample objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian prediction of future street scenes using synthetic likelihoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CONLL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<title level="m">Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder. In ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The mnist sequence dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">De</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jong</surname></persName>
		</author>
		<ptr target="https://edwin-de-jong.github.io/blog/mnist-sequence-data/" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional social pooling for vehicle trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiket</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scene induced multi-modal trajectory forecasting via planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiket</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph neural networks for modelling traffic participant interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Truong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Avoiding latent variable collapse with generative skip models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Nonparametric variational auto-encoders for hierarchical representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasoon</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dialogwae: Multimodal response generation with conditional wasserstein auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12352</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pixelvae: A latent variable model for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><forename type="middle">Ali</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Elbo surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew J Johnson</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The use of hyperbolic cosines in solving cubic polynomials. The Mathematical Gazette</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holmes</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural autoregressive flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kloeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ITSC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01434</idno>
		<title level="m">Videoflow: A flow-based generative model for video</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cyclical annealing schedule: A simple approach to mitigating kl vanishing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structured output learning with conditional generative flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stick-breaking variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Back to square one: probabilistic trajectory forecasting without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Pajouheshgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPs Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Preventing posterior collapse with delta-vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">R2p2: A reparameterized pushforward policy for diverse, precise generative path forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vernaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Variational approaches for auto-encoding generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04987</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Car-net: Clairvoyant attentive recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdinand</forename><surname>Legros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Voisin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Vesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Density estimation by dual ascent of the log-likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tabak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications in Mathematical Sciences</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01558</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving variational auto-encoders using householder flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vae with a vampprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5756" to="5766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Riemannian normalizing flow on variational wasserstein autoencoder for text modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhuang</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Improved variational autoencoders for text modeling using dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Infovae: Information maximizing variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02262</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-agent tensor fusion for contextual trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Latent normalizing flows for discrete sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
