<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Aging and Rejuvenation by Conditional Multi-Adversarial Autoencoder with Ordinal Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
							<email>jpzhang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
							<email>jwang@ist.psu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">College of Information Sciences and Technology</orgName>
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<postCode>16802</postCode>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Aging and Rejuvenation by Conditional Multi-Adversarial Autoencoder with Ordinal Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial aging and facial rejuvenation analyze a given face photograph to predict a future look or estimate a past look of the person. To achieve this, it is critical to preserve human identity and the corresponding aging progression and regression with high accuracy. However, existing methods cannot simultaneously handle these two objectives well. We propose a novel generative adversarial network based approach, named the Conditional Multi-Adversarial AutoEncoder with Ordinal Regression (CMAAE-OR). It utilizes an age estimation technique to control the aging accuracy and takes a high-level feature representation to preserve personalized identity. Specifically, the face is first mapped to a latent vector through a convolutional encoder. The latent vector is then projected onto the face manifold conditional on the age through a deconvolutional generator. The latent vector preserves personalized face features and the age controls facial aging and rejuvenation. A discriminator and an ordinal regression are imposed on the encoder and the generator in tandem, making the generated face images to be more photorealistic while simultaneously exhibiting desirable aging effects. Besides, a high-level feature representation is utilized to preserve personalized identity of the generated face. Experiments on two benchmark datasets demonstrate appealing performance of the proposed method over the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facial aging and facial rejuvenation, also referred to as age progression and age regression, aim to render a face photograph with natural aging and rejuvenating effects on the individual's face <ref type="bibr" target="#b1">[Fu et al., 2010]</ref>. It has broad applications, including facial appearance prediction, cross-age face recognition <ref type="bibr" target="#b5">[Park et al., 2010]</ref>, finding missing children, and movie entertainment . Although the problem has attracted much attention of the research community, there are serious challenges, primarily from the intrinsic complexity of aging in the physical world and a shortage of labeled aging photograph data. Generally speaking, aging accuracy and <ref type="figure">Figure 1</ref>: Example simulation results of our aging and rejuvenation process. The first image of each row is the provided face photograph, with the actual age marked. The other images are machine-generated face images for the specified ages of the same person.</p><p>identify permanence are commonly regarded as two crucial metrics to evaluate the quality of facial aging and rejuvenation in the recent literature <ref type="bibr" target="#b7">[Shu et al., 2015;</ref><ref type="bibr" target="#b8">Suo et al., 2010;</ref><ref type="bibr" target="#b9">Yang et al., 2016]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Early approaches were mainly based on the skin's anatomical structure and simulated the profile growth and facial muscle changes with respect to the elapsed time <ref type="bibr" target="#b6">[Ramanathan and Chellappa, 2008]</ref>. Although these methods provided novel insights for facial aging synthesis, they are difficult to generalize for other tasks because of their complex modeling techniques.</p><p>The later data-driven approaches can be roughly categorized into prototype-based methods <ref type="bibr" target="#b8">[Tiddeman et al., 2001;</ref><ref type="bibr" target="#b3">Kemelmacher-Shlizerman et al., 2014;</ref><ref type="bibr" target="#b7">Shu et al., 2015]</ref> and physical model based ones <ref type="bibr" target="#b8">[Suo et al., 2010;</ref><ref type="bibr" target="#b5">Park et al., 2010;</ref><ref type="bibr" target="#b8">Suo et al., 2012]</ref>. By dividing training data into several disjoint age groups, prototype-based approaches learn a transformation over these age groups <ref type="bibr" target="#b0">[Burt and Perrett, 1995;</ref><ref type="bibr" target="#b3">Kemelmacher-Shlizerman et al., 2014]</ref>. Because they only consider the general aging mechanism, they are simple and fast. However, they neglect personalized information, causing them to generate unrealistic images. Although <ref type="bibr" target="#b7">[Shu et al., 2015]</ref> utilized dictionary learning to estimate the age pattern of each age group from the corresponding sub-dictionary, this approach presents serious ghosting artifacts. Physical model based approaches, on the other hand, employ parametric models to simulate the aging mechanisms of the muscles <ref type="bibr" target="#b8">[Suo et al., 2012]</ref>, the wrinkle <ref type="bibr" target="#b6">[Ramanathan and Chellappa, 2008;</ref><ref type="bibr" target="#b8">Suo et al., 2010]</ref>, the skin, and the skull of a particular individual <ref type="bibr" target="#b4">[Lanitis et al., 2002;</ref>]. Nevertheless, they suffer from a complex modeling procedure with high computational cost. Moreover, it is difficult for these approaches to collect a large ground truth face dataset, with a long time span of each individual, to model the subtle aging mechanism.</p><p>Recently, the generative adversarial networks (GANs) have shown an impressive ability in generating synthetic images <ref type="bibr" target="#b2">[Goodfellow et al., 2014;</ref><ref type="bibr" target="#b2">Gauthier, 2014;</ref><ref type="bibr" target="#b5">Radford et al., 2015]</ref>, and facial aging and rejuvenation <ref type="bibr">Duong et al., 2017;</ref><ref type="bibr" target="#b11">Zhang et al., 2017;</ref>. For example,  transformed faces across different ages smoothly by modeling the intermediate transition states in a RNN model. And <ref type="bibr" target="#b11">[Zhang et al., 2017]</ref> proposed conditional adversarial autoencoder to simulate facial muscle sagging caused by aging. These approaches render faces with more appealing aging effects and less ghosting artifacts compared to the earlier methods. However, aging accuracy and identity permanence can hardly be achieved simultaneously. The reason is that they focus more on modeling facial transformation between age groups, where the age factor plays a dominant role while the identity information plays a subordinate role. Furthermore, learning facial aging between age groups does not allow the generation of facial images for an arbitrary age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our Approach</head><p>In this paper, we propose a novel GANs-based approach, named Conditional Multi-Adversarial Autoencoder with Ordinal Regression (CMAAE-OR), which combines the advantage of GANs in synthesizing visually plausible images and that of ordinal regression in accurate age estimation. Compared with existing methods, our method can simultaneously handle the identity permanence and aging accuracy better on facial aging and rejuvenation. Concretely, CMAAE-OR utilizes a convolutional encoder to extract a latent feature from an input face photograph, followed by projecting the feature onto the face manifold conditional on age through a deconvolutional generator. The encoder and the generator are trained with four parts, (1) an age-distance-based weighted squared Euclidean loss in the image space, (2) the identity loss to minimize the input-output distance by a latent feature representation, which embeds personalized characteristics from a pre-trained encoder, (3) the GAN loss that encourages generated faces to be indistinguishable from actual faces, and (4) the ordinal regression loss to force generated faces to exhibit desirable aging effect. These four parts simultaneously ensure that the resultant faces present desired aging effects and the identity properties remain stable. In contrast to the previous approaches that regarded an age group as a conditional input, the proposed method allows a specific age input and utilizes an age estimation technique to ensure the aging accu-racy. Consequently, CMAAE-OR produces more photorealistic and aging accurate images, as shown in <ref type="figure">Figure 1</ref>. The main contributions are summarized as follows:</p><p>1. The proposed method incorporates face verification and age estimation techniques to preserve identity permanence and achieve high aging accuracy. In addition, our framework accepts an arbitrary age as the conditional input, instead of a pre-defined discrete age group.</p><p>2. An age-distance-based weighted squared Euclidean loss in facial image space is utilized in our framework to emphasize the aging effect.</p><p>3. Experimental results illustrate the appealing performances of the proposed method in facial aging and rejuvenation. Besides, our method is robust against variations in pose, eyeglasses, and occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Method</head><p>We introduce the proposed Conditional Multi-Adversarial Autoencoder with Ordinal Regression (CMAAE-OR) in detail. Then the objective function of CMAAE-OR is described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conditional Multi-Adversarial Autoencoder with Ordinal Regression</head><p>In our framework, the provided actual face image is first mapped to a latent vector through a convolutional encoder E. Then the vector is projected onto the face manifold conditional with a desired age through a deconvolutional generator G. The latent vector preserves personalized face features and the age controls facial aging or rejuvenation. To predict aging trend well and keep person-specific information stable, a compound training procedure with four different loss functions is employed. Specifically, (1) an age-distancebased weighted squared Euclidean loss in the image space is used for eliminating input-output gap, (2) the identity loss is for minimizing the input-output distance in a high-level feature representation which embeds the personalized characteristics, (3) the GAN loss is for encouraging generated faces to be indistinguishable from the actual faces, and (4) the ordinal regression loss is for ensuring the aging accuracy of the generated faces. The detailed structure of the CMAAE-OR is shown as <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Encoder &amp; Generator: Facial aging and rejuvenation only require a forward pass through encoder E and a generator G. The encoder E maps the input face x to a feature vector, i.e., E(x) = z ∈ R n , where n is the dimension of the face feature. Given z and conditional age label , the generator G generates the output facex = G(z, ). Unlike existing GANrelated works <ref type="bibr" target="#b11">[Zhang et al., 2017;</ref>, the age label is a specific age with one dimension rather than an age group with a one-hot age label, so that a specific aged face can be generated.</p><p>Discriminator: According to the principle of conditional generative adversarial networks (cGANs) <ref type="bibr">[Mirza and Osindero, 2014]</ref>, the discriminator D on face images forces the generator G to yield more realistic faces. The goal of the generator G is to confuse the discriminator D through capturing the distribution of true face, whereas the optimization  (1) the GAN loss that encourages generated faces to be indistinguishable from the provided actual faces, (2) the ordinal regression loss that makes generated faces exhibit desirable aging effect, (3) the weighted squared Euclidean loss in the image space that eliminates the input-output gap, and (4) the identity loss to minimizes the input-output distance by a latent features z, which embeds personalized characteristics.</p><p>procedure of D is to distinguish the natural face images from the ones generated by G. The risk function of optimizing this mini-max two-player game can be written as:</p><formula xml:id="formula_0">V(D, G) = min G max D E x, ∼p data (x, ) log[D(x, )] +(1) E x, ∼p data(x, ) log[1 − D(G(x, ))]</formula><p>, where x denotes an actual face image following a certain distribution p data and is a conditional age label with one dimension. After the process converges, the distribution of the synthesized images p g is equivalent to p data . Accordingly, the training process alternately minimizes the following equations:</p><formula xml:id="formula_1">L gan−d = E x, ∼p data (x, ) [− log D(x, )] + (2) E x, ∼p data (x, ) [− log(1 − D(G(E(x), )))] , L gan−g = E x, ∼p data (x, ) [log(1 − D(G(E(x), )))] . (3)</formula><p>Ordinal Regression: Once the aforementioned procedure is completed, an age estimation technique is used to help the generated image become more accurate in the age-level. In our case, a CNN-based ordinal regression method R is introduced to estimate the age of the generated face image, because it has achieved a remarkable accuracy in the age estimation area <ref type="bibr" target="#b5">[Niu et al., 2016]</ref>. The regression loss for the generated image is written as</p><formula xml:id="formula_2">L regression = E x∼p data R(G(E(x), )) − 2 2 .</formula><p>(4) Here || · || 2 2 is the L 2 distance between feature representations. For more implementation details of ordinal regression R, readers are referred to <ref type="bibr" target="#b5">[Niu et al., 2016]</ref>.</p><p>Identity Preservation: Another core issue of facial aging and rejuvenation is to keep the person-dependent or personspecific properties consistent. By measuring the input-output distance in a proper feature space that is sensitive to the identify change and relatively robust to other variations, we incorporate an associate constraint into our proposed model. Specifically, we utilize an encoder E (pre-trained by training dataset) to extract a latent vector z, which preserves personal identity in high-level feature representation for each face image. Then the identity loss can be written as</p><formula xml:id="formula_3">L identity = ||E(G(x, )) − E(x)|| 2 2 .<label>(5)</label></formula><p>The architecture of the pre-trained encoder is the same as the encoder E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Objective Function</head><p>Besides the three aforementioned loss functions, an agedistance-based weighted squared Euclidean loss in the image space is adopted for further eliminating the input-output gap, e.g., the color aberration. Because the original squared Euclidean loss may eliminate the facial aging effect to some degree, we proposed an age-distance-based weighted squared Euclidean loss to avoid this issue. The proposed weighting strategy is based on the intuition that the higher the gap in age, the larger the gap in input-output faces. The formulation of this loss can be written as</p><formula xml:id="formula_4">L pixel = 1 ∆ × W × H × C ||G(x, ) − x|| 2 2 ,<label>(6)</label></formula><p>where W , H, and C correspond to the shape of image x (e.g. width, height, and channel) and ∆ = | in − out |. Here in and out are the age labels of the input face and the generated face, respectively. Finally, the objective functions for the generator G and the discriminator D are written as:</p><formula xml:id="formula_5">L G = λ p L pixel + λ i L identity + (7) λ g L gan−g + λ r L regression , L D = L gan−d ,<label>(8)</label></formula><p>where λ p , λ i , λ g and λ r are the coefficients of pixel loss, identity loss, GAN loss, and ordinal regression loss, respectively. These coefficients are trade-offs between face aging accuracy and identity performance.</p><p>In our framework, we first pre-train a convolutional encoder E as a face identity descriptor and a CNN-based ordinal regression R as an age estimator based on the training dataset. Then G and D are trained alternately by Eqs. <ref type="formula">(7)</ref> and <ref type="formula" target="#formula_5">(8)</ref> until optimality reaches. Finally, G learns the desired age transformation pattern and D becomes a reliable discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>We perform a comprehensive comparison between our proposed approach and several published methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>In our experiments, we utilized three datasets for training and evaluation: the MORPH dataset <ref type="bibr">[Ricanek and Tesafaye, 2006]</ref>, the UTKFace dataset <ref type="bibr" target="#b11">[Zhang et al., 2017]</ref>, and the FGNET dataset 1 .</p><p>The MORPH is a large publicly available aging database, consisting of subject's ethnicity, height, weight, and gender. It contains 55,608 facial images and the age of each subject ranges from 16 to 77 years, with the average age being approximately 33 years. The UTKFace is also a large-scale face dataset with long age span, ranging from 0 to 116 years. This dataset includes 23,709 facial images with annotations of age, gender, and ethnicity. The third facial aging dataset, the FGNET, consisting of only 1,002 images of 82 subjects, is used for testing in our work. In addition, images from these three datasets cover a wide variations in eyeglasses, pose, facial expression, illumination, occlusion, etc.</p><p>In order to make the training phase effective, we align all the faces according to 68 landmarks in each face <ref type="bibr" target="#b2">[Kazemi and Josephine, 2014]</ref>. Each facial image is cropped to 128 × 128 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The architecture of the CMAAE-OR is constructed as in <ref type="figure" target="#fig_0">Figure 2</ref>. Specifically, we normalize four parameters into [0, 1]. They are (1) the pixel values of the input images, (2) the output z of E by using a sigmoid activation function, (3) the value of the label through dividing the maximal age of each dataset, and (4) the output of the network G through using the sigmoid function. Furthermore, the desired age label is 1 http://www.prima.inrialpes.fr/FGnet/ concatenated to z, forming the input of G. Based on our experiences, such a normalization helps the training process converge faster. In E, G, and D, the convolution of stride 2 is employed instead of pooling (e.g., max pooling) because strided convolution is fully differentiable and allows the network to learn its own spatial downsampling <ref type="bibr" target="#b5">[Radford et al., 2015]</ref>. Note that we do not use the batch normalization (BN) for E and G because it blurs personal features and makes the generated faces drift far away from inputs in testing. However, BN will make the framework more stable if it is applied on D. All intermediate layers of each block (i.e., E, G, D, and R) use the ReLU as the activation function. Further, paddings are added to the layers to make the size of the input and the output identical.</p><p>The coefficients of four parameters λ p , λ i , λ g , and λ r are set to 0.10, 1.00, 1.00, and 0.02 for the MORPH, and 0.50, 1.00, 1.00 and 0.01 for the UTKFace, respectively. At the training stage, we employ Adam <ref type="bibr" target="#b4">[Kingma and Ba, 2014]</ref> with the initial learning rate of 1 × 10 −4 and the weight decay factor of 1 × 10 −5 . After the ordinal regression R and the encoder E are pre-trained, we alternatively update the discriminator D with GAN loss and the generator G with GAN loss, age estimation loss, pixel-level loss, and identity loss at every iteration. The networks are trained with a batch size of 100 for 200 epochs in total, which takes around 2.5 hours on four GTX 1080Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Comparison</head><p>Facial Aging and Rejuvenation: Given an input face and its target age label, CMAAE-OR generates the target age faces along the direction of facial aging or rejuvenation. We evaluate its performance for the FGNET and the MORPH datasets. For the MORPH, we randomly divide the whole dataset into the training set (80%) and the testing set (20%) without overlapping. For the FGNET, we utilize the UTKFace dataset as the training dataset and the FGNET as the testing set, following the setting in <ref type="bibr" target="#b11">[Zhang et al., 2017]</ref>. The facial aging and rejuvenation results of our method on these two datasets are shown in <ref type="figure" target="#fig_1">Figures 3 and 4</ref>, respectively. It can be seen that CMAAE-OR preserves the personal identity well even with a long age span and produces richer texture such as wrinkle in older faces, making older faces more realistic. Aging Accuracy and Effect of Ordinal Regression: To verify the accuracy of facial aging and rejuvenation of our method, an ordinal regression R is trained as age estimator (the architecture of R is shown in <ref type="figure" target="#fig_0">Figure 2)</ref>, which achieves the remarkable performance for the MORPH and the FGNET datasets (see the left column of <ref type="table" target="#tab_1">Table 1</ref>). Then we compared the performances of generating facial images by the CMAAE-OR with and without the ordinal regression network R, to justify the effectiveness of the ordinal regression in our networks. The age estimation errors of synthesized images under these conditions are shown in the right column of Table 1. It can be seen that the ordinal regression R improves the accuracy of synthesized facial images remarkably. Further, <ref type="figure">Figure 5</ref> shows the effects of R to synthesized images. It is clear that R helps the framework generate aging face with high accuracy. Although the outputs without R can present aging, its effect is subtle since R brings age-level details to the  <ref type="table" target="#tab_1">Table 1</ref>. Note that other published work should have similar MAE as our method without R. The reason is that those methods evaluate MAE in discrete age groups. That can cause the 50-aged face and the 60-aged being placed in the same group, producing a large MAE value.  <ref type="bibr" target="#b11">[Zhang et al., 2017]</ref>. As a prototype-based method, FT demo regards different age groups as aging prototypes to learn the aging pattern. Different from FT, CDL utilized dictionary learning to estimate age pattern between age groups. And RFA transformed faces across different ages by modeling the intermediate transition states in a RNN model, while CAAE utilized a conditional adversarial autoencoder network to achieve a bidirectional face aging. For fair comparison, we choose the same faces with their works as our input, and directly cite their synthetic results, as most of prior works did. From the results of <ref type="figure" target="#fig_2">Figure 6</ref>, it can be seen that the age changing of the synthetic face is not obvious in these prior works. In contrast, our method achieves facial aging with more clear changes. Moreover, our network simultaneously achieves age progression and regression in the same framework, and can generate facial image with an arbitrary age instead of a pre-defined discrete age group.</p><p>Robustness: As aforementioned, the input images may have large variation in eyeglasses, pose, and occlusion. To demonstrate the robustness of the CMAAE-OR, we select the faces with eyeglass variation, non-frontal pose, and occlusion, respectively, as shown in <ref type="figure">Figure 7</ref>. Note that the previous <ref type="figure">Figure 4</ref>: The results of the proposed method on the MORPH dataset. <ref type="figure">Figure 5</ref>: Effect of the ordinal regression, which forces the age of the generated face to be closer to the given value. The first column shows the original faces, and their actual ages are marked below them. The other columns are generated faces through the proposed framework, without (the upper row) or with (the lower row) ordinal regression. The generated faces fall in five ages as indicated above the columns.</p><p>works <ref type="bibr" target="#b3">Kemelmacher-Shlizerman et al., 2014]</ref> often apply face normalization to alleviate the variation of pose and expression but they may still suffer from the occlusion issue. In contrast, the CMAAE-OR generates faces without the need of removing these variations, paving the way to robust performance in real-world applications. <ref type="figure">Figure 7</ref>: Robustness to glasses, pose, and occlusion variations. The leftmost column shows the input faces, and the right columns are generated faces by CMAAE-OR from younger to older ages. In the first column, the first input face presents robustness to glasses, the second input shows only the face profile, and the last one is partially occluded by facial marks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, a novel GANs-based method, named the Conditional Multi-Adversarial Autoencoder with Ordinal Regression (CMAAE-OR), is proposed to predict facial aging and rejuvenation. It involves age estimation, face verification, and synthesis of visually plausible images as well as eliminating the input-output gap by using a weighted pixel-level penalty. Different from the previous approaches, this method can simultaneously address face aging accuracy and identity permanence well. To our knowledge, what's more, it is the first time to generate an aged face with a specific age instead of a discrete age group. This can help face aging prediction obtain higher accuracy and finer estimate. Finally, experimental results on both face aging and rejuvenation demonstrate the effectiveness and robustness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Structure of the proposed CMAAE-OR framework for facial aging and rejuvenation. A convolutional encoder E and a deconvolutional generator G learn the age transformation in tandem. The training of CMAAE-OR incorporates four different losses:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The results of the proposed method on the FGNET dataset. The first column shows the provided faces. The other columns are our results for both facial aging and facial rejuvenation. generated face. The detailed results are illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Comparison to prior works FT demo, CDL, RFA, and CAAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The comparisions between CMAAE-OR with oridnal regression (R) and CMAAE-OR without R. The performance is measured by the Mean Absolute Error (MAE) metric. MORPH 3.27 ± 0.02 1.48 ± 0.39 10.42 ± 0.73 FGNET 4.58 ± 0.08 3.62 ± 0.54 19.91 ± 0.82</figDesc><table><row><cell>Dataset</cell><cell>Actual Faces Estimator R</cell><cell>Synthesized Faces with R without R</cell></row><row><cell cols="3">Comparison with Prior Works: We compare our synthetic</cell></row><row><cell cols="3">results with several representative prior works, including FT</cell></row><row><cell cols="3">demo 2 , CDL: coupled dictionary learning [Shu et al., 2015],</cell></row><row><cell cols="3">RFA: recurrent facial aging [Wang et al., 2016], and CAAE:</cell></row><row><cell cols="3">conditional adversarial autoencoder</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Face Transformer (FT) demo. http://cherry.dcs. aber.ac.uk/transformer/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Perrett. Perception of age in adult Caucasian male faces: Computer graphic manipulation of shape and colour information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">D</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08617</idno>
	</analytic>
	<monogr>
		<title level="j">In Proc. R. Soc. Lond. B</title>
		<editor>Duong et al., 2017] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Ngan le, and Marios Savvides</editor>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page" from="137" to="143" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Temporal non-volume preserving approach to facial ageprogression and age-invariant face recognition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Age synthesis and estimation via faces: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1955" to="1976" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conditional generative adversarial nets for convolutional face generation. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
	<note>The 27th IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<editor>Ira Kemelmacher-Shlizerman, Supasorn Suwajanakorn, and Steven M</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward automatic simulation of aging effects on face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seitz ; Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanitis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv:1411.1784</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<publisher>Mirza and Osindero</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="442" to="455" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Conditional generative adversarial nets</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Ramanathan and Chellappa</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chellappa ; Narayanan</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<editor>Karl Ricanek and Tamirat Tesafaye</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
	<note>The 7th International Conference on Automatic Face and Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Personalized age progression with aging dictionary</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3970" to="3978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prototyping and transforming facial textures for perception research</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
		</imprint>
	</monogr>
	<note>Recurrent face aging</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face aging effect simulation using hidden factor analysis joint sparse representation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2493" to="2507" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning Face Age Progression: A Pyramid Architecture of GANs</title>
		<idno type="arXiv">arXiv:1711.10352</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Age Progression/Regression by Conditional Adversarial Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08423</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
