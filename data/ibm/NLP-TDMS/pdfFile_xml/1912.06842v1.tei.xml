<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained Recognition: Accounting for Subtle Differences between Similar Classes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
							<email>guolei.sun@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@inceptioniai.org</email>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-grained Recognition: Accounting for Subtle Differences between Similar Classes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main requisite for fine-grained recognition task is to focus on subtle discriminative details that make the subordinate classes different from each other. We note that existing methods implicitly address this requirement and leave it to a datadriven pipeline to figure out what makes a subordinate class different from the others. This results in two major limitations: First, the network focuses on the most obvious distinctions between classes and overlooks more subtle inter-class variations. Second, the chance of misclassifying a given sample in any of the negative classes is considered equal, while in fact, confusions generally occur among only the most similar classes. Here, we propose to explicitly force the network to find the subtle differences among closely related classes. In this pursuit, we introduce two key novelties that can be easily plugged into existing end-to-end deep learning pipelines. On one hand, we introduce "diversification block" which masks the most salient features for an input to force the network to use more subtle cues for its correct classification. Concurrently, we introduce a "gradient-boosting" loss function that focuses only on the confusing classes for each sample and therefore moves swiftly along the direction on the loss surface that seeks to resolve these ambiguities. The synergy between these two blocks helps the network to learn more effective feature representations. Comprehensive experiments are performed on five challenging datasets. Our approach outperforms existing methods using similar experimental setting on all five datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained recognition focuses on discriminating between children classes of a main parent category (e.g., cars <ref type="bibr" target="#b12">(Krause et al. 2013)</ref>, dogs <ref type="bibr" target="#b11">(Khosla et al. 2011)</ref>, birds <ref type="bibr" target="#b25">(Wah et al. 2011)</ref>, and aircrafts <ref type="bibr" target="#b17">(Maji et al. 2013)</ref>). Deep CNNs have excelled immensely on traditional visual recognition tasks where categories greater differ from each other. However, fine-grained visual categorization (FGVC) poses a significant challenge mainly due to the close resemblance between subcategories e.g., different species of the same bird. The challenge is compounded by the fact that the classifier has to Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="figure">Figure 1</ref>: Illustration of two novel components of our approach. Left: comparison between class activation maps obtained from the model with our diversification block (DB) and the one without DB. Our DB forces the network to capture more discriminative regions. With DB (below), the network finds beak, tail and feet of the bird as informative regions, while without DB (middle), the network only focuses on beak. Right: visual comparison in terms of 2-d tSNE (Van Der Maaten 2014) plot for features of 24 kinds of Walbler (confusing and difficult classes) in <ref type="bibr">CUB-200-2011</ref> between network trained with cross entropy (CE) (top) and our gradient-boosting loss (below). By focusing on difficult classes, our gradient-boosting loss can distinguish between hard classes which are not well separated by CE. be invariant to intra-class variations, e.g., pose, appearance and lighting changes.</p><p>Common deep learning based approaches for FGVC learn a mapping between input images and output labels. While doing so, a natural tendency during learning is to focus on only few distinguishing parts in an object to deal with confusing inter-class similarities and large intra-class variations (see <ref type="figure">Fig. 1</ref>). The analysis of attention based models provides the evidence that attention maps are often densely concentrated on a few parts, thus considering only a limited set of cues. In contrast, here we propose to spread the attention to consolidate a diverse set of relevant cues spread across the activation map. While we diversify attention at the feature level, we do the opposite at the prediction level, i.e., focus on only the most confusing cases to achieve better discriminability. Popular loss functions such as cross entropy, consider all classes to compute the error signal for parameter update. When closely related classes are present in the data, this leads to a weak supervision signal resulting in slower convergence and low recall rates. We show that selectively attending to the hard negative classes helps in achieving much faster convergence and higher accuracy.</p><p>Our approach can also be understood as a mechanism to enhance network generalization and avoid overfitting. This consideration is of particular relevance to FGVC, since the datasets are generally smaller due to the high cost of obtaining fine-grained annotations from experts. In effort to minimize loss on training data, a high-capacity network can end up associating unrelated concepts (such as those of background) to the fine-grained object itself. By concentrating on only the closely related classes and diversifying the model's attention, we are in fact regularizing the model to avoid overfitting the training samples. Our approach reduces classifiers confidence on training samples and therefore makes it more generalizable. We note that regularization schemes such as label-smoothing <ref type="bibr" target="#b23">(Szegedy et al. 2016</ref>) and maximum prediction entropy <ref type="bibr" target="#b5">(Dubey et al. 2018b</ref>) are related to ours, but significantly different as we impose regularization on both features and output predictions.</p><p>Our main contributions are as follows: • We introduce a gradient-boosting loss that seeks to resolve ambiguities among closely related classes by appropriately magnifying the gradient updates. • Our diversification block masks out the salient features in order to force the network to look for subtle differences between similar-looking categories. • The proposed method makes the convergence faster while outperforming existing methods on five datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Fine-grained classification has attracted much research attention in the recent years. Despite several attempts <ref type="bibr" target="#b32">(Yang et al. 2018;</ref>, FGVC is still an active research problem. To deal with the problem of subtle intra-class distance, many approaches focused on obtaining more relevant features <ref type="bibr" target="#b0">(Berg and Belhumeur 2013;</ref><ref type="bibr" target="#b16">Lin, RoyChowdhury, and Maji 2015;</ref><ref type="bibr" target="#b7">Gao et al. 2016;</ref><ref type="bibr" target="#b32">Yang et al. 2018;</ref>). One of the earliest but naive strategy was to exploit part annotations <ref type="bibr" target="#b0">(Berg and Belhumeur 2013)</ref> to locate the objects so that more informative features were used. Such an approach requires more labeling effort and has therefore limited scalability. Another stream of works <ref type="bibr" target="#b16">(Lin, RoyChowdhury, and Maji 2015;</ref><ref type="bibr" target="#b7">Gao et al. 2016;</ref><ref type="bibr" target="#b14">Li et al. 2018</ref>) developed complex pooling methods, so that complex local features can be used for classification. However, one obvious drawback of those methods is the high computation complexity. To deal with the problem of small fine-grained datasets, Cui et al. ) proposed a transfer learning scheme from selected subset of the source domain to target domain. However, it requires to re-train models on a subset of large datasets like ImageNet <ref type="bibr" target="#b20">(Russakovsky et al. 2015)</ref> and iNaturalist <ref type="bibr" target="#b24">(Van Horn et al. 2018)</ref>. Recent efforts <ref type="bibr" target="#b32">(Yang et al. 2018;</ref><ref type="bibr" target="#b1">Chen et al. 2019;</ref><ref type="bibr" target="#b35">Zheng et al. 2019;</ref><ref type="bibr" target="#b8">Ge, Lin, and Yu 2019)</ref> used only class labels to automatically locate informative regions. Specifically, <ref type="bibr" target="#b32">Yang et al. (Yang et al. 2018</ref>) adapted a Navigator-Teacher-Scrutinizer system under a multi-stage scheme. ) leveraged multiple channel attentions to learn several relevant regions. <ref type="bibr" target="#b28">Wang et al. (Wang, Morariu, and Davis 2018)</ref> used a bank of convolutional filters to capture discriminative regions in the feature maps. Chen <ref type="bibr" target="#b1">(Chen et al. 2019)</ref> deconstructed and reconstructed input images to find discriminative regions and features. <ref type="bibr" target="#b35">Zheng et al. (Zheng et al. 2019)</ref> proposed trilinear attention sampling network to learn features from different details. Despite the fact that the above methods perform well, they generally need to be trained in multiple stages or learn high-dimension features, resulting in increased training times. Another recent work <ref type="bibr" target="#b8">(Ge, Lin, and Yu 2019)</ref> developed a computationally complex, three-stage pipeline for fine-grained classification. Their framework requires a weakly supervised object detector, a mask-rcnn <ref type="bibr" target="#b10">(He et al. 2017)</ref> based instance segmentation and an LSTM for capturing the context. Moreover, the mask-rcnn needs to be pretrained on an additional dataset: MS-COCO <ref type="bibr" target="#b15">(Lin et al. 2014)</ref>. Our proposed diversification block adopts a novel way to find more relevant features by suppressing the most prominent discriminative regions in class activation maps <ref type="bibr" target="#b36">(Zhou et al. 2016</ref>) and thus forcing the network to find other informative regions. We note that hide-and-seek (Singh and Lee 2017) is related to ours, but largely different since our module works on feature maps and selectively suppresses discriminative regions. Our module is trained end-to-end with a computational cost nearly equal to the backbone.</p><p>Lately, FGVC strategies aimed to learn optimal classifiers on top of deep features have been proposed <ref type="bibr" target="#b4">(Dubey et al. 2018a;</ref><ref type="bibr" target="#b5">2018b)</ref>. <ref type="bibr" target="#b19">Qian et al. (Qian et al. 2015</ref>) employed a multi-stage framework which accepted pre-computed feature maps and learned the distance metric for classification. Dubey et al. <ref type="bibr" target="#b4">(Dubey et al. 2018a</ref>) adapted the idea from pairwise learning and used Siamese-like neural network. A triplet loss was used in <ref type="bibr" target="#b27">(Wang et al. 2016</ref>) to achieve better inter-class separation. The contrastive and triplet losses, however, increase the computational cost of training. <ref type="bibr" target="#b5">(Dubey et al. 2018b</ref>) proposed a maximum entropy loss for finegrained classification by using the principle of maximumentropy. All above methods do not specifically focus on differentiating confusing classes. Further, all the negative categories for a given sample are considered as equal. Our proposed gradient-boosting loss solves the problem by explicitly focusing on hard classes, incurs no additional cost and provides faster convergence rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce our method which can be easily plugged into any classification network. As shown in <ref type="figure">Fig. 2</ref>, to deploy our approach, we need to replace global pooling Figure 2: Overview of our overall architecture. Our method contains two novel components: diversification block and gradientboosting loss. The diversification block suppresses the discriminative regions of the class activation maps, and hence the network is forced to find alternative informative features. The gradient-booting loss focuses on difficult (confusing) classes for each image and boosts their gradient. As a result, the network moves swiftly (faster convergence) to discriminate the hard classes. layer and the last fully connected layer of the backbone network with a 1×1 convolution having output channels equal to the number of classes. Our method includes two novel components: (a) A diversification module which forces the network to capture more subtle features, rather than only the most obvious ones; (b) A gradient boosting loss which trains the network to focus on highly confusing classes. These two components will be addressed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Diversification Block</head><p>Consider the multi-class image classification task with C classes as shown in <ref type="figure">Fig. 2</ref>. Let I be a training image with ground-truth label l ∈ J, where J = {1, 2, ..., C} is the label set containing all labels. The input to our diversification block is the category-specific activation maps M ∈ R C×H×W , which is the output of the modified network. We denote M = {M c : c ∈ [1, C]}, where M c ∈ R H×W is the individual activation map corresponding to c th class.</p><p>Here, H and W refer to the height and width of the output activation maps.</p><p>The basic idea of our diversification block is to suppress the discriminative regions of the activation map M, so that the network is forced to look for other informative regions which is expected to enhance classification performance. In the following, we will target two relevant questions: (1) Where to suppress information? and (2) How to suppress?</p><p>Mask Generation Here, we explain the procedure to generate the mask that indicates the locations in M that are suppressed. Let B = {B c : c ∈ [1, C]}, where B c ∈ R H×W denotes the binary suppressing mask for its corresponding activation map M c . Each element in mask B c is in the domain {0, 1}, where 1 indicates the corresponding location will be suppressed while 0 means that no suppression will take place.</p><p>Peak suppression: First, we randomly suppress the peak locations of the activation maps because they are the most discriminative regions for the classifier. By suppressing the peaks, the network is forced to find alternative relevant regions in the image. Let P c ∈ R H×W be the peak map derived from c th object category map (M c ) such that:</p><formula xml:id="formula_0">Pc(i, j) = 1, if Mc(i, j) = max(Mc), 0, otherwise.<label>(1)</label></formula><p>Here, max(M c ) denotes the maximum of matrix M c . We suppress the peaks of different object categories with probability p peak . The masks B c to randomly hide the peaks are generated as follows:</p><formula xml:id="formula_1">B c = r c * P c , where r c ∼ Bernoulli(p peak ),<label>(2)</label></formula><p>where ' * ' denotes element-wise multiplication and r c is a Bernoulli random variable that has p peak probability of being 1.</p><p>Patch suppression: Peaks are the most discriminative regions, but there are other discriminative regions as well that encompass more subtle inter-class differences. In the following, we explain how to suppress locations other than peaks in the activation maps. We divide each M c into a grid of patches, where each fixed sized patch M [l,m] c ∈ R G×G is indexed by row l and column m. Lets assume the set of all such patches on the grid is given by:</p><formula xml:id="formula_2">G c = {M [l,m] c : l ∈ [1, W G ], m ∈ [1, H G ]}.<label>(3)</label></formula><p>After this operation, the activation map M c will be divided into (W × H)/G 2 patches. Let B c ∈ R H×W be the mask for randomly hiding patches for c th activation map M c . For each patch inside M c , we randomly hide it with probability p patch and set the elements of corresponding locations of B c as 1. Otherwise, the elements of B c are set to 0:</p><formula xml:id="formula_3">B c = {B [l,m] c ∈ [0, 1] ∼ Bernoulli(p patch )},<label>(4)</label></formula><p>where, 0, 1 ∈ R G×G and l, m are in the same range as Eq. 3. To consider only the non-peak locations, we then set the element of B c in the peak location of M c as 0,</p><formula xml:id="formula_4">B c (i, j) = 0, if M c (i, j) = max(M c ).<label>(5)</label></formula><p>The final suppressing mask for c th category is obtained as:</p><formula xml:id="formula_5">B c = B c + B c .<label>(6)</label></formula><p>Activation Suppression Factor Setting values that replace the suppressed features is of much importance to achieve good performance. Let M = {M c : c ∈ [1, C]} represents the category activation maps obtained after our diversification module, which is generated as follows.</p><formula xml:id="formula_6">M c (i, j) = M c (i, j), if B c (i, j) = 0, α * M c (i, j), if B c (i, j) = 1,<label>(7)</label></formula><p>where, α denotes the suppressing factor. Basically, we replace the values in the suppressing locations as α times of their initial values. In general, setting α to a low number will lead to good performance. Throughout our experiments, we set α as 0.1. After feature masking, we perform global average pooling to get the confidence scores s ∈ R 1×C as follows:</p><formula xml:id="formula_7">s = {s c : c ∈ [1, C]}, s c = AvgPool(M c ),<label>(8)</label></formula><p>where, AvgPool denotes global average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gradient-boosting Cross Entropy Loss</head><p>While diversification module aims at finding more subtle variations in the input images, our second contribution is a gradient-boosting loss function that specifically focuses on confusing classes to avoid misclassifications between them. We elaborate the proposed loss function below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The most widely used loss for image classification is cross entropy (CE) loss. For an image I, CE loss can be written as follows:</p><formula xml:id="formula_8">CE(s, l) = − log exp (s l ) i∈J exp (s i ) ,<label>(9)</label></formula><p>where l is the ground-truth label for image I. Here, the loss considers all negative classes equally. However, in finegrained classification, the ground-truth class is generally much closer to a related subset of classes than others. For example, in CUB-200-2011 <ref type="bibr" target="#b25">(Wah et al. 2011)</ref>, bird class of Acadian Flycatcher is more closer to categories such as Great Crested Flycatcher, Least Flycatcher, Olive sided Flycatcher and other kinds of Flycatcher, since they all belong to the same species. As a result, the network is prone to making mistakes among these similar (thus confusing) classes and predicting relatively higher confidence scores for them. Based on this observation, we argue that the loss should focus more on the confusing classes, rather than simply considering all negative classes equally for the normalization in Eq. 9. Hence, we propose a novel and simple gradient-boosting cross entropy (GCE) loss which focuses only on k negative classes with top-k highest confidence scores among all negative classes. Here, k simply means the number of negative classes to focus on. We will show in the next section, that the proposed loss basically boosts gradients to more swiftly resolve ambiguities between closely related confusing classes. We define J as the set of all negative classes, where J = {i : i ∈ [1, C] ∧ i = l}. Let s = {s i , i ∈ J } be the set containing confidence scores of all negative classes. We get the k th highest values of s by heap-max algorithm <ref type="bibr" target="#b2">(Chhavi 2018)</ref> and denote it as t k . Next, we split J into J &gt; and J &lt; by thresholding s using t k , defined as follows:</p><formula xml:id="formula_9">J &gt; = {i : i ∈ J ∧ s i ≥ t k } (10) J &lt; = {i : i ∈ J ∧ s i &lt; t k },<label>(11)</label></formula><p>where, J &gt; contains the negative classes whose confidence scores are within the top-k of all negative classes, and J &lt; is the set of negative classes whose confidence scores rank below the top-k classification scores. Instead of considering all negative classes in Eq. 9, our gradient-boosting cross entropy loss only focuses on confusing classes (J &gt; ). The negative classes in J &lt; do not contribute to the loss since the network can easily distinguish them from the ground-truth class. Our proposed loss is given by:</p><formula xml:id="formula_10">GCE(s, l) = − log exp (s l ) exp (s l ) + i∈J &gt; exp (s i ) .<label>(12)</label></formula><p>As shown in Eq. 10 and 12, GCE loss focuses only on J &gt; , containing k negative classes with top-k highest confidence scores. Here, k is a hyper-parameter (we found k = 15 works best in our experiments). When k = C, GCE is equivalent to CE.</p><p>In the following analysis, we will show how our loss can boost the gradient for both the ground-truth class and confusing negative classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Boosting</head><p>We analyze the loss from the perspective of gradient. For the original cross entropy (CE) loss, the gradient for s c is computed as:</p><formula xml:id="formula_11">∂ CE(s, l) ∂s c = exp (sc) i∈J exp (si) , c = l exp (sc) i∈J exp (si) − 1, c = l<label>(13)</label></formula><p>For our gradient-boosting cross entropy loss, the gradient for s c is computed as:</p><formula xml:id="formula_12">∂ GCE(s, l) ∂s c =    exp (sc) exp (s l )+ i∈J &gt; exp (si) , c ∈ J &gt; exp (sc) exp (s l )+ i∈J &gt; exp (si) − 1, c = l<label>(14)</label></formula><p>From our definition in Eq. 10 and Eq. 11, the following relation exists between J &gt; and J ,</p><formula xml:id="formula_13">J &gt; + {l} ⊂ J + {l} = J.<label>(15)</label></formula><p>As such, we obtain,</p><formula xml:id="formula_14">∂ GCE(s, l) ∂s c &gt; ∂ CE(s, l) ∂s c .<label>(16)</label></formula><p>We can see that for both the ground-truth class and confusing negative classes, the gradient of our proposed loss is larger than the gradient of the original cross entropy loss. With our novel loss, the network can focus on differentiating difficult classes from the ground-truth class and converge faster, which is validated by our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference</head><p>Our method is trained end-to-end in a single stage. The diversification block is only used during the training phase. As shown in <ref type="figure">Fig. 2</ref>, during the training phase, class activation maps are passed through our novel diversification block and then to the global average pooling. As a result, discriminative regions are randomly masked and the network is forced to find other relevant areas. During test phase, the whole class activation maps are passed to global average pooling directly, without being suppressed at any region so that all informative regions found during training phase contribute to the final confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We comprehensively evaluate our algorithm on CUB-200-2011 <ref type="bibr" target="#b25">(Wah et al. 2011)</ref>, Stanford Cars <ref type="bibr" target="#b12">(Krause et al. 2013)</ref>, FGVC Aircraft <ref type="bibr" target="#b17">(Maji et al. 2013)</ref>, and Stanford Dogs <ref type="bibr" target="#b11">(Khosla et al. 2011)</ref>, all of which are widely used for finegrained recognition. Statistics of all datasets are shown in <ref type="table">Table 1</ref>. We follow the same train/test splits as in the table. For evaluation metric, we use top-1 accuracy following <ref type="bibr" target="#b4">Dubey et al. 2018a;</ref><ref type="bibr" target="#b5">2018b)</ref>.</p><p>Furthermore, we also evaluate on the recent terrain dataset for terrain recognition: GTOS-mobile <ref type="bibr" target="#b31">(Xue, Zhang, and Dana 2018)</ref> dataset and GTOS (Ground Terrain in Outdoor Scenes) ) dataset, which have potential use for autonomous agents (automatic car). The datasets are large-scale, containing classes of outdoor ground terrain, i.e. glass, sand, soil, stone-cement, and so on. Since those terrain classes are closely related, visually similar and thus difficult to classify, we use this challenging dataset to evaluate our method. Following <ref type="bibr" target="#b31">(Xue, Zhang, and Dana 2018)</ref>, we use GTOS as training and GTOS-mobile as test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For fair comparisons with other methods <ref type="bibr" target="#b32">(Yang et al. 2018;</ref><ref type="bibr" target="#b28">Wang, Morariu, and Davis 2018)</ref>, we use an input image resolution of 448×448 in all experiments. We fine-tune pretrained network (ResNet-50 <ref type="bibr" target="#b9">(He et al. 2016</ref>)) using our proposed diversification block and gradient-boosting loss due to its popularity in existing works. Momentum SGD optimizer is used with an initial learning rate of 0.001, which decays by 0.1 for every 50 epochs. We set weight decay as 10 −4 . Our algorithm is implemented using Pytorch <ref type="bibr" target="#b18">(Paszke et al. 2017)</ref> using two Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results</head><p>Our method does not require any part-annotation and can be trained using only class labels. Moreover, it is parameterfree and does not increase the number of parameters compared to the ResNet-50 backbone. Our results are compared with the most recent and top-performing approaches evaluated under similar experimental setting. Several approaches such as RACNN , RAM <ref type="bibr" target="#b13">(Li et al. 2017)</ref>, and NTS-net <ref type="bibr" target="#b32">(Yang et al. 2018)</ref> extract multiple crops at different scale from an input image. The classification score obtained from these crops are averaged to predict the final class during inference. For fair comparison, we report our 'multi-scale' (five crops) results, in addition to the 'single-scale' using one crop from an image. The comparisons with various methods on four challenging fine-grained datasets, namely CUB-200-2011 <ref type="bibr" target="#b25">(Wah et al. 2011)</ref>, FGVC Aircraft <ref type="bibr" target="#b17">(Maji et al. 2013)</ref>, Stanford Cars <ref type="bibr" target="#b12">(Krause et al. 2013)</ref>, and Stanford Dogs <ref type="bibr" target="#b11">(Khosla et al. 2011)</ref>, are shown in <ref type="table" target="#tab_2">Table 2</ref>. Additionally, results for GTOS-mobile ) are shown in <ref type="table" target="#tab_3">Table 3</ref>. Overall, our proposed method outperforms previous methods on all five datasets.</p><p>We observe that our method achieves the best accuracy on birds classification task <ref type="table" target="#tab_2">(Table 2)</ref>. Specifically, our method obtains an accuracy of 88.6% which outperforms TASN (87.6%) <ref type="bibr" target="#b35">(Zheng et al. 2019)</ref>. TASN <ref type="bibr" target="#b35">(Zheng et al. 2019)</ref> performs well because it first uses a small network to find the attentive regions and then distills knowledge from various informational regions to the model. With a low parametric complexity, our method can capture more relevant regions by focusing on hard classes and diversifying informative areas in the class activation maps.</p><p>For other four datasets, our method also outperforms the compared methods. In Aircraft, we achieve 93.5% top-1 accuracy, surpassing NTS-net <ref type="bibr" target="#b32">(Yang et al. 2018</ref>) (91.4%). In Cars, we obtain 94.9%, outperforming the best performances: 93.8% of TASN <ref type="bibr" target="#b35">(Zheng et al. 2019)</ref>. In Dogs, we obtain 87.7% top-1 accuracy compared to 87.3% obtained by RACNN approach . Note that RACNN has much more parameters (429M) than our methods (23.9M). In GTOS-mobile, we show our result using ResNet-50 with "single scale", for fair comparison with Deep-TEN  and DEP <ref type="bibr" target="#b31">(Xue, Zhang, and Dana 2018)</ref>. We get 85.0%, which is 2.8% better than the current state-of-the-art 82.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone Resolution #Parameters</head><p>Accuracy CUB-200-2011 Aircrafts Cars Dogs RACNN  VGG  Methods Accuracy B-CNN <ref type="bibr" target="#b16">(Lin, RoyChowdhury, and Maji 2015)</ref> 75.4 Deep-TEN  76.1 DEP <ref type="bibr" target="#b31">(Xue, Zhang, and Dana 2018)</ref> 82.2 Ours (single scale) 85.0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To fully analyze our method, <ref type="table" target="#tab_4">Table 4</ref> provides a detailed ablation analysis on the key components of our method. It basically highlights the importance of diversification block and gradient-boosting loss. We conduct all ablation studies on CUB-200-2011 using the ResNet-50 <ref type="bibr" target="#b9">(He et al. 2016)</ref>. Diversification block (DB). DB is important because it diversifies the informative regions by forcing the network to find relevant parts other than the most obvious ones. Integrating DB block in the ResNet-50 backbone results in a performance improvement of 0.8% (from 85.5% to 86.3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy ResNet-50 85.5 ResNet-50+DB 86.3 ResNet-50+DB+Center loss <ref type="bibr" target="#b29">(Wen et al. 2016)</ref> 86.4 ResNet-50+DB+LGM loss <ref type="bibr" target="#b26">(Wan et al. 2018)</ref> 86.5 ResNet-50+DB+MaxEnt <ref type="bibr" target="#b5">(Dubey et al. 2018b)</ref> 86.2 Ours (single scale) 87.7 Ours (multi scale) 88.6 Gradient-boosting loss. Our gradient-boosting loss is another important component that shows significant improvement. Using this loss, we improve the results from 86.3% to 87.7% (an absolute gain of 1.4%).We also compare our loss with other recent losses that aim at achieving better discriminability: Center loss <ref type="bibr" target="#b29">(Wen et al. 2016)</ref>, LGM loss <ref type="bibr" target="#b26">(Wan et al. 2018)</ref>, and max entropy loss <ref type="bibr" target="#b5">(Dubey et al. 2018b)</ref>. The re-α 0 0.1 0.2 1.0 Accuracy 86.2 86.3 85.8 85.5 <ref type="table">Table 5</ref>: Ablation study on suppressing factor α. Keeping suppressing factor as small leads to good performance. sults show that gradient-boosting loss outperforms all these loss functions. Our loss targets on difficult/confusing classes and selectively boosts the gradients for them, while other losses consider all negative classes as equal.</p><p>Suppressing Factor. Here, we show a parameter sensitivity analysis on the suppressing factor α. Top-1 accuracy with respect to different α settings is shown in <ref type="table">Table 5</ref>. It shows that keeping α as a small value consistently leads to better performance than without using diversification block (α = 1). Specifically, α = 0.1 gives the best performance on CUB-200-2011 dataset.</p><p>Choices of k. Here, we show ablation study on k, the number of negative classes to focus on for gradient-boosting loss, in <ref type="figure" target="#fig_0">Fig. 3</ref>. It shows that by reducing k, our loss focuses on more confusing classes and achieves consistent improvement in top-1 accuracy.</p><p>Convergence Analysis. We compare the training curves of our methods and baseline (ResNet-50) in <ref type="figure" target="#fig_1">Fig. 5</ref>. It shows <ref type="figure">Figure 4</ref>: Class activation map (CAM) comparison between our method and baseline in different datasets. Top to below: original image, CAM of the ground-truth class of baseline, CAM of the ground-truth class of our method. While baseline only focuses on the most discriminative region, our method accurately diversifies attentions to other informative regions of the objects. that our method converges much faster than the baseline, and also attains a lower error rate on test set. Remarkably, the baseline achieves a lower error rate on training set after 50 epochs, but fails to generalize well to the test set. This shows that the baseline is prone to overfitting on the train set, which our method successfully avoids. Our method uses diversification block which prevents the network to only focus on the most discriminative regions, like the beak or head of a bird. In contrast, using diversification block, the network finds various informative areas, thus reducing overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>We qualitatively illustrate the comparison between our approach and the baseline in <ref type="figure">Fig. 4</ref>. We note that the diversification block indeed helps the network to find more discriminative regions in the image. In contrast, the baseline model generally focuses on the most obvious distinguishing patterns and its attention is limited to only a limited set of spatial locations. This explains why our approach generalizes better to test images, attaining a higher accuracy <ref type="figure" target="#fig_1">(Fig. 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">ImageNet Results</head><p>To validate the generality of gradient-boosting loss in visual recognition, we apply it to a ResNet-50 backbone on Ima- <ref type="figure">Figure 6</ref>: Training curves of using our loss and Cross Entropy (CE) on ImageNet. Our loss converges considerably faster than CE and also leads to a better performance.</p><p>geNet <ref type="bibr" target="#b20">(Russakovsky et al. 2015)</ref>. Here, we use the input size of 224×224 and follow the same training strategy as used in <ref type="bibr" target="#b9">(He et al. 2016)</ref>. Since our loss focuses on difficult classes, we apply it only half way (50 epochs) during training when easy categories are already well-classified. The comparison of training curve between using our loss and using cross entropy (CE) is shown in <ref type="figure">Fig. 6</ref>. It shows that the proposed loss converges much faster than the CE loss and achieves a lower error rate on the challenging ImageNet benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel approach to better discriminate closely related categories in fine-grained classification task. Our method has two novel components: (a) diversification block that forces the network to find subtle distinguishing features between each pair of classes and (b) gradient-boosting loss that specifically focuses on maximally separating the highly similar and confusing classes. Our approach not only outperforms existing methods on all studied fine-grained datasets, but also demonstrates much faster convergence rates. In comparison to previous methods, our solution is both simple and elegant, leads to higher accuracy and demonstrates better computational efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study on k for our loss inCUB-200-2011.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Training curves of our methods and the baseline (CE loss) on CUB-200-2011. Using our loss, our method converges faster and performs better than the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on four standard datasets. "-" means the information is not mentioned in the relevant paper. Our method outperforms existing approaches on four commonly used fine-grained datasets, and requires no additional parameters compared to the ResNet-50 backbone. Here, the parameters are computed on CUB-200-2011, having 200 output classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on GTOS-mobile.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation analysis on the CUB-200-2011. Our diversification block (DB) and gradient-boosting loss provide progressive improvements over the baseline.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Poof: Part-based onevs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">k largest(or smallest) elements in an arrayadded min heap method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chhavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domainspecific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum-entropy fine grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask r-cnn. ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-grained visual categorization via multi-stage metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. Sun, M</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">;</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jmlr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Accelerating t-sne using treebased algorithms</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking feature distribution for loss functions in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining discriminative triplets of patches for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Differential angular imaging for material recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep texture manifold for ground terrain recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep ten: Texture encoding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning multiattention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
