<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Wang</surname></persName>
							<email>wangsiwei@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Liang</surname></persName>
							<email>liangtingting@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
							<email>zhaoqijie@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
							<email>tangzhi@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>haibin.ling@stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In existing CNN based detectors, the backbone network is a very important component for basic feature 1 extraction, and the performance of the detectors highly depends on it. In this paper, we aim to achieve better detection performance by building a more powerful backbone from existing backbones like ResNet and ResNeXt. Specifically, we propose a novel strategy for assembling multiple identical backbones by composite connections between the adjacent backbones, to form a more powerful backbone named Composite Backbone Network (CBNet). In this way, CBNet iteratively feeds the output features of the previous backbone, namely high-level features, as part of input features to the succeeding backbone, in a stage-by-stage fashion, and finally the feature maps of the last backbone (named Lead Backbone) are used for object detection. We show that CBNet can be very easily integrated into most state-of-the-art detectors and significantly improve their performances. For example, it boosts the mAP of FPN, Mask R-CNN and Cascade R-CNN on the COCO dataset by about 1.5 to 3.0 percent. Meanwhile, experimental results show that the instance segmentation results can also be improved. Specially, by simply integrating the proposed CBNet into the baseline detector Cascade Mask R-CNN, we achieve a new state-of-the-art result on COCO dataset (mAP of 53.3) with single model, which demonstrates great effectiveness of the proposed CBNet architecture. Code will be made available on https://github.com/PKUbahuangliuhe/CBNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is one of the most fundamental problems in computer vision, which can serve a wide range of applications such as autonomous driving, intelligent video surveillance, remote sensing, and so on. In recent years, great progresses have been made for object detection thanks to the booming development of the deep convolutional networks <ref type="bibr" target="#b10">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, and a few excellent detectors have been proposed, e.g., SSD <ref type="bibr" target="#b17">(Liu et al. 2016)</ref>, Faster R-CNN <ref type="bibr" target="#b21">(Ren et al. 2015)</ref>, Retinanet <ref type="bibr" target="#b16">(Lin et al. 2018)</ref>, FPN <ref type="bibr" target="#b14">(Lin et al. 2017a</ref>), Mask R-CNN , Cascade R-CNN <ref type="bibr" target="#b1">(Cai and Vasconcelos 2018)</ref>   <ref type="bibr" target="#b13">(Lin et al. 2014)</ref> with different existing backbones and the proposed Composite Backbone Networks (Dual-ResNeXt152 and Triple-ResNeXt152), which are reproduced by Detectron <ref type="bibr" target="#b1">Cai and Vasconcelos 2018)</ref>. It shows that deeper and larger backbones bring better detection performance, while our Composite Backbone Network architecture can further strengthen the existing very powerful backbones for object detection such as ResNeXt152.</p><p>Generally speaking, in a typical CNN based object detector, a backbone network is used to extract basic features for detecting objects, which is usually designed for the image classification task and pretrained on the ImageNet dataset <ref type="bibr" target="#b3">(Deng et al. 2009</ref>). Not surprisingly, if a backbone can extract more representational features, its host detector will perform better accordingly. In other words, a more powerful backbone can bring better detection performance, as demonstrated in <ref type="table">Table 1</ref>. Hence, starting from AlexNet <ref type="bibr" target="#b10">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, deeper and larger (i.e., more powerful) backbones have been exploited by the state-of-the-art detectors, such as VGG (Simonyan and Zisserman 2014), ResNet , DenseNet <ref type="bibr" target="#b9">(Huang et al. 2017)</ref>, ResNeXt <ref type="bibr" target="#b27">(Xie et al. 2017)</ref>. Despite encouraging results achieved by the state-of-the-art detectors based on deep and large backbones, there is still plenty of room for performance improvement. Moreover, it is very expensive to achieve better detection performance by designing a novel more powerful backbone and pre-training it on ImageNet. In addition, since almost all of the existing backbone networks are originally designed for image classification task, directly employing them to extract basic features for object detection may result in suboptimal performance.</p><p>To deal with the issues mentioned above, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, we propose to assemble multiple identical backbones, in a novel way, to build a more powerful backbone for object detection. In particular, the assembled backbones are treated as a whole which we call Composite Backbone Network (CBNet). More specifically, CBNet consists of multiple identical backbones (specially called Assistant Backbones and Lead Backbone) and composite connections between neighbor backbones. From left to right, the output of each stage in an Assistant Backbone, namely higher-level features, flows to the parallel stage of the succeeding backbone as part of inputs through composite connections. Finally, the feature maps of the last backbone named Lead Backbone are used for object detection. Obviously, the features extracted by CBNet for object detection fuse the highlevel and low-level features of multiple backbones, hence improve the detection performance. It is worth mentioning that, we do not need to pretrain CBNet for training a detector integrated with it. For instead, we only need to initialize each assembled backbone of CBNet with the pretrained model of the single backbone which is widely and freely available today, such as ResNet and ResNeXt. In other words, adopting the proposed CBNet is more economical and efficient than designing a novel more powerful backbone and pre-training it on ImageNet.</p><p>On the widely tested MS-COCO benchmark <ref type="bibr" target="#b13">(Lin et al. 2014)</ref>, we conduct experiments by applying the proposed Composite Backbone Network to several state-of-the-art object detectors, such as FPN <ref type="bibr" target="#b14">(Lin et al. 2017a</ref>), Mask R-CNN  and Cascade R-CNN <ref type="bibr" target="#b1">(Cai and Vasconcelos 2018)</ref>. Experimental results show that the mAPs of all the detectors consistently increase by 1.5 to 3.0 percent, which demonstrates the effectiveness of our Composite Backbone Network. Moreover, with our Composite Backbone Network, the results of instance segmentation are also improved. Specially, using Triple-ResNeXt152, i.e., Com-posite Backbone Network architecture of three ResNeXt152 <ref type="bibr" target="#b27">(Xie et al. 2017</ref>) backbones, we achieve the new state-ofthe-art result on COCO dataset, that is, mAP of 53.3, outperforming all the published object detectors.</p><p>To summarize, the major contributions of this work are two-fold:</p><p>• We propose a novel method to build a more powerful backbone for object detection by assembling multiple identical backbones, which can significantly improve the performances of various state-of-the-art detectors. • We achieve the new state-of-the-art result on the MSCOCO dataset with single model, that is, the mAP of 53.3 for object detection.</p><p>In the rest of the paper, after reviewing related work in Sec. 2, we describe in details the proposed CBNet for object detection in Sec. 3. Then, we report the experimental validation in Sec. 4. Finally, we draw the conclusion in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Object detection Object detection is a fundamental problem in computer vision. The state-of-the-art methods for general object detection can be briefly categorized into two major branches. The first branch contains one-stage methods such as YOLO <ref type="bibr" target="#b20">(Redmon et al. 2016)</ref>, SSD <ref type="bibr" target="#b17">(Liu et al. 2016</ref>), Retinanet <ref type="bibr" target="#b15">(Lin et al. 2017b</ref>), FSAF <ref type="bibr" target="#b30">(Zhu, He, and Savvides 2019)</ref> and NAS-FPN <ref type="bibr" target="#b4">(Ghiasi, Lin, and Le 2019)</ref>. The other branch contains two-stage methods such as Faster R-CNN <ref type="bibr" target="#b21">(Ren et al. 2015)</ref>, FPN <ref type="bibr" target="#b14">(Lin et al. 2017a</ref>), Mask R-CNN , Cascade R-CNN <ref type="bibr" target="#b1">(Cai and Vasconcelos 2018)</ref> and Libra R-CNN <ref type="bibr" target="#b18">(Pang et al. 2019)</ref>. Although breakthrough has been made and encouraging results have been achieved by the recent CNN based detectors, there is still large room for performance improvement. For example, on MS COCO benchmark <ref type="bibr" target="#b13">(Lin et al. 2014)</ref>, the best publicly reported mAP is only 52.5 , which is achieved by model ensemble of four detectors.</p><p>Backbone for Object detection Backbone is a very important component of a CNN based detector to extract basic features for object detection. Following the original works (e.g., R-CNN <ref type="bibr" target="#b5">(Girshick et al. 2014)</ref> and OverFeat (Sermanet et al. 2013)) of applying deep learning to object detection, almost all of the recent detectors adopt the pretraining and fine-tuning paradigm, that is, directly use the networks which are pre-trained for ImageNet classification task as their backbones. For instance, VGG (Simonyan and Zisserman 2014), ResNet , ResNeXt <ref type="bibr" target="#b27">(Xie et al. 2017</ref>) are widely used by the state-of-the-art detectors. Since these backbone networks are originally designed for image classification task, directly employing them to extract basic features for object detection may result in suboptimal performance. More recently, two sophisticatedly designed backbones, i.e., DetNet  and FishNet , are proposed for object detection. These two backbones are specifically designed for the object detection task, and they still need to be pretrained for ImageNet classification task before training (fine tuning) the detector based on them. It is well known that designing and pretraining a novel and powerful backbone like them requires much manpower and computation cost. In an alternative way, we propose a more economic and efficient solution to build a more powerful backbone for object detection, by assembling multiple identical existing backbones (e.g., ResNet and ResNeXt). Recurrent Convolution Neural Network As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the proposed architecture of Composite Backbone Network is somewhat similar to an unfolded recurrent convolutional neural network (RCNN) (Liang and Hu 2015) architecture. However, the proposed CBNet is quite different from this network. First, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the architecture of CBNet is actually quite different, especially for the connections between the parallel stages. Second, in RCNN, the parallel stages of different time steps share the parameters, while in the proposed CBNet, the parallel stages of backbones do not share the parameters. Moreover, if we use RCNN as the backbone of a detector, we need to pretrain it on ImageNet. However, when we use CBNet, we do not need to pretrain it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>This section elaborates the proposed CBNet in detail. We first describe its architecture and variants in Section 3.1 and Section 3.2 respectively. And then, we describe the structure of detection network with CBNet in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture of CBNet</head><p>The architecture of the proposed CBNet consists of K identical backbones (K ≥ 2). Specially, we call the case of K = 2 (as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.a) as Dual-Backbone (DB) for simplicity, and the case of K=3 as Triple-Backbone (TB).</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the CBNet architecture consists of two types of backbones: the Lead Backbone B K and the Assistant Backbones B 1 , B 2 , ..., B K−1 . Each backbone comprises L stages (generally L = 5), and each stage consists of several convolutional layers with feature maps of the same size. The l-th stage of the backbone implements a nonlinear transformation F l (·).</p><p>In the traditional convolutional network with only one backbone, the l-th stage takes the output (denoted as x l−1 ) of the previous l − 1-th stage as input, which can be expressed as:</p><p>x l = F l (x l−1 ), l ≥ 2.</p><p>(1) Unlike this, in the CBNet architecture, we novelly employ Assistant Backbones B 1 , B 2 , ..., B k−1 to enhance the features of the Lead Backbone B k , by iteratively feeding the output features of the previous backbone as part of input features to the succeeding backbone, in a stage-by-stage fashion. To be more specific, the input of the l-th stage of the backbone B k is the fusion of the output of the previous l−1th stage of B k (denoted as x l−1 k ) and the output of the parallel stage of the previous backbone B k−1 (denoted as x l k−1 ). This operation can be formulated as following:</p><formula xml:id="formula_0">x l k = F l k (x l−1 k + g(x l k−1 )), l ≥ 2,<label>(2)</label></formula><p>where g(·) denotes the composite connection, which consists of a 1×1 convolutional layer and batch normalization layer to reduce the channels and an upsample operation. As a result, the output features of the l-th stage in B k−1 is transformed to the input of the same stage in B k , and added to the original input feature maps to go through the corresponding layers. Considering that this composition style feeds the output of the adjacent higher-level stage of the previous backbone to the succeeding backbone, we call it as Adjacent Higher-Level Composition (AHLC).</p><p>For object detection task, only the output of Lead Backbone x l K (l = 2, 3, ...L) are taken as the input of RPN/detection head, while the output of each stage of Assistant Backbones is forwarded into its adjacent backbone. Moreover, the B 1 , B 2 , ..., B K−1 in CBNet can adopt various backbone architectures, such as  or ResNeXt <ref type="bibr" target="#b27">(Xie et al. 2017)</ref>, and can be initialized from the pre-trained model of the single backbone directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Other possible composite styles</head><p>Same Level Composition (SLC) An intuitive and simple composite style is to fuse the output features from the same stage of backbones. This operation of Same Level Composite (SLC) can be formulated as:</p><formula xml:id="formula_1">x l k = F l k (x l−1 k + x l−1 k−1 ), l ≥ 2.<label>(3)</label></formula><p>To be more specific, <ref type="figure" target="#fig_2">Figure 3</ref>.b illustrates the structure of SLC when K = 2.</p><p>Adjacent Lower-Level Composition (ALLC) Contrary to AHLC, another intuitive composite style is to feed the output of the adjacent lower-level stage of the previous backbone to the succeeding backbone. This operation of Adjacent Lower-Level Composition (ALLC). The operation of Inverse Level Composite (ILC) can be formulated as:</p><formula xml:id="formula_2">x l k = F l k (x l−1 k + g(x l+1 k−1 )), l ≥ 2.<label>(4)</label></formula><p>To be more specific, <ref type="figure" target="#fig_2">Figure 3</ref>.c illustrates the structure of ILC when K = 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense</head><p>Higher-Level Composition (DHLC) In DenseNet <ref type="bibr" target="#b9">(Huang et al. 2017)</ref>, each layer is connected to all subsequent layers to build a dense connection in a stage. Inspired by it, we can utilize dense composite connection in our CBNet architecture. The operation of DHLC can be expressed as follows:</p><formula xml:id="formula_3">x l k = F l k (x l−1 k + L i=l g i (x i k−1 )), l ≥ 2.<label>(5)</label></formula><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>.d, when K = 2, we assemble the features from all the higher-level stages in the Assistant Backbone, and add the composite features to the output features of the previous stage in the Lead Backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture of detection network with CBNet</head><p>The CBNet architecture is applicable with various off-theshelf object detectors without additional modifications to the network architectures. In practice, we attach layers of the Lead Backbone with functional networks, RPN <ref type="bibr" target="#b21">(Ren et al. 2015)</ref> , detection head <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present experimental results on the bounding box detection task and instance segmentation task of the challenging MS-COCO benchmark <ref type="bibr" target="#b13">(Lin et al. 2014)</ref>. Following the protocol in MS-COCO, we use the trainval35k set for training, which is a union of 80k images from the train split and a random 35k subset of images from the 40k image validation split. We report COCO AP on the test-dev split for comparisons, which is tested on the evaluation server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Baselines methods in this paper are reproduced by ourselves based on the Detectron framework ). All the baselines are trained with single-scale strategy, except Cascade Mask R-CNN ResNeXt152. Specifically, the short side of input image is resized to 800, and the longer side is limited to 1333. We conduct experiments on a machine with 4 NVIDIA Titan X GPUs, CUDA 9.2 and cuDNN 7.1.4 for most experiments. In addition, we train Cascade Mask R-CNN with Dual-ResNeXt152 on a machine with 4 NVIDIA P40 GPUs and Cascade Mask R-CNN with Triple-ResNeXt152 on a machine with 4 NVIDIA V100 GPUs. The data augmentation is simply flipping the images. For most of the original baselines, batch size on a single GPU is two images. Due to the limitation of GPU memory for CBNet , we put one image on each GPU for training the detectors using CBNet. Meanwhile, we set the initial learning rate as the half of the default value and train for the same epoches as the original baselines. It is worth noting that, we do not change any other configuration of these baselines except the reduction of the initial learning rate and batch size. During the inference, we completely use the configuration in the original baselines . For Cascade Mask R-CNN with different backbones, we run both single-scale test and multi-scale test. And for other baseline detectors, we run single-scale test, in which the short side of input image is resized to 800, and the longer side is limited to 1333. It is noted that we do not utilize Soft-NMS <ref type="bibr" target="#b0">(Bodla et al. 2017</ref>) during the inference for fair comparison .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detection results</head><p>To demonstrate the effectiveness of the proposed CBNet, we conduct a series of experiments with the baselines of stateof-the-art detectors, i.e.,    <ref type="table" target="#tab_2">Table 2</ref>, we compare a baseline (provided by Detectron ) with its variants using the proposed CBNet, and one can see that our CBNet consistently improves all of these baselines with a significant margin. More specifically, the mAPs of these baselines increase by 1.5 to 3 percent. Furthermore, as presented in <ref type="table" target="#tab_5">Table 3</ref>, a new state-ofthe-art detection result of 53.3 mAP on the MS-COCO benchmark is achieved by Cascade Mask R-CNN baseline equipped with the proposed CBNet. Notably, this result is achieved just by single model, without any other improvement for the baseline besides taking CBNet as backbone. Hence, this result demonstrates great effectiveness of the proposed CBNet architecture.</p><p>Moreover, as shown in <ref type="table" target="#tab_2">Table 2</ref>, the proposed CBNet also improves the performances of the baselines for instance segmentation. Compared with bounding boxes prediction (i.e., object detection), pixel-wise classification (i.e., instance segmentation) tends to be more difficult and requires more representational features. And these results demonstrate the effectiveness of CBNet again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons of different composite styles</head><p>We further conduct experiments to compare the suggested composite style AHLC with other possible composite styles illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, including SLC, ALLC, and DHLC. All of these experiments are conducted based on the Dual-Backbone architecture and the baseline of FPN ResNet101.</p><p>SLC v.s. AHLC As presented in <ref type="table" target="#tab_4">Table 4</ref>, SLC gets even worse result than the original baseline. We think the major reason is that the architecture of SLC will bring serious parameter redundancy. To be more specific, the features extracted by the same stage of the two backbones in CBNet are similar, hence SLC cannot learn more semantic information than using single backbone. In other words, the network parameters are not fully utilized, but bring much difficulty on training, leading to a worse result.</p><p>ALLC v.s. AHLC As shown in <ref type="table" target="#tab_4">Table 4</ref>, there is a great gap between ALLC and AHLC. We infer that, in our CBNet, if we directly add the lower-level (i.e., shallower) features of the previous backbone to the higher-level (i.e., deeper) ones of the succeeding backbone, the semantic information of the latter ones will be largely harmed. On the contrary, if we add the deeper features of the previous backbone to the shallow ones of the succeeding backbone, the semantic information of the latter ones can be largely enhanced.</p><p>DHLC v.s. AHLC The results in <ref type="table" target="#tab_4">Table 4</ref> show that DHLC does not bring performance improvement as AHLC, although it adds more composite connections than AHLC. We infer that, the success of Composite Backbone Network lies mainly in the composite connections between adjacent stages, while the other composite connections do not enrich much feature since they are too far away.</p><p>Obviously, CBNets of these composite styles have same amount of the network parameter (i.e., about twice amount of the network parameters than single backbone), but only AHLC brings optimal detection performance improvement. These experiment results prove that only increasing parameters or adding additional backbone may not bring better result. Moreover, these experiment also show that composite connections should be added properly. Hence, these experiment results actually demonstrate that the suggested composite style AHLC is effective and nontrivial.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sharing weights for CBNet</head><p>Due to using more backbones, CBNet brings the sharp increase of network parameters. To further demonstrate that the improvement of detection performance mainly comes from the composite architecture rather than the increase of network parameters, we conduct experiments on FPN, with the configuration of sharing the weighs of two backbones in Dual-ResNet101, and the results are shown in <ref type="table" target="#tab_7">Table 5</ref>. We can see that when sharing the weights of backbones in CB-Net, the increment of parameters is negligible, but the detection result is still much better than the baseline (e.g., mAP 40.4 v.s. 39.4). However, when we do not share the weights, the improvement is not so much (mAP from 40.4 to 41.0), which proves that it is the composite architecture that boosts the performance dominantly, rather than the increase of network parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Number of backbones in CBNet</head><p>We conduct experiments to investigate the relationship between the number of backbones in CBNet and the detection performance by taking FPN-ResNet101 as the baseline, and the results are shown in <ref type="figure">Figure 4</ref>. It can be noted that the detection mAP steadily increases with the number of backbones, and tends to converge when the number of backbones reaches three. Hence, considering the speed and memory cost, we suggest to utilize Dual-Backbone and Triple-Backbone architectures.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">An accelerated version of CBNet</head><p>The major drawback of the proposed CBNet is that it will slows down the inference speed of the baseline detector since it uses more backbones to extract features thus increases the computation complexity. For example, as shown in <ref type="table" target="#tab_9">Table 6</ref>, DB increases the AP of FPN by 1.6 percent but slows down the detection speed from 8.1 fps to 5.5 fps. To alleviate this problem, we further propose an accelerated version of the CBNet as illustrated in <ref type="figure">Figure 5</ref>, by removing the two early stages of the Assistant Backbone. As demonstrated in <ref type="table" target="#tab_9">Table 6</ref>, this accelerated version can significantly improve the speed (from 5.5 fps to 6.9 fps) while not harming the detection accuracy (i.e., AP) a lot (from 41.0 to 40.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effectiveness of basic feature enhancement by CBNet</head><p>We think the root cause that our CBNet can performs much better than the single backbone network for object detection task is: it can extract more representational basic features than the original single backbone network which is originally designed for classification problem. To verify this, as illustrated in <ref type="figure">Figure 6</ref>, we visualize and compare the intermediate the feature maps extracted by our CBNet and the original single backbone in the detectors for some examples. The example image in <ref type="figure">Figure 6</ref> contains two foreground objects: a person and a tennis ball. Obviously, the person is the large-size object and the tennis ball is the small-size object. Hence, we correspondingly visualize the large scale feature maps (for detecting small objects) and the small scale feature maps (for detecting large objects) extracted by our CBNet and the original single backbone. One can see that, the feature maps extracted by our CBNet consistently have stronger activation values at the foreground object and weaker activation values at the background. This visualization example shows that our CBNet is more effective to extract representational basic features for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature of Res2</head><p>Feature of Res5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>For CBNet For original backbone <ref type="figure">Figure 6</ref>: Visualization comparison of the features extracted by our CBNet (Dual-ResNet101) and the original backbone (ResNet101). The baseline detector is FPN-ResNet101. For each backbone, we visualize the Res2 and Res5 according to the size of the foreground objects, by averaging feature maps along channel dimension. It is noteworthy that feature maps come from the CBNet are more representational since they have stronger activation values at the foreground object and weaker activation values at the background. Best view in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, a novel network architecture called Composite Backbone Network (CBNet) is proposed to boost the performance of state-of-the-art object detectors. CBNet consists of a series of backbones with same network structure and uses composite connections to link these backbones. Specifically, the output of each stage in a previous backbone flows to the parallel stage of the succeeding backbone as part of inputs through composite connections. Finally, the feature maps of the last backbone namely Lead Backbone are used for object detection. Extensive experimental results demonstrate that the proposed CBNet is beneficial for many stateof-the-art detectors, such as FPN, Mask R-CNN, and Cascade R-CNN, to improve their detection accuracy. To be more specific, the mAPs of the detectors mentioned above on the COCO dataset are increased by about 1.5 to 3 percent, and a new state-of-the art result on COCO with the mAP of 53.3 is achieved by simply integrating CBNet into the Cascade Mask R-CNN baseline. Simultaneously, experimental results show that it is also very effective to improve the instance segmentation performance. Additional ablation studies further demonstrate the effectiveness of the proposed architecture and the composite connection module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the proposed Composite Backbone Network (CBNet) architecture for object detection. CBNet assembles multiple identical backbones (Assistant Backbones and Lead Backbone) by composite connections between the parallel stages of the adjacent backbones. In this way, CBNet iteratively feeds the output features of the previous backbone as part of input features to the succeeding backbone, in a stage-by-stage fashion, and finally outputs the features of the last backbone namely Lead Backbone for object detection. The red arrows represent composite connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between (a). our proposed CBNet architecture (K = 2) and (b). the unrolled architecture of RCNN (Liang and Hu 2015)(T = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Four kinds of composite styles for Dual-Backbone architecture (an Assistant Backbone and a Lead Backbone). (a) Adjacent Higher-Level Composition (AHLC). (b) Same Level Composition (SLC). (c) Adjacent Lower-Level Composition (ALLC). (d) Dense Higher-Level Composition (DHLC). The composite connection denotes in blue boxes represents some simple operations, i.e., element-wise operation, scaling, 1×1 Conv layer and bn layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b20">Redmon et al. 2016;</ref><ref type="bibr" target="#b21">Ren et al. 2015;</ref><ref type="bibr" target="#b28">Zhang et al. 2018;</ref><ref type="bibr" target="#b14">Lin et al. 2017a;</ref><ref type="bibr" target="#b8">He et al. 2017;</ref><ref type="bibr" target="#b1">Cai and Vasconcelos 2018)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>FPN (Lin et al. 2017a), Mask R-CNN (He et al. 2017) and Cascade R-CNN (Cai and Vas-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Object detection results on the MS-COCO test-dev dataset using different number of backbones in CBNet architecture based on FPN ResNet101. An accelerated version of CBNet (K = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Backbone NetworkAP box AP 50 AP 75Table 1: Results of the state-of-the-art detector Cascade Mask R-CNN on the COCO test-dev dataset</figDesc><table><row><cell>ResNet50</cell><cell>41.5</cell><cell>60.1</cell><cell>45.5</cell></row><row><cell>ResNet101</cell><cell>43.3</cell><cell>61.7</cell><cell>47.2</cell></row><row><cell>ResNeXt101</cell><cell>45.9</cell><cell>64.4</cell><cell>50.2</cell></row><row><cell>ResNeXt152</cell><cell>48.3</cell><cell>67.0</cell><cell>52.8</cell></row><row><cell>Dual-ResNeXt152 (ours)</cell><cell>50.0</cell><cell>68.8</cell><cell>54.6</cell></row><row><cell>Triple-ResNeXt152 (ours)</cell><cell>50.7</cell><cell>69.8</cell><cell>55.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Baseline detectorSingle DB TB AP bbox AP50 AP75 AP mask AP50 AP75</figDesc><table><row><cell></cell><cell>39.4</cell><cell>61.5</cell><cell>42.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FPN + ResNet101</cell><cell>41.0</cell><cell>62.4</cell><cell>44.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>41.7</cell><cell>64.0</cell><cell>45.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>40.0</cell><cell>61.2</cell><cell>43.6</cell><cell>35.9</cell><cell>57.9</cell><cell>38.0</cell></row><row><cell>Mask R-CNN + ResNet101</cell><cell>41.8</cell><cell>62.9</cell><cell>45.6</cell><cell>37.0</cell><cell>59.5</cell><cell>39.3</cell></row><row><cell></cell><cell>42.4</cell><cell>64.0</cell><cell>46.7</cell><cell>38.1</cell><cell>59.9</cell><cell>40.8</cell></row><row><cell></cell><cell>42.8</cell><cell>62.1</cell><cell>46.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Cascade R-CNN + ResNet101</cell><cell>44.3</cell><cell>62.5</cell><cell>48.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>44.9</cell><cell>63.9</cell><cell>48.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>48.3</cell><cell>67.0</cell><cell>52.8</cell><cell>41.0</cell><cell>64.1</cell><cell>44.2</cell></row><row><cell>Cascade Mask R-CNN + ResNeXt152</cell><cell>50.0</cell><cell>68.8</cell><cell>54.6</cell><cell>42.0</cell><cell>64.6</cell><cell>45.6</cell></row><row><cell></cell><cell>50.7</cell><cell>69.8</cell><cell>55.5</cell><cell>43.3</cell><cell>66.9</cell><cell>46.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Detection results on the MS-COCO test-dev set. We report both object detection and instance segmentation results on four kinds of detectors to demonstrate the effectiveness of CBNet. Single: with/without baseline backbone.</figDesc><table><row><cell>DB: with/without</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Comparison between different composite styles,</cell></row><row><cell>the baseline is FPN ResNet101 (Lin et al. 2017a). DB:</cell></row><row><cell>with/without Dual-Backbone. "SLC" represents Same Level</cell></row><row><cell>Composition, "ALLC" represents Adajacent Lower-Level</cell></row><row><cell>Composition,"ADLC" is Adjacent Dense Level Composi-</cell></row><row><cell>tion and "AHLC" is Adjacent Higher-Level Composition.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Object detection comparison between our methods and state-of-the-art detectors on COCO test-dev set.</figDesc><table /><note>* : utilizing multi-scale testing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of with/without sharing weights for Dual-Backbone architecture. DB: with/without Dual-Backbone. Share: with/without sharing weights. AP box : detection results on COCO test-dev dataset. mb: the model size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison between the original DB and the accelerated vesion. DB: with/without Dual-Backbone. Ψ: with/without the acceleration modification of the CBNet architecture illustrated in Figure 5.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here and after, basic feature refers in particular to the features which are extracted by the backbone network and used as the input to other functional modules in the detector like detection head, RPN and FPN.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3367" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9333" to="9343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fishnet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="762" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04533</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
