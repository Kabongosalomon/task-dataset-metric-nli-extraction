<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
							<email>athar@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
							<email>mahadevan@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
							<email>aljosa.osep@tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix√©</surname></persName>
							<email>leal.taixe@tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing methods for instance segmentation in videos typically involve multi-stage pipelines that follow the tracking-by-detection paradigm and model a video clip as a sequence of images. Multiple networks are used to detect objects in individual frames, and then associate these detections over time. Hence, these methods are often non-end-toend trainable and highly tailored to specific tasks. In this paper, we propose a different approach that is well-suited to a variety of tasks involving instance segmentation in videos. In particular, we model a video clip as a single 3D spatio-temporal volume, and propose a novel approach that segments and tracks instances across space and time in a single stage. Our problem formulation is centered around the idea of spatio-temporal embeddings which are trained to cluster pixels belonging to a specific object instance over an entire video clip. To this end, we introduce (i) novel mixing functions that enhance the feature representation of spatio-temporal embeddings, and (ii) a single-stage, proposal-free network that can reason about temporal context. Our network is trained end-to-end to learn spatio-temporal embeddings as well as parameters required to cluster these embeddings, thus simplifying inference. Our method achieves stateof-the-art results across multiple datasets and tasks. Code and models are available at https://github.com/sabarim/STEm-Seg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of segmenting and tracking multiple objects in videos is becoming increasingly popular due to a surge in development of autonomous vehicles and robots that are able to perceive and accurately track surrounding objects. These advances are driven by the recent emergence of new datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b99">100]</ref> containing videos with dense, per-pixel annotations of object instances. The underlying task tackled in these datasets can be summarized as follows: given an input video containing multiple objects, each pixel has to be uniquely assigned to a specific object instance or to the background. State-of-the-art methods tackling these tasks <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b99">100]</ref> usually operate in a top-down fashion and follow the tracking-by-detection paradigm which is wellestablished in multi-object tracking (MOT) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b63">64]</ref>. Such methods usually employ multiple networks to detect objects in individual images <ref type="bibr" target="#b31">[32]</ref>, associate them over consecutive frames, and resolve occlusions using learned appearance models. Though these approaches yield high-quality results, they involve multiple networks, are computationally demanding, and not end-to-end trainable.</p><p>Inspired by the Perceptual Grouping Theory <ref type="bibr" target="#b67">[68]</ref>, we learn to segment object instances in videos in a bottom-up fashion by leveraging spatio-temporal embeddings. To this end, we propose an efficient, single-stage network that operates directly on a 3D spatio-temporal volume. We train the embeddings in a categoryagnostic setting, such that pixels belonging to the same object instance across the spatio-temporal volume are mapped to a single cluster in the embedding space. This way, we can infer object instances by simply assigning pixels to their respective clusters. Our method outperforms proposal-based methods for tasks involving pixel-precise tracking such as Unsupervised Video Object Segmentation (UVOS) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>, Video Instance Segmentation (VIS) <ref type="bibr" target="#b99">[100]</ref>, and Multi-Object Tracking and Segmentation (MOTS) <ref type="bibr" target="#b84">[85]</ref>.</p><p>To summarize, our contributions are the following: (i) We propose a unified approach for tasks involving instance segmentation in videos <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b99">100]</ref>. Our method performs consistently well under highly varied settings (see <ref type="figure" target="#fig_0">Fig. 1</ref>) such as automotive driving scenes <ref type="bibr" target="#b84">[85]</ref>, semantically diverse YouTube videos <ref type="bibr" target="#b99">[100]</ref> and scenarios where object classes are not pre-defined <ref type="bibr" target="#b10">[11]</ref>. (ii) We propose using spatio-temporal embeddings for the aforementioned set of tasks. To this end, we propose a set of mixing functions (Sec. 3.2) that improve performance by modifying the feature representation of these embeddings. Our method enables a simple inference procedure based on clustering within a 3D spatio-temporal volume, thus alleviating the need for external components for temporal association. (iii) We propose a single-stage network architecture which is able to effectively incorporate temporal context and learn the spatio-temporal embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image-level Instance Segmentation: Several existing methods for imagelevel instance segmentation, which is closely related to our task, operate in a top-down fashion by using a Region Proposal Network to generate object proposals <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b74">75]</ref>, which are then classified and segmented <ref type="bibr" target="#b31">[32]</ref>. Other methods operate in a bottom-up fashion by grouping pixels belonging to the same object instance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>. Novotny et al. <ref type="bibr" target="#b60">[61]</ref> introduce an embedding mixing function to overcome appearance ambiguity, and predict a displacement field from the instance-specific center, which is similar to Hough voting based methods for object detection <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b85">86]</ref>. Recent methods for 3D object detection and instance segmentation also follow this trend <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b105">106]</ref>. Neven et al. <ref type="bibr" target="#b58">[59]</ref> extend <ref type="bibr" target="#b17">[18]</ref> by training a network to predict object centers and clustering bandwidths, thus alleviating the need for density-based clustering algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">55]</ref>. This serves as a basis for our work. However, in contrast to our approach, the aforementioned methods are only suitable for image-level segmentation.</p><p>Video Segmentation: Temporally consistent object segmentation in videos can benefit several other tasks such as action/activity recognition <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>, video object detection <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b24">25]</ref> and object discovery <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b96">97]</ref>. Several bottomup methods segment moving regions by grouping based on optical flow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b97">98]</ref> or point trajectories <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b96">97]</ref>. By contrast, our method segments and tracks both moving and static objects. Other methods obtain video object proposals <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref> based on cues such as actions <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref> or image-level objects of interest <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b66">67]</ref>. Feature representations are then learned for these proposals in order to perform classification. Instead, we propose a single-stage approach that localizes objects with pixel-level precision.</p><p>Pixel-precise Tracking of Multiple Objects: Multi object tracking (MOT) has its roots in robotic perception <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b90">91]</ref>. Although some works extend MOT with pixel-precise masks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b64">65]</ref>, a much larger set of works can be found in the domain of Video Object Segmentation (VOS), which encompasses multiple tasks related to pixel-precise tracking. In the semi-supervised variant of VOS <ref type="bibr" target="#b9">[10]</ref>, the ground-truth masks of the objects which are meant to be tracked are given for the first frame. State-of-the-art approaches to this task <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b104">105]</ref> usually involve online fine-tuning on the first frame masks and/or follow the object proposal generation and mask propagation approach <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b104">105]</ref>. Chen et al. <ref type="bibr" target="#b13">[14]</ref> tackle this problem by learning embeddings from the first-frame masks, and then associating pixels in the remaining frames.</p><p>More relevant to our work is the task of unsupervised VOS <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b72">73]</ref>. Here, no ground truth information is provided at test-time, and the goal is to segment and track all dominant "foreground" objects in the video clip. Current stateof-the-art methods for this task are either proposal-based <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b106">107]</ref> or focus on foreground-background segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103]</ref>. Li et al. <ref type="bibr" target="#b48">[49]</ref> propose an approach that groups pixel embeddings based on objectness and optical flow cues. In contrast to ours, this method processes each frame separately, employs K-means clustering for object localization, and cannot separate different object instances. Wang et al. <ref type="bibr" target="#b88">[89]</ref> employ an attentive graph neural network <ref type="bibr" target="#b29">[30]</ref> and use differentiable message passing to propagate information across time; we compare our results to theirs in Sec. 4.5.</p><p>Recently, the task of Multi-Object Tracking and Segmentation (MOTS) <ref type="bibr" target="#b84">[85]</ref> and Video Instance Segmentation (VIS) <ref type="bibr" target="#b99">[100]</ref> were introduced. Voigtlaender et al. <ref type="bibr" target="#b84">[85]</ref> extended the KITTI <ref type="bibr" target="#b27">[28]</ref> and MOTChallenge <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b55">56]</ref> datasets with pixelprecise annotations, and proposed a baseline method that adapts Mask R-CNN <ref type="bibr" target="#b31">[32]</ref> to associate object proposals across time. Hu et al. <ref type="bibr" target="#b0">[1]</ref> use a mask network to filter foreground embeddings and perform mean-shift clustering to generate object masks, which are then associated over time using a distance threshold unlike our single-stage end-to-end method.</p><p>The YouTube-VIS <ref type="bibr" target="#b99">[100]</ref> contains a large number of YouTube videos with per-pixel annotations for a variety of object classes. Compared to MOTS, these videos contain fewer instances, but are significantly more diverse. In addition to adapting several existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b100">101]</ref> for this task, Yang et al. <ref type="bibr" target="#b99">[100]</ref> also proposed their own method (MaskTrack-RCNN) which extends Mask R-CNN with additional cues for data association. Methods such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b51">52]</ref> also rely on object proposals and/or heuristic post-processing to associate objects over time, unlike our end-to-end bottom-up approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>We tackle the task of instance segmentation in videos by modeling the video clip as a 3D spatio-temporal volume and using a network to learn an embedding for each pixel in that volume. This network is trained to push pixels belonging to different object instances towards different, non-overlapping clusters in the embedding space. This differs from most existing approaches, which first generate object detections per-frame, and then associate them over time. The following sections explain our problem formulation, the network architecture and loss functions employed, and the inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>As input, we assume a video clip with T frames of resolution H √ó W . Let us denote the set of RGB pixels in this clip with X ‚àà R N √ó3 where N = T √ó H √ó W . Assuming there are K object instances in this clip (K is unknown), our aim is to produce a segmentation that assigns each pixel in the clip to either the background, or to exactly one of these K instances. We design a network that predicts video instance tubes by clustering pixels simultaneously across space and time based on a learned embedding function. Instead of learning just the embedding function and then using standard density-based clustering algorithms <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref> to obtain instance tubes, we take inspiration from Neven et al. <ref type="bibr" target="#b58">[59]</ref> and design a network which estimates the cluster centers as well as their corresponding variances, thus enabling efficient inference. Formally, our network can be viewed as a function that maps the set of pixels X to three outputs; (1) E ‚àà R N √óE : an E-dimensional embedding for each pixel, (2) V ‚àà R N √óE + : a positive variance for each pixel and each embedding dimension, and (3) H ‚àà [0, 1] N : an object instance center heat-map.</p><p>Instance representation: The network is trained such that the embeddings belonging to the j th instance in the video clip are modeled by a multivariate Gaussian distribution N (¬µ j , Œ£ j ). Assuming that this instance comprises the set of pixel coordinates C j with cardinality N j , we denote the embeddings and variances output by the network at these coordinates using</p><formula xml:id="formula_0">E j ‚äÇ E, E j ‚àà R Nj √óE and V j ‚äÇ V, V j ‚àà R Nj √óE +</formula><p>, respectively. During training, C j (i.e., the ground truth mask tube for instance j) is known. Using it, the mean ¬µ j and covariance Œ£ j of the distribution are computed by averaging over the per-pixel outputs:</p><formula xml:id="formula_1">¬µ j = 1 N j e ‚àà Ej e ‚àà R E , Œ£ j = 1 N j diag Ô£´ Ô£≠ v ‚àà Vj v Ô£∂ Ô£∏ ‚àà R E√óE .<label>(1)</label></formula><p>This single distribution models all embeddings belonging to instance j across the entire clip (not individually for each frame). We can now use the distribution N (¬µ j , Œ£ j ) to compute the probability p ij of any embedding e i ‚àà E, anywhere in the input clip, of belonging to instance j:</p><formula xml:id="formula_2">p ij = 1 (2œÄ) E 2 |Œ£ j | 1 2 exp ‚àí 1 2 (e i ‚àí ¬µ j ) T Œ£ ‚àí1 j (e i ‚àí ¬µ j ) .<label>(2)</label></formula><p>Using Eq. 2 to compute p ij , ‚àÄ i ‚àà {1, .. , N }, we can obtain the set of pixels C j comprising the predicted mask tube for instance j by thresholding the probabilities at 0.5:</p><formula xml:id="formula_3">C j = { (x i , y i , t i ) | i ‚àà {1, .. , N }, p ij &gt; 0.5 } .<label>(3)</label></formula><p>Training: This way, the training objective can be formulated as one of learning the optimal parameters ¬µ opt j and Œ£ opt j which maximize the intersection-overunion (IoU) between the predicted and ground-truth mask tubes in the clip:</p><formula xml:id="formula_4">¬µ opt j , Œ£ opt j = argmax ¬µj ,Œ£ j C j ‚à© C j C j ‚à™ C j ,<label>(4)</label></formula><p>that is, all pixels in the ground truth mask tube C j should have probability larger than 0.5, and vice versa. A classification loss such as cross-entropy could be used to optimize the pixel probabilities. However, we employ the Lov√†sz hinge loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b103">104]</ref> (details in supplementary material), which is a differentiable, convex surrogate of the Jaccard index that directly optimizes the IoU. This formulation allows ¬µ j and Œ£ j to be implicitly learned by the network. Using Eqs. 1-4, we can define and optimize the distribution for every instance (i.e. ‚àÄj ‚àà {1, ..., K}). Note that only a single forward pass of the network is required regardless of the number of instances in the clip. This is in contrast to common approaches for Video Object Segmentation, which only process one instance at a time <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b94">95]</ref>.</p><p>We further remark that ours is a bottom-up approach which detects and tracks objects in a single step, thus mitigating the inherent drawback of top-down approaches that often require different networks/cues for single-image object detection and temporal association. A further advantage of our approach is that it can implicitly resolve occlusions, insofar as they occur within the clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding Representation</head><p>Under the formulation described in Sec. 3.1, the network can learn arbitrary representations for the embedding dimensions. However, it is also possible to fix the representation by using a mixing function œÜ : R E ‚Üí R E that modifies the embeddings E as follows: E ‚Üê {œÜ(e), | e ‚àà E }.</p><p>In <ref type="bibr" target="#b58">[59]</ref>, for the task of single-image instance segmentation, 2D embeddings were used (E = 2) in conjunction with a spatial coordinate mixing function œÜ xy (e i ) = e i + [x i , y i ] 3 . With this setting, the embeddings could be interpreted as offsets to the (x, y) coordinates of their respective locations. The network thus learned to cluster the embeddings belonging to a given object towards some object-specific point on the image. It follows that the predicted variances could be interpreted as the network's estimate of the size of the object along the x and y axes. We postulate that the reason for this formulation yielding good results is that the (x, y) coordinates already serve as a good initial feature for instance separation; the network can then enhance this representation by producing offsets which further improve the clustering behavior. Furthermore, this can be done in an end-to-end trainable fashion which allows the network to adjust the clustering parameters, i.e., the Gaussian distribution parameters, for each instance. In general, it has been shown that imparting spatial coordinate information to CNNs can improve performance for a variety of tasks <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>Compared to single-image segmentation, the task of associating pixels across space and time in in videos poses additional challenges, e.g., camera ego-motion, occlusions, appearance/pose changes. To tackle these challenges we propose (and experimentally validate in Sec. 4.3) the following extensions:</p><p>Spatio-temporal coordinating mixing: Since we operate on video clips instead of single images, a logical extension is to use 3D embeddings (E = 3) with a spatio-temporal coordinate mixing function</p><formula xml:id="formula_5">œÜ xyt (e i ) = e i + [x i , y i , t i ].</formula><p>Free dimensions: In addition to the spatial (and temporal) coordinate dimensions, it can be beneficial to include extra dimensions whose representation is left for the network to decide. The motivation here is to improve instance clustering quality by allowing additional degrees of freedom in the embedding space. From here on, we shall refer to these extra embedding dimensions as free dimensions. For example, if E = 4 with 2 spatial coordinate dimensions and 2 free dimensions, the mixing function is denoted as There is, however, a caveat with free dimensions: since the spatial (and temporal) dimensions already achieve reasonable instance separation, the network may converge to a poor local minimum by producing very large variances for the free dimensions instead of learning a discriminative feature representation. Consequently, the free dimensions may end up offering no useful instance separation during inference. We circumvent this problem at the cost of introducing one extra hyper-parameter by setting the variances for the free dimensions to a fixed value v free . We justify our formulation quantitatively using multiple datasets and different variants of the mixing function œÜ(¬∑) in Sec. 4.3.</p><formula xml:id="formula_6">œÜ xyff (e i ) = e i + [x i , y i , 0, 0].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>Since the ground truth mask tube is not known during inference, it is not possible to obtain ¬µ j and Œ£ j using Eq. 1. This is where the instance center heat map H comes into play. For each pixel c i = (x i , y i , t i ), the value H(c i ) ‚àà [0, 1] in the heat map at this location gives us the probability of the embedding vector E(c i ) at this location being an instance center. The sequential process of inferring object instances in a video clip is described in the following algorithm:</p><p>1. Identify the coordinates of the most likely instance center c j = argmax i H(c i ). 2. Find the corresponding embedding vector E(c j ) and variances V(c j ). 3. Using ¬µ j ‚Üê E(c j ) and Œ£ j ‚Üê diag (V(c j )), generate the 3D mask tube C j for this instance by computing per-pixel probabilities using Eq. 2, and then thresholding them as in Eq. 3. 4. Since the pixels in C j have now been assigned to an instance, the embeddings, variances and heat map probabilities at these pixel locations are masked out and removed from further consideration:</p><formula xml:id="formula_7">E ‚Üê E \ E j , V ‚Üê V \ V j , H ‚Üê H \ H j .<label>(5)</label></formula><p>5. Repeat steps 1-4 until either E = V = H = ‚àÖ, or the next highest probability in the heat map falls below some threshold.</p><p>Even though this final clustering step ( <ref type="figure" target="#fig_1">Fig. 2</ref>) depends on the number of instances in a clip, in practice, the application of Eq. 2 and 3 carries little computational overhead and its run-time is negligible compared to a forward pass.</p><p>Video clip stitching: Due to memory constraints, the clip length that can be input to the network is limited. In order to apply our framework to videos of arbitrary length, we split the input video into clips of length T with an overlap of T c frames between consecutive clips. Linear assignment <ref type="bibr" target="#b43">[44]</ref> is then used to associate the predicted tracklets in consecutive clips. The cost metric for this assignment is the IoU between tracks in overlapping frames. Our approach is currently near online because, given a new frame, the delay until its output becomes available is at most T ‚àí T c ‚àí 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Losses</head><p>Our model's loss function is a linear combination of three terms:</p><formula xml:id="formula_8">L total = L emb + L smooth + L center (6)</formula><p>Embedding loss L emb : As mentioned in Sec. 3.1, we use the Lov√†sz hinge loss to optimize the IoU between the predicted and ground truth masks for a given instance. The embedding loss for the entire input clip is calculated as the mean of the Lov√†sz hinge loss for all object instances in that clip.</p><p>Variance smoothness loss L smooth : To ensure that the variance values at every pixel belonging to an object are consistent, we employ a smoothness loss L smooth similar to <ref type="bibr" target="#b58">[59]</ref>. This regresses the variances V j for instance j to be close to the average value of all the variances for that instance, i.e. Var[V j ].</p><p>Instance center heat map loss L center : For all pixels belonging to instance j, the corresponding outputs in the sigmoid activated heat map H j are trained with an L 2 regression loss to match the output of Eq. 2. The outputs for background pixels are regressed to 0. During inference, this enables us to sample the highest values from the heat map which corresponds to the peak of the learned Gaussian distributions for the object instances in an input volume, as explained in Sec 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Architecture</head><p>The network ( <ref type="figure" target="#fig_2">Fig. 3)</ref> consists of an encoder with two decoders. The first decoder outputs the embeddings E and variances V, while the second outputs the instance center heat map H. The encoder comprises a backbone with Feature Pyramid Network (FPN) that produces feature maps at 4 different scales for each image in the input clip. The feature maps at each scale are then stacked along the temporal dimension before being input to each of the decoders. Our decoder consists of 3D convolutions and pooling layers which first compress the feature maps along the temporal dimension before gradually expanding them back to the input size. The underlying idea here is to allow the network to learn temporal context in order to enable pixel association across space and  time. To reduce the decoder's memory footprint and run-time, the large sized feature maps undergo a lower degree of temporal pooling (i.e., fewer convolution/normalization/pooling layers). We call our decoder a temporal squeezeexpand decoder (abbreviated as TSE decoder). In Sec. 4.5, we experimentally validate our network's stand-alone ability to learn spatio-temporal context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Category Prediction</head><p>Our task formulation is inherently category-agnostic. However, some datasets <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b99">100]</ref> require a category label for each predicted object track. For such cases we add an additional TSE decoder to the network that performs semantic segmentation for all pixels in the input clip, and that is trained using a standard cross-entropy loss. During inference, the logits for all pixels belonging to a given object instance are averaged, and the highest scoring category label is assigned. Note that this is merely a post-processing step; the instance clustering still happens in a category-agnostic manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>To demonstrate our method's effectiveness and generalization capability, we apply it to three different tasks and datasets involving pixel-precise segmentation and tracking of multiple objects in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>For all experiments, we use a ResNet-101 <ref type="bibr" target="#b32">[33]</ref> backbone initialized with weights from Mask R-CNN <ref type="bibr" target="#b31">[32]</ref> trained for image instance segmentation on COCO <ref type="bibr" target="#b49">[50]</ref>. The temporal squeeze-expand decoders are initialized with random weights. The network is optimized end-to-end using SGD with momentum 0.9 and an initial learning rate of 10 ‚àí3 which decays exponentially.</p><p>Augmented Images. Since the amount of publicly available video data with dense per-pixel annotations is limited, we utilize image instance segmentation datasets by synthesizing training clips from single images. We apply on-the-fly random affine transformations and motion blur and show that this technique is effective (Sec. 4.3) even though such augmented image sequences have little visual resemblance to video clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmarks</head><p>DAVIS Unsupervised: DAVIS <ref type="bibr" target="#b10">[11]</ref> is a popular video object segmentation dataset with 90 videos (60 for training and 30 for validation) containing multiple moving objects of diverse categories. Several DAVIS benchmarks have been introduced over the years; we evaluate our method on the 2019 Unsupervised Video Object Segmentation (UVOS) benchmark in which the salient "foreground objects" in each video have to be segmented and tracked. The evaluation measures employed are (i) J -score (the IoU between predicted and ground truth mask tubes), and (ii) F-score (accuracy of predicted mask boundaries against ground truth). The mean of those measures, J &amp;F, serves as the final score.</p><p>YouTube-VIS: The YouTube Video Instance Segmentation (YT-VIS) <ref type="bibr" target="#b99">[100]</ref> dataset contains 2,883 high quality YouTube videos with ‚àº131k object instances spanning 40 known categories. The task requires all objects belonging to the known category set to be segmented, tracked and assigned a category label. The evaluation measures used for this task are an adaptation of the Average Precision (AP) and Average Recall (AR) metrics used for image instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI-MOTS:</head><p>Multi-Object Tracking and Segmentation (MOTS) <ref type="bibr" target="#b84">[85]</ref> extends the KITTI multi-object tracking dataset <ref type="bibr" target="#b27">[28]</ref> with pixel-precise instance masks. It contains 21 videos (12 for training and 9 for validation) of driving scenes captured from a moving vehicle. The task here is to segment and track all car and pedestrian instances in the videos. The evaluation measures used are an extension of the CLEAR MOT measures <ref type="bibr" target="#b4">[5]</ref> to account for pixel-precise tracking (details in <ref type="bibr" target="#b84">[85]</ref>). We report these measures separately for each object class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Embedding formulation: We first ablate the impact of using different mixing functions for the embeddings (Sec. 3.2) on the DAVIS'19 val dataset with clip length T = 8, and summarize the results in Tab. 1(a). Compared to the identity function baseline œÜ identity , imparting a spatial coordinate offset (œÜ xy ) improves the J &amp;F from 57.3% to 61.6%. Adding another embedding dimension with a temporal coordinate offset, as in œÜ xyt , yields a further improvement to 62.6%.</p><p>Comparing the mixing function pairs (œÜ xyf , œÜ xyt ) where E = 3, and (œÜ xyff , œÜ xytf ) where E = 4, we note that having a free dimension is slightly better than having a temporal dimension since there is a difference of 0.2% J &amp;F for both pairs of functions. This is the case for both DAVIS'19 and YT-VIS, <ref type="bibr" target="#b3">4</ref> where œÜ xyff yields the best results. For KITTI-MOTS however, <ref type="bibr" target="#b3">4</ref>  free dimensions yield roughly the same performance. Our intuitive explanation is that for DAVIS and YT-VIS, object instances normally persist throughout the video clip. Therefore, in contrast to the spatial coordinates which serve as a useful feature for instance separation, the temporal coordinate provides no useful separation cue, thus rendering the temporal embedding dimension less effective. On the other hand, using a free dimension enables the network to learn a more discriminative feature representation that can better aid in separating instances. By contrast, objects in KITTI-MOTS driving scenes undergo fast motion and often enter/exit the scene midway through a clip. Thus, the temporal dimension becomes useful for instance separation.</p><p>Having additional embedding dimensions is beneficial, but only up to a certain point (i.e., E = 4). Beyond that, test-time performance drops, as can be seen by comparing œÜ xyff and œÜ xyfff . We conclude by noting that our proposed formulations for œÜ(¬∑) improve performance on video-related tasks compared to existing formulations for single-image tasks <ref type="bibr">[59,?]</ref>. For further discussion and results we refer to the supplementary material.</p><p>Temporal Window Size: Next, we investigate the effect of varying the input clip length on DAVIS'19 val (œÜ xyff is used throughout). As shown in Tab. 1(b), larger temporal length helps the TSE decoder to predict better quality mask tubes. Increasing the input clip length from T = 4 to T = 16 improves the J &amp;F from 62.2% to 64.7%, respectively. Above T = 24, the performance decreases.</p><p>Training Data: For the DAVIS Unsupervised task, we train on image datasets (COCO <ref type="bibr" target="#b49">[50]</ref> and Pascal VOC <ref type="bibr" target="#b23">[24]</ref>) in conjunction with video datasets (YT-VIS <ref type="bibr" target="#b99">[100]</ref> and DAVIS <ref type="bibr" target="#b10">[11]</ref>). As shown in Tab. 1(c), this combination yields 64.4% J &amp;F compared to 60.7% when using only video datasets, and 57.1% when using only image datasets. This highlights the benefit of using a combination of imageaugmented and video data. For this ablation, T = 8 and œÜ xyff were used.</p><p>Semantic Head: Since our network does not produce semantic labels for objects, we adapt it to tasks requiring such labels by adding a semantic segmentation decoder as explained in Sec. 3.6. Here, we compare the quality of our semantic output to an oracle by training our network on a custom train-validation   <ref type="bibr" target="#b31">[32]</ref> network trained with the same backbone and data as our method (see supplementary for details). Our method (64.7% J &amp;F) outperforms these baselines and the other published methods by a significant margin, even though we use a single, proposal-free network. AGNN <ref type="bibr" target="#b88">[89]</ref>, with the second best score of 61.1%, uses object proposals from Mask R-CNN <ref type="bibr" target="#b31">[32]</ref> on the salient regions detected by their network, and associates them over time using optical flow. We also list the top entries of the DAVIS'19 Challenge Workshop in gray. UnOVOST <ref type="bibr" target="#b106">[107]</ref> achieves a higher score (67.0%), but (i) uses several networks along with heuristic-based post-processing, (ii) is an order of magnitude slower (1 vs. 7FPS), and (iii) is highly tailored to this benchmark. To validate this, we adapted UnOVOST to KITTI-MOTS by re-training its networks and optimizing the post-processing parameters with grid search (for further details and analysis of this experiment, we refer to the supplementary). As can be seen in Tab. 4, it does not generalize well to the task of Multi-object Tracking and Segmentation.</p><p>Video Instance Segmentation (VIS): This task requires object instances to be segmented, tracked and also assigned a category label. We therefore adapt our network to this setting as explained in Sec. 3.6. We train jointly on YT-VIS <ref type="bibr" target="#b99">[100]</ref> and augmented images from COCO <ref type="bibr" target="#b49">[50]</ref> (for COCO, we only use the 20 object classes which overlap with the YT-VIS class set). Since this is a new task with few published works, we compare our method to various baselines and adaptions of existing works from <ref type="bibr" target="#b99">[100]</ref>    post-processing. Since MaskTrack-RCNN uses a ResNet-50 backbone, we also applied this backbone to our network and still improve the AP from 30.3 to 30.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Object Tracking and Segmentation (MOTS):</head><p>Here we again adapt the network for category prediction as required by the task formulation. We outline the results of comparing our method with TrackR-CNN <ref type="bibr" target="#b84">[85]</ref> and Un-OVOST <ref type="bibr" target="#b106">[107]</ref> in Tab. 4. Current top-performing method on MOTS is the twostage network that extends Mask-RCNN with a re-id head, trained to learn an appearance embedding vector for each object detection, used for data association. Our method achieves the highest sMOTSA score (50.4) on the pedestrian class, but TrackR-CNN performs better on the car class. However, for both classes, STEm-Seg suffers significantly fewer ID switches (IDS) compared to TrackR-CNN (76 vs. 93 for the car class and 14 vs. 78 for the pedestrian class). This measure is of particular interest to the tracking community since it directly reflects temporal association accuracy.</p><p>Similar to UnOVOST <ref type="bibr" target="#b106">[107]</ref>, TrackR-CNN does not generalize well to other tasks. To validate this, we retrained TrackR-CNN on the YT-VIS dataset. However, the resulting AP was less than 10. We assume this is due to TrackR-CNN that is forced to use a 14 √ó 14 ROI-Align layer <ref type="bibr" target="#b31">[32]</ref> due to memory constraints. This results in coarse segmentation masks which are heavily penalized by the AP measure. Furthermore, the ReID-based embeddings can only learn an ap-pearance model, which is a limitation in YT-VIS where similar looking objects often occur in the same scene.  Finally, we apply our method to the DAVIS'16 unsupervised benchmark <ref type="bibr" target="#b72">[73]</ref>, where the task is to produce a binary segmentation for the salient regions in a given video clip. Since separating object instances is not required, we simplify our network to one decoder with two output channels trained for binary segmentation using a bootstrapped cross-entropy loss <ref type="bibr" target="#b92">[93]</ref> on randomly selected clips from the YT-VIS and DAVIS datasets. Although competing methods are specifically engineered for this task, our simple setup obtains state-of-the-art results (Tab. 5). We note that while AGNN <ref type="bibr" target="#b88">[89]</ref> and AD-Net <ref type="bibr" target="#b101">[102]</ref> use a stronger DeepLabv3 <ref type="bibr" target="#b11">[12]</ref> backbone, we use additional video data for training. This is needed as we work with 3D input volumes. Since we do not perform post-processing, we report results from AD-Net <ref type="bibr" target="#b101">[102]</ref> without their DAVIS-specific post-processing for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Segmentation of Salient Regions in Videos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel bottom-up approach for instance segmentation in videos which models video clips as 3D space-time volumes and then separates object instances by clustering learned embeddings. We enhanced the feature representation of these embeddings using novel mixing functions which yield considerable performance improvements over existing formulations. We applied our method to multiple, diverse datasets and achieved state-of-the-art results under both category-aware and category-agnostic settings. We further showed that, compared to existing dataset-specific state-of-the-arts, our approach generalizes much better across different datasets. Finally, we validated our network's temporal context learning ability by performing a separate experiment on video saliency detection and showed that our good results also generalize there.</p><p>help with experiments and Francis Engelmann, Theodora Kontogianni, Paul Voigtlaender, Gulliem Bras√≥ and Aysim Toker for helpful discussions.</p><formula xml:id="formula_9">L total = L emb + L smooth + L center ,<label>(1)</label></formula><p>here L smooth is the variance smoothness loss, which ensures that the network outputs uniform variance values for every object instance. For example, if the network outputs the set of variances V j for the j th instance in a video clip, then the variance smoothness loss for this set of variances is denoted by L j smooth and is computed as:</p><formula xml:id="formula_10">L j smooth = 1 |V j | v‚ààVj (v ‚àív) 2 ,</formula><p>wherev is the mean of the variances in V j . Likewise, the loss can be computed for all object instances in the video clip and averaged. No loss is applied to the variances output for background pixels. L center is a regression loss, which ensures that pixels belonging to a foreground object instance have probability values in the instance center heat map H that match the probability obtained by applying Eq. 2 (main text) to the embedding vector at that pixel location.</p><p>L emb is the embedding loss, and is computed using the Lov√†sz extension of the hinge loss for binary segmentation, as explained below.</p><p>The Lov√†sz Hinge Loss: We use the Lov√†sz Hinge Loss <ref type="bibr" target="#b2">[3]</ref> to train the embeddings output by our network (L emb ). It is a convex surrogate of the Jaccard index which directly optimizes the Intersection over Union (IoU) between the predicted and the ground truth object mask tubes, thereby alleviating class imbalance issues that arise from using, e.g., the cross-entropy loss. In practice, we apply the Lov√†sz Hinge loss for binary segmentation. For a given video object instance prediction, we use F to denote the set of scores for each pixel in the video 1 , and denote by ‚àÜ J the set of incorrect pixel predictions 2 . The loss L emb can then be computed as follows:</p><formula xml:id="formula_11">L emb (F ) =‚àÜ J (h(F )),<label>(2)</label></formula><p>where‚àÜ J is the Lov√†sz extension of ‚àÜ J , and h is the hinge loss associated with a binary prediction. Here we provide only a high-level description of the loss function. For a more detailed explanation of this loss we refer the reader to <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Implementation Details</head><p>Hardware: We train our network using a batch size of 2 on a workstation with 2 Nvidia RTX TITAN GPUs and 64GB RAM. All inference experiments were performed on a workstation with a single Nvidia GTX 1080Ti GPU and 32GB RAM.</p><p>Training Schedule: For all tasks, the network is trained using an SGD optimizer with an initial learning rate of 10 ‚àí3 . The learning rate is initially constant and then starts to decay exponentially after a certain number of iterations up to 10 ‚àí5 . The exact number of iterations varies for each setting as follows:</p><p>-DAVIS'19 Unsupervised: 60k total iterations, decay begins after 20k iterations. -YouTube-VIS: 150k total iterations, decay begins after 60k iterations.</p><p>-KITTI-MOTS: 100k total iterations, decay begins after 40k iterations.</p><p>Image Augmented Sequences: As mentioned in Sec. 4.1, we train our network on clips from actual video data in addition to sequences that have been synthesized from static images using random affine transformations and motion blur. Doing so allows us to utilize a large amount of publicly available image instance segmentation data (e.g., COCO <ref type="bibr" target="#b49">[50]</ref>, Pascal-VOC <ref type="bibr" target="#b23">[24]</ref>, Mapillary Vistas <ref type="bibr" target="#b57">[58]</ref>) for training purposes. We experimentally verified the performance benefit of incorporating such data in Sec. 4.3.</p><p>These augmentations were applied using the imgaug 3 library, which, in addition to various transformations, also provides a built-in function that simulates image blur caused by camera motion. The affine transformations we apply consist of rotations in the range [‚àí10 ‚Ä¢ , 10 ‚Ä¢ ], translations of up to 10% of the image dimension along each axis, and scale variations in the range [0.8, 1.2]. We also apply small random offsets to the hue and saturation values of each image. All random transformations are independent of one another, i.e., we do not try to simulate consistent motion by sequentially applying the same transformation multiple times.</p><p>Video Data Augmentation: For training clips sampled from actual video data, random horizontal flipping is the only augmentation used. This is applied randomly to entire clips and not to individual frames within clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Baselines for DAVIS'19 Unsupervised</head><p>In Sec. 4.2 we compared our method to two simple proposal-based baselines: optical flow tracker (OF-Tracker) and Re-ID tracker (RI-Tracker), in addition to other published methods on DAVIS'19 unsupervised benchmark. For both, we generate per-frame mask proposals M ‚àà {m 1 , ..., m n } for all the objects in a video using a ResNet-101 based Mask R-CNN <ref type="bibr" target="#b31">[32]</ref>. To ensure a fair  comparison with our approach, we train the Mask R-CNN jointly on YouTube-VIS <ref type="bibr" target="#b99">[100]</ref>, DAVIS'19 <ref type="bibr" target="#b10">[11]</ref>, and augmented images from COCO <ref type="bibr" target="#b49">[50]</ref>, as well as Pascal-VOC <ref type="bibr" target="#b23">[24]</ref> dataset for 120k iterations. This network is initialized with weights from a model trained for image instance segmentation on COCO. We use SGD with a momentum of 0.9 and an initial learning rate of 10 ‚àí3 with exponential decay. The mask proposals generated by this re-trained Mask R-CNN network are then linked over time using optical flow and re-id for OF-Tracker and RI-Tracker, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OF-Tracker:</head><p>We use PWC-Net <ref type="bibr" target="#b78">[79]</ref> to generate optical flow for each subsequent pair of frames in the DAVIS'19 validation set. The optical flow is then used to warp m i+1 onto m i for each frame pair {i, i+1} to generate a set of warped masks per-frame W ‚àà {w 1 , ..., w n‚àí1 } for a video sequence. A simple linear assignment based on object overlap between the warped frame w i and the proposal m i is then used to associate the objects in the adjacent video frames. The associated object IDs are further propagated forward throughout the video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RI-Tracker:</head><p>For the RI-Tracker, we train a re-id network with a ResNet-50 <ref type="bibr" target="#b32">[33]</ref> backbone on the DAVIS'19 <ref type="bibr" target="#b10">[11]</ref> training set. The network is trained using a batch hard triplet loss <ref type="bibr" target="#b33">[34]</ref> on randomly selected triplets from a random video sequence for 25k iterations. This network is then used to generate re-id vectors for all the object proposals in M , which are further associated over time using linear assignment based on the Euclidean distance between embedding vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Extended Ablations for Embedding Mixing Function</head><p>In Sec. 4.3, we ablated the impact of using different mixing functions œÜ(¬∑) that modify the embedding representation as discussed in Sec. <ref type="bibr" target="#b2">3</ref> It can be seen that both DAVIS'19 and YouTube-VIS give consistent results: for the same number of total embedding dimensions (E), having a free dimension is more beneficial than having a temporal coordinate dimension. For KITTI-MOTS, however, the trend differs. In particular, we obtain similar performance with œÜ xyt (72.5 and 48.9 sMOTSA on the car and pedestrian class, respectively) and œÜ xyff (73.2 and 47.3 sMOTSA). In Tab. 4 (main text), we reported the results for œÜ xyt since the mean sMOTSA score for the two categories (60.70) is slightly better than that of œÜ xyff <ref type="bibr">(60.25)</ref>. We attribute this difference in part to the fact that the temporal coordinate is a more useful feature for instance separation in KITTI-MOTS than in DAVIS'19 due to the fact that object instances undergo faster motion and often enter/exit the scene mid-way through a video clip. Furthermore, the performance trends for the car and pedestrian classes seem to follow different patterns, e.g., while œÜ xyfff yields the highest sMOTSA for the car class (73.4), it is significantly lower for the pedestrian class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. UnOVOST Training on KITTI-MOTS</head><p>In Sec. 4.2, we reported the performance of UnOVOST <ref type="bibr" target="#b106">[107]</ref>, the highest-scoring workshop submission for the DAVIS'19 Unsupervised Challenge <ref type="bibr" target="#b10">[11]</ref>, for the task of Multi-object Tracking and Segmentation (MOTS) using the KITTI-MOTS dataset <ref type="bibr" target="#b84">[85]</ref>. We obtained the implementation from the authors <ref type="bibr" target="#b106">[107]</ref> and retrained and tuned the model as follows:</p><p>-We initialized a Mask R-CNN <ref type="bibr" target="#b31">[32]</ref> network with a ResNet-101 <ref type="bibr" target="#b79">[80]</ref> backbone with weights from an off-the-shelf model trained for instance segmentation on the COCO dataset <ref type="bibr" target="#b49">[50]</ref>. We then altered the output layers to predict two categories, i.e.. car and pedestrian, and trained the network for 60k iterations on Mapillary Vistas <ref type="bibr" target="#b57">[58]</ref> and KITTI-MOTS datasets. The training data and the backbone is thus identical to the one used for our STEm-Seg network. -We trained a ReID network on image instance crops from KITTI-MOTS using a triplet loss <ref type="bibr" target="#b75">[76]</ref> and batch-hard sampling <ref type="bibr" target="#b33">[34]</ref>.</p><p>The two most important hyper-parameters in UnOVOST are the IoU thresholds used for pruning object detections and for associating object detections based on optical flow, respectively. We performed a grid search for these two parameters on the KITTI-MOTS validation set in order to optimize the sMOTSA score. Our observation was that the UnOVOST framework is fairly insensitive to these parameters; however, the final scores on KITTI-MOTS are consistently low (see Tab. 4 in the paper). Qualitative analysis of the results showed that the ReID network frequently makes spurious associations. We postulate that this is because object instances in KITTI-MOTS frequently have similar appearances. This differs from the object instances in DAVIS whose appearances usually differ since they span a large variety of object classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Adaptation of TrackR-CNN to YouTube-VIS</head><p>As discussed in Sec. 4.2, we adapted the publicly available implementation 4 of TrackR-CNN <ref type="bibr" target="#b84">[85]</ref> to the task of Video Instance Segmentation and evaluated it on the Youtube-VIS dataset <ref type="bibr" target="#b99">[100]</ref>. To this end, we initialized the parameters of the network, which overlap with Mask R-CNN <ref type="bibr" target="#b31">[32]</ref> with weights from a model trained for instance segmentation on COCO <ref type="bibr" target="#b49">[50]</ref> and Mapillary Vistas <ref type="bibr" target="#b57">[58]</ref>.</p><p>In the original implementation, a class-specific re-identification embedding head was used. This was feasible for KITTI-MOTS, where there are only two object classes. In YouTube-VIS, however, there are 40 object classes, and several occur infrequently in the dataset. Furthermore, video sequences are significantly shorter, and there are usually only 1-2 objects of the same class present in a video clip. For that reason, we adapted the TrackR-CNN architecture and kept a single ReID head that is shared among all object classes. We trained the network under this setting using a batch size of 8 images for 400k iterations and evaluated multiple intermediate checkpoints. Despite these efforts, the highest AP score obtained was less than 10%.</p><p>A major performance bottleneck we identified is a low-resolution 14x14 RoI-Align <ref type="bibr" target="#b31">[32]</ref> layer used in TrackR-CNN that limit the memory usage to a reasonable level. This suffices for KITTI-MOTS, which contains small pedestrian instances and cars with simple shapes, but results in very coarse segmentation masks on the YouTube-VIS dataset which contains a diverse set of objects that cover a large area of the image. The AP measure heavily penalizes such coarse segmentation as it is computed by taking the average over a set of IoU thresholds ranging from 0.5 to 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Additional Qualitative Results</head><p>In this section, we provide additional qualitative results on the validation split of all three datasets, DAVIS'19 <ref type="bibr" target="#b10">[11]</ref> in <ref type="figure" target="#fig_0">Fig. 1</ref>, YouTube-VIS <ref type="bibr" target="#b99">[100]</ref> in <ref type="figure" target="#fig_1">Fig. 2</ref> and KITTI-MOTS <ref type="bibr" target="#b84">[85]</ref> in <ref type="figure" target="#fig_2">Fig. 3</ref>. As can be seen, our method can reliably segment and track a large variety of objects in diverse scenarios, and is fairly robust to scale changes and brief occlusions.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our method is applicable to multi-object segmentation tasks such as VOS (top), VIS (middle) and MOTS (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>For an input video clip, our network produces embeddings (E), variances (V), and instance center heat map (H). H contains one peak per object for the entire spatiotemporal volume (ca for the rider, c b for the horse). E(ca), E(c b ) and V(ca), V(c b ) are the corresponding embeddings and variances at ca and c b , respectively. These quantities are then used to define the Gaussian distribution for each object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The network has an encoder-decoder structure. GN: Group Normalization<ref type="bibr" target="#b91">[92]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>time, it is robust to sudden pose changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 1 :Fig. 2 :Fig. 3 :</head><label>123</label><figDesc>Additional qualitative results on DAVIS'19. STEm-Seg generates consistently good results under varied scenarios. E.g., in the motocross-jump sequence (fifth row ) it demonstrates robustness to a large change in scale. In the bike-packing sequence (bottom row ) time Additional qualitative results on YouTube-VIS (YT-VIS) [100]. Most of the semantically challenging animal categories are successfully segmented by STEm-Seg. It also captures some fine object details such as the skateboard (top row ) and the surfboard (third row ) well.time Additional qualitative results on KITTI-MOTS. Our method successfully tracks and segments cars and pedestrians in automotive scenarios, even when observed from a large distance (sixth row from the bottom) and bridges occlusions (fifth row ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>temporal and</figDesc><table><row><cell cols="2">Mixing Function J &amp;F E</cell><cell cols="2">Clip Length (T ) J &amp;F</cell><cell>(c)</cell><cell>Training Data</cell><cell>J &amp;F</cell></row><row><cell>œÜ identity</cell><cell>57.3 2</cell><cell>4</cell><cell>62.2</cell><cell></cell><cell>Images</cell><cell>57.1</cell></row><row><cell>œÜxy</cell><cell>61.6 2</cell><cell>8</cell><cell>64.4</cell><cell></cell><cell>Video</cell><cell>60.7</cell></row><row><cell>œÜxyt</cell><cell>62.6 3</cell><cell>16</cell><cell>64.7</cell><cell></cell><cell>Images + Video</cell><cell>64.4</cell></row><row><cell>œÜ xyf</cell><cell>62.8 3</cell><cell>24</cell><cell>63.1</cell><cell></cell><cell></cell></row><row><cell>œÜ xytf</cell><cell>64.2 4</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Object Category AP AR</cell></row><row><cell>œÜ xyff œÜ xyfff</cell><cell>64.4 4 62.4 5</cell><cell></cell><cell></cell><cell>(d)</cell><cell>Oracle Predicted</cell><cell>33.0 34.5 24.7 31.8</cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Table 1: Ablation studies on DAVIS'19 val: (a): Impact of different embedding mixing</cell></row><row><cell cols="7">functions; (b): Effect of temporal context; (c): Analysis of training data; (d): Impact</cell></row><row><cell cols="5">of Semantic head on a custom validation split of YT-VIS.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>OF RI J &amp;F J Mean J Recall J Decay F Mean F Recall F Decay fps</figDesc><table><row><cell></cell><cell></cell><cell cols="3">DAVIS 2019 Unsupervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method P/D KIS  *  [15]</cell><cell>59.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UnOVOST  *  [107]</cell><cell>67.0</cell><cell>65.6</cell><cell>75.5</cell><cell>0.3</cell><cell>68.4</cell><cell>75.9</cell><cell>3.7</cell><cell>&lt; 1</cell></row><row><cell>RVOS [83]</cell><cell>41.2</cell><cell>36.8</cell><cell>40.2</cell><cell>0.5</cell><cell>45.7</cell><cell>46.4</cell><cell>1.7</cell><cell>20+</cell></row><row><cell>AGNN [89]</cell><cell>61.1</cell><cell>58.9</cell><cell>65.7</cell><cell>11.7</cell><cell>63.2</cell><cell>67.1</cell><cell>14.3</cell><cell>-</cell></row><row><cell>OF-Tracker</cell><cell>54.6</cell><cell>53.4</cell><cell>60.9</cell><cell>-1.3</cell><cell>55.9</cell><cell>63</cell><cell>1.1</cell><cell>‚àº1</cell></row><row><cell>RI-Tracker</cell><cell>56.9</cell><cell>55.5</cell><cell>63.3</cell><cell>2.7</cell><cell>58.2</cell><cell>64.4</cell><cell>6.4</cell><cell>&lt; 1</cell></row><row><cell>Ours</cell><cell>64.7</cell><cell>61.5</cell><cell>70.4</cell><cell>-4</cell><cell>67.8</cell><cell>75.5</cell><cell>1.2</cell><cell>7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>DAVIS'19 validation results for the unsupervised track. P/D: Proposals/Detections, OF: optical flow, RI: Re-Id, * : uses heuristic post-processing.split of YT-VIS. This was done because ground truth annotations for the official validation set are not publicly available. The results presented in Tab. 1(d) show that using oracle category labels improves AP performance by 8.3 from 24.7 to 33.0. This suggests that our results on the official validation set could be further improved by using a better semantic classifier. We leave this for future work.</figDesc><table /><note>4.4 Comparison with state of the art Video Object Segmentation: Tab. 2 summarizes our results on the DAVIS'19 unsupervised validation set. OF-Tracker and RI-Tracker are our own optical flow and re-id baselines which use proposals from a Mask-RCNN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>in Tab. 3. As can be seen, our method performs best with respect to all evaluation metrics. Compared to MaskTrack-RCNN [100], we improve the AP from 30.3 to 34.6, even though MaskTrack-RCNN uses a two stage object detector and incorporates additional cues during</figDesc><table><row><cell></cell><cell cols="4">YouTube Video Instance Segmentation</cell><cell></cell></row><row><cell>Method</cell><cell>FF P/D OF</cell><cell cols="5">AP AP@50 AP@75 AR@1 AR@10</cell></row><row><cell>OSMN MaskProp [101]</cell><cell></cell><cell>23.4</cell><cell>36.5</cell><cell>25.7</cell><cell>28.9</cell><cell>31.1</cell></row><row><cell>FEELVOS [84]</cell><cell></cell><cell>26.9</cell><cell>42.0</cell><cell>29.7</cell><cell>29.9</cell><cell>33.4</cell></row><row><cell>IoUTracker+ [100]</cell><cell></cell><cell>23.6</cell><cell>39.2</cell><cell>25.5</cell><cell>26.2</cell><cell>30.9</cell></row><row><cell>OSMN [101]</cell><cell></cell><cell>27.5</cell><cell>45.1</cell><cell>29.1</cell><cell>28.6</cell><cell>33.1</cell></row><row><cell>DeepSORT [90]</cell><cell></cell><cell>26.1</cell><cell>42.9</cell><cell>26.1</cell><cell>27.8</cell><cell>31.3</cell></row><row><cell>MaskTrack R-CNN [100]</cell><cell></cell><cell>30.3</cell><cell>51.1</cell><cell>32.6</cell><cell>31.0</cell><cell>35.5</cell></row><row><cell>SeqTracker [100]</cell><cell></cell><cell>27.5</cell><cell>45.7</cell><cell>28.7</cell><cell>29.7</cell><cell>32.5</cell></row><row><cell>Ours (ResNet-50)</cell><cell></cell><cell>30.6</cell><cell>50.7</cell><cell>33.5</cell><cell>31.6</cell><cell>37.1</cell></row><row><cell>Ours</cell><cell></cell><cell>34.6</cell><cell>55.8</cell><cell>37.9</cell><cell>34.4</cell><cell>41.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>YouTube-VIS validation results. P/D: Proposals/Detections, FF: First Frame Proposals, OF: Optical Flow.</figDesc><table><row><cell></cell><cell cols="2">KITTI MOTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Car</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pedestrian</cell><cell></cell></row><row><cell>Method</cell><cell cols="8">P/D OF RI sMOTSA MOTSA MOTSP IDS sMOTSA MOTSA MOTSP IDS</cell></row><row><cell>UnOVOST[107]</cell><cell>50.7</cell><cell>60.2</cell><cell>85.6</cell><cell>151</cell><cell>33.4</cell><cell>47.7</cell><cell>76.0</cell><cell>68</cell></row><row><cell>MaskRCNN+maskprop[85]</cell><cell>75.1</cell><cell>86.6</cell><cell>87.1</cell><cell>-</cell><cell>45.0</cell><cell>63.5</cell><cell>75.6</cell><cell>-</cell></row><row><cell>TrackRCNN[85]</cell><cell>76.2</cell><cell>87.8</cell><cell>87.2</cell><cell>93</cell><cell>46.8</cell><cell>65.1</cell><cell>75.7</cell><cell>78</cell></row><row><cell>Ours</cell><cell>72.7</cell><cell>83.8</cell><cell>87.2</cell><cell>76</cell><cell>50.4</cell><cell>66.1</cell><cell>77.7</cell><cell>14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>KITTI MOTS validation set results for Car and Pedestrian class. P/D: Proposals/Detections, OF: optical flow, RI: Re-Id.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results on DAVIS'16 val. for the unsupervised track. OF: optical flow, CRF: post-processing using CRF.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table I :</head><label>I</label><figDesc>Ablation studies on the Impact of different embedding mixing functions on DAVIS '19, YouTube-VIS (YT-VIS) and KITTI MOTS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>.2. In Tab. 1(a) of the main text, we reported the results of this ablation on the DAVIS'19 Unsupervised validation set. Here, we provide extended results of applying different œÜ(¬∑) on the YouTube-VIS [100] and KITTI-MOTS [85] datasets in Tab. I. The results for the DAVIS'19 Unsupervised validation set have also been repeated for reference.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This notation denotes element-wise addition between ei and [xi, yi]</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">results for various œÜ(¬∑) on YT-VIS and KITTI-MOTS are given in supplementary</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is practically just the logit value for the probability computed in Eq. 2 (main text).<ref type="bibr" target="#b1">2</ref> Obtained by thresholding the probabilities as in Eq. 3 of the main text and comparing against the ground truth mask.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/aleju/imgaug</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/VisualComputingInstitute/TrackR-CNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This project was funded, in parts, by ERC Consolidator Grant DeeVise (ERC-2017-COG-773161), EU project CROWDBOT (H2020-ICT-2017-779942) and the Humboldt Foundation through the Sofja Kovalevskaja Award. Computing resources for several experiments were granted by RWTH Aachen University under project 'rwth0519'. We thank Sebastian Hennen for</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Loss Function</head><p>As explained in Sec. 3.1 of the paper, we use a loss function that is a linear combination of three components:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning a spatio-temporal embedding for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</author>
		<idno>arXiv:1912:08969v</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Online video seeds for temporal window objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimization of the jaccard index for image segmentation with the lov√°sz hinge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The lov√°sz-softmax loss: a tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rannen Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: The clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIVP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">High-speed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AVSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-target tracking by lagrangian relaxation to mincost network flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00737</idno>
		<title level="m">The 2019 DAVIS challenge on VOS: unsupervised multi-object segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Key instance selection for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2019 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03715</idno>
		<title level="m">Towards segmenting everything that moves</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic instance segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal feature augmented network for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D-BEVIS: Birds-Eye-View Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dual embedding learning for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The estimation of the gradient of a density function, with applications in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IJCNN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08465</idno>
		<title level="m">Seq-nms for video object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Tube convolutional neural network (t-cnn) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An efficient 3d CNN for action/object segmentation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Robust object tracking by hierarchical association of detection responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On the analysis of accumulative difference pictures from image sequences of real world scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Nagel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="206" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pointgroup: Dual-set point grouping for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unsupervised object discovery and tracking in video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">MOTChallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Coupled object detection and tracking from static cameras and moving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1683" to="1698" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Instance embedding transfer to unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spatio-temporal attention network for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">hdbscan: Hierarchical density based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Astels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">MOT16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Joint tracking and segmentation of multiple targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The Mapillary Vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Semi-convolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Video object segmentation using spacetime memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">A boosted particle filter: Multitarget detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taleghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Track, then decide: Categoryagnostic vision-based multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O≈°ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Large-scale object mining for object discovery from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O≈°ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">4d generic video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O≈°ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Organizing objects and scenes. Foundations of cognitive psychology: Core readings pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="189" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Hierarchical video representation with trajectory binary partition tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Palou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Geodesic active contours and level sets for the detection and tracking of moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="266" to="280" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Video segmentation using teacher-student adaptation in a human robot interaction (HRI) setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>J√§gersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>American Ass. of Art. Intelligence</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Towards 3D object recognition via classification of arbitrary object tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teichman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">RVOS: end-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marqu√©s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gir&amp;apos;o I Nieto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">MOTS: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Pixel consensus voting for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Video object discovery and co-segmentation with extremely weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">An empirical study of detectionbased video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Onboard contextual classification of 3D point clouds with learned high-order markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Pfinder: Real-time tracking of the human body</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="780" to="785" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Object discovery in videos as foreground motion clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Evaluation of super-voxel methods for early video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">YouTube-VOS: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Anchor diffusion for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S K</forename><surname>Yeong Jun Koh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Learning submodular losses with the lov√°sz hinge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Dmm-net: Differentiable mask-matching network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Spatial semantic embedding network: Fast 3d instance segmentation with deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03169</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking for the 2019 Unsupervised DAVIS Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Zulfikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2019 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
