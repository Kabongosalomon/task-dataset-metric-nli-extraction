<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Transformer for Unaligned Multimodal Language Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung</forename><forename type="middle">Hubert</forename><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>and Bosch Center for AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Transformer for Unaligned Multimodal Language Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>https://github.com/yaohungt/Multimodal-Transformer</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human language possesses not only spoken words but also nonverbal behaviors from vision (facial attributes) and acoustic (tone of voice) modalities <ref type="bibr">(Gibson et al., 1994)</ref>. This rich information provides us the benefit of understanding human behaviors and intents . Nevertheless, the heterogeneities across modalities often increase the difficulty of analyzing human language. For example, the receptors for audio and vision streams may vary with variable receiving frequency, and hence we may not obtain optimal mapping between them. A frowning face may relate to a pessimistically word spoken in the past. That is to say, multimodal language sequences  [Bottom] Illustration of crossmodal attention weights between text ("spectacle") and vision/audio. often exhibit "unaligned" nature and require inferring long term dependencies across modalities, which raises a question on performing efficient multimodal fusion.</p><p>To address the above issues, in this paper we propose the Multimodal Transformer (MulT), an end-to-end model that extends the standard Transformer network <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> to learn representations directly from unaligned multimodal streams. At the heart of our model is the crossmodal attention module, which attends to the crossmodal interactions at the scale of the entire utterances. This module latently adapts streams from one modality to another (e.g., vision → language) by repeated reinforcing one modality's features with those from the other modalities, regardless of the need for alignment. In comparison, one common way of tackling unaligned multimodal sequence is by forced word-aligning before training <ref type="bibr" target="#b22">(Poria et al., 2017;</ref><ref type="bibr">Zadeh et al., 2018a,b;</ref><ref type="bibr">Tsai et al., 2019;</ref><ref type="bibr" target="#b21">Pham et al., 2019;</ref><ref type="bibr" target="#b12">Gu et al., 2018)</ref>: manually preprocess the visual and acoustic features by aligning them to the resolution of words. These approaches would then model the multimodal interactions on the (already) aligned time steps and thus do not directly consider long-range crossmodal contingencies of the original features. We note that such wordalignment not only requires feature engineering that involves domain knowledge; but in practice, it may also not always be feasible, as it entails extra meta-information about the datasets (e.g., the exact time ranges of words or speech utterances). We illustrate the difference between the word-alignment and the crossmodal attention inferred by our model in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>For evaluation, we perform a comprehensive set of experiments on three human multimodal language benchmarks: CMU-MOSI <ref type="bibr" target="#b35">(Zadeh et al., 2016)</ref>, CMU-MOSEI <ref type="bibr" target="#b36">(Zadeh et al., 2018b)</ref>, and IEMOCAP <ref type="bibr" target="#b2">(Busso et al., 2008)</ref>. Our experiments show that MulT achieves the state-of-theart (SOTA) results in not only the commonly evaluated word-aligned setting but also the more challenging unaligned scenario, outperforming prior approaches by a margin of 5%-15% on most of the metrics. In addition, empirical qualitative analysis further suggests that the crossmodal attention used by MulT is capable of capturing correlated signals across asynchronous modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Human Multimodal Language Analysis. Prior work for analyzing human multimodal language lies in the domain of inferring representations from multimodal sequences spanning language, vision, and acoustic modalities. Unlike learning multimodal representations from static domains such as image and textual attributes <ref type="bibr" target="#b18">(Ngiam et al., 2011;</ref><ref type="bibr" target="#b25">Srivastava and Salakhutdinov, 2012)</ref>, human language contains time-series and thus requires fusing time-varying signals <ref type="bibr" target="#b15">(Liang et al., 2018;</ref><ref type="bibr">Tsai et al., 2019)</ref>. Earlier work used early fusion approach to concatenate input features from different modalities <ref type="bibr" target="#b14">(Lazaridou et al., 2015;</ref><ref type="bibr" target="#b18">Ngiam et al., 2011)</ref> and showed improved performance as compared to learning from a single modality. More recently, more advanced models were proposed to learn representations of human multimodal language. For example, <ref type="bibr" target="#b12">Gu et al. (2018)</ref> used hierarchical attention strategies to learn multimodal representations, <ref type="bibr" target="#b31">Wang et al. (2019)</ref> adjusted the word representations using accompanying non-verbal behaviors, <ref type="bibr" target="#b21">Pham et al. (2019)</ref> learned robust multimodal representations using a cyclic translation objective, and Dumpala et al. (2019) explored cross-modal autoencoders for audio-visual alignment. These previous approaches relied on the assumption that multimodal language sequences are already aligned in the resolution of words and considered only short-term multimodal interactions. In contrast, our proposed method requires no alignment assumption and defines crossmodal interactions at the scale of the entire sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>Network. Transformer network <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> was first introduced for neural machine translation (NMT) tasks, where the encoder and decoder side each leverages a self-attention <ref type="bibr" target="#b19">(Parikh et al., 2016;</ref><ref type="bibr" target="#b16">Lin et al., 2017;</ref><ref type="bibr" target="#b29">Vaswani et al., 2017)</ref> transformer. After each layer of the self-attention, the encoder and decoder are connected by an additional decoder sublayer where the decoder attends to each element of the source text for each element of the target text. We refer the reader to <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> for a more detailed explanation of the model. In addition to NMT, transformer networks have also been successfully applied to other tasks, including language modeling <ref type="bibr" target="#b4">(Dai et al., 2018;</ref><ref type="bibr" target="#b1">Baevski and Auli, 2019)</ref>, semantic role labeling <ref type="bibr" target="#b26">(Strubell et al., 2018)</ref>, word sense disambiguation <ref type="bibr" target="#b27">(Tang et al., 2018)</ref>, learning sentence representations <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, and video activity recognition <ref type="bibr" target="#b30">(Wang et al., 2018)</ref>. This paper absorbs a strong inspiration from the NMT transformer to extend to a multimodal setting. Whereas the NMT transformer focuses on unidirectional translation from source to target texts, human multimodal language time-series are neither as well-represented nor discrete as word embeddings, with sequences of each modality having vastly different frequencies. Therefore, we propose not to explicitly translate from one modality to the others (which could be extremely challenging), but to latently adapt elements across modalities via the attention. Our model (MulT) therefore has no encoder-decoder structure, but it is built up from multiple stacks of pairwise and bidirectional crossmodal attention blocks that directly attend to low-level features (while removing the self-attention). Empirically, we show that our proposed approach improves beyond standard transformer on various human multimodal language tasks.</p><formula xml:id="formula_0">Crossmodal Transformer (V ! L) (A ! L) (A ! V ) (V ! A) Transformer Transformer Transformer Predictionŷ Concatenation Conv1D Conv1D Conv1D Positional Embedding XL 2 R TL⇥dL XV 2 R TV ⇥dV XA 2 R TA⇥dA Crossmodal Attention Self Attention Block i = 1,...,D Crossmodal Transformer (L ! V ) Crossmodal Transformer (L ! A) Multimodal Transformer ZL 2 R TL⇥2d ZV 2 R TV ⇥2d ZA 2 R TA⇥2d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we describe our proposed Multimodal Transformer (MulT) ( <ref type="figure" target="#fig_1">Figure 2</ref>) for modeling unaligned multimodal language sequences. At the high level, MulT merges multimodal timeseries via a feed-forward fusion process from multiple directional pairwise crossmodal transformers. Specifically, each crossmodal transformer (introduced in Section 3.2) serves to repeatedly reinforce a target modality with the low-level features from another source modality by learning the attention across the two modalities' features. A MulT architecture hence models all pairs of modalities with such crossmodal transformers, followed by sequence models (e.g., self-attention transformer) that predicts using the fused features. The core of our proposed model is crossmodal attention module, which we first introduce in Section 3.1. Then, in Section 3.2 and 3.3, we present in details the various ingredients of the MulT architecture (see <ref type="figure" target="#fig_1">Figure 2</ref>) and discuss the difference between crossmodal attention and classical multimodal alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Crossmodal Attention</head><p>We consider two modalities α and β, with two (potentially non-aligned) sequences from each of them denoted X α ∈ R Tα×dα and X β ∈ R T β ×d β , respectively. For the rest of the paper, T (·) and d (·) are used to represent sequence length and feature dimension, respectively. Inspired by the decoder transformer in NMT <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> that translates one language to another, we hypothesize a good way to fuse crossmodal information is providing a latent adaptation across modalities; i.e., β to α. Note that the modalities consider in our paper may span very different domains such as facial attributes and spoken words.</p><p>We define the Querys as Q α = X α W Qα , Keys</p><formula xml:id="formula_1">as K β = X β W K β , and Values as V β = X β W V β , where W Qα ∈ R dα×d k , W K β ∈ R d β ×d k and W V β ∈ R d β ×dv are weights. The latent adapta- tion from β to α is presented as the crossmodal attention Y α := CM β→α (X α , X B ) ∈ R Tα×dv : Y α = CM β→α (X α , X β ) = softmax Q α K β √ d k V β = softmax X α W Qα W K β X β √ d k X β W V β .</formula><p>(1)</p><p>Note that Y α has the same length as Q α (i.e., T α ), but is meanwhile represented in the feature space of V β . Specifically, the scaled (by √ d k ) softmax in Equation <ref type="formula">(1)</ref> computes a score matrix softmax (·) ∈ R Tα×T β , whose (i, j)-th entry measures the attention given by the i-th time step of modality α to the j-th time step of modality β. Hence, the i-th time step of Y α is a weighted summary of V β , with the weight determined by i-th row in softmax(·). We call Equation (1) a singlehead crossmodal attention, which is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>(a).</p><p>Following prior works on transformers <ref type="bibr" target="#b29">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b6">Devlin et al., 2018;</ref><ref type="bibr" target="#b4">Dai et al., 2018)</ref>, we add a residual connection to the crossmodal attention computation. Then, another positionwise feed-forward sublayer is injected to complete a crossmodal attention block (see <ref type="figure" target="#fig_3">Figure 3</ref>(b)). Each crossmodal attention block adapts directly from the low-level feature sequence (i.e., Z <ref type="figure" target="#fig_3">Figure 3</ref>(b)) and does not rely on self-attention, which makes it different from the NMT encoderdecoder architecture <ref type="bibr" target="#b29">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b23">Shaw et al., 2018</ref>) (i.e., taking intermediate-level features). We argue that performing adaptation from low-level feature benefits our model to preserve the low-level information for each modality. We leave the empirical study for adapting from intermediate-level features (i.e., Z  </p><formula xml:id="formula_2">[0] β in</formula><formula xml:id="formula_3">[i−1] β ) in Ablation Study in Section 4.3. softmax ✓ Q↵K &gt; p dk ◆ V 2 R T↵⇥dv CM !↵(X↵, X ) softmax ✓ Q↵K &gt; p dk ◆ Q↵ 2 R T↵⇥dk K 2 R T ⇥dk V 2 R T ⇥dv X↵ 2 R T↵⇥d↵ X 2 R T ⇥d↵ WQ ↵ WK WV Modality ↵ Modality (a) Crossmodal attention CM β→α (Xα, X β ) between sequences Xα, X β from distinct modalities. Multi-head Positionwise Feed-forward ⇥D Layers Layer 0 Z [0] ↵ Z [0] Crossmodal Transformer ( ! ↵) Block i ( ! ↵) CM !↵(Z [i 1] !↵ , Z [0] ) Z [i 1] !↵ Z [i] !↵ Z [D] !↵ Q↵ K V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Architecture</head><p>Three major modalities are typically involved in multimodal language sequences: language (L), video (V ), and audio (A) modalities. We denote with X {L,V,A} ∈ R T {L,V,A} ×d {L,V,A} the input feature sequences (and the dimensions thereof) from these 3 modalities. With these notations, in this subsection, we describe in greater details the components of Multimodal Transformer and how crossmodal attention modules are applied.</p><p>Temporal Convolutions. To ensure that each element of the input sequences has sufficient awareness of its neighborhood elements, we pass the input sequences through a 1D temporal convolutional layer:</p><formula xml:id="formula_4">X {L,V,A} = Conv1D(X {L,V,A} , k {L,V,A} ) ∈ R T {L,V,A} ×d</formula><p>(2) where k {L,V,A} are the sizes of the convolutional kernels for modalities {L, V, A}, and d is a common dimension. The convolved sequences are expected to contain the local structure of the sequence, which is important since the sequences are collected at different sampling rates. Moreover, since the temporal convolutions project the features of different modalities to the same dimension d, the dot-products are admittable in the crossmodal attention module.</p><p>Positional Embedding. To enable the sequences to carry temporal information, following <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>, we augment positional embedding (PE) toX {L,V,A} : {L,V,A} are the resulting low-level positionaware features for different modalities. We leave more details of the positional embedding to Appendix A.</p><formula xml:id="formula_5">Z [0] {L,V,A} =X {L,V,A} + PE(T {L,V,A} , d) (3) where PE(T {L,V,A} , d) ∈ R T {L,V,</formula><p>Crossmodal Transformers. Based on the crossmodal attention blocks, we design the crossmodal transformer that enables one modality for receiving information from another modality. In the following, we use the example for passing vision (V ) information to language (L), which is denoted by "V → L". We fix all the dimensions (d {α,β,k,v} ) for each crossmodal attention block as d.</p><p>Each crossmodal transformer consists of D layers of crossmodal attention blocks (see <ref type="figure" target="#fig_3">Figure  3</ref>(b)). Formally, a crossmodal transformer computes feed-forwardly for i = 1, . . . , D layers:</p><formula xml:id="formula_6">Z [0] V →L = Z [0] L Z [i] V →L = CM [i],mul V →L (LN(Z [i−1] V →L ), LN(Z [0] V )) + LN(Z [i−1] V →L ) Z [i] V →L = f θ [i] V →L (LN(Ẑ [i] V →L )) + LN(Ẑ [i] V →L ) (4)</formula><p>where f θ is a positionwise feed-forward sublayer parametrized by θ, and CM</p><p>[i],mul V →L means a multihead (see <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> for more details) version of CM V →L at layer i (note: d should be divisible by the number of heads). LN means layer normalization <ref type="bibr">(Ba et al., 2016)</ref>.</p><p>In this process, each modality keeps updating its sequence via low-level external information from the multi-head crossmodal attention module. At every level of the crossmodal attention block, the low-level signals from source modality are transformed to a different set of Key/Value pairs to interact with the target modality. Empirically, we find that the crossmodal transformer learns to correlate meaningful elements across modalities (see Section 4 for details). The eventual MulT is based on modeling every pair of crossmodal interactions. Therefore, with 3 modalities (i.e., L, V, A) in consideration, we have 6 crossmodal transformers in total (see <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention Transformers and Prediction.</head><p>As a final step, we concatenate the outputs from the crossmodal transformers that share the same target modality to yield</p><formula xml:id="formula_7">Z {L,V,A} ∈ R T {L,V,A} ×2d . For example, Z L = [Z [D] V →L ; Z [D]</formula><p>A→L ]. Each of them is then passed through a sequence model to collect temporal information to make predictions. We choose the self-attention transformer <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>. Eventually, the last elements of the sequences models are extracted to pass through fully-connected layers to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion about Attention &amp; Alignment</head><p>When modeling unaligned multimodal language sequences, MulT relies on crossmodal attention blocks to merge signals across modalities. While the multimodal sequences were (manually) aligned to the same length in prior works before training <ref type="bibr" target="#b36">(Zadeh et al., 2018b;</ref><ref type="bibr" target="#b15">Liang et al., 2018;</ref><ref type="bibr">Tsai et al., 2019;</ref><ref type="bibr" target="#b21">Pham et al., 2019;</ref><ref type="bibr" target="#b31">Wang et al., 2019)</ref>, we note that MulT looks at the nonalignment issue through a completely different lens. Specifically, for MulT, the correlations between elements of multiple modalities are purely based on attention. In other words, MulT does not handle modality non-alignment by (simply) aligning them; instead, the crossmodal attention encourages the model to directly attend to elements in other modalities where strong signals or relevant information is present. As a result, MulT can capture long-range crossmodal contingencies in a way that conventional alignment could not easily reveal. Classical crossmodal alignment, on the other hand, can be expressed as a special (step di-   <ref type="bibr" target="#b36">(Zadeh et al., 2018b)</ref> 45.0 76.9 77.0 0.71 0.54 RAVEN <ref type="bibr" target="#b31">(Wang et al., 2019)</ref> 50.0 79.1 79.5 0.614 0.662 MCTN <ref type="bibr" target="#b21">(Pham et al., 2019)</ref> 49 agonal) crossmodal attention matrix (i.e., monotonic attention <ref type="bibr" target="#b32">(Yu et al., 2016)</ref>). We illustrate their differences in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we empirically evaluate the Multimodal Transformer (MulT) on three datasets that are frequently used to benchmark human multimodal affection recognition in prior works <ref type="bibr" target="#b21">(Pham et al., 2019;</ref><ref type="bibr">Tsai et al., 2019;</ref><ref type="bibr" target="#b15">Liang et al., 2018)</ref>. Our goal is to compare MulT with prior competitive approaches on both word-aligned (by word, which almost all prior works employ) and unaligned (which is more challenging, and which MulT is generically designed for) multimodal language sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Each task consists of a word-aligned (processed in the same way as in prior works) and an unaligned version. For both versions, the multimodal features are extracted from the textual (GloVe word embeddings <ref type="bibr" target="#b20">(Pennington et al., 2014)</ref>), visual (Facet (iMotions, 2017)), and acoustic (CO-VAREP <ref type="bibr" target="#b5">(Degottex et al., 2014)</ref>) data modalities. A more detailed introduction to the features is included in Appendix D.</p><p>For the word-aligned version, following <ref type="bibr" target="#b34">(Zadeh et al., 2018a;</ref><ref type="bibr">Tsai et al., 2019;</ref><ref type="bibr" target="#b21">Pham et al., 2019)</ref>, we first use P2FA <ref type="bibr" target="#b33">(Yuan and Liberman, 2008)</ref> to obtain the aligned timesteps (segmented w.r.t. words) for audio and vision streams, and we then perform averaging on the audio and vision features within these time ranges. All sequences in the word-aligned case have length 50. The process remains the same across all the datasets. On the other hand, for the unaligned version, we keep the original audio and visual features as extracted, without any word-segmented alignment or manual subsampling. As a result, the lengths of each modality vary significantly, where audio and vision sequences may contain up to &gt; 1, 000 time steps. We elaborate on the three tasks below. <ref type="bibr" target="#b35">(Zadeh et al., 2016</ref>) is a human multimodal sentiment analysis dataset consisting of 2,199 short monologue video clips (each lasting the duration of a sentence). Acoustic and visual features of CMU-MOSI are extracted at a sampling rate of 12.5 and 15 Hz, respectively (while textual data are segmented per word and expressed as discrete word embeddings). Meanwhile, CMU-MOSEI <ref type="bibr" target="#b36">(Zadeh et al., 2018b</ref>) is a sentiment and emotion analysis dataset made up of 23,454 movie review video clips taken from YouTube (about 10× the size of CMU-MOSI). The unaligned CMU-MOSEI sequences are extracted at a sampling rate of 20 Hz for acoustic and 15 Hz for vision signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CMU-MOSI &amp; MOSEI. CMU-MOSI</head><p>For both CMU-MOSI and CMU-MOSEI, each sample is labeled by human annotators with a sentiment score from -3 (strongly negative) to 3 (strongly positive). We evaluate the model performances using various metrics, in agreement with those employed in prior works: 7-class accuracy (i.e., Acc 7 : sentiment score classification in Z ∩ [−3, 3]), binary accuracy (i.e., Acc 2 : positive/negative sentiments), F1 score, mean absolute error (MAE) of the score, and the correlation of the model's prediction with human. Both tasks are frequently used to benchmark models' ability to fuse multimodal (sentiment) information <ref type="bibr" target="#b22">(Poria et al., 2017;</ref><ref type="bibr" target="#b34">Zadeh et al., 2018a;</ref><ref type="bibr" target="#b15">Liang et al., 2018;</ref><ref type="bibr">Tsai et al., 2019;</ref><ref type="bibr" target="#b21">Pham et al., 2019;</ref><ref type="bibr" target="#b31">Wang et al., 2019)</ref>.</p><p>IEMOCAP. IEMOCAP <ref type="bibr" target="#b2">(Busso et al., 2008)</ref> consists of 10K videos for human emotion analysis. As suggested by <ref type="bibr" target="#b31">Wang et al. (2019)</ref>, 4 emotions (happy, sad, angry and neutral) were selected for emotion recognition. Unlike CMU-MOSI and CMU-MOSEI, this is a multilabel task (e.g., a person can be sad and angry simultaneously). Its multimodal streams consider fixed sampling rate on audio (12.5 Hz) and vision (15 Hz) signals. We follow <ref type="bibr" target="#b22">(Poria et al., 2017;</ref><ref type="bibr" target="#b31">Wang et al., 2019;</ref><ref type="bibr">Tsai et al., 2019)</ref> to report the binary classification accuracy and the F1 score of the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We choose Early Fusion LSTM (EF-LSTM) and Late Fusion LSTM (LF-LSTM) as baseline models, as well as Recurrent Attended Variation Embedding Network (RAVEN) <ref type="bibr" target="#b31">(Wang et al., 2019)</ref> and Multimodal Cyclic Translation Network (MCTN) <ref type="bibr" target="#b21">(Pham et al., 2019)</ref>, that achieved SOTA results on various word-aligned human multimodal language tasks. To compare the models comprehensively, we adapt the connectionist temporal classification (CTC) <ref type="bibr" target="#b11">(Graves et al., 2006)</ref> method to the prior approaches (e.g., EF-LSTM, MCTN, RAVEN) that cannot be applied directly to the unaligned setting. Specifically, these models train to optimize the CTC alignment objective and the human multimodal objective simultaneously. We leave more detailed treatment of the CTC module to Appendix B. For fair comparisons, we control the number of parameters of all models to be approximately the same. The hyperparameters are reported in Appendix C. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Analysis</head><p>Word-Aligned Experiments. We first evaluate MulT on the word-aligned sequencesthe "home turf" of prior approaches modeling human multimodal language <ref type="bibr" target="#b24">(Sheikh et al., 2018;</ref><ref type="bibr">Tsai et al., 2019;</ref><ref type="bibr" target="#b21">Pham et al., 2019;</ref><ref type="bibr" target="#b31">Wang et al., 2019)</ref>. The upper part of the  with CTC) by 10%-15% on most attributes. Empirically, we find that MulT converges faster to better results at training when compared to other competitive approaches (see <ref type="figure" target="#fig_7">Figure 5</ref>). In addition, while we note that in general there is a performance drop on all models when we shift from the word-aligned to unaligned multimodal timeseries, the impact MulT takes is much smaller than the other approaches. We hypothesize such performance drop occurs because the asynchronous (and much longer) data streams introduce more difficulty in recognizing important features and computing the appropriate attention.</p><p>Ablation Study. To further study the influence of the individual components in MulT, we perform comprehensive ablation analysis using the unaligned version of CMU-MOSEI. The results are shown in <ref type="table" target="#tab_5">Table 4</ref>. First, we consider the performance for only using unimodal transformers (i.e., language, audio or vision only). We find that the language transformer outperforms the other two by a large margin. For example, for the Acc h 2 metric, the model improves from 65.6 to 77.4 when comparing audio only to language only unimodal transformer. This fact aligns with the observations in prior work <ref type="bibr" target="#b21">(Pham et al., 2019)</ref>, where the authors found that a good language network could already achieve good performance at inference time.</p><p>Second, we consider 1) a late-fusion transformer that feature-wise concatenates the last elements of three self-attention transformers; and 2) an early-fusion self-attention transformer that takes in a temporal concatenation of three asynchronous sequences [X L ,X V ,X A ] ∈ MOSEI. We found that the crossmodal attention has learned to correlate certain meaningful words (e.g., "movie", "disappointing") with segments of stronger visual signals (typically stronger facial motions or expression change), despite the lack of alignment between original L/V sequences. Note that due to temporal convolution, each textual/visual feature contains the representation of nearby elements.</p><formula xml:id="formula_8">R (T L +T V +T A )×dq (see Section 3.2). Empirically,</formula><p>we find that both EF-and LF-Transformer (which fuse multimodal signals) outperform unimodal transformers.</p><p>Finally, we study the importance of individual crossmodal transformers according to the target modalities (i.e., using</p><formula xml:id="formula_9">[V, A → L], [L, A → V ], or [L, V → A] network)</formula><p>. As shown in <ref type="table" target="#tab_5">Table 4</ref>, we find crossmodal attention modules consistently improve over the late-and earlyfusion transformer models in most metrics on unaligned CMU-MOSEI. In particular, among the three crossmodal transformers, the one where language(L) is the target modality works best. We also additionally study the effect of adapting intermediate-level instead of the low-level features from source modality in crossmodal attention blocks (similar to the NMT encoder-decoder architecture but without self-attention; see Section 3.1). While MulT leveraging intermediatelevel features still outperform models in other ablative settings, we empirically find adapting from low-level features works best. The ablations suggest that crossmodal attention concretely benefits MulT with better representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>To understand how crossmodal attention works while modeling unaligned multimodal data, we empirically inspect what kind of signals MulT picks up by visualizing the attention activations. <ref type="figure" target="#fig_8">Figure 6</ref> shows an example of a section of the crossmodal attention matrix on layer 3 of the V → L network of MulT (the original matrix has dimension T L × T V ; the figure shows the attention corresponding to approximately a 6-sec short window of that matrix). We find that crossmodal at-tention has learned to attend to meaningful signals across the two modalities. For example, stronger attention is given to the intersection of words that tend to suggest emotions (e.g., "movie", "disappointing") and drastic facial expression changes in the video (start and end of the above vision sequence). This observation advocates one of the aforementioned advantage of MulT over conventional alignment (see Section 3.3): crossmodal attention enables MulT to directly capture potentially long-range signals, including those offdiagonals on the attention matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In the paper, we propose Multimodal Transformer (MulT) for analyzing human multimodal language. At the heart of MulT is the crossmodal attention mechanism, which provides a latent crossmodal adaptation that fuses multimodal information by directly attending to low-level features in other modalities. Whereas prior approaches focused primarily on the aligned multimodal streams, MulT serves as a strong baseline capable of capturing long-range contingencies, regardless of the alignment assumption. Empirically, we show that MulT exhibits the best performance when compared to prior methods.</p><p>We believe the results of MulT on unaligned human multimodal language sequences suggest many exciting possibilities for its future applications (e.g., Visual Question Answering tasks, where the input signals is a mixture of static and time-evolving signals). We hope the emergence of MulT could encourage further explorations on tasks where alignment used to be considered necessary, but where crossmodal attention might be an equally (if not more) competitive alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Positional Embedding</head><p>A purely attention-based transformer network is order-invariant. In other words, permuting the order of an input sequence does not change transformer's behavior or alter its output. One solution to address this weakness is by embedding the positional information into the hidden units <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>.</p><p>Following <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>, we encode the positional information of a sequence of length T via the sin and cos functions with frequencies dictated by the feature index. In particular, we define the positional embedding (PE) of a sequence X ∈ R T ×d (where T is length) as a matrix where:</p><formula xml:id="formula_10">PE[i, 2j] = sin i 10000 2j d PE[i, 2j + 1] = cos i 10000 2j d</formula><p>for i = 1, . . . , T and j = 0, d 2 . Therefore, each feature dimension (i.e., column) of PE are positional values that exhibit a sinusoidal pattern. Once computed, the positional embedding is added directly to the sequence so that X + PE encodes the elements' position information at every time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Connectionist Temporal Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connectionist</head><p>Temporal Classification (CTC) <ref type="bibr" target="#b11">(Graves et al., 2006)</ref> was first proposed for unsupervised Speech to Text alignment. Particularly, CTC is often combined with the output of recurrent neural network, which enables the model to train end-to-end and simultaneously infer speech-text alignment without supervision. For the ease of explanation, suppose the CTC module now are aiming at aligning an audio signal sequence [a 1 , a 2 , a 3 , a 4 , a 5 , a 6 ] with length 6 to a textual sequence "I am really really happy" with length 5. In this example, we refer to audio as the source and texts as target signal, noting that the sequence lengths may be different between the source to target; we also see that the output sequence may have repetitive element (i.e., "really"). The CTC <ref type="bibr" target="#b11">(Graves et al., 2006)</ref> module we use comprises two components: alignment predictor and the CTC loss.</p><p>First, the alignment predictor is often chosen as a recurrent networks such as LSTM, which performs on the source sequence then outputs the possibility of being the unique words in the target sequence as well as a empty word (i.e., x). In our example, for each individual audio signal, the alignment predictor provides a vector of length 5 regarding the probability being aligned to <ref type="bibr">[x, 'I', 'am', 'really', 'happy']</ref>.</p><p>Next, the CTC loss considers the negative loglikelihood loss from only the proper alignment for the alignment predictor outputs. The proper alignment, in our example, can be results such as i) <ref type="bibr">[x, 'I', 'am', 'really', 'really', 'happy'];</ref> ii) <ref type="bibr">['I', 'am', x, 'really', 'really', 'happy'];</ref> iii) <ref type="bibr">['I', 'am', 'really', 'really', 'really', '</ref>happy']; iv) <ref type="bibr">['I', 'I', 'am', 'really', 'really', 'happy']</ref> In the meantime, some examples of the suboptimal/failure cases would be i) <ref type="bibr">[x, x, 'am', 'really', 'really', 'happy'];</ref> ii) ['I', 'am', 'I', 'really', 'really', 'happy'];</p><p>iii) <ref type="bibr">['I', 'am', x, 'really', x, 'happy']</ref> When the CTC loss is minimized, it implies the source signals are properly aligned to target signals.</p><p>To sum up, in the experiments that adopting the CTC module, we train the alignment predictor while minimizing the CTC loss. Then, excluding the probability of blank words, we multiply the probability outputs from the alignment predictor to source signals. The source signal is hence resulting in a pseudo-aligned target singal. In our example, the audio signal is then transforming to a audio signal [a 1 , a 2 , a 3 , a 4 , a 5 ] with sequence length 5, which is pseudo-aligned to <ref type="bibr">['I', 'am', 'really', 'really', 'happy']</ref>. <ref type="table" target="#tab_6">Table 5</ref> shows the settings of the various MulTs that we train on human multimodal language tasks. As previously mentioned, the models are contained at roughly the same sizes as in prior works for the purpose of fair comparison. For hyperparameters such as the dropout rate and number of heads in crossmodal attention module, we perform a basic grid search. We decay the learning rate by a factor of 10 when the validation performance plateaus. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Features</head><p>The features for multimodal datasets are extracted as follows:</p><p>-Language. We convert video transcripts into pre-trained Glove word embeddings (glove.840B.300d) <ref type="bibr" target="#b20">(Pennington et al., 2014)</ref>. The embedding is a 300 dimensional vector.</p><p>-Vision. We use Facet (iMotions, 2017) to indicate 35 facial action units, which records facial muscle movement <ref type="bibr" target="#b9">(Ekman et al., 1980;</ref><ref type="bibr" target="#b8">Ekman, 1992)</ref> for representing per-frame basic and advanced emotions.</p><p>-Audio. We use COVAREP <ref type="bibr" target="#b5">(Degottex et al., 2014)</ref> for extracting low level acoustic features. The feature includes 12 Mel-frequency cepstral coefficients (MFCCs), pitch tracking and voiced/unvoiced segmenting features, glottal source parameters, peak slope parameters and maxima dispersion quotients. Dimension of the feature is 74.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example video clip from movie reviews. [Top]: Illustration of word-level alignment where video and audio features are averaged across the time interval of each spoken word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture for MulT on modalities (L, V, A). The crossmodal transformers, which suggests latent crossmodal adaptations, are the core components of MulT for multimodal fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A crossmodal transformer is a deep stacking of several crossmodal attention blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Architectural elements of a crossmodal transformer between two time-series from modality α and β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>A} ×d computes the (fixed) embeddings for each position index, and Z [0]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>in the form of a crossmodal attention matrix A learned crossmodal attention in MulT =little attention weight Figure 4: An example of visualizing alignment using attention matrix from modality β to α. Multimodal alignment is a special (monotonic) case for crossmodal attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Validation set convergence of MulT when compared to other baselines on the unaligned CMU-MOSEI task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of sample crossmodal attention weights from layer 3 of [V → L] crossmodal transformer on CMU-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results for multimodal sentiment analysis on CMU-MOSI with aligned and non-aligned multimodal sequences. h means higher is better and means lower is better. EF stands for early fusion, and LF stands for late fusion.</figDesc><table><row><cell>Metric</cell><cell>Acc h 7 Acc h 2</cell><cell>F1 h MAE Corr h</cell></row><row><cell cols="3">(Word Aligned) CMU-MOSI Sentiment</cell></row><row><cell>EF-LSTM</cell><cell cols="2">33.7 75.3 75.2 1.023 0.608</cell></row><row><cell>LF-LSTM</cell><cell cols="2">35.3 76.8 76.7 1.015 0.625</cell></row><row><cell>RMFN (Liang et al., 2018)</cell><cell cols="2">38.3 78.4 78.0 0.922 0.681</cell></row><row><cell>MFM (Tsai et al., 2019)</cell><cell cols="2">36.2 78.1 78.1 0.951 0.662</cell></row><row><cell>RAVEN (Wang et al., 2019)</cell><cell cols="2">33.2 78.0 76.6 0.915 0.691</cell></row><row><cell>MCTN (Pham et al., 2019)</cell><cell cols="2">35.6 79.3 79.1 0.909 0.676</cell></row><row><cell>MulT (ours)</cell><cell cols="2">40.0 83.0 82.8 0.871 0.698</cell></row><row><cell cols="3">(Unaligned) CMU-MOSI Sentiment</cell></row><row><cell cols="3">CTC (Graves et al., 2006) + EF-LSTM 31.0 73.6 74.5 1.078 0.542</cell></row><row><cell>LF-LSTM</cell><cell cols="2">33.7 77.6 77.8 0.988 0.624</cell></row><row><cell>CTC + MCTN (Pham et al., 2019)</cell><cell cols="2">32.7 75.9 76.4 0.991 0.613</cell></row><row><cell>CTC + RAVEN (Wang et al., 2019)</cell><cell cols="2">31.7 72.7 73.1 1.076 0.544</cell></row><row><cell>MulT (ours)</cell><cell cols="2">39.1 81.1 81.0 0.889 0.686</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for multimodal sentiment analysis on (relatively large scale) CMU-MOSEI with aligned and nonaligned multimodal sequences.</figDesc><table><row><cell>Metric</cell><cell>Acc h 7 Acc h 2</cell><cell>F1 h MAE Corr h</cell></row><row><cell cols="3">(Word Aligned) CMU-MOSEI Sentiment</cell></row><row><cell>EF-LSTM</cell><cell cols="2">47.4 78.2 77.9 0.642 0.616</cell></row><row><cell>LF-LSTM</cell><cell cols="2">48.8 80.6 80.6 0.619 0.659</cell></row><row><cell>Graph-MFN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results for multimodal emotions analysis on IEMOCAP with aligned and non-aligned multimodal sequences.</figDesc><table><row><cell>Task</cell><cell cols="2">Happy</cell><cell>Sad</cell><cell></cell><cell cols="2">Angry</cell><cell cols="2">Neutral</cell></row><row><cell>Metric</cell><cell>Acc h</cell><cell>F1 h</cell><cell>Acc h</cell><cell>F1 h</cell><cell>Acc h</cell><cell>F1 h</cell><cell>Acc h</cell><cell>F1 h</cell></row><row><cell cols="5">(Word Aligned) IEMOCAP Emotions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EF-LSTM</cell><cell>86.0</cell><cell>84.2</cell><cell>80.2</cell><cell>80.5</cell><cell>85.2</cell><cell>84.5</cell><cell>67.8</cell><cell>67.1</cell></row><row><cell>LF-LSTM</cell><cell>85.1</cell><cell>86.3</cell><cell>78.9</cell><cell>81.7</cell><cell>84.7</cell><cell>83.0</cell><cell>67.1</cell><cell>67.6</cell></row><row><cell>RMFN (Liang et al., 2018)</cell><cell>87.5</cell><cell>85.8</cell><cell>83.8</cell><cell>82.9</cell><cell>85.1</cell><cell>84.6</cell><cell>69.5</cell><cell>69.1</cell></row><row><cell>MFM (Tsai et al., 2019)</cell><cell>90.2</cell><cell>85.8</cell><cell>88.4</cell><cell>86.1</cell><cell>87.5</cell><cell>86.7</cell><cell>72.1</cell><cell>68.1</cell></row><row><cell>RAVEN (Wang et al., 2019)</cell><cell>87.3</cell><cell>85.8</cell><cell>83.4</cell><cell>83.1</cell><cell>87.3</cell><cell>86.7</cell><cell>69.7</cell><cell>69.3</cell></row><row><cell>MCTN (Pham et al., 2019)</cell><cell>84.9</cell><cell>83.1</cell><cell>80.5</cell><cell>79.6</cell><cell>79.7</cell><cell>80.4</cell><cell>62.3</cell><cell>57.0</cell></row><row><cell>MulT (ours)</cell><cell>90.7</cell><cell>88.6</cell><cell>86.7</cell><cell>86.0</cell><cell>87.4</cell><cell>87.0</cell><cell>72.4</cell><cell>70.7</cell></row><row><cell></cell><cell cols="4">(Unaligned) IEMOCAP Emotions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTC (Graves et al., 2006) + EF-LSTM 76.2</cell><cell>75.7</cell><cell>70.2</cell><cell>70.5</cell><cell>72.7</cell><cell>67.1</cell><cell>58.1</cell><cell>57.4</cell></row><row><cell>LF-LSTM</cell><cell>72.5</cell><cell>71.8</cell><cell>72.9</cell><cell>70.4</cell><cell>68.6</cell><cell>67.9</cell><cell>59.6</cell><cell>56.2</cell></row><row><cell>CTC + RAVEN (Wang et al., 2019)</cell><cell>77.0</cell><cell>76.8</cell><cell>67.6</cell><cell>65.6</cell><cell>65.0</cell><cell>64.1</cell><cell>62.0</cell><cell>59.5</cell></row><row><cell>CTC + MCTN (Pham et al., 2019)</cell><cell>80.5</cell><cell>77.5</cell><cell>72.0</cell><cell>71.7</cell><cell>64.9</cell><cell>65.6</cell><cell>49.4</cell><cell>49.3</cell></row><row><cell>MulT (ours)</cell><cell>84.8</cell><cell>81.9</cell><cell>77.7</cell><cell>74.1</cell><cell>73.9</cell><cell>70.2</cell><cell>62.5</cell><cell>59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>, 2, and 3 show the results</cell></row><row><cell>of MulT and baseline approaches on the word-</cell></row><row><cell>aligned task. With similar model sizes (around</cell></row><row><cell>200K parameters), MulT outperforms the other</cell></row><row><cell>competitive approaches on different metrics on all</cell></row><row><cell>tasks, with the exception of the "sad" class results</cell></row><row><cell>on IEMOCAP.</cell></row><row><cell>Unaligned Experiments. Next, we evaluate</cell></row><row><cell>MulT on the same set of datasets in the unaligned</cell></row><row><cell>setting. Note that MulT can be directly applied to</cell></row><row><cell>unaligned multimodal stream, while the baseline</cell></row><row><cell>models (except for LF-LSTM) require the need of</cell></row><row><cell>additional alignment module (e.g., CTC module).</cell></row><row><cell>The results are shown in the bottom part of Ta-</cell></row><row><cell>ble 1, 2, and 3. On the three benchmark datasets,</cell></row><row><cell>MulT improves upon the prior methods (some</cell></row><row><cell>1 All experiments are conducted on 1 GTX-1080Ti</cell></row><row><cell>GPU. The code for our model and experiments can</cell></row><row><cell>be found in https://github.com/yaohungt/</cell></row><row><cell>Multimodal-Transformer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>An ablation study on the benefit of MulT's crossmodal transformers using CMU-MOSEI.).</figDesc><table><row><cell></cell><cell></cell><cell cols="3">(Unaligned) CMU-MOSEI</cell></row><row><cell>Description</cell><cell></cell><cell cols="2">Sentiment</cell><cell></cell></row><row><cell></cell><cell>Acc h 7</cell><cell>Acc h 2</cell><cell>F1 h</cell><cell>MAE Corr h</cell></row><row><cell cols="3">Unimodal Transformers</cell><cell></cell><cell></cell></row><row><cell>Language only</cell><cell>46.5</cell><cell>77.4</cell><cell>78.2</cell><cell>0.653 0.631</cell></row><row><cell>Audio only</cell><cell>41.4</cell><cell>65.6</cell><cell>68.8</cell><cell>0.764 0.310</cell></row><row><cell>Vision only</cell><cell>43.5</cell><cell>66.4</cell><cell>69.3</cell><cell>0.759 0.343</cell></row><row><cell cols="5">Late Fusion by using Multiple Unimodal Transformers</cell></row><row><cell>LF-Transformer</cell><cell>47.9</cell><cell>78.6</cell><cell>78.5</cell><cell>0.636 0.658</cell></row><row><cell cols="5">Temporally Concatenated Early Fusion Transformer</cell></row><row><cell>EF-Transformer</cell><cell>47.8</cell><cell>78.9</cell><cell>78.8</cell><cell>0.648 0.647</cell></row><row><cell cols="3">Multimodal Transfomers</cell><cell></cell><cell></cell></row><row><cell>Only [V, A → L] (ours) Only [L, A → V ] (ours) Only [L, V → A] (ours) MulT mixing intermediate-level features (ours)</cell><cell>50.5 48.2 47.5 50.3</cell><cell>80.1 79.7 79.2 80.5</cell><cell>80.4 80.2 79.7 80.6</cell><cell>0.605 0.670 0.611 0.651 0.620 0.648 0.602 0.674</cell></row><row><cell>MulT (ours)</cell><cell>50.7</cell><cell>81.6</cell><cell>81.6</cell><cell>0.591 0.691</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters of Multimodal Transformer (MulT) we use for the various tasks. The "# of Crossmodal Blocks" and "# of Crossmodal Attention Heads" are for each transformer.</figDesc><table><row><cell></cell><cell>CMU-MOSEI</cell><cell>CMU-MOSI</cell><cell>IEMOCAP</cell></row><row><cell>Batch Size</cell><cell>16</cell><cell>128</cell><cell>32</cell></row><row><cell>Initial Learning Rate</cell><cell>1e-3</cell><cell>1e-3</cell><cell>2e-3</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Transformers Hidden Unit Size d</cell><cell>40</cell><cell>40</cell><cell>40</cell></row><row><cell># of Crossmodal Blocks D</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell># of Crossmodal Attention Heads</cell><cell>8</cell><cell>10</cell><cell>10</cell></row><row><cell>Temporal Convolution Kernel Size (L/V /A)</cell><cell>(1 or 3)/3/3</cell><cell>(1 or 3)/3/3</cell><cell>3/3/5</cell></row><row><cell>Textual Embedding Dropout</cell><cell>0.3</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>Crossmodal Attention Block Dropout</cell><cell>0.1</cell><cell>0.2</cell><cell>0.25</cell></row><row><cell>Output Dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Gradient Clip</cell><cell>1.0</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell># of Epochs</cell><cell>20</cell><cell>100</cell><cell>30</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by DARPA HR00111990016, AFRL FA8750-18-C-0014, NSF IIS1763562 #1750439 #1722822, Apple, Google focused award, and Samsung. We would also like to acknowledge NVIDIAs GPU support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transformer-xl: Language modeling with longer-term dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Covarepa collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audio-visual fusion for sentiment classification using cross-modal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupayan</forename><surname>Sri Harsha Dumpala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil Kumar</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopparapu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An argument for basic emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; emotion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="169" to="200" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial signs of emotional experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Wallace V Freisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ancoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1125</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Kathleen Rita Gibson, and Tim Ingold. 1994. Tools, language and cognition in human evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Gibson</surname></persName>
		</author>
		<imprint>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal affective analysis using hierarchical attention strategy with word-level alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangning</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Facial expression analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02598</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multimodal language analysis with recurrent multistage fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyin</forename><surname>Paul Pu Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prismatic</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL, System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
	<note>Louis-Philippe Morency, and Barnabas Poczos</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentiment analysis using imperfect views from spoken language and acoustic modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupayan</forename><surname>Sri Harsha Dumpala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil Kumar</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopparapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</title>
		<meeting>Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linguistically-informed self-attention for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5027" to="5038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Why self-attention? a targeted evaluation of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongbo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08946</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning factorized multimodal representations</title>
		<editor>Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
	<note>Non-local neural networks</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Words can shift: Dynamically adjusting word representations using nonverbal behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Online segment to segment neural transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08194</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speaker identification on the scotus corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3878</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Memory fusion network for multiview sequential learning</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
