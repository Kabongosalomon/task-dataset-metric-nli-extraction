<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">General E(2) -Equivariant Steerable CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
							<email>m.weiler@uva.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Cesa</surname></persName>
							<email>cesa.gabriele@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">QUVA Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">General E(2) -Equivariant Steerable CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of E(2)-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group E(2) and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. E(2)-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions. * Equal contribution, author ordering determined by random number generator. † This research has been conducted during an internship at QUVA lab, University of Amsterdam. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1911.08251v2 [cs.CV] 6 Apr 2021 1 r2 _ act = Rot2dOnR2(N=8) 2 feat _ type _ in = FieldType(r2 _ act, 3 * [r2 _ act.trivial _ repr]) 3 feat _ type _ out = FieldType(r2 _ act, 10 * [r2 _ act.regular _ repr]) 4 conv _ op = R2Conv(feat _ type _ in, feat _ type _ out, kernel _ size=5)</p><p>8 Note that this prevents equivariance from being exact for groups which are not symmetries of the grid. Specifically, for Z 2 only subgroups of D4 are exact symmetries which motivated their use in [6, 10, 1]. <ref type="bibr" target="#b8">9</ref> The same decomposition was used in a different context in Section 2.4.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The equivariance of neural networks under symmetry group actions has in the recent years proven to be a fruitful prior in network design. By guaranteeing a desired transformation behavior of convolutional features under transformations of the network input, equivariant networks achieve improved generalization capabilities and sample complexities compared to their non-equivariant counterparts. Due to their great practical relevance, a big pool of rotation-and reflection-equivariant models for planar images has been proposed by now. Unfortunately, an empirical survey, reproducing and comparing all these different approaches, is still missing.</p><p>An important step in this direction is given by the theory of Steerable CNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> which defines a very general notion of equivariant convolutions on homogeneous spaces. In particular, steerable CNNs describe E(2)-equivariant (i.e. rotation-and reflection-equivariant) convolutions on the image plane R 2 . The feature spaces of steerable CNNs are thereby defined as spaces of feature fields, characterized by a group representation which determines their transformation behavior under transformations of the input. In order to preserve the specified transformation law of feature spaces, the convolutional kernels are subject to a linear constraint, depending on the corresponding group representations. While this constraint has been solved for specific groups and representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, no general solution strategy has been proposed so far. In this work we give a general strategy which reduces the solution of the kernel space constraint under arbitrary representations to much simpler constraints under single, irreducible representations.</p><p>Specifically for the Euclidean group E(2) and its subgroups, we give a general solution of this kernel space constraint. As a result, we are able to implement a wide range of equivariant models, covering regular GCNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, classical Steerable CNNs <ref type="bibr" target="#b0">[1]</ref>, Harmonic Networks <ref type="bibr" target="#b11">[12]</ref>, gated Harmonic Networks <ref type="bibr" target="#b1">[2]</ref>, Vector Field Networks <ref type="bibr" target="#b12">[13]</ref>, Scattering Transforms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> and entirely new architectures, in one unified framework. In addition, we are able to build hybrid models, mixing different field types (representations) of these networks both over layers and within layers.  We further propose a group restriction operation, allowing for network architectures which are decreasingly equivariant with depth. This is useful e.g. for natural images which show low level features like edges in arbitrary orientations but carry a sense of preferred orientation globally. An adaptive level of equivariance accounts for the resulting loss of symmetry in the hierarchy of features.</p><p>Since the theory of steerable CNNs does not give a preference for any choice of group representation or equivariant nonlinearity, we run an extensive benchmark study, comparing different equivariance groups, representations and nonlinearities. We do so on MNIST 12k, rotated MNIST SO(2) and reflected and rotated MNIST O(2) to investigate the influence of the presence or absence of certain symmetries in the dataset. A drop in replacement of our equivariant convolutional layers is shown to yield significant gains over non-equivariant baselines on CIFAR10, CIFAR100 and STL-10.</p><p>Beyond the applications presented in this paper, our contributions are of relevance for general steerable CNNs on homogeneous spaces <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and gauge equivariant CNNs on manifolds <ref type="bibr" target="#b4">[5]</ref> since these models obey the same kind of kernel constraints. More specifically, 2-dimensional manifolds, endowed with an orthogonal structure group O(2) (or subgroups thereof), necessitate exactly the kernel constraints solved in this paper. Our results can therefore readily be transferred to e.g. spherical CNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> or more general models of geometric deep learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General E(2) -Equivariant Steerable CNNs</head><p>Convolutional neural networks process images by extracting a hierarchy of feature maps from a given input signal. The convolutional weight sharing ensures the inference to be translation-equivariant which means that a translated input signal results in a corresponding translation of the feature maps. However, vanilla CNNs leave the transformation behavior of feature maps under more general transformations, e.g. rotations and reflections, undefined. In this work we devise a general framework for convolutional networks which are equivariant under the Euclidean group E <ref type="bibr" target="#b1">(2)</ref>, that is, under isometries of the plane R 2 . We work in the framework of steerable CNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> which provides a quite general theory for equivariant CNNs on homogeneous spaces, including Euclidean spaces R d as a specific instance. Sections 2.2 and 2.3 briefly review the theory of Euclidean steerable CNNs as described in <ref type="bibr" target="#b1">[2]</ref>. The following subsections explain our main contributions: a decomposition of the kernel space constraint into irreducible subspaces <ref type="bibr">(2.4)</ref>, their solution for E(2) and subgroups <ref type="bibr">(2.5)</ref>, an overview on the group representations used to steer features, their admissible nonlinearities and their use in related work <ref type="bibr">(2.6)</ref>, the group restriction operation (2.7) and implementation details (2.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Isometries of the Euclidean plane R 2</head><p>The Euclidean group E(2) is the group of isometries of the plane R 2 , consisting of translations, rotations and reflections. Characteristic patterns in images often occur at arbitrary positions and in arbitrary orientations. The Euclidean group therefore models an important factor of variation of image features. This is especially true for images without a preferred global orientation like satellite imagery or biomedical images but often also applies to low level features of globally oriented images.</p><p>One can view the Euclidean group as being constructed from the translation group (R 2 , +) and the orthogonal group O(2) = {O ∈ R 2×2 | O T O = id 2×2 } via the semidirect product operation as E(2) ∼ = (R 2 , +) O <ref type="bibr" target="#b1">(2)</ref>. The orthogonal group thereby contains all operations leaving the origin invariant, i.e. continuous rotations and reflections. In order to allow for different levels of equivariance and to cover a wide spectrum of related work we consider subgroups of the Euclidean group of the form (R 2 , +) G, defined by subgroups G ≤ O <ref type="bibr" target="#b1">(2)</ref>. Specifically, G could be either the special orthogonal group SO <ref type="bibr" target="#b1">(2)</ref>, the group ({±1}, * ) of the reflections along a given axis, the cyclic groups C N , the dihedral groups D N or the orthogonal group O(2) itself. While SO(2) describes continuous rotations (without reflections), C N and D N contain N discrete rotations by angles multiple of 2π N and, in the case of D N , reflections. C N and D N are therefore discrete subgroups of order N and 2N , respectively. For an overview over the groups and their interrelations see <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Since the groups (R 2 , +) G are semidirect products, one can uniquely decompose any of their elements into a product tg where t ∈ (R 2 , +) and g ∈ G <ref type="bibr" target="#b2">[3]</ref> which we will do in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">E(2) -steerable feature fields</head><p>Steerable CNNs define feature spaces as spaces of steerable feature fields f : R 2 → R c which associate a c-dimensional feature vector f (x) ∈ R c to each point x of a base space, in our case the plane R 2 . In contrast to vanilla CNNs, the feature fields of steerable CNNs are associated with a transformation law which specifies their transformation under actions of E(2) (or subgroups) and therefore endows features with a notion of orientation 1 . Formally, a feature vector f (x) encodes the coefficients of a coordinate independent geometric feature relative to a choice of reference frame or, equivalently, image orientation (see Appendix A). scalar field ρ(g) = 1 vector field ρ(g) = g An important example are scalar feature fields s : R 2 → R, describing for instance gray-scale images or temperature and pressure fields. The Euclidean group acts on scalar fields by moving each pixel to a new position, that is, s(x) → s (tg) −1 x = s g −1 (x − t) for some tg ∈ (R 2 , +) G; see <ref type="figure" target="#fig_1">Figure 1</ref>, left. Vector fields v : R 2 → R 2 , like optical flow or gradient images, on the other hand transform as v(x) → g · v g −1 (x − t) . In contrast to the case of scalar fields, each vector is therefore not only moved to a new position but additionally changes its orientation via the action of g ∈ G; see <ref type="figure" target="#fig_1">Figure 1</ref>, right.</p><p>The transformation law of a general feature field f : R 2 → R c is fully characterized by its type ρ.</p><p>Here ρ : G → GL(R c ) is a group representation, specifying how the c channels of each feature vector f (x) mix under transformations. A representation satisfies ρ(gg) = ρ(g)ρ(g) and therefore models the group multiplication gg as multiplication of c × c matrices ρ(g) and ρ(g); see Appendix B. More specifically, a ρ-field transforms under the induced representation <ref type="bibr" target="#b22">23</ref> Ind</p><formula xml:id="formula_0">(R 2 ,+) G G ρ of (R 2 , +) G as f (x) → Ind (R 2 ,+) G G ρ (tg) · f (x) := ρ(g) · f g −1 (x − t) .<label>(1)</label></formula><p>As in the examples above, it transforms feature fields by moving the feature vectors from g −1 (x − t) to a new position x and acting on them via ρ(g). We thus find scalar fields to correspond to the trivial representation ρ(g) = 1 ∀g ∈ G which reflects that the scalar values do not change when being moved. Similarly, a vector field corresponds to the standard representation ρ(g) = g of G.</p><p>In analogy to the feature spaces of vanilla CNNs comprising multiple channels, the feature spaces of steerable CNNs consist of multiple feature fields f i : R 2 → R ci , each of which is associated with its own type ρ i : G → GL(R ci ). A stack f = i f i of feature fields is then defined to be concatenated from the individual feature fields and transforms under the direct sum ρ = i ρ i of the individual representations. Since the direct sum representation is block diagonal, the individual feature fields are guaranteed to transform independently from each other. A common example for a stack of feature fields are RGB images f: R 2 → R 3 . Since the color channels transform independently under rotations we identify them as three independent scalar fields. The stacked field representation is thus given by the direct sum</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">E(2) -steerable convolutions</head><p>In order to preserve the transformation law of steerable feature spaces, each network layer is required to be equivariant under the group actions. As proven for Euclidean groups <ref type="bibr" target="#b3">4</ref> in <ref type="bibr" target="#b1">[2]</ref>, the most general equivariant linear map between steerable feature spaces, transforming under ρ in and ρ out , is given by convolutions with G-steerable kernels 5 k : R 2 → R cout×cin , satisfying a kernel constraint</p><formula xml:id="formula_1">k(gx) = ρ out (g)k(x)ρ in (g −1 ) ∀g ∈ G, x ∈ R 2 .<label>(2)</label></formula><p>Intuitively, this constraint determines the form of the kernel in transformed coordinates gx in terms of the kernel in non-transformed coordinates x and thus its response to transformed input fields. It ensures that the output feature fields transform as specified by Ind ρ out when the input fields are being transformed by Ind ρ in ; see Appendix D.1 for a proof.</p><p>Since the kernel constraint is linear, its solutions form a linear subspace of the vector space of unconstrained kernels considered in conventional CNNs. It is thus sufficient to solve for a basis of the G-steerable kernel space in terms of which the equivariant convolutions can be parameterized. The lower dimensionality of the restricted kernel space enhances the parameter efficiency of steerable CNNs over conventional CNNs similarly to the increased parameter efficiency of CNNs over MLPs by translational weight sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Irrep decomposition of the kernel constraint</head><p>The kernel constraint <ref type="bibr" target="#b1">(2)</ref> in principle needs to be solved individually for each pair of input and output types ρ in and ρ out to be used in the network <ref type="bibr" target="#b5">6</ref> . Here we show how the solution of the kernel constraint for arbitrary representations can be reduced to much simpler constraints under irreducible representations (irreps). Our approach relies on the fact that any representation of a finite or compact group decomposes under a change of basis into a direct sum of irreps, each corresponding to an invariant subspace of the representation space R c on which ρ acts. Denoting the change of basis by Q, this means that one can always write ρ = Q −1 i∈I ψ i Q where ψ i are the irreducible representations of G and the index set I encodes the types and multiplicities of irreps present in ρ. A decomposition can be found by exploiting basic results of character theory and linear algebra <ref type="bibr" target="#b28">[29]</ref>.</p><p>The decomposition of ρ in and ρ out in the kernel constraint (2) leads to k(gx) = Q −1 out i∈Iout</p><formula xml:id="formula_2">ψ i (g) Q out k(x) Q −1 in j∈Iin ψ −1 j (g) Q in ∀g ∈ G, x ∈ R 2 ,</formula><p>which, defining a kernel relative to the irrep bases as κ := Q out kQ −1 in , implies</p><formula xml:id="formula_3">κ(gx) = i∈Iout ψ i (g) κ(x) j∈Iin ψ −1 j (g) ∀g ∈ G, x ∈ R 2 .</formula><p>The left and right multiplication with a direct sum of irreps reveals that the constraint decomposes into independent constraints</p><formula xml:id="formula_4">κ ij (gx) = ψ i (g) κ ij (x) ψ −1 j (g) ∀g ∈ G, x ∈ R 2 where i ∈ I out , j ∈ I in<label>(3)</label></formula><p>on blocks κ ij in κ corresponding to invariant subspaces of the full space of equivariant kernels; see Appendix E for a visualization. In order to solve for a basis of equivariant kernels satisfying the original constraint <ref type="bibr" target="#b1">(2)</ref>, it is therefore sufficient to solve the irrep constraints (3) to obtain bases for each block, revert the change of basis and take the union over different blocks. Specifically, given d ij -dimensional bases κ ij 1 , · · ·, κ ij dij for the blocks κ ij of κ, we get a d = ij d ij -dimensional basis</p><formula xml:id="formula_5">k 1 , · · · , k d := i∈Iout j∈Iin Q −1 out κ ij 1 Q in , · · · , Q −1 out κ ij dij Q in<label>(4)</label></formula><p>of solutions of (2). Here κ ij denotes a block κ ij being filled at the corresponding location of a matrix of the shape of κ with all other blocks being set to zero; see Appendix E. The completeness of the basis found this way is guaranteed by construction if the bases for each block ij are complete. <ref type="bibr" target="#b3">4</ref> Proofs for more general cases can be found in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. <ref type="bibr" target="#b4">5</ref> As k : R 2 → R cout×c in returns a matrix of shape (cout, cin) for each position x ∈ R 2 , its discretized version can be represented by a tensor of shape (cout, cin, X, Y ) as usually done in deep learning frameworks. <ref type="bibr" target="#b5">6</ref> A numerical solution technique which is based on a Clebsch-Gordan decomposition of tensor products of irreps has been proposed in <ref type="bibr" target="#b1">[2]</ref>. While this technique would generalize to arbitrary representations it becomes prohibitively expensive for larger representations as considered here; see Appendix G.</p><p>Note that while this approach shares some basic ideas with the solution strategy proposed in <ref type="bibr" target="#b1">[2]</ref>, it is computationally more efficient for large representations; see Appendix G. We want to emphasize that this strategy for reducing the kernel constraint to irreducible representations is not restricted to subgroups of O(2) but applies to steerable CNNs in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">General solution of the kernel constraint for O(2) and subgroups</head><p>In order to build isometry-equivariant CNNs on R 2 we need to solve the irrep constraints (3) for the specific case of G being O(2) or one of its subgroups. For this purpose note that the action of G on R 2 is norm-preserving, that is, |g.x| = |x| ∀g ∈ G, x ∈ R 2 . The constraints (2) and (3) therefore only restrict the angular parts of the kernels but leave their radial parts free. Since furthermore all irreps of G correspond to one unique angular frequency (see Appendix F.2), it is convenient to expand the kernel w.l.o.g. in terms of an (angular) Fourier series</p><formula xml:id="formula_6">κ ij αβ x(r, φ) = A αβ,0 (r) + ∞ µ=1 A αβ,µ (r) cos(µφ) + B αβ,µ (r) sin(µφ)<label>(5)</label></formula><p>with real-valued, radially dependent coefficients A αβ,µ : R + → R and B αβ,µ : R + → R for each matrix entry κ ij αβ of block κ ij . By inserting this expansion into the irrep constraints (3) and projecting on individual harmonics we obtain constraints on the Fourier coefficients, forcing most of them to be zero. The vector spaces of G-steerable kernel blocks κ ij satisfying the irrep constraints (3) are then parameterized in terms of the remaining Fourier coefficients. The completeness of this basis follows immediately from the completeness of the Fourier basis. Similar approaches have been followed in simpler settings for the cases of C N in <ref type="bibr" target="#b6">[7]</ref>, SO(2) in <ref type="bibr" target="#b11">[12]</ref> and SO(3) in <ref type="bibr" target="#b1">[2]</ref>.</p><p>The resulting bases for the angular parts of kernels for each pair of irreducible representations of O(2) are shown in <ref type="table" target="#tab_2">Table 2</ref>. It turns out that each basis element is harmonic and associated to one unique angular frequency. Appendix F gives an explicit derivation and the resulting bases for all possible pairs of irreps for all groups G ≤ O(2) following the strategy presented in this section. The analytical solutions for SO(2), ({±1}, * ), C N and D N are found in Tables 8, 10, 11 and 12. Since these groups are subgroups of O(2), they enforce a weaker kernel constraint as compared to O(2). As a result, the bases for G &lt; O(2) are higher dimensional, i.e. they allow for a wider range of kernels. A higher level of equivariance therefore leads simultaneously to a guaranteed behavior of the inference process under transformations and on the other hand to an improved parameter efficiency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Group representations and nonlinearities</head><p>A question which so far has been left open is which field types, i.e. which representations ρ of G, should be used in practice. Considering only the convolution operation with G-steerable kernels for the moment, it turns out that any change of basis P to an equivalent representation ρ := P −1 ρP is irrelevant. To see this, consider the irrep decomposition ρ = Q −1 i∈I ψ i Q used in the solution of the kernel constraint to obtain a basis {k i } d i=1 of G-steerable kernels as defined by Eq. (4). Any equivalent representation will decompose into ρ = Q −1 i∈I ψ i Q with Q = QP for some P and therefore result in a kernel basis {P −1 out k i P in } d i=1 which entirely negates changes of bases between equivalent representations. It would therefore w.l.o.g. suffice to consider direct sums of irreps ρ = i∈I ψ i as representations only, reducing the question on which representations to choose to the question on which types and multiplicities of irreps to use.</p><p>In practice, however, convolution layers are interleaved with other operations which are sensitive to specific choices of representations. In particular, nonlinearity layers are required to be equivariant under the action of specific representations. The choice of group representations in steerable CNNs therefore restricts the range of admissible nonlinearities, or, conversely, a choice of nonlinearity allows only for certain representations. In the following we review prominent choices of representations found in the literature in conjunction with their compatible nonlinearities.</p><p>All equivariant nonlinearities considered here act spatially localized, that is, on each feature vector f (x) ∈ R cin for all x ∈ R 2 individually. They might produce different types of output fields</p><formula xml:id="formula_7">ρ out : G → GL(R cout ), that is, σ : R cin → R cout , f (x) → σ(f (x)</formula><p>). As proven in Appendix D.2, it is sufficient to require the equivariance of σ under the actions of ρ in and ρ out , i.e. σ • ρ in (g) = ρ out (g)•σ ∀g ∈ G, for the nonlinearities to be equivariant under the action of induced representations when being applied to a whole feature field as σ(f )(x) := σ(f (x)).</p><p>A general class of representations are unitary representations which preserve the norm of their representation space, that is, they satisfy |ρ unitary (g)f (x)| = f (x) ∀ g ∈ G. As proven in Appendix D.2.2, nonlinearities which solely act on the norm of feature vectors but preserve their orientation are equivariant w.r.t. unitary representations. They can in general be decomposed in</p><formula xml:id="formula_8">σ norm : R c → R c , f (x) → η |f (x)| f (x)</formula><p>|f (x)| for some nonlinear function η : R ≥0 → R ≥0 acting on the norm of feature vectors. Norm-ReLUs, defined by η(|f (x)|) = ReLU(|f (x)| − b) where b ∈ R + is a learned bias, were used in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2]</ref>. In <ref type="bibr" target="#b27">[28]</ref>, the authors consider squashing nonlinearities</p><formula xml:id="formula_9">η(|f (x)|) = |f (x)| 2 |f (x)| 2 +1</formula><p>. Gated nonlinearities were proposed in <ref type="bibr" target="#b1">[2]</ref> as conditional version of norm nonlinearities. They act by scaling the norm of a feature field by learned sigmoid gates 1 1+e −s(x) , parameterized by a scalar feature field s. All representations considered in this paper are unitary such that their fields can be acted on by norm-nonlinearities. This applies specifically also to all irreducible representations ψ i of G ≤ O(2) which are discussed in detail in Section F.2.</p><p>A common choice of representations of finite groups like C N and D N are regular representations. Their representation space R |G| has dimensionality equal to the order of the group, e.g. R N for C N and R 2N for D N . The action of the regular representation is defined by assigning each axis e g of R |G| to a group element g ∈ G and permuting the axes according to ρ G reg (g)e g := eg g . Since this action is just permuting channels of ρ G reg -fields, it commutes with pointwise nonlinearities like ReLU; a proof is given in Appendix D.2.3. While regular steerable CNNs were empirically found to perform very well, they lead to high dimensional feature spaces with each individual field consuming |G| channels. Regular steerable CNNs were investigated for planar images in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref>, for spherical CNNs in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5]</ref> and for volumetric convolutions in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Further, the translation of feature maps of conventional CNNs can be viewed as action of the regular representation of the translation group.</p><p>Closely related to regular representations are quotient representations. Instead of permuting |G| channels indexed by G, they permute |G|/|H| channels indexed by cosets gH in the quotient space G/H of a subgroup H ≤ G. Specifically, they act on axes e gH of R |G|/|H| as defined by ρ G/H quot (g)e gH := eg gH . This definition covers regular representations as a special case for the trivial subgroup H = {e}. As permutation representations, quotient representations allow for pointwise nonlinearities; see Appendix D.2.3. A benefit of quotient representations over regular representation is that they require by a factor of |H| less channels per feature field. On the other hand, they enforce more symmetries in the feature fields which result in a restriction of the G-steerable kernel basis; see Appendix C for details and intuitive examples. Quotient representations were considered in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Both regular and quotient representations can be viewed as being induced from the trivial representation of a subgroup H ≤ G, specifically ρ G reg = Ind G {e} 1 and ρ G quot = Ind G H 1. More generally, any representationρ : H → GL(R c ) can be used to define an induced representations 7 ρ ind = Ind G Hρ : G → GL(R c·|G:H| ). Here |G : H| denotes the index of H in G which corresponds to |G|/|H| if G and H are both finite. Admissible nonlinearities of induced representations can be constructed from valid nonlinearities ofρ. For more information on inductions we refer to Appendix B.</p><p>Regular and quotient fields can furthermore be acted on by nonlinear pooling operators. Via a group pooling or projection operation max : R c → R, f (x) → max(f (x)) the works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31]</ref> extract the maximum value of a regular or quotient field. The invariance of the maximum operation implies that the resulting features form scalar fields. Since group pooling operations discard information on the feature orientations entirely, vector field nonlinearities σ vect : R N → R 2 for regular representations of C N were proposed in <ref type="bibr" target="#b12">[13]</ref>. Vector field nonlinearities do not only keep the maximum response max(f (x)) but also its index arg max(f (x)). This index corresponds to a rotation angle θ = 2π N arg max(f (x)) which is used to define a vector field with elements v(x) = max(f (x))(cos(θ), sin(θ)) T . The equivariance of this operation is proven in D.2.4.</p><p>In general, any pair of feature fields f 1 : R 2 → R c1 and f 2 : R 2 → R c2 can be combined via the tensor product operation f 1 ⊗ f 2 . Given that the individual fields transform under arbitrary representations ρ 1 and ρ 2 , their product transforms under the tensor product representation ρ 1 ⊗ρ 2 : G → GL(R c1·c2 ). Since the tensor product is a nonlinear transformation by itself, no additional nonlinearity needs to be applied. Tensor product nonlinearities have been discussed in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Any pair f 1 and f 2 of feature fields can furthermore be concatenated by taking their direct sum f 1 ⊕ f 2 : R 2 → R c1+c2 which we used in Section 2.2 to define feature spaces comprising multiple feature fields. The concatenated field transforms according to the direct sum representation as</p><formula xml:id="formula_10">ρ 1 ⊕ ρ 2 (g) f 1 ⊕ f 2 := ρ 1 (g)f 1 ⊕ ρ 2 (g)f 2 .</formula><p>Since each constituent f i of a concatenated field transforms independently it can be viewed as an individual feature and is equivariant under the action of its corresponding nonlinearity σ i ; see Section D.2.1.</p><p>The theory of steerable CNNs does not prefer any of the here presented representations or nonlinearities over each other. We are therefore extensively comparing different choices in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Group restrictions and inductions</head><p>The key idea of equivariant networks is to exploit symmetries in the distribution of characteristic patterns in signals. The level of symmetry present in data might thereby vary over different length scales. For instance, natural images typically show small features like edges or intensity gradients in arbitrary orientations and reflections. On a larger length scale, however, the rotational symmetry is broken as manifested in visual patterns exclusively appearing upright but still in different reflections. Each individual layer of a convolutional network should therefore be adapted to the symmetries present in the length scale of its fields of view.</p><p>A loss of symmetry can be implemented by restricting the equivariance constraints at a certain depth to a subgroup (R 2 , +) H ≤ (R 2 , +) G, where H ≤ G; e.g. from rotations and reflections G = O(2) to mere reflections H = ({±1}, * ) in the example above. This requires the feature fields produced by a layer with a higher level of equivariance to be reinterpreted in the following layer as fields transforming under a subgroup. Specifically, a ρ-field, transforming under a representation ρ : G → GL(R c ), needs to be reinterpreted as aρ-field, whereρ : H → GL(R c ) is a representation of the subgroup H ≤ G. This is naturally achieved by definingρ to be the restricted representatioñ</p><formula xml:id="formula_11">ρ := Res G H (ρ) : H → GL(R c ), h → ρ(h) ,<label>(6)</label></formula><p>defined by restricting the domain of ρ to H. Since a subsequent H-steerable convolution layers can map fields of arbitrary representations we can readily process the resulting Res G H (ρ)-field further. Conversely, it is also possible that local patterns are aligned, while patterns which emerge on a larger scale are more symmetrically distributed. This can be exploited by lifting a ρ field, transforming under H ≤ G, to an induced Ind G H ρ field, transforming under G. The effect of enforcing different levels of equivariance through group restrictions is investigated empirically in Sections 3.2, 3.4, 3.5 and 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Implementation details</head><p>E(2)-steerable CNNs rely on convolutions with O(2)-steerable kernels. Our implementation therefore involves 1) computing a basis of steerable kernels, 2) the expansion of a steerable kernel in terms of this basis with learned expansion coefficients and 3) running the actual convolution routine. Since the kernel basis depends only on the chosen representations it is precomputed before training.</p><p>Given an input and output representation ρ in and ρ out of G ≤ O(2), we first precompute a basis {k 1 , . . . k d } of G-steerable kernels satisfying Eq. <ref type="bibr" target="#b1">(2)</ref>. In order to solve the kernel constraint we compute the types and multiplicities of irreps in the input and output representations using character theory <ref type="bibr" target="#b28">[29]</ref>. The change of basis can be obtained by solving the linear system of equations ρ(g) = Q −1 [ i∈I ψ i (g)]Q ∀g ∈ G. For each pair ψ i , ψ j of irreps occurring in ρ out and ρ in we retrieve the analytical solutions {κ ij 1 , . . . , κ ij dij } listed in Appendix F.3. Together with the change of basis matrices Q in and Q out , they fully determine the angular parts of the basis {k 1 , . . . , k d } of G-steerable kernels via Eq. (4). Since the kernel space constraint affects only the angular behavior of the kernels we are free to choose any radial profile. Following <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b1">[2]</ref>, we choose Gaussian radial profiles exp 1 2σ 2 (r R) 2 of width σ, centered at radii R = 1, . . . , s/2 . In practice, we consider digitized signals on a pixel grid 8 Z 2 . Correspondingly, we sample the analytically found kernel basis {k 1 , . . . , k d } on a square grid of size s×s to obtain their numerical representation of shape (d, c out , c in , s, s). In this process it is important to prevent aliasing effects. Specifically, each basis kernel corresponds to one particular angular harmonic; see <ref type="table" target="#tab_2">Table 2</ref>. When being sampled with a too low rate, a basis kernel can appear as a lower harmonic and might therefore introduce non-equivariant kernels to the sampled basis. For this reason, preventing aliasing is necessary to guarantee (approximate) equivariance. In order to ensure a faithful discretization, note that each Gaussian radial profile defines a ring whose circumference, and thus angular sampling rate, is proportional to its radius. It is therefore appropriate to bandlimit the kernel basis by a cutoff frequency which is chosen in proportion to the rings' radii. Since the basis kernels are harmonics of specific angular frequencies this is easily implemented by discarding high frequency solutions.</p><p>In typical applications the feature spaces are defined to be composed of multiple independent feature fields. Since the corresponding representations are block diagonal, this implies that the actual constraint (2) decomposes into multiple simpler constraints 9 which we leverage in our implementation to improve its computational efficiency. Assuming the output and input representations of a layer to be given by ρ out = γ ρ out,γ and ρ in = δ ρ in,δ respectively, the constraint on the full kernel space is equivalent to constraints on its blocks k γδ which map between the independent fields transforming under ρ in,δ and ρ out,γ . Our implementation therefore computes a sampled basis k γδ 1 , . . . , k γδ d γδ of k γδ for each pair (ρ in,δ , ρ out,γ ) of input and output representations individually.</p><p>At runtime, the convolution kernels are expanded by contracting the sampled kernel bases with learned weights. Specifically, each basis k γδ 1 , . . . , k γδ d γδ , realized by a tensor of shape (d γδ , c out,γ , c in,δ , s, s), is expanded into the corresponding block k γδ of the kernel by contracting it with a tensor of learned parameters of shape (d γδ ). This process is sped up further by batching together multiple occurrences of the same pair of representations and thus block bases.</p><p>The resulting kernels are then used in a standard convolution routine. In practice we find that the time spent on the actual convolution of reasonably sized images outweighs the cost of the kernel expansion. In evaluation mode the parameters are not updated such that the kernel needs to be expanded only once and can then be reused. E(2)-steerable CNNs therefore have no computational overhead in comparison to conventional CNNs at test time.</p><p>Our implementation is provided as a PyTorch extension which is available at https://github.com/ QUVA-Lab/e2cnn. The library provides equivariant versions of many neural network operations, including G-steerable convolutions, nonlinearities, mappings to produce invariant features, spatial pooling, batch normalization and dropout. Feature fields are represented by geometric tensors, which are wrapping a torch.Tensor object and augment it, among other things, with their transformation law under the action of a symmetry group. This allows for a dynamic type-checking which prevents the user from applying operations to geometric tensors whose transformation law does not match the transformation law expected by the operation. The user interface hides most complications on group theory and solutions of the kernel space constraint and requires the user only to specify the transformation laws of feature spaces. For instance, a C 8 -equivariant convolution operation, mapping a RGB image, identified as three scalar fields, to ten regular feature fields, would be instantiated by:</p><p>Everything the user has to do is to specify that the group C 8 acts on R 2 by rotating it (line 1) and to define the types ρ in = 3 i=1 1 and ρ out = 10 i=1 ρ C4 reg of the input and output feature fields (lines 2 and 3), which are subsequently passed to the constructor of the steerable convolution (line 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Since the framework of general E(2)-equivariant steerable CNNs supports many choices of groups, representations and nonlinearities, we first run an extensive benchmark study over the space of supported models in Section 3.1. As the performance of the models depends heavily on the level of symmetry present in the data, we evaluate each model on three different versions of the MNIST dataset: on untransformed digits, randomly rotated digits and simultaneously rotated and reflected digits, corresponding to transformations in {e}, in SO(2) and O(2), respectively. Section 3.2 further investigates the effect of the invariance of models to different global symmetries. We discuss how too high levels of invariance can hurt the performance and demonstrate how it can be prevented via group restrictions. The convergence rate of equivariant models is being explored in Section 3.3. In addition, we present two models which improve upon the previous state of the art on MNIST rot in Section 3.4.</p><p>The insights from these benchmark experiments are then applied to build several models for classifying CIFAR-10 and CIFAR-100 images in Section 3.5 and STL-10 images in Section 3.6. These datasets consist of globally aligned, natural images which allows us to validate that exploiting local symmetries is advantageous. In order to compare the benefit from different local symmetries we restrict the equivariance of our models to different subgroups at varying depths. All models are built by replacing the non-equivariant convolutions of well established baseline models with our G-steerable convolutions. Despite not tuning any hyperparameters, we find that all of our models significantly outperform the baselines. This holds true even for settings in which a strong auto-augmentation policy is being used.</p><p>All of our experiments are found in a dedicated repository at https://github.com/gabri95/ e2cnn_experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model benchmarking on transformed MNIST datasets</head><p>We first perform a comprehensive benchmarking to compare the impact of the different design choices covered in this work. Each model is evaluated on three different versions of the MNIST dataset, each of which consists of 12000 training images and 50000 test images. As non-transformed version we use the official MNIST 12k dataset. The official rotated MNIST dataset, often called MNIST rot, differs from MNIST 12k by a random SO(2) rotation of each digit. In addition we build MNIST O(2) whose digits are both rotated and reflected. These datasets allow us to study the benefit from different levels of G-steerability in the presence or absence of certain symmetries. In order to not disadvantage models with lower levels of equivariance and since it would be done in real scenarios we train all models using augmentation by the transformations present in the corresponding dataset. <ref type="table" target="#tab_7">Table 3</ref> shows the test errors of 57 different models on the three MNIST variants. The first four columns state the equivariance groups, representations, nonlinearities and invariant maps which distinguish the models. Column five cites related work which proposed the corresponding model design. The statistics of each entry are averaged over (at least) 6 samples. All models in these experiments are derived from the base architecture described in <ref type="table" target="#tab_0">Table 13</ref> in Appendix H. The actual width of each model is adapted such that the number of parameters is approximately preserved. Note that this results in different numbers of channels, depending on the parameter efficiency of the corresponding models. All models apply some form of invariant mapping to scalar fields followed by spatial pooling after the last convolutional layer such that the predictions are guaranteed to be invariant under the equivariance group of the model. The number of invariant features passed to the fully connected classifier is approximately kept constant by adapting the width of the last convolutional layer to the invariant mapping used. In the remainder of this subsection we will guide through the results presented in <ref type="table" target="#tab_7">Table 3</ref>. For more information on the training setup, see Appendix H.1.</p><p>Regular steerable CNNs: Due to their popularity we first cover steerable CNNs whose features transform under regular representations of C N and D N for varying orders N . Note that these models correspond to group convolutional CNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. For the dihedral models we choose a vertical reflection axis. We use ELUs <ref type="bibr" target="#b34">[35]</ref>    <ref type="table" target="#tab_7">Table 3</ref>). For MNIST O(2) and MNIST rot the prediction accuracies improve with N but start to saturate at approximately 8 to 12 rotations. On MNIST O(2) the D N models perform consistently better than the C N models of the same order N . This is the case since the dihedral models are guaranteed to generalize over reflections which are present in the dataset. All equivariant models outperform the non-equivariant CNN baseline.</p><p>On MNIST rot, the accuracy of the C N -equivariant models improve significantly in comparison to their results on MNIST O(2) since the intra-class variability is reduced. In contrast, the test errors of the D N -equivariant models is the same on both datasets. The reason for this result is the reflection invariance of the D N models which implies that they can't distinguish between reflected digits. For N = 1 the dihedral model is purely reflection-but not rotation invariant and therefore performs even worse than the CNN baseline. This issue is resolved by restricting the dihedral models after the penultimate convolution to C N ≤ D N , such that the group pooling after the final convolution results in only C N -invariant features. This model, denoted in the figure by D N | 5 C N , achieves a slightly better accuracy than the pure C N -equivariant model since it can leverage local reflectional symmetries <ref type="bibr" target="#b9">10</ref> .</p><p>For MNIST 12k the non-restricted D N models perform again worse than the C N models since they are insensitive to the chirality of the digits. In order to explain the non-monotonic trend of the curves <ref type="bibr" target="#b9">10</ref> The group restricted models are not listed in <ref type="table" target="#tab_7">Table 3</ref>   <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> 1.93 ± 0.04 0.82 ± 0.02 0.95 ± 0.04</p><formula xml:id="formula_12">10 C 20 regular ρ reg [7] 1.95 ± 0.05 0.83 ± 0.05 0.94 ± 0.06 11 C 4 5ρ reg ⊕2ρ C 4/ C 2 quot ⊕2ψ 0 [1] 2.43 ± 0.05 1.03 ± 0.05 1.01 ± 0.03 12 C 8 5ρ reg ⊕2ρ C 8/ C 2 quot ⊕2ρ C 8/ C 4 quot ⊕2ψ 0 - 2.03 ± 0.05 0.84 ± 0.05 0.91 ± 0.02 13 C 12 5ρ reg ⊕2ρ C 12/ C 2 quot ⊕2ρ C 12/ C 4 quot ⊕3ψ 0 - 2.04 ± 0.04 0.81 ± 0.02 0.95 ± 0.02 14 C 16 5ρ reg ⊕2ρ C 16/ C 2 quot ⊕2ρ C 16/ C 4 quot ⊕4ψ 0 - 2.00 ± 0.01 0.86 ± 0.04 0.98 ± 0.04 15 C 20 quotient 5ρ reg ⊕2ρ C 20/ C 2 quot ⊕2ρ C 20/ C 4 quot ⊕5ψ 0 ELU - 2.01 ± 0.05 0.83 ± 0.03 0.96 ± 0.04 16 regular/scalar ψ 0 conv −−→ ρ reg G-pool − −−− → ψ 0 ELU, G-pooling [6, 36] 2.02 ± 0.02 0.90 ± 0.03 0.93 ± 0.04 17 regular/vector ψ 1 conv −−→ ρ reg vector pool − −−−−− → ψ 1 vector field [13, 37] 2.12 ± 0.02 1.07 ± 0.03 0.78 ± 0.03 18 C 16 mixed vector ρ reg ⊕ψ 1 conv − − → 2ρ reg vector −−→ pool ρ reg ⊕ψ 1 ELU, vector field G-pooling - 1.87 ± 0.03 0.83 ± 0.02 0.63 ± 0.02 19 D 1 - 3.40 ± 0.07 3.44 ± 0.10 0.98 ± 0.03 20 D 2 - 2.42 ± 0.07 2.39 ± 0.04 1.05 ± 0.03 21 D 3 - 2.17 ± 0.06 2.15 ± 0.05 0.94 ± 0.02 22 D 4 [6, 1, 38] 1.88 ± 0.04 1.87 ± 0.04 1.69 ± 0.03 23 D 6 [8] 1.77 ± 0.06 1.77 ± 0.04 1.00 ± 0.03 24 D 8 - 1.68 ± 0.06 1.73 ± 0.03 1.64 ± 0.02 25 D 12 - 1.66 ± 0.05 1.65 ± 0.05 1.67 ± 0.01 26 D 16 - 1.62 ± 0.04 1.65 ± 0.02 1.68 ± 0.04 27 D 20 regular ρ reg ELU G-pooling - 1.64 ± 0.06 1.62 ± 0.05 1.69 ± 0.03 28 D 16 regular/scalar ψ 0,0 conv −−→ ρ reg G-pool − −−− → ψ 0,0 ELU, G-pooling - 1.92 ± 0.03 1.88 ± 0.07 1.74 ± 0.04 29 irreps ≤ 1 1 i=0 ψ i - 2.98 ± 0.04 1.38 ± 0.09 1.29 ± 0.05 30 irreps ≤ 3 3 i=0 ψ i - 3.02 ± 0.18 1.38 ± 0.09 1.27 ± 0.03 31 irreps ≤ 5 5 i=0 ψ i - 3.24 ± 0.05 1.44 ± 0.10 1.36 ± 0.04 32 irreps ≤ 7 7 i=0 ψ i - 3.30 ± 0.11 1.51 ± 0.10 1.40 ± 0.07 33 C-irreps ≤ 1 1 i=0 ψ C i [12]</formula><p>3.39 ± 0.10 1.47 ± 0.06 1.42 ± 0.04</p><formula xml:id="formula_13">34 C-irreps ≤ 3 3 i=0 ψ C i [12]</formula><p>3.48 ± 0. <ref type="bibr" target="#b15">16</ref> 1.51 ± 0.05 1.53 ± 0.07   of the C N and D N models, notice that some of the digits are approximately related by symmetry transformations <ref type="bibr" target="#b10">11</ref> . If these transformations happen to be part of the equivariance group w.r.t. which the model is invariant the predictions are more likely to be confused. This is mostly the case for N being a multiple of 2 or 4 or for large orders N , which include almost all orientations. Once again, the restricted models, here D N | 5 {e} and C N | 5 {e}, show the best results since they exploit local symmetries but preserve information on the global orientation. Since the restricted dihedral model generalizes over local reflections, its performance is consistently better than that of the restricted cyclic model.</p><formula xml:id="formula_14">35 C-irreps ≤ 5 5 i=0 ψ C i - 3.59 ± 0.08 1.59 ± 0.05 1.55 ± 0.06 36 C-irreps ≤ 7 7 i=0 ψ C i ELU,</formula><formula xml:id="formula_15">irreps ≤ 3 3 i=0 ψ i ELU, shared gate norm - 2.20 ± 0.06 1.01 ± 0.03 1.03 ± 0.03 45 irreps = 0 ψ 0,0 ELU - - 5.46 ± 0.46 5.21 ± 0.29 3.98 ± 0.04 46 irreps ≤ 1 ψ 0,0 ⊕ ψ 1,0 ⊕ 2ψ 1,1 - 3.31 ± 0.17 3.37 ± 0.18 3.05 ± 0.09 47 irreps ≤ 3 ψ 0,0 ⊕ ψ 1,0 3 i=1 2ψ 1,i - 3.42 ± 0.03 3.41 ± 0.10 3.86 ± 0.09 48 irreps ≤ 5 ψ 0,0 ⊕ ψ 1,0 5 i=1 2ψ 1,i - 3.59 ± 0.13 3.78 ± 0.31 4.17 ± 0.15 49 irreps ≤ 7 ψ 0,0 ⊕ ψ 1,0 7 i=1 2ψ 1,i ELU, norm-ReLU O(2)-conv2triv - 3.84 ± 0.25 3.90 ± 0.18 4.57 ± 0.27 50 Ind-irreps ≤ 1 Ind ψ SO(2) 0 ⊕ Ind ψ SO(2) 1 - 2.72 ± 0.05 2.70 ± 0.11 2.39 ± 0.07 51 Ind-irreps ≤ 3 Ind ψ SO(2) 0 3 i=1 Ind ψ SO(2) i - 2.66 ± 0.07 2.65 ± 0.12 2.25 ± 0.06 52 Ind-irreps ≤ 5 Ind ψ SO(2) 0 5 i=1 Ind ψ SO(2) i - 2.71 ± 0.11 2.84 ± 0.10 2.39 ± 0.09 53 Ind-irreps ≤ 7 Ind ψ SO(2) 0 7 i=1 Ind ψ SO(2) i ELU, Ind norm-ReLU Ind-conv2triv - 2.80 ± 0.12 2.85 ± 0.06 2.25 ± 0.08 54 O(2)-conv2triv - 2.39 ± 0.05 2.38 ± 0.07 2.28 ± 0.07 55 irreps ≤ 3 ψ 0,0 ⊕ ψ 1,0 3 i=1 2ψ 1,i ELU,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quotient representations:</head><p>As an alternative to regular representations we experiment with some mixtures of quotient representations of C N (rows <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. These models differ from the regular models by enforcing more symmetries in the feature fields and thus kernels. The individual feature fields are lower dimensional; however, by fixing the number of parameters, the models use more different fields which in this specific case leads to approximately the same number of channels and therefore compute and memory requirements. We do not observe any significant difference in performance between regular and quotient representations. Appendix C gives more intuition on our specific choices of quotient representations and which symmetries they enforce. Note that the space of possible quotient representations and their multiplicities is very large and still needs to be investigated more thoroughly.</p><p>Group pooling and vector field nonlinearities: For C <ref type="bibr" target="#b15">16</ref> we implement a group pooling network (row 16) and a vector field network (row 17). These models map regular feature fields, produced by each convolutional layer, to scalar fields and vector fields, respectively; see Section 2.6. These pooling operations compress the features in the regular fields, which can lead to lower memory and compute requirements. However, since we fix the number of parameters, the resulting models are ultimately much wider than the corresponding regular steerable CNNs. Since the pooling operations lead to a loss of information, both models perform worse than their purely regular counterpart on MNIST O(2) and MNIST rot. Surprisingly, the group pooling network, whose features are orientation unaware, performs better than the vector field network. On MNIST 12k the group pooling network closes up with the regular steerable CNN while the vector field network achieves an even better result. We further experiment with a model which applies vector field nonlinearities to only half of the regular fields and preserves the other half (row 18). This model is on par with the regular model on both transformed MNIST versions but achieves the overall best result on MNIST 12k. Similar to the case of C 16 , the group pooling network for D 16 (row 28) performs worse than the corresponding regular model, this time also on MNIST 12k. The models in rows 29-32 are inspired by Harmonic Networks <ref type="bibr" target="#b11">[12]</ref> and consist of irrep fields with the same multiplicity up to a certain threshold. All models apply ELUs on scalar fields and norm-ReLUs (see Section 2.6) on higher order fields. The projection to invariant features is done via a convolution to scalar features (conv2triv) in the last convolutional layer. We find that irrep fields up to order 1 and 3 perform equally well while higher thresholds yield worse results. The original implementation of Harmonic Networks considered complex irreps of SO(2) which results in a lower dimensional steerable kernel basis as discussed in Appendix F.5. We reimplemented these models and found that their reduced kernel space leads to consistently worse results (rows <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>.</p><p>For the model containing irreps up to order 3 we implemented some alternative variants. For instance, the model in row 38 does not convolve to trivial features in the last layer but computes these by taking the norms of all non-scalar fields. This does not lead to significantly different results. Appendix I discusses all variations in detail. <ref type="bibr" target="#b10">11</ref> E.g. 9 and 6 (6 and 9) or 5 and 2 (2 and 5) are related by a rotations by π and might therefore be confused by all models C 2k and D 2k for k ∈ N. Similarly, and (4 and 7) are related by a reflection and a rotation by π/2 and might be confused by all models D 4k .  <ref type="table">Table 4</ref>: Effect of the group restriction operation at different depths of the network on MNIST rot and MNIST 12k. Before restriction, the models are equivariant to a larger symmetry group than the group of global symmetries of the corresponding dataset. A restriction at larger depths leads to an improved accuracy. All restricted models perform better than non-restricted, and hence globally invariant, models.</p><p>By far the best results are achieved by the models in rows 41-44, which replace the norm-ReLUs with gated nonlinearities, see Section 2.6. This observation is in line with the results presented in <ref type="bibr" target="#b1">[2]</ref>, where gated nonlinearities were proposed.</p><p>O(2) models: As for SO <ref type="formula" target="#formula_1">(2)</ref>, we are investigating O(2)-equivariant models whose features transform under irreps up to a certain order and apply norm-ReLUs (rows 46-49). In this case we choose twice the multiplicity of 2-dimensional fields than scalar fields, which reflects the multiplicity of irreps contained in the regular representation of O <ref type="bibr" target="#b1">(2)</ref>. 1,0 (see Appendix F.2), followed by taking the absolute value of the latter (O(2)-conv2triv). We again find that higher irrep thresholds yield worse results, this time already starting from order 1. In particular, these models perform worse than their SO(2)-equivariant counterparts even on MNIST O(2). This suggests that the kernel constraint for this particular choice of representations is too restrictive.</p><p>If only scalar fields, corresponding to the trivial irrep ψ O(2) 0,0 , are chosen, the kernel constraint becomes k(gx) = k(x) ∀g ∈ O(2) and therefore allows for isotropic kernels only. This limits the expressivity of the model so severely that it performs even worse than a conventional CNN on MNIST rot and MNIST 12k while being on par for MNIST O(2), see row 45. Note that isotropic kernels correspond to vanilla graph convolutional networks (cf. the results and discussion in <ref type="bibr" target="#b4">[5]</ref>).</p><p>In order to improve the performance of O(2)-steerable CNNs, we propose to use representations Ind</p><formula xml:id="formula_16">O(2) SO(2) ψ SO(2) k</formula><p>, which are induced from the irreps of SO(2) (see Appendix B for more details on induction). By the definition of induction, this leads to pairs of fields which transform according to ψ SO(2) k under rotations but permute under reflections. The multiplicity of the irreps of O(2) contained in this induced representation coincides with the multiplicities chosen in the pure O(2) irrep models. However, the change of basis, relating both representations, does not commute with the nonlinearities, such that the networks behave differently. We apply Ind norm-ReLU nonlinearities to the induced O(2) models which compute the norm of each of the permuting subfields individually but share the norm-ReLU parameters (the bias) to guarantee equivariance. In order to project to final, invariant features, we first apply a convolution producing Ind</p><formula xml:id="formula_17">O(2) SO(2) ψ SO(2) 0 fields (Ind-conv2triv).</formula><p>Since these transform like the regular representation of ({±1}, * ) ∼ = O(2)/ SO(2), we can simply apply G-pooling over the two reflections. The results, given in rows 50-53, show that these models perform significantly better than the O(2) irreps models and outperform the SO(2) irrep models on MNIST O(2). More specific details on all induced O(2) model operations are given in Appendix I.</p><p>We again build models which apply gated nonlinearities. As for SO <ref type="bibr" target="#b1">(2)</ref>, this leads to a greatly improved performance of the pure irrep models, see rows 54-55. In addition we adapt the gated nonlinearity to the induced irrep models (rows 56-57). Here we apply an independent gate to each of the two permuting sub-fields (Ind gate). In order to be equivariant, the gates need to permute under reflections as well, which is easily achieved by deriving them from Ind </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MNIST group restriction experiments</head><p>All transformed MNIST datasets clearly show local rotational and reflectional symmetries but differ in the level of symmetry present at the global scale. While the D N and O(2)-equivariant models in the last section could exploit these local symmetries, their global invariance leads to a considerable loss of information. On the other hand, models which are equivariant to the symmetries present at the global scale of the dataset only are not able to generalize over all local symmetries. The proposed group restriction operation allows for models which are locally equivariant but are globally invariant only to the level of symmetry present in the data. <ref type="table">Table 4</ref> reports the results of models which are restricted after different layers. Specifically, on MNIST rot the D 16 -equivariant model introduced in the last section is restricted to C 16 , while we restrict the D <ref type="bibr" target="#b15">16</ref> and the C 16 model to the trivial group {e} on MNIST 12k. The overall trend is that a restriction at later stages of the model improves the performance. All restricted models perform significantly better than the invariant models. <ref type="figure" target="#fig_2">Figure 2</ref> shows that this behavior is consistent for different orders N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">On the convergence of Steerable CNNs</head><p>In our experiments we find that steerable CNNs converge significantly faster than non-equivariant CNNs. <ref type="figure" target="#fig_18">Figure 3</ref> shows this behavior for the regular C N -steerable CNNs from Section 3.1 in comparison to a conventional CNN, corresponding to N = 1, on MNIST rot. The rate of convergence thereby increases with the order N and, as already observed in <ref type="figure" target="#fig_2">Figure 2</ref>, saturates at approximately N = 8. All models share approximately the same number of parameters.</p><p>The faster convergence of equivariant networks is explained by the fact that they generalize over G-transformed images by design which reduces the amount of intra-class variability which they have to learn <ref type="bibr" target="#b11">12</ref> . In contrast, a conventional CNN has to learn to classify all transformed versions of each image explicitly which requires either an increased batch size or more training iterations. The enhanced data efficiency of E(2)-steerable CNNs can therefore lead to a reduced training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Competitive MNIST rot experiments</head><p>As a final experiment on MNIST rot we are replicating the regular C 16 model from <ref type="bibr" target="#b6">[7]</ref> which was the previous SOTA. It is mostly similar to the models evaluated in the previous sections but is wider, uses larger kernel sizes and adds additional fully connected layers; see <ref type="table" target="#tab_0">Table 14</ref> in the Appendix. As reported in <ref type="table" target="#tab_10">Table 5</ref>, our reimplementation matches the accuracy of the original model. Replacing the regular feature fields with the quotient representations used in Section 3.1 leads to slightly better results. We refer to Appendix C for more insights on the improved performance of the quotient model. A further significant improvement and a new state of the art is being achieved by a D 16 -equivariant model, which is restricted to C <ref type="bibr" target="#b15">16</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CIFAR experiments</head><p>The statistics of natural images are typically invariant under global translations and reflections but are not under global rotations. Here we are investigating the practical benefit of G-steerable convolutions for such images by classifying CIFAR-10 and CIFAR-100. For this purpose we implement several D N and C N -equivariant versions of WideResNet <ref type="bibr" target="#b40">[41]</ref>. Different levels of equivariance, stated in the model specifications in <ref type="table" target="#tab_11">Table 6</ref>, are thereby used in the three main blocks of the network (i.e. between pooling layers). Regular representations are used throughout the whole model except for the last convolution which maps to a scalar field to produce invariant predictions. For a fair comparison we scale the width of all layers such that the number of parameters of the original wrn28/10 model is approximately preserved. Note that, due to their enhanced parameter efficiency, our models become wider than conventional CNNs. Since this implies a higher computational cost, we add an equivariant model, marked by an additional *, which has about the same number of channels as the non-equivariant wrn28/10. For rotation order N = 8 we are further using 5 × 5 kernels to mitigate the discretization artifacts of steering 3 × 3 kernels by 45 degrees. All runs use the same training procedure as reported in <ref type="bibr" target="#b40">[41]</ref> and Appendix H.3. We want to emphasize in particular that we perform no further hyperparameter tuning.</p><p>The results of the D 1 D 1 D 1 model in <ref type="table" target="#tab_11">Table 6</ref> confirm that incorporating the global symmetries of the data already yields a significant boost in accuracy. Interestingly, the C 8 C 4 C 1 model, which is purely rotation but not reflection-equivariant, achieves better results, which shows that it is worthwhile to leverage local rotational symmetries. Both symmetries are respected simultaneously by the wrn28/10 D 8 D 4 D 1 model. While this model performs better than the two previous ones on CIFAR-10, it surprisingly yields slightly worse result on CIFAR-100. This might be due to the higher dimensionality of its feature fields which, despite the model having more channels in total, leads to less independent fields. The best results (without using auto augment) are obtained by the D 8 D 4 D 4 model which suggests that rotational symmetries are useful even on a larger scale. The small wrn28/10* D 8 D 4 D 1 model shows a remarkable gain compared to the non-equivariant wrn28/10 baseline despite not being computationally more expensive. To investigate whether equivariance is useful even when a powerful data augmentation policy is available, we further rerun both D 8 D 4 D 1 models with AutoAugment (AA) <ref type="bibr" target="#b41">[42]</ref>. As without AA, both the computationally cheap wrn28/10* model and the wider wrn28/10 version outperform the wrn28/10 baseline by a large margin.  In order to test whether the previous results generalize to natural images of higher resolution we run additional experiments on STL-10 <ref type="bibr" target="#b43">[44]</ref>. While this dataset was originally intended for semi-supervised learning tasks, its 5000 training images are also being used for supervised classification in the low data regime <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">STL-10 experiments</head><p>We adapt the experiments in <ref type="bibr" target="#b42">[43]</ref> by replacing the nonequivariant convolutions of their wrn16/8 model, which was the previous supervised SOTA, with D N -steerable convolutions. As in the CIFAR experiments, all intermediate features transform according to regular representations. A final, invariant prediction is generated via a convolution to scalar fields. We are again using steerable convolutions as a mere drop-in replacement, that is, we use the same training setting and hyperparameters as in the original paper. The four adapted models, reported in <ref type="table" target="#tab_13">Table 7</ref>, are equivariant under either the action of D 1 in all blocks or the actions of D 8 , D 4 and D 1 in the respective blocks. For both choices we build a large model, whose width is scaled up to approximately match the number of parameters of the baseline, and a small model, which preserves the number of channels and thus compute and memory requirements, but is more parameter efficient.  As expected, all models improve significantly over the baseline with larger models outperforming smaller ones. However, due to their extended equivariance, the small D 8 D 4 D 1 model performs better than the large D 1 D 1 D 1 model. In comparison to the CIFAR experiments, rotational equivariance seems to give a more significant boost in accuracy. This is expected since the higher resolution of 96 × 96 pixels of the STL-10 images allows for more detailed local patterns which occur in arbitrary orientations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>A wide range of rotation-and reflection-equivariant models has been proposed in the recent years.</p><p>In this work we presented a general theory of E(2)-equivariant steerable CNNs which describes many of these models in one unified framework. By analytically solving the kernel constraint for any representation of O(2) or its subgroups we were able to reproduce and systematically compare these models. We further proposed a group restriction operation which allows us to adapt the level of equivariance to the symmetries present on the corresponding length scale. When using G-steerable convolutions as drop in replacement for conventional convolution layers we obtained significant improvements on CIFAR-10, CIFAR-100 and STL-10 without additional hyperparameter tuning. While the kernel expansion leads to a small overhead during train time, the final kernels can be stored such that during test time steerable CNNs are computationally not more expensive than conventional CNNs of the same width. Due to the enhanced parameter efficiency of equivariant models it is a common practice to adapt the model width to match the parameter cost of conventional CNNs. Our results show that even non-scaled models outperform conventional CNNs in accuracy. Since steerable CNNs converge faster than non-equivariant CNNs, they can even be cheaper to train.</p><p>We believe that equivariant CNNs will in the long term become the default choice for tasks like biomedical imaging, where symmetries are present on a global scale. The impressive results on natural images demonstrate the great potential of applying E(2)-steerable CNNs to more general vision tasks which involve only local symmetries. Future research still needs to investigate the wide range of design choices of steerable CNNs in more depth and collect evidence on whether our findings generalize to different settings. We hope that our library <ref type="bibr" target="#b12">13</ref> will help equivariant CNNs to be adopted by the community and facilitate further research. The E(2)-equivariant steerable CNNs considered in this work were derived in the classical framework of steerable CNNs on Euclidean spaces R d (or more general homogeneous spaces) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. This formulation considers active transformations of signals, in our case translations, rotations and reflections of images. Specifically, an active transformation by a group element tg ∈ (R 2 , +) G moves signal values from x to g −1 (x − t); see Eq. (1) and <ref type="figure" target="#fig_8">Figure 5</ref>, top left. The proven equivariance properties of the proposed E(2)-equivariant steerable CNNs guarantee the specified transformation behavior of the feature spaces under such active transformations. However, our derivations so far don't prove any equivariance guarantees for local, independent rotations or reflections of small patches in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General E(2) -Equivariant Steerable CNNs Appendix</head><p>The appropriate framework for analyzing local transformations is given by Gauge Equivariant Steerable CNNs <ref type="bibr" target="#b4">[5]</ref>. In contrast to active transformations, Gauge Equivariant CNNs consider passive gauge transformations; see <ref type="figure" target="#fig_8">Figure 5</ref>, right. Adapted to our specific setting, each feature vector f (x) is being expressed relative to a local reference frame (or gauge) e 1 (x), e 2 (x) at x ∈ R 2 . A gauge transformation formalizes a change of local reference frames by the action of position dependent elements g(x) of the gauge group (or structure group), in our case rotations and reflections in G ≤ O(2). Since gauge transformations act independently on each position, they model independent transformation of local patches in an image. As derived in <ref type="bibr" target="#b4">[5]</ref>, the demand for local gauge equivariance results in the same kernel constraint as in Eq. (2). This implies that our models are automatically locally gauge equivariant <ref type="bibr" target="#b13">14</ref> .</p><p>More generally, the kernel constraint (2) applies to arbitrary 2-dimensional Riemannian manifolds M with structure groups G ≤ O(2). The presented solutions of the kernel space constraint therefore describe spherical CNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> or convolutional networks on triangulated meshes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> for different choices of structure groups and group representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B A short primer on group representation theory</head><p>Linear group representations model abstract algebraic group elements via their action on some vector space, that is, by representing them as linear transformations (matrices) on that space. Representation theory forms the backbone of Steerable CNNs since it describes the transformation law of their feature spaces. It is furthermore widely used to describe fields and their transformation behavior in physics.</p><p>Formally, a linear representation ρ of a group G on a vector space (representation space) R n is a group homomorphism from G to the general linear group GL(R n ) (the group of invertible n × n matrices), i.e. it is a map</p><formula xml:id="formula_18">ρ : G → GL(R n ) such that ρ(g 1 g 2 ) = ρ(g 1 )ρ(g 2 ) ∀g 1 , g 2 ∈ G .</formula><p>The requirement to be a homomorphism, i.e. to satisfy ρ(g 1 g 2 ) = ρ(g 1 )ρ(g 2 ), ensures the compatibility of the matrix multiplication ρ(g 1 )ρ(g 2 ) with the group composition g 1 g 2 which is necessary for a well defined group action. Note that group representations do not need to model the group faithfully (which would be the case for an isomorphism instead of a homomorphism).</p><p>A simple example is the trivial representation ρ : G → GL(R) which maps any group element to the identity, i.e. ∀g ∈ G ρ(g) = 1. The 2-dimensional rotation matrices ψ(θ) = cos (θ) sin (θ) sin (θ) cos (θ) are an example of a representation of SO(2) (whose elements are identified by a rotation angle θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equivalent representations</head><p>Two representations ρ and ρ on R n are called equivalent iff they are related by a change of basis Q ∈ GL(R n ), i.e. ρ (g) = Qρ(g)Q −1 for each g ∈ G. Equivalent representations behave similarly since their composition is basis independent as seen by</p><formula xml:id="formula_19">ρ (g 1 )ρ (g 2 ) = Qρ(g 1 )Q −1 Qρ(g 2 )Q −1 = Qρ(g 1 )ρ(g 2 )Q −1 .</formula><p>Direct sums Two representations can be combined by taking their direct sum. Given representations</p><formula xml:id="formula_20">ρ 1 : G → GL(R n ) and ρ 2 : G → GL(R m ), their direct sum ρ 1 ⊕ρ 2 : G → GL(R n+m ) is defined as (ρ 1 ⊕ ρ 2 )(g) = ρ 1 (g) 0 0 ρ 2 (g) ,</formula><p>i.e. as the direct sum of the corresponding matrices. Its action is therefore given by the independent actions of ρ 1 and ρ 2 on the orthogonal subspaces R n and R m in R n+m . The direct sum admits an obvious generalization to an arbitrary number of representations ρ i :</p><formula xml:id="formula_21">i ρ i (g) = ρ 1 (g) ⊕ ρ 2 (g) ⊕ . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Irreducible representations</head><p>The action of a representation might leave a subspace of the representation space invariant. If this is the case there exists a change of basis to an equivalent representation which is decomposed into the direct sum of two independent representations on the invariant subspace and its orthogonal complement. A representation is called irreducible if no non-trivial invariant subspace exists.</p><p>Any representation ρ : G → R n of a compact group G can therefore be decomposed as</p><formula xml:id="formula_22">ρ(g) = Q i∈I ψ i (g) Q −1</formula><p>where I is an index set specifying the irreducible representations ψ i contained in ρ and Q is a change of basis. In proofs it is therefore often sufficient to consider irreducible representations which we use in Section 2.4 to solve the kernel constraint.</p><p>Regular and quotient representations A commonly used representation in equivariant deep learning is the regular representation. The regular representation of a finite group G acts on a vector space R |G| by permuting its axes. Specifically, associating each axis e g of R |G| to an element g ∈ G, the representation of an elementg ∈ G is a permutation matrix which maps e g to eg g . For instance, the regular representation of the group C 4 with elements {p π 2 |p = 0, . . . , 3} is instantiated by: </p><formula xml:id="formula_23">φ 0 π 2 π 3π 2 ρ C4 reg (φ)      1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1           0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0           0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0           0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0      A vector v = g v g e g in R |G| can be interpreted as a scalar function v : G → R, g → v g on G. Since ρ(h)v = g v g e hg = g v h −</formula><formula xml:id="formula_24">Res G H (ρ) : H → GL(R n ), h → ρ H (h)</formula><p>Induced Representations Instead of restricting a representation from a group G to a subgroup H ≤ G, it is also possible to induce a representation of H to a representation of G. In order to keep the presentation accessible we will first only consider the case of finite groups G and H.</p><p>Let ρ : H → GL(R n ) be any representation of a subgroup H of G. The induced representation Ind G H (ρ) is then defined on the representation space R n|G|/|H| which can be seen as one copy of R n for each of the |G|/|H| cosets gH in the quotient set G/H. For the definition of the induced representation it is customary to view this space as the tensor product R |G|/|H| ⊗ R n and to write vectors in this space as 15</p><formula xml:id="formula_25">w = gH e gH ⊗ w gH ∈ R n |G| |H| ,<label>(7)</label></formula><p>where e gH is a basis vector of R |G|/|H| , associated to the coset gH, and w gH is some vector in the representation space R n of ρ. Intuitively, Ind G H (ρ) acts on R n|G|/|H| by i) permuting the |G|/|H| subspaces associated to the cosets gH and ii) acting on each of these subspaces via ρ.</p><p>To formalize this intuition, note that any element g ∈ G can be identified by the coset gH to which it belongs and an element h(g) ∈ H which specifies its position within this coset. Hereby h : G → H expresses g relative to an arbitrary representative 16 R(gH) ∈ G of gH and is defined as h(g) := R(gH) −1 g from which it immediately follows that g is decomposed relative to R as</p><formula xml:id="formula_26">g = R(gH)h(g) .<label>(8)</label></formula><p>The action of an elementg ∈ G on a coset gH ∈ G/H is naturally given byggH ∈ G/H. This action defines the aforementioned permutation of the n-dimensional subspaces in R n|G|/|H| by sending e gH in Eq. <ref type="bibr" target="#b6">(7)</ref> to eg gH . Each of the n-dimensional, translated subspacesggH, is in addition transformed by the action of ρ h(gR(gH)) . This H-component h(gR(gH)) = R(ggH) −1g R(gH) of thẽ g action within the cosets accounts for the relative choice of representatives R(ggH) and R(gH).</p><p>Overall, the action of Ind G H (ρ(g)) is given by</p><formula xml:id="formula_27">Ind G H ρ (g) gH e gH ⊗ w gH := gH eg gH ⊗ ρ h(gR(gH)) w gH ,<label>(9)</label></formula><p>which can be visualized as: Note that a vector in R |G|/|H| ⊗ R n is in one-to-one correspondence to a function f : G/H → R n . The induced representation can therefore equivalently be defined as acting on the space of such functions as 17</p><formula xml:id="formula_28">Ind G H ρ(g) ·              . . . w gH . . . . . . . . .              =              . . . . . . . . . ρ(h(gR(gH)))w gH . . .             </formula><formula xml:id="formula_29">[Ind G H ρ(g) · f ](gH) = ρ(h(gR(g −1 gH)))f (g −1 gH) .<label>(10)</label></formula><p>This definition generalizes to non-finite groups where the quotient space G/H is not necessarily finite anymore.</p><p>For the special case of semidirect product groups G = N H it is possible to choose representatives of the cosets gH such that the elements h(gR(g H)) = h(g) become independent of the cosets <ref type="bibr" target="#b2">[3]</ref>. This simplifies the action of the induced representation to </p><formula xml:id="formula_30">[Ind G H ρ(g) · f ](gH) = ρ(h(g)) f (g −1 gH)<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C An intuition for quotient representation fields</head><p>The quotient representations of C N in rows 11-15 of <ref type="table" target="#tab_7">Table 3</ref> and in <ref type="table" target="#tab_10">Table 5</ref> are all of the form ρ In our experiments we mostly used quotients by C 2 and C 4 since we assumed the corresponding symmetric patterns ( | and +) to be most frequent in MNIST. As hypothesized, our model, which uses the representations 5ρ reg ⊕ 2ρ <ref type="bibr" target="#b15">16</ref> , improves slightly upon a purely regular model with the same number of parameters, see <ref type="table" target="#tab_10">Table 5</ref>. By mixing regular, quotient and trivial <ref type="bibr" target="#b18">19</ref> representations, our model keeps a certain level of expressiveness in its feature fields but incorporates a-priori known symmetries and compresses the model.</p><formula xml:id="formula_31">C 16/C 2 quot ⊕ 2ρ C 16/C 4 quot ⊕ 4ψ 0 of C</formula><p>We want to emphasize that quotient representations are expected to severely harm the model performance if the assumed symmetry does not actually exist in the data or is unimportant for the inference. Since the space of possible quotient representations and their multiplicities is very large, it might be necessary to apply some form of neural architecture search to find beneficial combinations. As a default choice we recommend the user to work with regular representations.</p><p>Further, note that the intuition given above is specific for the case of quotient representations ρ G/N quot where N G is a normal subgroup (which is always the case for C N ). Since normal subgroups imply gN = N g ∀g ∈ G by definition, the action of the quotient representation by any element n ∈ N is given by ρ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Equivariance of E(2) -steerable CNNs D.1 Equivariance of E(2) -steerable convolutions</head><p>Assume two feature fields f in : R 2 → R cin of type ρ in and f out : R 2 → R cout of type ρ out to be given. Under actions of the Euclidean group these fields transform as</p><formula xml:id="formula_32">f in (x) → Ind (R 2 ,+) G G ρ in (gt)f in (x) := ρ in (g) f in g −1 (x − t) f out (x) → Ind (R 2 ,+) G G ρ out (gt)f out (x) := ρ out (g)f out g −1 (x − t) .</formula><p>18 Or more generally, any possible pattern. <ref type="bibr" target="#b18">19</ref> Trivial representations ψ 0 ∼ = ρ G/G quot can themself be seen as an extreme case of quotient representations which are invariant to the full group G.</p><p>Here we show that the G-steerability (2) of convolution kernels is sufficient to guarantee the equivariance of the mapping. We therefore define the convolution (or correlation) operation of a feature field with a G-steerable kernel k : R 2 → R cout×cin as usual by</p><formula xml:id="formula_33">f out (x) := (k * f in ) (x) = R 2 k(y)f in (x + y) dy .</formula><p>The convolution with a transformed input field then gives</p><formula xml:id="formula_34">R 2 dy k(y) Ind (R 2 ,+) G G ρ in (gt) f in (x + y) = R 2 dy k(y)ρ in (g)f in g −1 (x + y − t) = R 2 dy ρ out (g)k(g −1 y)ρ in (g) −1 ρ in (g)f in g −1 (x + y − t) = ρ out (g) R 2 dỹ k(ỹ)f in g −1 (x − t) +ỹ = ρ out (g)f out g −1 (x − t) = Ind (R 2 ,+) G G ρ out (gt)f out (x) ,</formula><p>i.e. it satisfies the desired equivariance condition k * Ind</p><formula xml:id="formula_35">(R 2 ,+) G G ρ in (gt)f in = Ind (R 2 ,+) G G ρ out (gt) k * f in .</formula><p>We used the kernel steerability (2) in the second step to identify k(x) with ρ out (g)k(g −1 x)ρ in (g −1 ). In the third step we substitutedỹ = g −1 y which does not affect the integral measure since det ∂y ∂ỹ = |det(g)| = 1 for an orthogonal transformation g ∈ G.</p><p>A proof showing the G-steerability of the kernel to not only be sufficient but necessary is given in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Equivariance of spatially localized nonlinearities</head><p>We consider nonlinearities of the form</p><formula xml:id="formula_36">σ : R cin → R cout , f (x) → σ f (x) ,</formula><p>which act spatially localized on feature vectors f (x) ∈ R cin . These localized nonlinearities are used to define nonlinearitiesσ acting on entire feature fields f : R 2 → R cin by mapping each feature vector individually, that is,</p><formula xml:id="formula_37">σ : f →σ(f ) such thatσ(f )(x) := σ f (x) .</formula><p>In order forσ to be equivariant under the action of induced representations it is sufficient to require</p><formula xml:id="formula_38">σ • ρ in (g) = ρ out (g) • σ ∀g ∈ G since thenσ Ind (R 2 ,+) G G ρ in (gt)f (x) = σ ρ in (g)f (g −1 (x − t)) = ρ out (g)σ f (g −1 (x − t)) = ρ out (g)σ f (g −1 (x − t)) = Ind (R 2 ,+) G G ρ out σ(f )(x) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1 Equivariance of individual subspace nonlinearities w.r.t. direct sum representations</head><p>The feature spaces of steerable CNNs comprise multiple feature fields f i : R 2 → R cin,i which are concatenated into one big feature field f :</p><formula xml:id="formula_39">R 2 → R i cin,i defined by f := i f i . By definition f (x) transforms under ρ in = i ρ in,i if each f i (x) transforms under ρ in,i . If σ i : R cin,i → R cout,i is an equivariant nonlinearity satisfying σ i • ρ in,i (g) = ρ out,i (g) • σ i for all g ∈ G, then i σ i • i ρ in,i (g) = i ρ out,i (g) • i σ i ∀g ∈ G,</formula><p>i.e. the concatenation of feature fields respects the equivariance of the individual nonlinearities. Here we defined i σ i : R i cin,i → R i cout,i as acting individually on the corresponding</p><formula xml:id="formula_40">f i (x) in f (x), that is, ( i σ i ) ( i f i (x)) = i (σ i • proj i ) (f (x)).</formula><p>To proof this statement consider without loss of generality the case of two feature fields f 1 and f 2 with corresponding representations ρ 1 and ρ 2 and equivariant nonlinearities σ 1 and σ 2 . Then it follows for all g ∈ G that</p><formula xml:id="formula_41">σ 1 ⊕ σ 2 (ρ 1,in ⊕ ρ 2,in )(g) f 1 ⊕ f 2 = σ 1 (ρ 1 (g) f 1 ) ⊕ σ 2 (ρ 2 (g) f 2 ) = ρ 1 (g) σ 1 (f 1 ) ⊕ ρ 2 (g) σ 2 (f 2 ) = (ρ 1,out ⊕ ρ 2,out )(g) σ 1 ⊕ σ 2 f 1 ⊕ f 2 .</formula><p>The general case follows by induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 Equivariance of norm nonlinearities w.r.t. unitary representations</head><p>We define unitary representations to preserve the norm of feature vectors, i.e. ρ iso (g)f (</p><formula xml:id="formula_42">x) = f (x) ∀g ∈ G. Norm nonlinearities are functions of the type σ norm f (x) := η |f (x)| f (x)</formula><p>|f (x)| , where η : R ≥0 → R is some nonlinearity acting on norm of a feature vector. Since norm nonlinearities preserve the orientation of feature vectors they are equivariant under the action of unitary representations:</p><formula xml:id="formula_43">σ norm ρ iso (g)f (x) = η ρ iso (g)f (x) ρ iso (g)f (x) ρ iso (g)f (x) = η f (x) ρ iso (g)f (x) f (x) = ρ iso (g)σ norm f (x) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.3 Equivariance of pointwise nonlinearities w.r.t. regular and quotient representations</head><p>Quotient representations act on feature vectors by permuting their entries according to the group composition as defined by ρ G/H quot (g)e gH := eg gH . The permutation of vector entries commutes with pointwise nonlinearities σ : R → R which are being applied to each entry of a feature vector individually: Expressing the feature vector as f (x) = gH e gH ⊗ f gH (x) this is seen by:</p><formula xml:id="formula_44">σ pt ρ G/H quot (g) f (x) = σ pt   ρ G/H quot (g) gH∈G/H f gH (x) e gH   = σ pt   gH∈G/H f gH (x) eg gH   = gH∈G/H σ pt f gH (x) eg gH = ρ G/H quot (g) gH∈G/H σ pt f gH (x) e gH = ρ G/H quot (g)σ pt f (x) .</formula><formula xml:id="formula_45">σ Ind G H ρ in (g) f (x) =σ   Ind G H ρ in (g) gH∈G/H e gH ⊗ f gH (x)   =σ   gH∈G/H Ind G H ρ in (g) (e gH ⊗ f gH (x))   =σ   gH∈G/H eg gH ⊗ ρ in (h(gr(gH))f gH (x)   = gH∈G/H eg gH ⊗ σ ρ in (h(gr(gH))f gH (x) = gH∈G/H eg gH ⊗ ρ out (h(gr(gH))σ f gH (x) = Ind G H ρ out (g) gH∈G/H e gH ⊗ σ (f gH (x)) = Ind G H ρ out (g)σ f (x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Visualizations of the irrep kernel constraint</head><p>The irrep kernel constraint</p><formula xml:id="formula_46">κ(gx) = i∈Iout ψ i (g) κ(x) j∈Iin ψ −1 j (g) ∀g ∈ G, x ∈ R 2 decomposes into independent constraints κ ij (gx) = ψ i (g) κ ij (x) ψ −1 j (g) ∀g ∈ G, x ∈ R 2 where i ∈ I out , j ∈ I in ,</formula><p>on invariant subspaces corresponding to blocks κ ij of κ. This is the case since the direct sums of irreps on the right hand side are block diagonal:</p><formula xml:id="formula_47">    κ i1j1 (gx) κ i1j2 (gx) . . . κ i2j1 (gx) κ i2j2 (gx) . . . . . . . . . . . .     κ(gx) =     ψ i1 (g) ψ i2 (g) . . .     i∈Iout ψ i (g) ·     κ i1j1 (x) κ i1j2 (x) . . . κ i2j1 (x) κ i2j2 (x) . . . . . . . . . . . .     κ(x) ·     ψ 1 j1 (g) ψ 1 j2 (g) . . .     j∈Iin ψ 1 j (g)</formula><p>A basis κ ij 1 , · · · , κ ij dij for the space of G-steerable kernels satisfying the independent constraints (3) on κ ij contributes to a part of the full basis</p><formula xml:id="formula_48">k 1 , · · · , k d := i∈Iout j∈Iin Q −1 out κ ij 1 Q in , · · · , Q −1 out κ ij dij Q in .<label>(12)</label></formula><p>of G-steerable kernels satisfying the original constraint (2). Here we defined a zero-padded block</p><formula xml:id="formula_49">κ ij :=     0 0 0 0 κ ij 0 0 0 0     .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Solutions of the kernel constraints for irreducible representations</head><p>In this section we are deriving analytical solutions of the kernel constraints</p><formula xml:id="formula_50">κ ij (gx) = ψ i (g) κ ij (x) ψ −1 j (g) ∀g ∈ G, x ∈ R 2<label>(13)</label></formula><p>for irreducible representations ψ i of O(2) and its subgroups. The linearity of the constraint implies that the solution space of G-steerable kernels forms a linear subspace of the unrestricted kernel space k ∈ L 2 R 2 cout×cin of square integrable functions k : R 2 → R cout×cin .</p><p>Section F.1 introduces the conventions, notation and basic properties used in the derivations. Since our numerical implementation is on the real field we are considering real-valued irreps. It is in general possible to derive all solutions considering complex valued irreps of G ≤ O <ref type="bibr" target="#b1">(2)</ref>. While this approach would simplify some steps it comes with an overhead of relating the final results back to the real field which leads to further complications, see Appendix F.5. An overview over the real-valued irreps of G ≤ O(2) and their properties is given in Section F.2.</p><p>We present the analytical solutions of the irrep kernel constraints for all possible pairs of irreps in Section F.3. Specifically, the solutions for SO(2) are given in <ref type="table" target="#tab_15">Table 8</ref> while the solutions for O(2), ({±1}, * ), C N and D N are given in <ref type="table" target="#tab_16">Table 9</ref>, <ref type="table" target="#tab_0">Table 10</ref>, <ref type="table" target="#tab_0">Table 11</ref> and <ref type="table" target="#tab_0">Table 12</ref> respectively.</p><p>Our derivation of the irrep kernel bases is motivated by the observation that the irreps of O(2) and subgroups are harmonics, that is, they are associated to one particular angular frequency. This suggests that the kernel constraint (13) decouples into simpler constraints on individual Fourier modes. In the derivations, presented in Section F.4, we are therefore defining the kernels in polar coordinates x = x(r, φ) and expand them in terms of an orthogonal, angular, Fourier-like basis. A projection on this orthogonal basis then yields constraints on the expansion coefficients. Only specific coefficients are allowed to be non-zero; these coefficients parameterize the complete space of G-steerable kernels satisfying the irrep constraint <ref type="bibr" target="#b12">(13)</ref>. The completeness of the solution follows from the completeness of the orthogonal basis.</p><p>We start with deriving the bases for the simplest cases SO <ref type="formula" target="#formula_1">(2)</ref>  We denote a 2×2 orthonormal matrix with positive determinant, i.e. rotation matrix for an angle θ, by:</p><formula xml:id="formula_51">ψ(θ) = cos (θ) sin (θ) sin (θ) cos (θ)</formula><p>We define the orthonormal matrix with negative determinant corresponding to a reflection with respect to the horizontal axis as: Moreover, these properties will be useful later:</p><formula xml:id="formula_52">ψ(θ)ξ(s) = ξ(s)ψ(sθ) (14) ξ(s) −1 = ξ(s) T = ξ(s) (15) ψ(θ) −1 = ψ(θ) T = ψ(−θ) (16) ψ(θ 1 )ψ(θ 2 ) = ψ(θ 1 + θ 2 ) = ψ(θ 2 )ψ(θ 1 )<label>(17)</label></formula><p>tr(ψ(θ)ξ(−1)) = tr cos (θ) sin (θ) sin (θ) cos (θ) = 0 <ref type="bibr" target="#b17">(18)</ref> tr(ψ(θ)) = tr cos (θ) sin (θ) sin (θ) cos (θ) = 2 cos(θ)</p><formula xml:id="formula_53">(19) w 1 cos(α) + w 2 sin(α) = w 1 cos(β) + w 2 sin(β) ∀w 1 , w 2 ∈ R ⇔ ∃t ∈ Z s.t. α = β + 2tπ (20)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Irreducible representations of G≤O(2)</head><p>Special Orthogonal group SO(2): SO(2) irreps would decompose into complex irreps of U(1) on the complex field but since we are implementing the theory with real-valued variables we will not consider these. Except for the trivial representation ψ 0 , all the other irreps are 2-dimensional rotation matrices with frequencies k ∈ N + . 27</p><formula xml:id="formula_54">-ψ C N N/2 (r θ ) = cos N 2 θ ∈ {±1} since θ ∈ {p 2π N } N −1 p=0</formula><p>Dihedral groups D N : Similarly, D N consists of irreps of O(2) up to frequency N/2 .</p><formula xml:id="formula_55">-ψ D N 0,0 (r θ s) = 1 -ψ D N 1,0 (r θ s) = s -ψ D N 1,k (r θ s) = cos (kθ) sin (kθ) sin (kθ) cos (kθ) 1 0 0 s = ψ(kθ)ξ(s), k ∈ {1, . . . , N −1 2 }</formula><p>If N is even, there are two 1-dimensional irreps:     Cyclic groups C N ψ m ψ n ψ 0 ψ N/2 (if N even) ψ n with n ∈ N + and 1 ≤ n &lt; N/2</p><formula xml:id="formula_56">-ψ D N 0,N/2 (r θ s) = cos N 2 θ ∈ {±1} since θ ∈ {p 2π N } N −1 p=0 -ψ D N 1,N/2 (r θ s) = s cos N 2 θ ∈ {±1} since θ ∈ {p 2π N } N −1</formula><formula xml:id="formula_57">ψ i,m ψ j,n ψ 0,0 ψ 1,0 ψ 1,n , n ∈ N + ψ 0,0 1 ∅ sin(nφ) cos(nφ) ψ 1,0 ∅ 1 cos(nφ) sin(nφ)</formula><formula xml:id="formula_58">ψ i ψ j ψ 0 ψ 1 ψ 0 cos µ(φ − β) sin µ(φ − β) ψ 1 sin µ(φ − β) cos µ(φ − β)</formula><formula xml:id="formula_59">ψ 0 cos(tN φ) , sin(tN φ) cos t + 1 2 N φ , sin t + 1 2 N φ sin((n + tN )φ) cos((n + tN )φ) , cos((n + tN )φ) sin((n + tN )φ) ψ N/2 (N even) cos t + 1 2 N φ , sin t + 1 2 N φ cos(tN φ) , sin(tN φ) sin n + t+ 1 2 N φ cos n + t+ 1 2 N φ , cos n + t+ 1 2 N φ sin n + t+ 1 2 N φ ψ m , m ∈ N + 1 ≤ m &lt; N/2 sin((m + tN )φ) cos((m + tN )φ)</formula><p>,     <ref type="table" target="#tab_0">Table 12</ref>: Bases for the angular parts of DN -steerable kernels for different pairs of input and output fields irreps ψj,n and ψi,m. The full basis is found by instantiating these solutions for each t ∈ Z ort ∈ N. The different types of irreps are explained in Appendix F.2. The solutions here shown are for a group action where the reflection is defined around the horizontal axis (β = 0). For different axes β = 0 substitute φ with φ − β.</p><formula xml:id="formula_60">cos((m + tN )φ) sin((m + tN )φ) sin m + t+ 1 2 N φ cos m + t+ 1 2 N φ , cos m + t+ 1 2 N φ sin m + t+ 1 2 N φ cos (m−n + tN)φ sin (m−n + tN)φ sin (m−n + tN)φ cos (m−n + tN)φ ,</formula><formula xml:id="formula_61">ψ 1,0 sin(tN φ) cos(tN φ) sin t + 1 2 N φ cos t + 1 2 N φ cos ((n + tN )φ) sin ((n + tN )φ) ψ 0,N/2 (N even) cos t + 1 2 N φ sin t + 1 2 N φ cos(tN φ) sin(tN φ) sin n + t+ 1 2 N φ cos n + t+ 1 2 N φ ψ 1,N/2 (N even) sin t + 1 2 N φ cos t + 1 2 N φ sin(tN φ) cos(tN φ) cos n + t+ 1 2 N φ sin n + t+ 1 2 N φ ψ 1,m , m ∈ N + 1 ≤ m &lt; N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Derivations of the kernel constraints</head><p>Here we solve the kernel constraints for the irreducible representations of G ≤ O(2). Since the irreps of G are either 1-or 2-dimensional, we distinguish between mappings between 2-dimensional irreps, mappings from 2-to 1-dimensional and 1-to 2-dimensional irreps and mappings between 1-dimensional irreps. We are first exclusively considering positive radial parts r &gt; 0 in the following sections. The constraint at the origin r = 0 requires some additional considerations which we postpone to Section F.4.6.</p><p>F.4.1 Derivation for SO <ref type="formula" target="#formula_1">(2)</ref> 2-dimensional irreps:</p><p>We first consider the case of 2-dimensional irreps both in the input and in output, that is, ρ out = ψ The last four matrices are equal to the first four, except for their opposite frequency. Moreover, the second matrices of each row are equal to the first matrices, with a phase shift of π 2 added. Therefore, we can as well write: </p><formula xml:id="formula_62">κ(r,φ) = ∞ µ= ∞ γ∈{0, π 2 } w 0,</formula><formula xml:id="formula_63">(φ) = ψ(µφ + γ)ξ(s) µ ∈ Z, γ ∈ 0, π 2 , s ∈ ({±1}, * )<label>(21)</label></formula><p>of the unrestricted kernel space which we will constrain in the following by demanding</p><formula xml:id="formula_64">κ(φ + θ) = ψ SO(2) m (r θ )κ(φ)ψ SO(2) n (r θ ) −1 ∀φ, θ ∈ [0, 2π),<label>(22)</label></formula><p>where we dropped the unrestricted radial part.</p><p>We solve for a basis of the subspace satisfying this constraint by projecting both sides on the basis elements defined above. The inner product on L 2 S 1 2×2 is hereby defined as</p><formula xml:id="formula_65">k 1 , k 2 = 1 4π dφ k 1 (φ), k 2 (φ) F = 1 4π dφ tr k 1 (φ) T k 2 (φ) ,</formula><p>where ·, · F denotes the Frobenius inner product between 2 matrices.</p><p>First consider the projection of the lhs of the kernel constraint <ref type="formula" target="#formula_1">(22)</ref>  </p><formula xml:id="formula_66">b µ ,γ ,s , R θ κ = 1 4π dφ tr b µ ,γ ,s (φ) T (R θ κ) (φ) = 1 4π dφ tr b µ ,γ ,s (φ) T κ(φ + θ) .</formula><p>By expanding the kernel in the linear combination of the basis we further obtain</p><formula xml:id="formula_67">= 1 4π dφ tr b µ ,γ ,s (φ) T µ γ s w s,γ,µ ψ (µ(φ + θ) + γ) ξ(s) ,</formula><p>which, observing that the trace, sums and integral commute, results in:</p><formula xml:id="formula_68">= 1 4π µ γ s w s,γ,µ tr dφ b µ ,γ ,s (φ) T ψ (µ(φ + θ) + γ) ξ(s) = 1 4π µ γ s w s,γ,µ tr dφ (ψ(µ φ + γ )ξ(s )) T ψ (µ(φ + θ) + γ) ξ(s) = 1 4π µ γ s w s,γ,µ tr dφ ξ(s ) T ψ(µ φ + γ ) T ψ (µ(φ + θ) + γ) ξ(s)</formula><p>Using the properties in Eq. (15) and (16) then yields:</p><formula xml:id="formula_69">= 1 4π µ γ s w s,γ,µ tr dφ ξ(s )ψ(−µ φ − γ )ψ (µ(φ + θ) + γ) ξ(s) = 1 2 µ γ s w s,γ,µ tr ξ(s )ψ(γ − γ ) 1 2π dφ ψ((µ − µ )φ) ψ(µθ)ξ(s)</formula><p>In the integral, each cell of the matrix ψ((µ − µ )φ) contains either a sine or cosine. As a result, if µ − µ = 0, all these integrals evaluate to 0. Otherwise, the cosines on the diagonal evaluate to 1, while the sines integrate to 0. The whole integral evaluates to δ µ,µ id 2×2 , such that Recall the propetries of the trace in Eq. (18), <ref type="bibr" target="#b18">(19)</ref>. If s * s = −1, i.e. s = s, the matrix has a trace of 0:</p><formula xml:id="formula_70">= 1 2 γ s w s,γ,µ δ s ,s 2 cos(s (γ − γ + µ θ))</formula><p>Since cos(−α) = cos(α) and s ∈ {±1}:</p><formula xml:id="formula_71">= 1 2 γ s w s,γ,µ δ s ,s 2 cos(γ − γ + µ θ) = γ w s ,γ,µ cos((γ − γ ) + µ θ)</formula><p>Next consider the projection of the rhs of Eq. <ref type="formula" target="#formula_1">(22)</ref>:</p><formula xml:id="formula_72">b µ ,γ ,s , ψ SO(2) m (r θ )κ(·)ψ SO(2) n (r θ ) −1 = 1 4π dφ tr b µ ,γ ,s (φ) T ψ SO(2) m (r θ )κ(φ)ψ SO(2) n (r θ ) −1 = 1 4π dφ tr b µ ,γ ,s (φ) T ψ(mθ)κ(φ)ψ(−nθ)</formula><p>An expansion of the kernel in the linear combination of the basis yields: or, explicitly, with γ ∈ {0, π 2 } and cos(α + π 2 ) = − sin(α):</p><formula xml:id="formula_73">= 1 4π dφ tr b µ ,γ ,s (φ) T ψ(mθ) µ γ s w s,γ,µ ψ (µφ + γ) ξ(s) ψ(−nθ) = 1 4π µ γ s w s,γ,µ tr dφ b µ ,γ ,s (φ) T ψ(mθ)ψ (µφ + γ) ξ(s)ψ(−nθ) = 1 4π µ γ s w s,γ,µ tr dφ ξ(s )ψ(−µ φ − γ )ψ(mθ)ψ (µφ + γ) ξ(s)ψ(−nθ)</formula><formula xml:id="formula_74">w s ,0,µ cos(µ θ − γ ) − w s , π 2 ,µ sin(µ θ − γ ) = w s ,0,µ cos((m − ns )θ − γ ) − w s , π 2 ,µ sin((m − ns )θ − γ ) ∀θ ∈ [0, 2π)</formula><p>Using the property in Eq. <ref type="bibr" target="#b19">(20)</ref> then implies that for each θ in [0, 2π) there exists a t ∈ Z such that:</p><formula xml:id="formula_75">⇔ µ θ − γ = (m − ns )θ − γ + 2tπ ⇔ (µ − (m − ns ))θ = 2tπ<label>(23)</label></formula><p>Since the constraint needs to hold for any θ ∈ [0, 2π) this results in the condition µ = m − sn on the frequencies occurring in the SO(2)-steerable kernel basis. Both γ and s are left unrestricted such that we end up with the four-dimensional basis</p><formula xml:id="formula_76">K SO(2) ψm←ψn = b µ,γ,s (φ) = ψ µφ + γ ξ(s) µ = (m sn), γ ∈ 0, π 2 , s ∈ {±1}<label>(24)</label></formula><p>for the angular parts of equivariant kernels for m, n &gt; 0. This basis is explicitly written out in the lower right cell of <ref type="table" target="#tab_15">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-dimensional irreps:</head><p>For the case of 1-dimensional irreps in both the input and output, i.e. ρ out = ρ in = ψ SO(2) 0 the kernel has the form κ ij : R 2 → R 1×1 . As a scalar function in L 2 (R 2 ), it can be expressed by the Fourier decomposition of its angular part:</p><formula xml:id="formula_77">κ(r, φ) = w 0,0 + ∞ µ=1 γ∈{0, π 2 } w µ,γ (r) cos(µφ + γ)</formula><p>As before, we can w.l.o.g. drop the dependency on the radial part as it is not restricted by the constraint. We are therefore considering the basis</p><formula xml:id="formula_78">b µ,γ (φ) = cos(µφ + γ) µ ∈ N, γ ∈ {0} if µ = 0 {0, π/2} otherwise<label>(25)</label></formula><p>of angular kernels in L 2 (S 1 ) 1×1 . The kernel constraint in Eq. (3) then requires</p><formula xml:id="formula_79">κ(φ + θ) = ψ SO(2) m (r θ )κ(φ)ψ SO(2) n (r θ ) −1 ∀θ, φ ∈ [0, 2π) ⇔ κ(φ + θ) = κ(φ) ∀θ, φ ∈ [0, 2π),</formula><p>i.e. the kernel has to be invariant to rotations.</p><p>Again, we find the space of all solutions by projecting both sides on the basis defined above. Here, the projection of two kernels is defined through the standard inner product k 1 ,</p><formula xml:id="formula_80">k 2 = 1 2π dφk 1 (φ)k 2 (φ) on L 2 (S 1 ).</formula><p>We first consider the projection of the lhs:</p><formula xml:id="formula_81">b µ ,γ , R θ κ = 1 2π dφ b µ ,γ (φ) (R θ κ) (φ) = 1 2π dφ b µ ,γ (φ)κ(φ + θ)</formula><p>As before we expand the kernel in the linear combination of the basis: With cos(α) cos(β) = 1 2 (cos(α − β) + cos(α + β)) this results in:</p><formula xml:id="formula_82">= µ,γ w µ,γ 1 2π dφ 1 2 cos(µ φ + γ − µφ − µθ − γ) + cos(µ φ + γ + µφ + µθ + γ) = µ,γ w µ,γ 1 2 1 2π dφ cos((µ − µ)φ + (γ − γ) − µθ) + 1 2π dφ cos((µ + µ)φ + (γ + γ) + µθ) = µ,γ w µ,γ 1 2 (δ µ,µ cos((γ − γ) − µθ) + δ µ,−µ cos((γ + γ) + µθ))</formula><p>Since µ, µ ≥ 0 and µ = −µ imply µ = µ = 0 this simplifies further to</p><formula xml:id="formula_83">= 1 2 γ w µ ,γ (cos((γ − γ) − µ θ) + δ µ ,0 cos(γ + γ)) .</formula><p>A projection of the rhs yields:</p><formula xml:id="formula_84">b µ ,γ , κ = 1 2π dφ b µ ,γ (φ)κ(φ) = µ,γ w µ,γ 1 2π dφ b µ ,γ (φ) cos(µφ + γ) = µ,γ w µ,γ 1 2π dφ cos(µ φ + γ ) cos(µφ + γ) = 1 2 γ w µ ,γ (cos(γ − γ) + δ µ ,0 cos((γ + γ)))</formula><p>The projections are required to coincide for all rotations: We consider two cases:</p><p>• µ = 0 In this case, the basis in Eq.(25) is restricted to the single case γ = 0 (as γ = π 2 and µ = 0 together lead to a null basis element). Then: γ w 0,γ (cos(−γ) + cos(γ)) = γ w 0,γ (cos(−γ) + cos(γ))</p><p>As γ ∈ {0, π 2 } and cos(± π 2 ) = 0: ⇔ w 0,0 (cos(0) + cos(0)) = w 0,0 (cos(0) + cos(0)) ⇔ w 0,0 = w 0,0 which is always true. • µ &gt; 0 Here:</p><formula xml:id="formula_85">γ w µ ,γ cos((γ − γ) − µ θ) = γ w µ ,γ cos(γ − γ) ∀θ ∈ [0, 2π)</formula><p>⇔ w µ ,0 cos(γ µ θ) + w µ , π 2 sin(γ µ θ) = w µ ,0 cos(γ ) + w µ , π 2 sin(γ ) ∀θ ∈ [0, 2π)</p><formula xml:id="formula_86">⇔ −µ θ = 2tπ ∀θ ∈ [0, 2π),</formula><p>where Eq. <ref type="bibr" target="#b19">(20)</ref> was used in the last step. From the last equation one can see that µ must be zero. Since this contradicts the assumption that µ ≥ 0, no solution exists.</p><p>This results in a one dimensional basis of isotropic (rotation invariant) kernels</p><formula xml:id="formula_87">K SO(2) ψm←ψn = b 0,0 (φ) = 1<label>(26)</label></formula><p>for m = n = 0, i.e. trivial representations. The basis is presented in the upper left cell of <ref type="table" target="#tab_15">Table 8</ref>. . The corresponding kernel κ ij : R 2 → R 2×1 can be expanded in the following generalized Fourier series on L 2 (R 2 ) 2×1 :</p><formula xml:id="formula_88">κ(r, φ) = ∞ µ=0 A 0,µ (r) cos(µφ) 0 + B 0,µ (r) sin(µφ) 0 + A 1,µ (r) 0 cos(µφ) + B 1,µ (r) 0 sin(µφ)</formula><p>As before, we perform a change of basis to produce a non-sparse basis</p><formula xml:id="formula_89">κ(r, φ) = ∞ µ=−∞ γ∈{0, π 2 } w γ,µ (r) cos(µφ + γ) sin(µφ + γ) .</formula><p>Dropping the radial parts as usual, this corresponds to the complete basis :</p><formula xml:id="formula_90">b µ,γ (φ) = cos(µφ + γ) sin(µφ + γ) µ ∈ Z, γ ∈ 0, π 2<label>(27)</label></formula><p>of angular kernels on L 2 (S 1 ) 2×1 .</p><p>The constraint in Eq. (3) requires the kernel space to satisfy</p><formula xml:id="formula_91">κ(φ + θ) = ψ SO(2) m (r θ )κ(φ)ψ SO(2) 0 (r θ ) −1 ∀θ, φ ∈ [0, 2π) ⇔ κ(φ + θ) = ψ SO(2) m (r θ )κ(φ) ∀θ, φ ∈ [0, 2π).</formula><p>We again project both sides of this equation on the basis elements defined above where the projection on L 2 (S 1 ) 2×1 is defined by k 1 ,</p><formula xml:id="formula_92">k 2 = 1 2π dφ k 1 (φ) T k 2 (φ).</formula><p>Consider first the projection of the lhs</p><formula xml:id="formula_93">b µ ,γ , R θ κ = 1 2π dφ b µ ,γ (φ) T (R θ κ) (φ) = 1 2π dφ b µ ,γ (φ) T κ(φ + θ) ,</formula><p>which, after expanding the kernel in terms of the basis reads:</p><formula xml:id="formula_94">= µ,γ w µ,γ 1 2π dφ b µ ,γ (φ) T cos(µ(φ + θ) + γ) sin(µ(φ + θ) + γ) = µ,γ w µ,γ 1 2π dφ [cos(µ φ + γ ) sin(µ φ + γ )] cos(µ(φ + θ) + γ) sin(µ(φ + θ) + γ) = µ,γ w µ,γ 1 2π dφ cos((µ − µ)φ + (γ − γ) − µθ).</formula><p>As before, the integral is non-zero only if the frequency is 0, i.e. iff µ − µ = 0 and thus:</p><formula xml:id="formula_95">= γ w µ ,γ cos((γ − γ) − µ θ) 36</formula><p>For the rhs we obtain:</p><formula xml:id="formula_96">b µ ,γ , ψ SO(2) m (r θ )κ(·) = 1 2π dφ b µ ,γ (φ) T ψ m (r θ )κ(φ) = µ,γ w µ,γ 1 2π dφ b µ ,γ (φ) T ψ m (r θ ) cos(µφ + γ) sin(µφ + γ) = µ,γ w µ,γ 1 2π dφ [cos(µ φ + γ ) sin(µ φ + γ )] ψ m (r θ ) cos(µφ + γ) sin(µφ + γ) = µ,γ w µ,γ 1 2π dφ cos(µ φ + γ − µφ − γ − mθ) = µ,γ w µ,γ 1 2π dφ cos((µ − µ)φ + γ − γ − mθ) .</formula><p>The integral is non-zero only if the frequency is 0, i.e. µ − µ = 0:</p><formula xml:id="formula_97">= γ w µ ,γ cos(γ − γ − mθ)</formula><p>Requiring the projections to be equal implies</p><formula xml:id="formula_98">b µ ,γ , R θ κ = b µ ,γ , ψ m (r θ )κ(·) ∀θ ∈ [0, 2π) ⇔ γ w µ ,γ cos(γ − γ − µ θ) = γ w µ ,γ cos(γ − γ − mθ) ∀θ ∈ [0, 2π) ⇔ w µ ,0 cos(γ −µ θ) + w µ , π 2 sin(γ −µ θ) = w µ ,0 cos(γ −mθ) + w µ , π 2 sin(γ −mθ) ∀θ ∈ [0, 2π) ⇔ γ − µ θ = γ − mθ + 2tπ ∀θ ∈ [0, 2π) ⇔ µ θ = mθ + 2tπ ∀θ ∈ [0, 2π),</formula><p>where we made use of Eq. (20) once again. It follows that µ = m, resulting in the two-dimensional basis</p><formula xml:id="formula_99">K SO(2) ψm←ψn = b m,γ (φ) = cos(mφ + γ) sin(mφ + γ) γ ∈ 0, π 2<label>(28)</label></formula><p>of equivariant kernels for m &gt; 0 and n = 0. This basis is explicitly given in the lower left cell of <ref type="table" target="#tab_15">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">and 1-dimensional irreps:</head><p>The case for 2-dimensional input and 1-dimensional output representations, i.e. ρ in = ψ SO(2) n and ρ out = ψ SO(2) 0 , is identical to the previous one up to a transpose. The final two-dimensional basis for m = 0 and n &gt; 0 is therefore given by </p><p>as shown in the upper right cell of <ref type="table" target="#tab_15">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4.2 Derivation for the reflection group</head><p>The action of the reflection group ({±1}, * ) on R 2 depends on a choice of reflection axis, which we specify by an angle β. More precisely, the element s ∈ ({±1}, * ) acts on x = (r, φ) ∈ R 2 as</p><formula xml:id="formula_101">s.x(r, φ) := x(r, s.φ) := x(r, 2βδ s,−1 + sφ) = x(r, φ) if s = 1 x(r, 2β − φ) if s = 1 .</formula><p>The kernel constraint for the reflection group is therefore being made explicit by</p><formula xml:id="formula_102">κ(r, s.φ) = ρ out (s)κ(r, φ)ρ in (s) −1 ∀s ∈ ({±1}, * ), φ ∈ [0, 2π) ⇔ κ(r, δ s,−1 2β + sφ) = ρ out (s)κ(r, φ)ρ in (s) −1 ∀s ∈ ({±1}, * ), φ ∈ [0, 2π) ⇔ κ(r, δ s,−1 2β + sφ) = ρ out (s)κ(r, φ)ρ in (s) ∀s ∈ ({±1}, * ), φ ∈ [0, 2π) ,</formula><p>where we used the identity s −1 = s. For s = +1 the constraint is trivially true. We will thus in the following consider the case s = −1, that is,</p><formula xml:id="formula_103">κ(r, 2β − φ) = ρ out (−1)κ(r, φ)ρ in (−1) ∀φ ∈ [0, 2π) .</formula><p>In order to simplify this constraint further we define a transformed kernel κ (r, φ) := κ(r, φ + β) which is oriented relative to the reflection axis. The transformed kernel is then required to satisfy</p><formula xml:id="formula_104">κ (r, β − φ) = ρ out (−1)κ (r, φ − β)ρ in (−1) ∀φ ∈ [0, 2π) ,</formula><p>which, with the change of variables φ = φ − β , reduces to the constraint for equivariance under reflections around the x-axis, i.e. the case for β = 0 :</p><formula xml:id="formula_105">κ (r, −φ ) = ρ out (−1)κ (r, φ )ρ in (−1) ∀φ ∈ [0, 2π) .</formula><p>As a consequence we can retrieve kernels equivariant under reflections around the β-axis through κ(r, φ) := κ (r, φ − β) .</p><p>We will therefore without loss of generality consider the case β = 0 only in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-dimensional irreps:</head><p>The reflection group ({±1}, * ) has only two irreps, namely the trivial representation ψ (with i, j ∈ {0, 1}) and the usual 1-dimensional Fourier basis for scalar functions in L 2 (S 1 ) as before:</p><formula xml:id="formula_106">b µ,γ (φ) = cos(µφ + γ) µ ∈ N, γ ∈ {0} if µ = 0 {0, π/2} otherwise<label>(30)</label></formula><p>Defining the reflection operator S by its action (S κ) (φ) := κ(−φ), we require the projections of both sides of the kernel constraint on the same basis element to be equal as usual. Specifically, for a particular basis b µ ,γ :</p><formula xml:id="formula_107">b µ ,γ , S κ = b µ ,γ , ψ ({±1}, * ) i (−1)κ(·)ψ ({±1}, * ) j (−1)</formula><p>The lhs implies</p><formula xml:id="formula_108">b µ ,γ , S κ = µ,γ w µ,γ 1 2π dφ b µ ,γ (φ)b µ,γ (−φ) = µ,γ w µ,γ 1 2π dφ cos(µ φ + γ ) cos(−µφ + γ) = µ,γ w µ,γ 1 2π dφ 1 2 (cos((µ + µ)φ + (γ − γ)) + cos((µ − µ)φ + (γ + γ))) = γ w µ ,γ 1 2 (cos(γ + γ) + δ µ ,0 cos(γ − γ)) while the rhs leads to b µ ,γ , ψ ({±1}, * ) m (−1)κ(·)ψ ({±1}, * ) n (−1) = µ,γ w µ,γ 1 2π dφ b µ ,γ (φ)ψ ({±1}, * ) i (−1)b µ,γ (φ)ψ ({±1}, * ) j (−1) = µ,γ w µ,γ 1 2π dφ cos(µ φ + γ )(−1) i cos(µφ + γ)(−1) j = (−1) i+j µ,γ w µ,γ 1 2π dφ cos(µ φ + γ ) cos(µφ + γ) = (−1) i+j γ w µ ,γ 1 2 (cos(γ − γ) + δ µ ,0 cos(γ + γ)) .</formula><p>Now, we require both sides to be equal, that is, As γ ∈ {0, π 2 } and cos(± π 2 ) = 0:</p><p>⇔ w 0,0 1 2 (cos(0) + cos(0)) = (−1) i+j w 0,0 1 2 (cos(−0) + cos(0))</p><formula xml:id="formula_109">⇔ w 0,0 = (−1) i+j w 0,0</formula><p>Which is always true when i = j, while it enforces w 0,0 = 0 when i = j. • µ &gt; 0 In this case we get:</p><formula xml:id="formula_110">γ w µ ,γ 1 2 cos(γ + γ) = (−1) i+j γ w µ ,γ 1 2 cos(γ − γ)</formula><p>⇔ (1 − (−1) i+j )w µ ,0 cos(γ ) = (1 + (−1) i+j )w µ, π 2 sin(γ ) If i + j ≡ 0 mod 2, the equation becomes sin(γ ) = 0 and, so, γ = 0. Otherwise, it becomes cos(γ ) = 0, which means γ = π 2 . Shortly, γ = (i + j mod 2) π 2 . As a result, only half of the basis for β = 0 is preserved:</p><formula xml:id="formula_111">K ({±1}, * ),β=0 ψi←ψj = b µ,γ (φ) = cos(µφ + γ) µ ∈ N, γ = (i + j mod 2) π 2 , µ &gt; 0 ∨ γ = 0<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>39</head><p>The solution for a general reflection axis β is therefore given by</p><formula xml:id="formula_112">K ({±1}, * ),β ψi←ψj = b µ,γ (φ) = cos(µ(φ β) + γ) µ ∈ N, γ = (i + j mod 2) π 2 , µ &gt; 0 ∨ γ = 0<label>(32)</label></formula><p>which is visualized in <ref type="table" target="#tab_0">Table 10</ref> for the different cases of irreps for i, j ∈ {0, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4.3 Derivation for O(2)</head><p>The orthogonal group O(2) is the semi-direct product between the rotation group SO(2) and the reflection group ({±1}, * ), i.e. O(2) ∼ = SO(2) ({±1}, * ). This justifies a decomposition of the constraint on O(2)-equivariant kernels as the union of the constraints for rotations and reflections. Consequently, the space of O(2)-equivariant kernels is the intersection between the spaces of SO(2)and reflection-equivariant kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sufficiency:</head><p>Assume a rotation-and reflection-equivariant kernel, i.e. a kernel which for all r ∈ R + 0 and φ ∈ [0, 2π) satisfies This observation allows us to derive the kernel space for O(2) by intersecting the previously derived kernel space of SO(2) with the kernel space of the reflection group:</p><formula xml:id="formula_113">κ(r, r θ φ) = Res O(2) SO(2) ρ out (r θ ) κ(r, φ) Res O(2) SO(2) ρ in −1 (r θ ) ∀ r θ ∈ SO(2) = ρ out (r θ ) κ(r, φ) ρ −1 in (r θ )</formula><formula xml:id="formula_114">K O(2) ρout←ρin = κ | κ(r, g.φ) = ρ out (g)κ(r, φ)ρ in (g) −1 ∀ g ∈ O(2) = κ | κ(r, r θ .φ) = ρ out (r θ )κ(r, φ)ρ in (r θ ) −1 ∀ r θ ∈ SO(2) ∩ κ | κ(r, s.φ) = ρ out (s)κ(r, φ)ρ in (s) −1 ∀ s ∈ ({±1}, * )</formula><p>As O(2) contains all rotations, it does also contain all reflection axes. Without loss of generality, we define s ∈ O(2) as the reflection along the x-axis. A reflection along any other axis β is associated with the group element r 2β s ∈ O(2), i.e. the combination of a reflection with a rotation of 2β. As a result, we consider the basis for reflection equivariant kernels derived for β = 0 in Eq. <ref type="bibr" target="#b30">(31)</ref>.</p><p>Therefore, to derive a basis associated to a pair of input and output representations ρ in and ρ out , we restrict the representations to SO(2) and the reflection group, compute the two bases using the results found in Appendix F.4.1 and in Appendix F.4.2, and, finally, take their intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-dimensional irreps:</head><p>The </p><formula xml:id="formula_115">0 µ ∈ N ∪ b 01 µ, π 2 (φ) = 0 sin(µφ) 0 0 µ ∈ N + ∪ b 10 µ, π 2 (φ) = 0 0 sin(µφ) 0 µ ∈ N + ∪ b 11 µ,0 (φ) = 0 0 0 cos(µφ) µ ∈ N .</formula><p>Through the same change of basis applied in the first paragraph of Appendix F.4.1, we get the following equivalent basis for the same space: In Appendix F.4.1, a basis for SO(2)-equivariant kernels with respect to a ψ SO(2) n input field and ψ SO(2) m output field was derived starting from the basis in Eq. <ref type="bibr" target="#b20">(21)</ref>. Notice that the basis of reflectionequivariant kernels in Eq. (33) contains exactly half of the elements in Eq. <ref type="bibr" target="#b20">(21)</ref>, indexed by γ = 0. A basis for O(2)-equivariant kernels can be found by repeating the derivations in Appendix F.4.1 for SO(2)-equivariant kernels using only the subset in Eq. (33) of reflection-equivariant kernels. The resulting two-dimensional O(2)-equivariant basis, which includes the SO(2)-equivariance conditions (µ = m − sn) and the reflection-equivariance conditions (γ = 0), is given by</p><formula xml:id="formula_116">K O(2) ψi,m←ψj,n = b µ,0,s (φ) = ψ(µφ)ξ(s) µ = m − sn, s ∈ {±1} ,<label>(34)</label></formula><p>where i = j = 1 and m, n &gt; 0. See the bottom right cell in <ref type="table" target="#tab_16">Table 9</ref>. In order to solve the O(2) kernel constraint consider again the reflectional constraint and the SO(2) constraint. Bases for reflection-equivariant kernels with above representations were derived in Appendix F.4.2 and are shown in Eq. <ref type="bibr" target="#b30">(31)</ref>. These bases form a subset of the Fourier basis in Eq. <ref type="bibr" target="#b24">(25)</ref> which is being indexed by γ = (i + j mod 2) π 2 . On the other hand, the full Fourier basis was restricted by the SO(2) constraint to satisfy µ = 0 and therefore γ = 0, see Eq. <ref type="bibr" target="#b25">(26)</ref>. Intersecting both constraints therefore implies i = j, resulting in the O(2)-equivariant basis</p><formula xml:id="formula_117">K O(2) ψi,m←ψj,n = {b 0,0 (φ) = 1} if i = j, ∅ else<label>(35)</label></formula><p>for m, n = 0 which is shown in the top left cell in <ref type="table" target="#tab_16">Table 9</ref>. Following the same strategy as before we find the reflectional constraints for these representations to be given by , and therefore to decompose into two independent constraints on the entries κ 00 and κ 10 . Solving for a basis for each entry and taking their union as before we get <ref type="bibr" target="#b20">21</ref> b 00</p><formula xml:id="formula_118">µ (φ) = cos(µφ + j π 2 ) 0 µ∈N ∪ b 10 µ (φ) = 0 sin(µφ − j π 2 ) µ∈N ,</formula><p>which, through a change of basis, can be rewritten as</p><formula xml:id="formula_119">b µ,j π 2 (φ) = cos(µφ + j π 2 ) sin(µφ + j π 2 ) µ∈Z .<label>(36)</label></formula><p>We intersect this basis with the basis of SO(2) equivariant kernels with respect to a Res output field as derived in Appendix F.4.1. Both constraints, that is, γ = j π 2 for the reflection group and µ = m for SO(2) (see Eq. <ref type="formula" target="#formula_1">(27)</ref>), define the one-dimensional basis for O(2)-equivariant kernels for n = 0, m &gt; 0 and i = 1 as</p><formula xml:id="formula_120">K O(2) ψi,m←ψj,n = b µ,j π 2 (φ) = cos(µφ + j π 2 ) sin(µφ + j π 2 ) µ = m ,<label>(37)</label></formula><p>see the bottom left cell in <ref type="table" target="#tab_16">Table 9</ref>. is identical to the previous basis up to a transpose, i.e. it is given by</p><formula xml:id="formula_121">K O(2) ψi,m←ψj,n = b µ,i π 2 (φ) = cos(µφ + i π 2 ) sin(µφ + i π 2 ) µ = n ,<label>(38)</label></formula><p>where j = 1, n &gt; 0 and m = 0. This case is visualized in the top right cell of <ref type="table" target="#tab_16">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4.4 Derivation for C N</head><p>The derivations for C N coincide mostly with the derivations done for SO(2) with the difference that the projected constraints need to hold for discrete angles θ ∈ p 2π N | p = 0, . . . , N − 1 only. Furthermore, C N has one additional 1-dimensional irrep of frequencyN/2 if (and only if) N is even.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-dimensional irreps:</head><p>During the derivation of the solutions for SO(2)'s 2-dimensional irreps in Appendix F.4.1, we assumed continuous angles only in the very last step. The constraint in Eq. (23) therefore holds for C N as well. Specifically, it demands that for each θ ∈ {p 2π N | p = 0, . . . , N − 1} there exists a t ∈ Z such that:</p><formula xml:id="formula_122">(µ − (m − ns ))θ = 2tπ ⇔ (µ − (m − ns ))p 2π N = 2tπ ⇔ (µ − (m − ns ))p = tN</formula><p>The last result corresponds to a system of N linear congruence equations modulo N which require N to divide (µ − (m − ns ))p for each non-negative integer p smaller than N . Note that solutions of the constraint for p = 1 already satisfy the constraints for p ∈ 2, . . . , N − 1 such that it is sufficient to consider</p><formula xml:id="formula_123">(µ − (m − ns ))1 = tN ⇔ µ = m − ns + tN .</formula><p>The resulting basis</p><formula xml:id="formula_124">K C N ψm←ψn = b µ,γ,s (φ) = ψ(µφ + γ)ξ(s) µ = m sn + tN, γ ∈ 0, π 2 s ∈ {±1} t∈Z<label>(39)</label></formula><p>for m, n &gt; 0 thus coincides mostly with the basis 24 for SO(2) but contains solutions for aliased frequencies, defined by adding tN . The bottom right cell in <ref type="table" target="#tab_0">Table 11</ref> gives the explicit form of this basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-dimensional irreps:</head><p>The same trick could be applied to solve the remaining three cases. However, since C N has an additional one dimensional irrep of frequency N/2 for even N it is convenient to rederive all cases. We therefore consider ρ out = ψ C N m and ρ in = ψ C N n , where m, n ∈ {0, N/2}. Note that ψ C N m (θ), ψ C N n (θ) ∈ {±1} for θ ∈ {p 2π N | p = 0, . . . , N − 1}. We use the same Fourier basis <ref type="bibr" target="#b39">(40)</ref> and the same projection operators as used for SO <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_125">b µ,γ (φ) = cos(µφ + γ) µ ∈ N, γ ∈ {0} if µ = 0 {0, π/2} otherwise</formula><p>Since the lhs of the kernel constraint does not depend on the representations considered its projection b µ ,γ , R θ κ is the same found for SO <ref type="formula" target="#formula_1">(2)</ref>:</p><formula xml:id="formula_126">b µ ,γ , R θ κ = 1 2 γ w µ ,γ (cos((γ − γ) − µ θ) + δ µ ,0 cos(γ + γ))</formula><p>For the rhs we find</p><formula xml:id="formula_127">b µ ,γ , ψ C N m (r θ )κ ψ C N n (r θ ) −1 = 1 2π dφ b µ ,γ (φ)ψ C N m (r θ )κ(φ)ψ C N n (r θ ) −1 ,</formula><p>which by expanding the kernel in the linear combination of the basis and writing the respresentations out yields:</p><formula xml:id="formula_128">= µ,γ w µ,γ 1 2π dφ b µ ,γ (φ) cos(mθ)b µ,γ (φ) cos(nθ) −1 = µ,γ w µ,γ 1 2π</formula><p>dφ cos(µ φ + γ ) cos(mθ) cos(µφ + γ) cos(nθ) <ref type="bibr">−1</ref> Using the property in Eq. <ref type="formula" target="#formula_1">(20)</ref>:</p><formula xml:id="formula_129">⇔ ∃t ∈ Z s.t. γ − µ θ = γ + (±m ± n)θ + 2tπ ⇔ ∃t ∈ Z s.t. µ θ = (±m ± n)θ + 2tπ</formula><p>Using θ = p 2π N :</p><formula xml:id="formula_130">⇔ ∃t ∈ Z s.t. µ p 2π N = (±m ± n)p 2π N + 2tπ ⇔ ∃t ∈ Z s.t. µ p = (±m ± n)p + tN ⇔ ∃t ∈ Z s.t. (±m ± n + µ )p = tN</formula><p>In both cases µ = 0 and µ &gt; 0 we thus find the constraints</p><formula xml:id="formula_131">∀p ∈ {0, 1, . . . , N − 1} ∃t ∈ Z s.t. (±m ± n + µ )p = tN .</formula><p>It is again sufficient to consider the constraint for p = 1 which results in solutions with frequencies µ = ±m ± n + tN . As (±m ± n) ∈ 0, ± N 2 , ±N , all valid solutions are captured by µ = (m + n mod N ) + tN , resulting in the basis</p><formula xml:id="formula_132">K C N ψm←ψn = b µ,γ (φ) = cos(µφ+γ) µ = (m+n mod N )+tN, γ ∈ 0, π 2 , µ = 0∨γ = 0 t∈N<label>(41)</label></formula><p>for n, m ∈ 0, N 2 . See the top left cells in <ref type="table" target="#tab_0">Table 11</ref>.</p><p>1 and 2-dimensional irreps Next consider a 1-dimensional irrep ρ in = ψ C N n with n ∈ {0, N 2 } in the input and a 2-dimensional irrep ρ out = ψ C N m in the output. We derive the solutions by projecting the kernel constraint on the basis introduced in Eq. <ref type="bibr" target="#b26">(27)</ref>.</p><p>For the lhs the projection coincides with the result found for SO(2) as before:</p><formula xml:id="formula_133">b µ ,γ , R θ κ = γ w µ ,γ cos((γ − γ) − µ θ)</formula><p>An expansion and projection of rhs gives:</p><formula xml:id="formula_134">b µ ,γ , ψ C N m (r θ )κ(·)ψ C N n (r θ ) −1 = 1 2π dφ b µ ,γ (φ) T ψ C N m (r θ )κ(φ)ψ C N n (r θ ) −1 = µ,γ w µ,γ 1 2π dφ b µ ,γ (φ) T ψ C N m (r θ ) cos(µφ + γ) sin(µφ + γ) ψ C N n (r θ ) −1 = µ,γ w µ,γ 1 2π dφ [cos(µ φ + γ ) sin(µ φ + γ )] ψ C N m (r θ ) cos(µφ + γ) sin(µφ + γ) ψ C N n (r θ ) 1 = µ,γ w µ,γ 1 2π dφ cos(µ φ + γ − µφ − γ − mθ) ψ C N n (r θ ) −1 .</formula><p>The integral is non-zero only if the frequency is 0, i.e. iff µ = µ:</p><formula xml:id="formula_135">= γ w µ ,γ cos(γ − γ − mθ)ψ C N n (r θ ) −1 = γ w µ ,γ cos(γ − γ − mθ) cos(±nθ)</formula><p>Since ±nθ = pπ for some p ∈ N one has sin(±nθ) = 0 which allows to add the following zero summand and simplify:</p><formula xml:id="formula_136">= γ w µ ,γ (cos(γ − γ − mθ) cos(±nθ) − sin(γ − γ − mθ) sin(±nθ)) = γ w µ ,γ cos(γ − γ − (m ± n)θ)</formula><p>Requiring the projections to be equal then yields:</p><formula xml:id="formula_137">b µ ,γ , R θ κ = b µ ,γ , ψ C N m (r θ )κ(·)ψ C N n (r θ ) −1 ∀θ ∈ p 2π N ⇔ γ w µ ,γ cos(γ − γ − µ θ) = γ w µ ,γ cos(γ − γ − (m ± n)θ) ∀θ ∈ p 2π N ⇔ w µ ,0 cos(γ µ θ)+w µ , π 2 sin(γ µ θ) = w µ ,0 cos(γ (m±n)θ)+w µ , π 2 sin(γ (m±n)θ) ∀θ ∈ p 2π N</formula><p>Using the property in Eq. <ref type="bibr" target="#b19">(20)</ref>, this requires that for each θ there exists a t ∈ Z such that:</p><formula xml:id="formula_138">⇔ γ − µ θ = γ − (m ± n)θ + 2tπ ∀θ ∈ p 2π N ⇔ µ θ = (m ± n)θ + 2tπ ∀θ ∈ p 2π N Since θ = p 2π N with p ∈ {0, . . . , N − 1} we find that ⇔ µ p 2π N = (m ± n)p 2π N + 2tπ ∀p ∈ {0, . . . , N 1} ⇔ µ p = (m ± n)p + tN ∀p ∈ {0, . . . , N 1} ⇔ µ = (m ± n) + tN ⇔ µ − (m ± n) = tN ,</formula><p>which implies that N needs to divide µ − (m ± n). It follows that the condition holds also for any other p. This gives the basis</p><formula xml:id="formula_139">K C N ψm←ψn = b µ,γ (φ) = cos(µφ + γ) sin(µφ + γ) µ = (m ± n) + tN, γ ∈ 0, π 2 t∈Z<label>(42)</label></formula><p>for m &gt; 0 and n ∈ 0, N 2 ; see the bottom left cells in <ref type="table" target="#tab_0">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">and 1-dimensional irreps:</head><p>The basis for 2-dimensional input and 1-dimensional output representations, i.e. ρ in = ψ C N n and ρ out = ψ C N m with n &gt; 0 and m ∈ {0, N 2 }, is identical to the previous one up to a transpose:  <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_140">K C N ψm←ψn = b µ,</formula><p>In contrast to the case of O(2)-equivariant kernels, the choice of reflection axis β is not irrelevant since D N does not act transitively on axes. More precisely, the action of D N defines equivalence classes β ∼ = β ⇔ ∃ 0 ≤ n &lt; N : β = β + n 2π N of axes which can be labeled by representatives β ∈ [0, 2π N ). For the same argument considered in Appendix F.4.2 we can without loss of generality consider reflections along the axis β = 0 in our derivations and retrieve kernels κ , equivariant to reflections along a general axis β, as κ (r, φ) = κ(r, φ − β).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-dimensional irreps:</head><p>For 2-dimensional input and output representations ρ in = ψ D N 1,n and ρ out = ψ D N 1,m , the final basis is</p><formula xml:id="formula_141">K D N ψi,m←ψj,n = b µ,0,s (φ) = ψ(µφ)ξ(s) µ = m − sn + tN, s ∈ {±1} t∈Z<label>(44)</label></formula><p>where i = j = 1 and m, n &gt; 0. These solutions are written out explicitly in the bottom right of <ref type="table" target="#tab_0">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-dimensional irreps:</head><p>D N has 1-dimensional representations ρ in = ψ D N j,n and ρ out = ψ D N i,m for m, n ∈ {0, N 2 }. In these cases we find the bases</p><formula xml:id="formula_142">K D N ψi,m←ψj,n = b µ,γ (φ) = cos(µφ + γ) µ = (m + n mod N ) + tN, γ = (i + j mod 2) π 2 , µ = 0 ∨ γ = 0 t∈N<label>(45)</label></formula><p>which are shown in the top left cells of <ref type="table" target="#tab_0">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">and 2-dimensional irreps:</head><p>For 1-dimensional input and 2-dimensional output representations, that is, ρ in = ψ D N j,n and ρ out = ψ D N 1,m with i = 1, m &gt; 0 and n ∈ {0, N 2 }, the kernel basis is given by:</p><formula xml:id="formula_143">K D N ψi,m←ψj,n = b µ,γ (φ) = cos(µφ + γ) sin(µφ + γ) µ = (m ± n) + tN, γ = j π 2 t∈Z<label>(46)</label></formula><p>See the bottom left of <ref type="table" target="#tab_0">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">and 1-dimensional irreps:</head><p>Similarly, for 2-dimensional input and 1-dimensional output representations ρ in = ψ D N 1,n and ρ out = ψ D N i,m with j = 1, n &gt; 0 and m ∈ {0, N 2 }, we find: (47) <ref type="table" target="#tab_0">Table 12</ref> shows these solutions in its top right cells.</p><formula xml:id="formula_144">K D N ψi,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4.6 Kernel constraints at the origin</head><p>Our derivations rely on the fact that the kernel constraints restrict only the angular parts of the unconstrained kernel space L 2 (R 2 ) cout×cin which suggests an independent solution for each radius r ∈ R + ∪ {0}. Particular attention is required for kernels defined at the origin, i.e. when r = 0. The reason for this is that we are using polar coordinates (r, φ) which are ambiguous at the origin where the angle is not defined. In order to stay consistent with the solutions for r &gt; 0 we still define the kernel at the origin as an element of L 2 (S 1 ) cout×cin . However, since the coordinates (0, φ) map to the same point for all φ ∈ [0, 2π), we need to demand the kernels to be angularly constant, that is, κ(φ) = κ(0). This additional constraint restricts the angular Fourier bases used in the previous derivations to zero frequencies only. Apart from this, the kernel constraints are the same for r = 0 and r &gt; 0 which implies that the G-steerable kernel bases at r = 0 are given by restricting the bases derived in F.4.1, F.4.2, F.4.3, F.4.4 and F.4.5 to the elements indexed by frequencies µ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Complex valued representations and Harmonic Networks</head><p>Instead of considering real (irreducible) representations we could have derived all results using complex representations, acting on complex feature maps. For the case of O(2) and D N this would essentially not affect the derivations since their complex and real irreps are equivalent, that is, they coincide up to a change of basis. Conversely, all complex irreps of SO(2) and C N are 1-dimensional which simplifies the derivations in complex space. However, the solution spaces of complex Gsteerable kernels need to be translated back to a real valued implementation. This translation has some not immediately obvious pitfalls which can lead to an underparameterized implementation in real space. In particular, Harmonic Networks <ref type="bibr" target="#b11">[12]</ref> were derived with a complete solution in complex space; however, their real valued implementation is using a G-steerable kernel space of half the dimensionality as ours. We will in the following explain why this is the case.</p><p>In the complex field, the irreps of SO(2) are given by ψ C k (θ) = e ikθ ∈ C with frequencies k ∈ Z. Notice that these complex irreps are indexed by positive and negative frequencies while their real counterparts, defined in Appendix F.2, only involve non-negative frequencies. As in <ref type="bibr" target="#b11">[12]</ref> we consider complex feature fields f C : R 2 → C which are transforming according to complex irreps of SO(2). A complex input field f C in : R 2 → C of type ψ C n is mapped to a complex output field f C out : R 2 → C of type ψ C m via the cross-correlation</p><formula xml:id="formula_145">f C out = k C f C in . (48)</formula><p>with a complex filter k C : R 2 → C. The (angular part of the) complete space of equivariant kernels between f C in and f C out was in <ref type="bibr" target="#b11">[12]</ref> proven to be parameterized by</p><formula xml:id="formula_146">k C (φ) = w e i(m−n)φ ,</formula><p>where w ∈ C is a complex weight which scales and phase-shifts the complex exponential. We want to point out that an equivalent parametrization is given in terms of the real and imaginary parts w Re and w Im of the weight w, i.e.</p><formula xml:id="formula_147">k C (φ) = w Re e i(m−n)φ + i w Im e i(m−n)φ = w Re e i(m−n)φ + w Im e i((m−n)φ+π/2) .<label>(49)</label></formula><p>The real valued implementation of Harmonic Networks models the complex feature fields f C of type ψ C k (θ) by splitting them in two real valued channels f R := (f Re , f Im ) T which contain their real and imaginary part. The action of the complex irrep ψ C k (θ) is modeled accordingly by a rotation matrix of the same, potentially negative 22 frequency. A real valued implementation of the cross-correlation (48) is built using a real kernel k : R 2 → R 2×2 as specified by</p><formula xml:id="formula_148">f Re out f Im out = k Re −k Im k Im k Re f Re in f Im in .</formula><p>The complex steerable kernel (49) is then given by</p><formula xml:id="formula_149">k(φ) = w Re cos ((m n)φ) sin ((m n)φ) sin ((m n)φ) cos ((m n)φ) + w Im sin ((m n)φ) cos ((m n)φ) cos ((m n)φ) sin ((m n)φ) = w Re ψ ((m − n)φ) + w Im ψ (m − n)φ + π 2 (50)</formula><p>While this implementation models the complex Harmonic Networks faithfully in real space, it does not utilize the complete SO(2)-steerable kernel space when the real feature fields are interpreted as fields transforming under the real irreps ψ R k as done in our work. More specifically, the kernel space used in (50) is only 2-dimensional while our basis <ref type="bibr" target="#b23">(24)</ref> for the same case is 4-dimensional. The additional solutions with frequency m + n are missing.</p><p>The lower dimensionality of the complex solution space can be understood by analyzing the relationship between SO(2)'s real and complex irreps. On the complex field, the real irreps become reducible and decomposes into two 1-dimensional complex irreps with opposite frequencies:</p><formula xml:id="formula_150">ψ R k (θ) = 1 √ 2 1 −i −i 1 e ikθ 0 0 e −ikθ 1 √ 2 1 i i 1</formula><p>Indeed, SO(2) has only half as many real irreps as complex ones since positive and negative frequencies are conjugated to each other, i.e. they are equivalent up to a change of basis:</p><formula xml:id="formula_151">ψ R k (θ) = ξ(−1)ψ R −k (θ)ξ(−1)</formula><p>. It follows that a real valued implementation of a complex ψ C k fields as a 2dimensional ψ R k fields implicitly adds a complex ψ C −k field. The intertwiners between two real fields of type ψ R n and ψ R m therefore do not only include the single complex intertwiner between complex fields of type ψ C n and ψ C m , but four complex intertwiners between fields of type ψ C ±n and ψ C ±m . The real parts of these intertwiners correspond to our four dimensional solution space.</p><p>In conclusion, <ref type="bibr" target="#b11">[12]</ref> indeed found the complete solution on the complex field. However, by implementing the network on the real field, negative frequencies are implicitly added to the feature fields which allows for our larger basis <ref type="bibr" target="#b23">(24)</ref> of steerable kernels to be used without adding an overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Alternative approaches to compute kernel bases and their complexities</head><p>The main challenge of building steerable CNNs is to find the space of solutions of the kernel space constraint in Eq. 2. Several recent works tackle this problem for the very specific case of features which transform under irreducible representations of SE(3) ∼ = (R 3 , +) SO(3). The strategy followed in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref> is based on well known analytical solutions and does not generalize to arbitrary representations. In contrast, <ref type="bibr" target="#b1">[2]</ref> present a numerical algorithm to solve the kernel space constraint. While this algorithm was only applied to solve the constraints for irreps, it generalizes to arbitrary representations. However, the computational complexity of the algorithm scales unfavorably in comparison to the approach proposed in this work. We will in the following review the kernel space solution algorithm of <ref type="bibr" target="#b1">[2]</ref> for general representations and discuss its complexity in comparison to our approach.</p><p>The algorithm proposed in <ref type="bibr" target="#b1">[2]</ref> is considering the same kernel space constraint k(gx) = ρ out (g)k(x)ρ −1 in (g) ∀g ∈ G as in this work. By vectorizing the kernel the constraint can be brought in the form</p><formula xml:id="formula_152">vec(k) (gx) = ρ out ⊗ ρ −1 in T (g) vec(k) (x) = (ρ out ⊗ ρ in ) (g) vec(k) (x) ,</formula><p>where the second step assumes the input representation to be unitary, that is, to satisfy ρ −1 in = ρ T in . A Clebsch-Gordan decomposition, i.e. a decomposition of the tensor product representation into a direct sum of irreps ψ j of G, then yields 23 vec(k) (gx) = Q −1 J∈J ψ J (g) Q vec(k) (x) <ref type="bibr" target="#b22">23</ref> For the irreps of SO(3) it is well known that J = {|j − l|, . . . , j + l} and |J | = 2 min(j, l) + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>50</head><p>Through a change of variables η(x) := Q vec(k)(x) this simplifies to</p><formula xml:id="formula_153">η(gx) = J∈J ψ J (g)η(x)</formula><p>which, in turn, decomposes into |J | independent constraints</p><formula xml:id="formula_154">η J (gx) = ψ J (g)η J (x) .</formula><p>Each of these constraints can be solved independently to find a basis for each η J . The kernel basis is then found by inverting the change of basis and the vectorization, i.e. by computing k(x) = unvec Q −1 η(x) .</p><p>For the case that ρ in = ψ j and ρ out = ψ l are Wigner D-matrices, i.e. irreps of SO(3), the change of basis Q is given by the Clebsch-Gordan coefficients of SO(3). These well known solutions were used in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref> to build the basis of steerable kernels. Conversely, the authors of <ref type="bibr" target="#b1">[2]</ref> solve for the change of basis Q numerically. Given arbitrary unitary representations ρ in and ρ out the numerical algorithm solves for the change of basis in</p><formula xml:id="formula_155">ρ in ⊗ ρ out (g) = Q −1 J∈J ψ J (g) Q ∀g ∈ G ⇔ 0 = Q ρ in ⊗ ρ out (g) − J∈J ψ J (g) Q ∀g ∈ G .</formula><p>This linear constraint on Q, which is a specific instance of the Sylvester equation, can be solved by vectorizing Q, i.e.</p><formula xml:id="formula_156">I ⊗ ρ in ⊗ ρ out (g) − J∈J ψ J (g) ⊗ I vec(Q) = 0 ∀g ∈ G ,</formula><p>where I is the identity matrix on R dim(ρin⊗ρout) = R dim(ρin) dim(ρout) and vec(Q) ∈ R dim(ρin) 2 dim(ρout) 2 . In principle there is one Sylvester equation for each group element g ∈ G, however, it is sufficient to consider the generators of G only, since the solutions found for the generators will automatically hold for all group elements. One can therefore stack the matrices I ⊗ ρ in ⊗ ρ out (g) − J∈J ψ J (g) ⊗ I for the generators of G into a bigger matrix and solve for Q as the null space of this stacked matrix. The linearly independent solutions Q J in the null space correspond to the Clebsch-Gordan coefficients for J ∈ J .</p><p>This approach does not rely on the analytical Clebsch-Gordan coefficients, which are only known for specific groups and representations, and therefore works for any choice of representations. However, applying it naively to large representations can be extremely expensive. Specifically, computing the null space to solve the (stacked) Sylvester equation for G generators of G via a SVD, as done in <ref type="bibr" target="#b1">[2]</ref>, scales as O dim(ρ in ) 6 dim(ρ out ) 6 G . This is the case since the matrix which is multiplying vec(Q) is of shape dim(ρ in ) 2 dim(ρ out ) 2 G × dim(ρ in ) 2 dim(ρ out ) 2 . Moreover, the change of basis matrix Q itself has shape dim(ρ in ) dim(ρ out ) × dim(ρ in ) dim(ρ out ) which implies that the change of variables 24 from η to k has complexity O dim(ρ in ) 2 dim(ρ out ) 2 . In <ref type="bibr" target="#b1">[2]</ref> the authors only use irreducible representations which are relatively small such that the bad complexity of the algorithm is negligible.</p><p>In comparison, the algorithm proposed in this work is based on an individual decomposition of the representations ρ in and ρ out into irreps and leverages the analytically derived kernel space solutions between the irreps of G ≤ O(2). The independent decomposition of the input and output representations leads to a complexity of only O dim(ρ in ) 6 + dim(ρ in ) 6 G . We further apply the input and output changes of basis Q in and Q out independently to the irreps kernel solutions κ ij which leads to a complexity of O dim(ρ in ) dim(ρ out ) 2 + dim(ρ out ) dim(ρ in ) 2 . The improved complexity of our implementation makes working with large representations as used in this work, for instance dim(ρ D20 reg ) = 40, possible.    <ref type="bibr" target="#b6">[7]</ref>). Each fully connected layer follows a dropout layer with p = 0.3; the first two fully connected layers are followed by batch normalization and ELU. The width of each layer is expressed in terms of regular feature fields of a C16 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 Benchmarking on transformed MNIST datasets</head><p>Each model reported in Sections 3.1, 3.2 and 3.3 is derived from the architecture reported in <ref type="table" target="#tab_0">Table 13</ref>.</p><p>The width of each model's layers is thereby scaled such that the total number of parameters is matched and the relative width of layers coincides with that reported in Table13. Training is performed with a batch size of 64 samples, using the Adam optimizer. The learning rate is initialized to 10 −3 and decayed exponentially by a factor of 0.8 per epoch, starting after a burn in phase of 10 epochs. We train each model for 30 epochs and test the model which performed best on the validation set. A weight decay of 10 −7 is being used for all convolutional layers and the first fully connected layer. In all experiments, we build steerable bases with Gaussian radial profiles of width σ = 0.6 for all except the outermost ring where we use σ = 0.4. We apply a strong bandlimiting policy which permits frequencies up to 0, 2, 2 for radii 0, 1, 2 in a 5×5 kernel and up to 0, 2, 3, 2 for radii 0, 1, 2, 3 in a 7×7 kernel. The strong cutoff in the rings of maximal radius is motivated by our empirical observation that these rings introduce a relatively high equivariance error for higher frequencies. This is the case since the outermost ring ranges out of the sampled kernel support. During training, data augmentation with continuous rotations and reflections is performed (if these are present in the dataset) to not disadvantage non-equivariant models. In the models using group restriction, the restriction operation is applied after the convolution layers but before batch normalization and non-linearities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Competitive runs on MNIST rot</head><p>In <ref type="table" target="#tab_10">Table 5</ref> we report the performances of some of our best models. Our experiments are based on the best performing, C 16 -equivariant model of <ref type="bibr" target="#b6">[7]</ref> which defined the state of the art on rotated MNIST at the time of writing. We replicate their model architecture, summarized in <ref type="table" target="#tab_0">Table 14</ref>, though our models have a different frequency bandlimit and width σ for the Gaussian radial profiles as discussed in the previous subsection. As before, the table reports the width of each layer in terms of number of fields in the C 16 regular model.</p><p>As commonly done, we train our final models on the 10000 + 2000 training and validation samples.</p><p>Training is performed for 40 epochs with an initial learning rate 0.015, which is being decayed by a factor of 0.8, starting after 15 epochs. As before, we use the Adam optimizer with a batch size of 64, this time using L1 and L2 regularization with a weight of 10 −7 . The fully connected layers are additionally regularized using dropout with a probability of p = 0.3. We are again using train time augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 CIFAR experiments</head><p>The equivariant models used in the experiments on CIFAR-10 and CIFAR-100 are adapted from the original WideResNet models by replacing conventional with G-steerable convolutions and scaling the number of feature fields such that the total number of parameters is preserved. For blocks which are equivariant under D 8 or C 8 we use 5 × 5 kernels instead of 3 × 3 kernels to allow for higher frequencies. All models use regular feature fields in all but the final convolution layer, which maps to a scalar field (conv2triv) to produce invariant predictions. We use a frequency cut-off of 3 times the ring's radius, e.g. 0, 3, 6 for rings of radii 0, 1, 2. These higher bandlimits in comparison to the MNIST experiments are motivated by the fact that the corresponding bases introduce small discretization errors, which is no problem for the classification of natural images. In the contrary, this leads to the models having a strong bias towards being equivariant, but might allow them to break equivariance if necessary. The widths of the bases' rings is chosen to be σ = 0.45 in all rings.</p><p>The training process is the same as used for WideResNets: we train for 200 epochs with a batch size of 128. We optimize the model with SGD, using an initial learning rate of 0.1, momentum 0.9 and a weight decay of 5 · 10 −4 . The learning rate is decayed by a factor of 0.2 every 60 epochs. We perform a standard data augmentation with random crops, horizontal flips and normalization. No CutOut is done during the normal experiments but it is used in the AutoAugment policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4 STL-10 experiments</head><p>The models for our STL-10 experiments are adapted from <ref type="bibr" target="#b42">[43]</ref>. However, according to an issue <ref type="bibr" target="#b24">25</ref> in the authors' GitHub repository, the publication states some model parameters and the training setup wrongly. Our adaptations are therefore based on the setting reported on GitHub. Specifically, we use patches of 60 × 60 pixels for cutout and the stride of the first convolution layer in the first block is 2 instead of 1. Moreover, we normalize input features using CIFAR-10 statistics. Though these statistics are very close to the statistics of STL-10, they might, as the authors of <ref type="bibr" target="#b42">[43]</ref> suggest, cause non-negligible changes in performance because of the small training set size of STL-10.</p><p>As before, regular feature fields are used throughout the whole model except for the last convolution layer which maps to trivial fields. In the small model, which does not preserve the number of parameters but the number of channels, we still scale up the number of output channels of the very first convolution layer (before the first residual block). As the first convolution layer originally has 16 output channels and our model is initially equivariant to D 8 (whose regular representation spans 16 channels), the first convolution layer would only be able to learn 1 single independent filter (repeated 16 times, rotated and reflected). Hence, we increase the number of output channels of the first convolution layer by the square root of the group size ( √ 16 = 4) leading to 4 · 16 = 64 channels, i.e. 64/16 = 4 regular fields. We use a ring width of σ = 0.6 for the kernel basis except for the outermost ring where we use σ = 0.4 and use a frequency cut-off factor of 3 for the rings' radii, i.e. cutoffs of 0, 3, 6, . . . .</p><p>We are again exactly replicating the training process as reported in the publication <ref type="bibr" target="#b42">[43]</ref>. Only the labeled subset of the training set is used, that is, the 100000 unlabeled training images are discarded. Training is performed for 1000 epochs with a batch size of 128, using SGD with Nesterov momentum of 0.9 and weight decay of 5 · 10 −4 . The learning rate is initialized to 0.1 and decayed by a factor of 5 at 300, 400, 600 and 800 epochs. During training, we perform data augmentation by zero-padding with 12 pixels and randomly cropping patches of 96 × 96 pixels, mirroring them horizontally and applying CutOut.</p><p>In the data ablation study, reported in <ref type="figure" target="#fig_6">Figure 4</ref>, we use the same models and training procedure as in the main experiment on the full STL-10 dataset. For every single run, we generate new datasets by mixing the original training, validation and test set and sample reduced datasets such that all classes are balanced. The results are averaged over 4 runs on each of the considered training set sizes of 250, 500, 1000, 2000 or 4000. The validation and test sets contain 1000 and 8000 images, which are resampled in each run as well. I Additional information on the irrep models SO(2) models We experiment with some variants (rows 37-44) of the Harmonic Network model in row 30 of <ref type="table" target="#tab_7">Table 3</ref>, varying in either the non-linearity or the invariant map applied. All of these models are therefore to be analyzed relative to this baseline. First, we try to use squashing nonlinearities <ref type="bibr" target="#b44">[45]</ref> (row 37) instead of norm-ReLUs on each non-trivial irrep. This variant performs consistently worse than the original model. In the baseline variant, we generate invariant features via a convolution to scalar fields in the last layer (conv2triv). This, however, reduces the utilization of high frequency irrep fields in the penultimate layer. The reason for this is that the kernel space for mappings from high frequency-to scalar fields consists of kernels of a high angular frequency, which will be cut off by our bandlimiting. To overcome this problem, we propose to instead compute the norms of all non-trivial fields to produce invariant features. This enables us to use all irreps in the output of the last convolutional layer. However, we find that combining invariant norm mappings with norm-ReLUs does not improve on the baseline model, see row 38. In row 39 we consider a variant which applies norm-ReLUs on the direct sum of multiple non-trivial irrep fields, each with multiplicity 1, together (shared norm-ReLU), while the scalar fields are still being acted on by ELUs. This is legitimate since the direct sum of unitary representations is itself unitary. After the last convolutional layer, the invariant projection preserves the trivial fields but computes the norm of each composed field. This model significantly outperforms all previous variants on all datasets. The model in row 40 additionally merges the scalar fields to such combined fields instead of treating them independently. This architecture performs significantly worse than the previous variants.</p><p>We further explore four different variations which are applying gated nonlinearities (rows 41-44). These models distinguish from each other by 1) their mapping to invariant features and 2) whether the gate is being applied to each non-trivial field independently or being shared between multiple non-trivial fields. We find that the second choice, i.e. sharing gates, does not significantly affect the performances (row 41 vs. 42 and 43 vs. 44). However, mapping to invariant features by taking the norm of all non-trivial fields performs consistently better than applying conv2triv. Overall, gated nonlinearities perform significantly better than any other choice of nonlinearity on the tested SO(2) irrep models.</p><p>O(2) models Here we will give more details on the O(2)-specific operations which we introduce to improve the performance of the O(2)-equivariant models, reported in rows 45-57 of <ref type="table" target="#tab_7">Table 3</ref>. 1,0 , that is, to scalar fields f 0,0 and sign-flip fields f 1,0 in equal proportions. Since the latter are not invariant under reflections, we are in addition taking their absolute value. The resulting, invariant output features are then multiple fields f 0,0 ⊕ |f 1,0 |. The motivation for not convolving to trivial representations of O(2) directly via conv2triv is that the steerable kernel space for mappings between irreps of O(2) does not allow for mapping between ψ O(2) 0,0 and ψ O(2) 1,0 (see <ref type="table" target="#tab_16">Table 9</ref>), which would lead to dead neurons.</p><p>The models in rows 50-53, 56 and 57 operate on Ind </p><p>that is, it coincides with the regular representation of the reflection group. Similarly, for k &gt; 0, it is for allr given by the 4 × 4 matrices</p><formula xml:id="formula_158">Ind O(2) SO(2) ψ SO(2) k&gt;0 (rs) =                                   ψ SO(2) k&gt;0 (r) 0 0 ψ SO(2) k&gt;0 (−r)      fors = +1      0 ψ SO(2) k&gt;0 (r) ψ SO(2) k&gt;0 (−r) 0      fors = −1 .</formula><p>We adapt the conv2triv and norm invariant maps, as well as the norm-ReLU and the gated nonlinearities to operate on Ind -field for k &gt; 0 as a whole, that is, to compute the norm of both subfields together. Instead, we apply two individual norm-ReLUs to the subfields. Since the fields permute under reflections, we need to choose the bias parameter of the two norm-ReLUs to be equal.</p><p>• Ind gate: Similarly, we could apply a single gate to each Ind O(2) SO(2) ψ SO(2) k -field. However, we apply an individual gate to each subfield. In this case it is necessary that the gates permute together with the Ind The difference between both bases is that the induced representations disentangle the action of reflections into a permutation, while the direct sum of irreps is modeling reflections in each of its sub-vectorfields independently as an inversion of the vector direction and rotation orientation. Note the analogy to the better performance of regular representations in comparison to a direct sum of the respective irreps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>* ) ∼ = D 1 (R 2 , +) ({±1}, * ) dihedral 2N D N ∼ = C N ({±1}, * ) (R 2 , +) D N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Transformation behavior of ρ-fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>summarizes the results for all regular steerable CNNs on all variants of MNIST (rows 2-10 and 19-27 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>SO( 2 )SO 1 ,</head><label>21</label><figDesc>irrep models: The feature fields of all SO(2)-equivariant models which we consider are defined to transform under irreducible representations; see Appendix B and F.2. Note that this covers scalar fields and vector fields which transform under ψ respectively. Overall these models are not competitive compared to the regular steerable CNNs. This result is particularly important for SE(3) ∼ = (R 3 , +) SO(3)-equivariant CNNs whose feature fields are often transforming under the irreps of SO(3) [39, 2, 33, 20, 34].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>O</head><label></label><figDesc>Invariant predictions are computed by convolving in equal proportion to fields which transform under trivial irreps ψ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>O( 2 ) 16 Figure 3 :</head><label>2163</label><figDesc>SO(2) ψ SO(2) 0 fields instead of scalar fields. The gated induced irrep model achieves the best results among all O(2)-steerable networks, however, it is still not competitive compared to the D N models with large N . Validation errors and losses during the training of a conventional CNN and CN -equivariant models on MNIST rot. Networks with higher levels of equivariance converge significantly faster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Data ablation study on STL-10. The equivariant models yield significantly improved results on all dataset sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4</head><label>4</label><figDesc>reports the results of a data ablation study which investigates the performance of the D 8 D 4 D 1 models for smaller training set sizes. The results validate that the gains from incorporating equivariance are consistent over all training sets. More information on the exact training procedures is given in Appendix H.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>AFigure 5 :</head><label>5</label><figDesc>Local gauge equivariance of E(2)-steerable CNNs Different viewpoints on transformations of signals on R 2 . Top Left: In our work we considered active rotations of the signal while keeping the coordinate frames fixed. Bottom Left: The equivalent, passive interpretation views the transformation as a global rotation of reference frames (a global gauge transformation). Right: Local gauge transformations rotate reference frames independently from each other. E(2)-steerable CNNs are equivariant w.r.t. both global and local gauge transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>1g eg the regular representation corresponds to a left translation [ρ(h)v](g) = v h −1 g of such functions. Very similarly, the quotient representation ρ G/H quot of G w.r.t. a subgroup H acts on R |G|/|H| by permuting its axes. Labeling the axes by the cosets gH in the quotient space G/H, it can be defined via its action ρ G/H quot (g)e gH = eg gH . An intuitive explanation of quotient representations is given in Appendix C. Regular and trivial representations are two specific cases of quotient representations obtained by choosing H = {e} or H = G, respectively. Vectors in the representation space R |G|/|H| can be viewed as scalar functions on the quotient space G/H. The action of the quotient representations on v then corresponds to a left translation of these functions on G/H. Restricted representations Any representation ρ : G → GL(R n ) can be uniquely restricted to a representation of a subgroup H of G by restricting its domain of definition:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>gH</head><label></label><figDesc>ggH =gR(gH)H Both quotient representations and regular representations can be viewed as being induced from trivial representations of a subgroup. Specifically, let ρ {e} triv : {e} → GL(R) = {(+1)} be the trivial representation of the the trivial subgroup. Then Ind G {e} ρ {e} triv : G → GL(R |G| ) is the regular representation which permutes the cosets g{e} of G/{e} ∼ = G which are in one to one relation to the group elements themself. For ρ H triv : H → GL(R) = {(+1)} being the trivial representation of an arbitrary subgroup H of G, the induced representation Ind G H ρ H triv : G → GL(R |G|/|H| ) permutes the cosets gH of H and thus coincides with the quotient representation ρ G/H quot .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>which corresponds to Eq. (1) for the group G = E(2) = (R 2 , +) O(2), subgroup H = O(2) and quotient space G/H = E(2)/ O(2) = R 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>C, 17 ..</head><label>17</label><figDesc>N /C M quot with C M ≤ C N . By the definition of quotient representations, given in Section 2.6, this implies features which are invariant under the action of C M . For instance, ρ C N /C2 quot -fields encode features like lines, which are invariant under rotations by π. Similarly, ρ C N /C4 quot features are invariant under rotations by π/2, and therefore describe features like a cross. The N/M channels of a ρ C N /C M quot -field respond to different orientations of these patterns, e.g. to + and × for the two channels of ρ C8/C4 quot . A few more examples are given by the 16/2 = 8 channels of ρ C16/C2 quot , which respond to the patterns −, − , − , The rhs. of Eq. (9) corresponds to [Ind G H ρ(g) · f ](ggH) = ρ(h(gR(gH)))f (gH).20 respectively, or the 16/4 = 4 channels of ρIn principle, each of these patterns 18 could be encoded by a regular feature field of C N . A regular field of type ρ C N reg comprises N instead of N/M channels, which detect arbitrary patterns in N orientations, for instance, , , , , , , and for N = 8. In the case of C M -symmetric patterns, e.g. crosses for M = 4, this becomes As evident from this example, the repetition after N/M orientations (here 8/4 = 2), introduces a redundancy in the responses of the regular feature fields. A quotient representation ρ C N /C M quot addresses this redundancy by a-priori assuming the C M symmetry to be present and storing only the N/M non-redundant responses. If symmetric patterns are important for the learning task, a quotient representation can therefore save computational, memory and parameter cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>quot (n)e gN = e ngN = e nN g = e N g = e gN , that it, it describes N -invariant feature fields. The quotient representations ρ G/H quot for general, potentially non-normal subgroups H ≤ G also imply certain symmetries in the feature fields but are not necessarily H-invariant. For instance, the quotient representation ρ D N /C N quot is invariant under rotations since C N is a normal subgroup of D N ∼ = C N ({±1}, * ), while the quotient representation ρ D N /({±1}, * ) quot is not invariant since ({±1}, * ) is not a normal subgroup of D N . In the latter case one has instead ρ D N /({±1}, * ) quot (s)e r({±1}, * ) = e sr({±1}, * ) = e r({±1}, * ) for s = +1 e r −1 s({±1}, * ) = e r −1 ({±1}, * ) for s = −1 for all s ∈ ({±1}, * ) and representatives r ∈ C N . The feature fields are therefore not invariant under the action of ({±1}, * ) but become reversed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>− 1 p=0</head><label>1</label><figDesc>Any pointwise nonlinearity is therefore equivariant under the action of quotient representations. The same holds true for regular representations ρ G reg = ρ G/{e} quot which are a special case of quotient representations for the choice H = {e} D.2.4 Equivariance of vector field nonlinearities w.r.t. regular and standard representationsVector field nonlinearities map an N -dimensional feature field which is transforming under the regular representation ρ C N reg of C N to a vector field. As the elements of C N correspond to rotations by angles θ p ∈ p 2π N N we can write the action of the cyclic group in this specific case as ρ C N reg (θ)e θ := eθ +θ and the feature vector as f (x) = θ∈C N f θ (x)e θ . In this convention vector field nonlinearities are defined asσ vec f (x) := max f (x) cos(arg max f (x)) sin(arg max f (x)) .The maximum operation max : R N → R thereby returns the maximal field value which is invariant under transformations of the regular input field. Observe thatarg max ρ C N reg (θ)f (x) = arg max ρ C N reg (θ) θ∈C N f θ (x)e θ = arg max θ∈C N f θ (x)eθ +θ =θ + arg max (f (x))such thatσ vec ρ C N reg (θ)f (x) = max f (x) cos(θ + arg max f (x)) sin(θ + arg max f (x)) = max f (x) cos(θ) − sin(θ) sin(θ) cos(θ) cos(arg max f (x)) sin(arg max f (x)) transforms under the standard representation ρ(θ) = cos(θ) − sin(θ) sin(θ) cos(θ) of C N . This proofs that the resulting feature field indeed transforms like a vector field. The original paper [13] used a different convention arg max : R N → {0, . . . , N 1}, returning the integer index of the maximal vector entry. This leads to a corresponding rotation angle θ = 2π N arg max(f (x)) ∈ C N in terms of which the vector field nonlinearity reads σ vec f (x) = max f (x) cos(θ) sin(θ) D.2.5 Equivariance of nonlinearities w.r.t. induced representations Consider a group H &lt; G with two representations ρ in : H → GL(R cin ) and ρ out : H → GL(R cout ). Suppose we are given an equivariant non-linearity σ : R cin → R cout with respect to the actions of ρ in and ρ out , that is, ρ out (h) • σ = σ • ρ in (h) ∀h ∈ H. Then an induced non-linearityσ, equivariant w.r.t. the induced representations Ind G H ρ in and Ind G H ρ out of G, can be defined as applying σ independently to each of the |G : H| different c in -dimensional subspaces of the representation space which are being permuted by the action of Ind G H ρ in , see Appendix B. The permutation of the subspaces commutes with the individual action of the nonlinearityσ on the subspaces, while the non-linearity σ itself commutes with the transformation within the subspaces through ρ by assumption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>F. 1 N N − 1 p=0</head><label>11</label><figDesc>and ({±1}, * ) in sections F.4.1 and F.4.2.The G-steerable kernel basis for O(2) forms a subspace of the kernel basis for SO<ref type="bibr" target="#b1">(2)</ref> such that it can be easily derived from this solution by adding the additional constraint coming from the reflectional symmetries in ({±1}, * ) ∼ = O(2)/ SO<ref type="bibr" target="#b1">(2)</ref>. This additional constraint is imposed in Section F.4.3. Since C N is a subgroup of discrete rotations in SO(2) their derivation is mostly similar. However, the discreteness of rotation angles leads to N systems of linear congruences modulo N in the final step. This system of equations is solved in Section F.4.4. Similar to how we derived the kernel basis for O(2) from SO(2), we derive the basis for D N from C N by adding reflectional constraints from ({±1}, * ) ∼ = D N / C N in Section F.4.5. Conventions, notation and basic properties Throughout this section we denote rotations in SO<ref type="bibr" target="#b1">(2)</ref> and C N by r θ with θ ∈ [0, 2π) and θ ∈ p 2π respectively. Since O(2) ∼ = SO(2) ({±1}, * ) can be seen as a semidirect product of rotations and reflections we decompose orthogonal group elements into a unique product g = r θ s ∈ O(2) where s ∈ ({±1}, * ) is a reflection and r θ ∈ SO<ref type="bibr" target="#b1">(2)</ref>. Similarly, we write g = r θ s ∈ D N for the dihedral group D N ∼ = C N ({±1}, * ), in this case with r θ ∈ C N .The action of a rotation r θ on R 2 in polar coordinates x(r, φ) is given by r θ .x(r, φ) = x(r, r θ .φ) = x(r, φ + θ). An element g = r θ s of O<ref type="bibr" target="#b1">(2)</ref> or D N acts on R 2 as g.x(r, φ) = x(r, r θ s.φ) = x(r, sφ + θ) where the symbol s denotes both group elements in ({±1}, * ) and numbers in {±1}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>orthonormal matrix with negative determinant, i.e. reflection with respect to the axis 2θ, as: cos (θ) sin (θ) sin (θ) cos (θ) = cos (θ) sin (θ) sin (θ) cos (θ) can express any orthonormal matrix in the form: s ∈ ({±1}, * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>2 }</head><label>2</label><figDesc>) = cos (kθ) sin (kθ) sin (kθ) cos (kθ) = ψ(kθ), k ∈ N + Orthogonal group O(2): O(2) has two 1-dimensional irreps: the trivial representation ψ 0,0 and a representation ψ 1,0 which assigns ±1 to reflections. The other representations are rotation matrices precomposed by a reflection. kθ)ξ(s), k ∈ N + Cyclic groups C N : The irreps of C N are identical to the irreps of SO(2) up to frequency N/2 . Due to the discreteness of rotation angles, higher frequencies would be aliased. -ψ C N 0 (r θ ) = 1 -ψ C N k (r θ ) = cos (kθ) sin (kθ) sin (kθ) cos (kθ) = ψ(kθ), k ∈ {1, . . . , N −1If N is even, there is an additional 1-dimensional irrep corresponding to frequency N 2 = N 2 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>p=0F. 3</head><label>3</label><figDesc>Reflection group ({±1}, * ) ∼ = D 1 ∼ = C 2 :The reflection group ({±1}, * ) is isomorphic to D 1 and C 2 . For this reason, it has the same irreps of these two groups: Analytical solutions of the irrep kernel constraints Special Orthogonal Group SO<ref type="bibr" target="#b1">(2)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>11 :</head><label>11</label><figDesc>Bases for the angular parts of CN -steerable kernels for different pairs of input and output fields irreps ψn and ψm. The full basis is found by instantiating these solutions for each t ∈ Z ort ∈ N. The different types of irreps are explained in Appendix F.2. Dihedral groups D N ψ i,m ψ j,n ψ 0,0 ψ 1,0 ψ 0,N/2 (if N even) ψ 1,N/2 (if N even) ψ 1,n with n ∈ N + and 1 ≤ n &lt; N/2 ψ 0,0 cos(tN φ) sin(tN φ) cos t + 1 2 N φ sin t + 1 2 N φ sin((n + tN )φ) cos((n + tN )φ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>/ 2 sin</head><label>2</label><figDesc>((m + tN )φ) cos((m + tN )φ) cos((m + tN )φ) sin((m + tN )φ) sin m + t+ 1 2 N φ cos m + t+ 1 2 N φ cos m + t+ 1 2 N φ sin m + t+ 1 2 N φ cos (m−n + tN)φ sin (m−n + tN)φ sin (m−n + tN)φ cos (m−n + tN)φ , cos (m+n + tN)φ sin (m+n + tN)φ sin (m+n + tN)φ cos (m+n + tN)φ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>2 sin µφ + π 2 sin µφ + π 2 cos µφ + π 2 + 2 sin µφ + π 2 sin µφ + π 2 cos µφ + π 2 + 2 sin µφ + π 2 sin µφ + π 2 cos µφ + π 2 + 2 sin µφ + π 2 sin µφ + π 2 cos µφ + π 2 .</head><label>2222222222222222</label><figDesc>) sin (kθ) sin (kθ) cos(kθ)   . This means that the kernel has the form κ ij : R 2 → R 2×2 . To reduce clutter we will from now on suppress the indices ij corresponding to the input and output irreps in the input and output fields.We expand each entry of the kernel κ in terms of an (angular) Fourier series 20 κ(r, φ) = convenience, perform a change of basis to a different, non-sparse, orthogonal basis κ(r, φ) = ∞ µ=0 w 0,0,µ (r) cos (µφ) sin (µφ) sin (µφ) cos (µφ) + w 0,1,µ (r) cos µφ + π w 1,0,µ (r) cos (µφ) sin (µφ) sin (µφ) cos (µφ) + w 1,1,µ (r) cos µφ + π w 2,0,µ (r) cos ( µφ) sin ( µφ) sin ( µφ) cos ( µφ) + w 2,1,µ (r) cos µφ + π w 3,0,µ (r) cos ( µφ) sin ( µφ) sin ( µφ) cos ( µφ) + w 3,1,µ (r) cos µφ + π</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>on a generic basis element b µ ,γ ,s (φ) = ψ(µ φ + γ )ξ(s ). Defining the operator R θ by (R θ κ) (φ) := κ(φ + θ), the projection gives:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>w s,γ,µ tr (ξ(s )ψ(γ − γ )ψ(µ θ)ξ(s)) , which, using the property in Eq. (14) leads to = 1 2 γ s w s,γ,µ tr (ψ(s (γ − γ + µ θ))ξ(s * s)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>w s,γ,µ tr ξ(s )ψ(γ γ )ψ(mθ) 1 2π dφ ψ((µ µ )φ) ξ(s)ψ( nθ)Again, the integral evaluates to δ µ,µ id 2×2 := 1 2 µ γ s w s,γ,µ δ µ,µ tr (ξ(s )ψ(γ − γ )ψ(mθ)ξ(s)ψ(−nθ)) = 1 2 γ s w s,γ,µ tr (ξ(s )ψ(γ − γ )ψ(mθ)ξ(s)ψ(−nθ)) = 1 2 γ s w s,γ,µ tr (ψ(s (γ − γ + mθ − nsθ))ξ(s * s))For the same reason as before, the trace is not zero if and only if s = s: w s,γ,µ δ s ,s 2 cos(s (γ − γ + mθ − nsθ)) Since cos(−α) = cos(α) and s ∈ {±1}: = γ w s ,γ,µ cos(γ − γ + mθ − ns θ) = γ w s ,γ,µ cos((γ − γ ) + (m − ns )θ) Finally, we require the two projections to be equal for all rotations in SO(2), that is, γ w s ,γ,µ cos((γ − γ ) + µ θ) = γ w s ,γ,µ cos((γ − γ ) + (m − ns )θ) ∀θ ∈ [0, 2π),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>= µ,γ w µ,γ 1 2π</head><label>1</label><figDesc>dφ b µ ,γ (φ) cos(µφ + µθ + γ) = µ,γ w µ,γ 1 2π dφ cos(µ φ + γ ) cos(µφ + µθ + γ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>b µ ,γ , R θ κ = b µ ,γ , κ ∀θ ∈ [0, 2π) γ w µ ,γ (cos((γ −γ)−µ θ) + δ µ ,0 cos((γ +γ))) = γ w µ ,γ (cos(γ −γ)+δ µ ,0 cos((γ +γ))) ∀θ ∈ [0, 2π)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>1 and 2 -</head><label>2</label><figDesc>dimensional irreps: Finally, consider the case of a 1-dimensional irrep in the input and a 2-dimensional irrep in the output, that is, ρ out = ψ SO(2) m and ρ in = ψ SO(2) 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>KSO( 2 )</head><label>2</label><figDesc>ψm←ψn = b n,γ (φ) = [cos(nφ + γ) sin(nφ + γ)] γ ∈ 0, π 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>= s. Therefore only the 1-dimensional case with a kernel of form κ : R 2 → R 1×1 exists. Note that we can write the irreps out as ψ({±1}, * ) f (s) = s f , in particular ψ ({±1}, * ) f (−1) = (−1) f . Consider the output and input irreps ρ out = ψ ({±1}, * ) i and ρ in = ψ ({±1}, * ) j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>γ w µ ,γ 1 2 (</head><label>2</label><figDesc>cos(γ + γ) + δ µ ,0 cos(γ γ)) = ( 1) γ γ) + δ µ ,0 cos(γ + γ)) and again consider two cases for µ :• µ = 0 The basis in Eq.(30) is restricted to the single case γ = 0. Hence: −γ) + cos(γ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>1 ( 1 ( 1 (</head><label>111</label><figDesc>and κ(r, sφ) = Res O(2) ({±1}, * ) ρ out (s) κ(r, φ) Res O(2) ({±1}, * ) ρ in −s) ∀ s ∈ ({±1}, * )= ρ out (s) κ(r, φ) ρ −1 in (s) . Then, for any g = r θ s ∈ O(2), the kernel constraint becomes:κ(r, gφ) = ρ out (g) κ(r, φ) ρ −1 in (g) ⇔ κ(r, r θ sφ) = ρ out (r θ s) κ(r, φ) ρ −1 in (r θ s) ⇔ κ(r, r θ sφ) = ρ out (r θ )ρ out (s) κ(r, φ) ρ −1 in (s)ρ −1 in (r θ ) . Applying reflection-equivariance this equation simplifies to ⇔ κ(r, r θ sφ) = ρ out (r θ ) κ(r, sφ) ρ −1 in (r θ ), which, applying rotation-equivariance yields ⇔ κ(r, r θ sφ) = κ(r, r θ sφ) .Hence any kernel satisfying both SO(2) and reflection constraints is also O(2) equivariant.Necessity:Trivially, O(2) equivariance implies equivariance under SO(2) and reflections. Specifically, for any r ∈ R + 0 and φ ∈ [0, 2π), the equation κ(r, gφ) = ρ out (g) κ(r, φ) ρ −1 in (g) ∀ g = r θ s ∈ O(2) implies κ(r, r θ φ) = ρ out (r θ ) κ(r, φ) ρ −1 in (r θ ) = Res O(2) SO(2) ρ out (r θ ) κ(r, φ) Res O(2) SO(2) ρ in −r θ ) ∀ r θ ∈ SO(2) and κ(r, sφ) = ρ out (s) κ(r, φ) ρ −1 in * ) ρ out (s) κ(r, φ) Res O(2) ({±1}, * ) ρ in −s) ∀ s ∈ ({±1}, * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>(O</head><label></label><figDesc>restriction of any 2-dimensional irrep ψ O(2) 1,n of O(2) to the reflection group decomposes into the direct sum of the two 1-dimensional irreps of the reflection group, i.e. into the diagonal matrix Res O(2) ({±1}, * ) ψ 1,n (s) = ψIt follows that the restricted kernel space constraint decomposes into independent constraints on each entry of the original kernel. Specifically, for output and input representations ρ out = ψ * ) ρ in (s) We can therefore solve for a basis for each entry individually following Appendix F.4.2 to obtain the complete basis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>=</head><label></label><figDesc>b µ,s (φ) = ψ(µφ)ξ(s) µ∈Z,s∈{±1} . (33) On the other hand, 2-dimensional O(2) representations restrict to the SO(2) irreps of the corresponding frequency, i.e. Res O(2) SO(2) ρ in = Res O(2) SO(2) ψ O(2) 1,n (r θ ) = ψ SO(2) n (r θ ) (r θ ) = ψ SO(2) m (r θ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>1 - 1 .O</head><label>11</label><figDesc>dimensional irreps: O(2) has two 1-dimensional irreps ψ O(2) 0,0 and ψ O(2) 1,0 (see Appendix F.2). Both are trivial under rotations and each of them corresponds to one of the two reflection group's irreps, i.e. Considering output and input representations ρ out = ψ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>1 and 2 -</head><label>2</label><figDesc>dimensional irreps: Now we consider the 2-dimensional output representation ρ out = ψ O(2) 1,m and the 1-dimensional input representation ρ in = ψ O(2) j,0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head></head><label></label><figDesc>* ) ρ in (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>O( 2 )</head><label>2</label><figDesc>SO(2) ρ in = ψ SO(2) 0 input field and Res O(2) SO(2) ρ out = ψ SO(2) m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>2 and 1 -</head><label>1</label><figDesc>dimensional irreps: As already argued in the case for SO(2), the basis for 2-dimensional input representations ρ in = ψ O(2) 1,n and 1-dimensional output representations ρ out = ψ O(2) i,0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head></head><label></label><figDesc>m←ψj,n = b µ,γ (φ) = [cos(µφ + γ) sin(µφ + γ)] µ = (±m + n) + tN, γ = i π 2 t∈Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>• O( 2 )O</head><label>2</label><figDesc>-conv2triv: As invariant map of the O(2) irrep models in rows 46-49 and 54 we are designing a last convolution layer which is mapping to an output representation ρ out = ψ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head></head><label></label><figDesc>representations are induced from the irreps of SO<ref type="bibr" target="#b1">(2)</ref>. Per definition, this representation acts on fea-ture vectors f in R dim(ψ SO(2) k ) ⊗ R | O(2):SO(2)| ,which we treat in the following as functions f : O(2)/ SO(2) → R dim(ψ SO(2) k ) . We further identify the coset s SO(2) in the quotient space O(2)/ SO(2) by its representative R(s SO(2)) := s ∈ ({±1}, * ) in the reflection group. Eq. 10 defines the action of the induced representation on a feature vector by Ind f (s SO(2)) := ψ SO(2) k h rsR((rs) −1 s SO(2)) f (rs) −1 s SO(2) = ψ SO(2) k h(rs) f ss SO(2) = ψ SO(2) k r) f ss SO(2) for s = +1 ψ SO(2) k r −1 ) f ss SO(2) for s = −1 , 54 where we used Eq. 8 to compute h(rs) := R rs SO(2) −1r s = s −1r s = r for s = +1 r −1 for s = −1 . Intuitively, this action describes a permutation of the subfields (indexed by s) via the reflections and a rotation of the subfields byr andr −1 , respectively. Specifically, for k = 0, the induced representation is for allr instantiated byInd</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>O( 2 )SO( 2 )--</head><label>22</label><figDesc>-fields as follows: • Ind-conv2triv: Instead of applying O(2)-conv2triv to compute invariant features, we apply convolutions to Ind fields which are invariant under rotations but behave like regular ({±1}, * )-fields under reflections. These fields are subsequently mapped to a scalar field via G-pooling, i.e. by taking the maximal response over the two subfields. • Ind-norm: An alternative invariant map is defined by computing the norms of the subfields of each final Ind field and applying G-pooling over the result.• Ind norm-ReLU: It would be possible to apply a norm-ReLU to a Ind</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head></head><label></label><figDesc>which contain two permuting scalar fields.Empirically we find that Ind O(2) SO(2) models perform much better than pure irrep models, despite both of them being equivalent up to a change of basis. In particular, the induced representations decompose for some change of basis matrices Q 0 and Q &gt;0 into:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview over the different groups covered in our framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Bases for the angular parts of O(2)-steerable kernels satisfying the irrep constraint (3) for different pairs of input field irreps ψj and output field irreps ψi.The different types of irreps are explained in Appendix F.2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Test errors of CN and DN regular steerable CNNs for different orders N for all three MNIST variants. Left: All equivariant models improve upon the non-equivariant CNN baseline on MNIST O(2). The error decreases before saturating at around 8 to 12 orientations. Since the dataset contains reflected digits, the DN -equivariant models perform better than their CN counterparts. Middle: Since the intraclass variability of MNIST rot is reduced, the performances of the CN model and the baseline CNN improve on this dataset. In contrast, the DN models are invariant to reflections such that they can't distinguish between MNIST O(2) and MNIST rot. For N = 1 this leads to a worse performance than that of the baseline. Restricted dihedral models, denoted by DN |5CN , make use of the local reflectional symmetries but are not globally invariant. This makes them perform even better than the CN models. Right: On MNIST 12k the globally invariant models CN and DN don't yield better results than the baseline, however, the restricted (i.e. non-invariant) models CN |5{e} and DN |5{e} do. For more details see the main text.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">MNIST O(2)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">MNIST rot</cell><cell></cell><cell></cell><cell></cell><cell cols="3">MNIST 12k</cell><cell></cell></row><row><cell></cell><cell>3 4 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CN DN CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CN DN DN|5CN CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CN DN CN|5{e} DN|5{e} CNN</cell></row><row><cell>test error (%)</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell>4</cell><cell>8</cell><cell>N</cell><cell>12</cell><cell>16</cell><cell>20</cell><cell>4</cell><cell>8</cell><cell>N</cell><cell>12</cell><cell>16</cell><cell>20</cell><cell>4</cell><cell>8</cell><cell>N</cell><cell>12</cell><cell>16</cell><cell>20</cell></row><row><cell cols="20">Figure 2: very well. The reason for this is that feature vectors, transforming under regular representations, can</cell></row><row><cell cols="8">encode any function on the group.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">as pointwise nonlinearities and perform group pooling (see</cell></row><row><cell cols="20">Section 2.6) as invariant map after the final convolution. Overall, regular steerable CNNs perform</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, 7, 9, 10]  2.40 ± 0.05 1.02 ± 0.03 0.99 ± 0.036 C 6 [8]2.08 ± 0.03 0.89 ± 0.03 0.84 ± 0.02 7 C 8<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> 1.96 ± 0.04 0.84 ± 0.02 0.89 ± 0.03</figDesc><table><row><cell></cell><cell cols="2">group representation</cell><cell>nonlinearity</cell><cell>invariant map</cell><cell cols="3">citation MNIST O(2) MNIST rot MNIST 12k</cell></row><row><cell>1</cell><cell>{e}</cell><cell>(conventional CNN)</cell><cell>ELU</cell><cell>-</cell><cell>-</cell><cell>5.53 ± 0.20</cell><cell>2.87 ± 0.09 0.91 ± 0.06</cell></row><row><cell>2</cell><cell>C 1</cell><cell></cell><cell></cell><cell></cell><cell>[7, 9]</cell><cell>5.19 ± 0.08</cell><cell>2.48 ± 0.13 0.82 ± 0.01</cell></row><row><cell>3</cell><cell>C 2</cell><cell></cell><cell></cell><cell></cell><cell>[7, 9]</cell><cell>3.29 ± 0.07</cell><cell>1.32 ± 0.02 0.87 ± 0.04</cell></row><row><cell>4</cell><cell>C 3</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>2.87 ± 0.04</cell><cell>1.19 ± 0.06 0.80 ± 0.03</cell></row><row><cell cols="5">5 [6, 18 C 4 C 12</cell><cell>[7]</cell><cell>1.95 ± 0.07</cell><cell>0.80 ± 0.03 0.89 ± 0.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">but are discussed in Section 3.2.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>9 C 16</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Extensive comparison of G-steerable CNNs for different choices of groups G, representations, nonlinearities and final G-invariant maps on three transformed MNIST datasets. Multiplicities of representations are reported in relative terms; the actual multiplicities are integer multiples with a depth dependent factor. All models apply a G-invariant map after the convolutions to guarantee an invariant prediction. Citations give credit to the works which proposed the corresponding model design. For reference see Sections 2.6, 3.1, C and I.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>after the penultimate layer.</figDesc><table><row><cell>model</cell><cell>group</cell><cell cols="2">representation test error (%)</cell></row><row><cell>[6]</cell><cell>C4</cell><cell cols="2">regular/scalar 3.21 ± 0.0012</cell></row><row><cell>[6]</cell><cell>C4</cell><cell>regular</cell><cell>2.28 ± 0.0004</cell></row><row><cell>[12]</cell><cell>SO(2)</cell><cell>irreducible</cell><cell>1.69</cell></row><row><cell>[40]</cell><cell>-</cell><cell>-</cell><cell>1.2</cell></row><row><cell>[13]</cell><cell>C17</cell><cell cols="2">regular/vector 1.09</cell></row><row><cell>Ours</cell><cell>C16</cell><cell>regular</cell><cell>0.716 ± 0.028</cell></row><row><cell>[7]</cell><cell>C16</cell><cell>regular</cell><cell>0.714 ± 0.022</cell></row><row><cell>Ours</cell><cell>C16</cell><cell>quotient</cell><cell>0.705 ± 0.025</cell></row><row><cell cols="3">Ours D16|5C16 regular</cell><cell>0.682 ± 0.022</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Final runs on MNIST rot D1 D1 D1 3.36 ± 0.08 17.97 ± 0.11 wrn28/10* D8 D4 D1 3.28 ± 0.10 17.42 ± 0.33 wrn28/10 C8 C4 C1 3.20 ± 0.04 16.47 ± 0.22 wrn28/10 D8 D4 D1 3.13 ± 0.17 16.76 ± 0.40 wrn28/10 D8 D4 D4 2.91 ± 0.13 16.22 ± 0.31 D8 D4 D1 AA 2.39 ± 0.11 15.55 ± 0.13 wrn28/10 D8 D4 D1 AA 2.05 ± 0.03 14.30 ± 0.09</figDesc><table><row><cell>model</cell><cell></cell><cell cols="2">CIFAR-10 CIFAR-100</cell></row><row><cell>wrn28/10</cell><cell>[41]</cell><cell>3.87</cell><cell>18.80</cell></row><row><cell>wrn28/10 wrn28/10</cell><cell cols="2">[42] AA 2.6 ± 0.1</cell><cell>17.1 ± 0.3</cell></row><row><cell>wrn28/10*</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Test errors on CIFAR (AA=autoaugment)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Test errors of different equivariant models on the STL-10 dataset. Models with * are not scaled to the same number of parameters as the original model but preserve the number of channels of the baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Bases for the angular parts of SO(2)-steerable kernels satisfying the irrep constraint (3) for different pairs of input field irreps ψn and output field irreps ψm. The different types of irreps are explained in F.2.</figDesc><table><row><cell>Orthogonal Group O(2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Bases for the angular parts of O(2)-steerable kernels satisfying the irrep constraint (3) for different pairs of input field irreps ψj,n and output field irreps ψi,m. The different types of irreps are explained in F.2.</figDesc><table /><note>Reflection group ({±1}, * )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Bases for the angular parts of ({±1}, * )-steerable kernels satisfying the irrep constraint (3) for different pairs of input field irreps ψj and output field irreps ψi for i, j ∈ {0, 1}. The different types of irreps are explained in F.2. The group is assumed to act by reflecting over an axis defined by the angle β. Note that the bases shown here are a special case of the bases shown inTable 12since ({±1}, * ) ∼ = D1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>As already shown in Section 2.5, we can w.l.o.g. consider the kernels as being defined only on the angular component φ ∈ [0, 2π) = S 1 by solving only for a specific radial component r. As a result, we consider the basis b µ,γ,s</figDesc><table><row><cell>γ,µ (r)</cell><cell cols="2">cos (µφ+γ) sin (µφ+γ) sin (µφ+γ) cos (µφ+γ)</cell><cell>+w 1,γ,µ (r)</cell><cell>cos (µφ+γ) sin (µφ+γ) sin (µφ+γ) cos (µφ+γ)</cell></row><row><cell cols="5">Notice that the first matrix evaluates to ψ(µφ + γ)ξ(1) = ψ(µφ + γ) while the second evaluates to</cell></row><row><cell cols="4">ψ(µφ + γ)ξ(−1). Hence, for s ∈ {±1} we can compactly write:</cell></row><row><cell></cell><cell>∞</cell><cell></cell><cell></cell></row><row><cell cols="2">κ(r, φ) =</cell><cell cols="3">w s,γ,µ (r)ψ(µφ + γ)ξ(s)</cell></row><row><cell></cell><cell>µ=−∞ γ∈{0, π 2 } s∈{±1}</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Derivation for D N A solution for D N can easily be derived by repeating the process done for O(2) in Appendix F.4.3 but starting from the bases derived for C N in Appendix F.4.4 instead of those for SO</figDesc><table><row><cell>F.4.5</cell><cell></cell></row><row><cell>γ (φ) = [cos(µφ + γ) sin(µφ + γ)] µ = (±m+n)+tN, γ ∈ 0,</cell><cell>π 2 t∈Z</cell></row><row><cell>for n &gt; 0 and m ∈ 0, N 2 . See the top right cells in Table 11.</cell><cell>(43)</cell></row><row><cell>47</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>H Additional information on the training setup</figDesc><table><row><cell>layer</cell><cell>output fields</cell></row><row><cell>conv block 7 × 7 (pad 1)</cell><cell>16</cell></row><row><cell>conv block 5 × 5 (pad 2)</cell><cell>24</cell></row><row><cell>max pooling 2 × 2</cell><cell>24</cell></row><row><cell>conv block 5 × 5 (pad 2)</cell><cell>32</cell></row><row><cell>conv block 5 × 5 (pad 2)</cell><cell>32</cell></row><row><cell>max pooling 2 × 2</cell><cell>32</cell></row><row><cell>conv block 5 × 5 (pad 2)</cell><cell>48</cell></row><row><cell>conv block 5 × 5</cell><cell>64</cell></row><row><cell>invariant projection</cell><cell>64</cell></row><row><cell>global average pooling</cell><cell>64</cell></row><row><cell>fully connected</cell><cell>64</cell></row><row><cell>fully connected + softmax</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 13 :</head><label>13</label><figDesc></figDesc><table><row><cell></cell><cell>layer</cell><cell>output fields</cell></row><row><cell></cell><cell>conv block 9 × 9</cell><cell>24</cell></row><row><cell></cell><cell>conv block 7 × 7 (pad 3)</cell><cell>32</cell></row><row><cell></cell><cell>max pooling 2 × 2</cell><cell>32</cell></row><row><cell></cell><cell>conv block 7 × 7 (pad 3)</cell><cell>36</cell></row><row><cell></cell><cell>conv block 7 × 7 (pad 3)</cell><cell>36</cell></row><row><cell></cell><cell>max pooling 2 × 2</cell><cell>36</cell></row><row><cell></cell><cell>conv block 7 × 7 (pad 3)</cell><cell>64</cell></row><row><cell></cell><cell>conv block 5 × 5</cell><cell>96</cell></row><row><cell></cell><cell>invariant projection</cell><cell>96</cell></row><row><cell></cell><cell>global average pooling</cell><cell>96</cell></row><row><cell></cell><cell>fully connected</cell><cell>96</cell></row><row><cell></cell><cell>fully connected</cell><cell>96</cell></row><row><cell>Basic model architecture from which</cell><cell>fully connected + softmax</cell><cell>10</cell></row><row><cell>all models for the MNIST benchmarks in Tables 3</cell><cell></cell><cell></cell></row><row><cell>and 4 are being derived. Each convolution block</cell><cell></cell><cell></cell></row><row><cell>includes a convolution layer, batch-normalization</cell><cell></cell><cell></cell></row><row><cell>and a nonlinearity. The first fully connected layer</cell><cell></cell><cell></cell></row><row><cell>is followed by batch-normalization and ELU. The</cell><cell></cell><cell></cell></row><row><cell>width of each layer is expressed as the number of</cell><cell></cell><cell></cell></row><row><cell>fields of a regular C16 model with approximately</cell><cell></cell><cell></cell></row><row><cell>the same number of parameters.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Model architecture for the final MNISTrot experiments (replicated from</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">i=1 1 = id 3×3 of three trivial representations. While the input and output types of steerable CNNs are given by the learning task, the user needs to specify the types ρ i of intermediate feature fields as hyperparameters, similar to the choice of channels for vanilla CNNs. We discuss different choices of representations in Section 2.6 and investigate them empirically in Section 3.1.<ref type="bibr" target="#b0">1</ref> Steerable feature fields can therefore be seen as fields of capsules<ref type="bibr" target="#b27">[28]</ref>.<ref type="bibr" target="#b1">2</ref> Induced representations are the most general transformation laws compatible with convolutions<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.<ref type="bibr" target="#b2">3</ref> Note that this simple form of the induced representation is a special case for semidirect product groups.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The induction from H to G considered here is conceptually equivalent to the induction (1) from G to G (R 2 , +) but applies to representations acting on a single feature vector rather than on a full feature field.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Mathematically, G-steerable CNNs classify equivalence classes of images defined by the equivalencerelation f ∼ f ⇔ ∃ tg ∈ (R 2 , +) G s.t. f (x) = f g −1 (x − t). Instead, MLPs learn to classify each image individually and conventional CNNs classify equivalence classes defined by translations, i.e. above equivalence classes for G = {e}. For more details see Section 2 of<ref type="bibr" target="#b6">[7]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">The library is available at https://github.com/QUVA-Lab/e2cnn.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Conversely, the equivariance under local gauge transformations g(x) ∈ O(2) implies the equivariance under active isometries. In the case of the Euclidean space R 2 these isometries are given by the Euclidean group E(2).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">The vector can equivalently be expressed as w = gH wgH , however, we want to make the tensor product basis explicit.<ref type="bibr" target="#b15">16</ref> Formally, a representative for each coset is chosen by a map R : G/H → G such that it projects back to the same coset, i.e. R(gH)H = gH. This map is therefore a section of the principal bundle G π → G/H with fibers isomorphic to H and the projection given by π(g) := gH.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">For brevity, we suppress that frequency 0 is associated to only half the number of basis elements which does not affect the validity of the derivation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">Notice that for µ = 0 some of the elements of the set are zero and are therefore not part of the basis. We omit this detail to reduce clutter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">This establishes an isomorphism between ψ C k (θ) and ψ R |k| (θ) depending on the sign of k.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24">No inversion from Q to Q −1 is necessary if the Sylvester equation is solved directly for Q −1 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25">https://github.com/uoguelph-mlrg/Cutout/issues/2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Taco Cohen for fruitful discussions on an efficient implementation and helpful feedback on the paper and Daniel Worrall for elaborating on the real valued implementation of Harmonic Networks.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since cos(nθ) ∈ {±1} the inverses can be dropped and terms can be collected via trigonometric identities:</p><p>We require the projections to be equal for each</p><p>Again, we consider two cases for µ :</p><p>• µ = 0 : The basis in Eq. <ref type="formula">(40)</ref>  If cos((±m ± n)θ) = 1, the coefficient w 0,0 is forced to 0. Conversely:</p><p>(±m ± n)θ = 2tπ</p><p>Using θ = p 2π N :</p><p>⇔ w µ ,0 cos(γ − µ θ) + w µ , π 2 sin(γ − µ θ) = cos((±m ± n)θ) w µ ,0 cos(γ ) + w µ , π 2 sin(γ ) Since (±m ± n)θ ∈ { π, 0, π} we have cos((±m ± n)θ) = ±1, therefore:</p><p>⇔ w µ ,0 cos(γ µ θ) + w µ , π 2 sin(γ µ θ) = = w µ ,0 cos(γ + (±m ± n)θ) + w µ , π 2 sin(γ + (±m ± n)θ)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Steerable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D steerable CNNs: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Intertwiners between induced representations (with applications to the theory of equivariant neural networks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10743</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A general theory of equivariant CNNs on homogeneous spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02017</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkay</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Jorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><forename type="middle">S</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hexaconv</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Roto-translation covariant convolutional networks for medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maxime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitko</forename><surname>Lafarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eppenhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Josien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remco</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting cyclic symmetry in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the generalization of equivariance and convolution in neural networks to the action of compact groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combined scattering for rotation invariant texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="68" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1687</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep roto-translation scattering for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clebsch-Gordan Nets: A Fully Fourier Space Spherical Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning SO(3) equivariant representations with spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">DeepSphere: Efficient spherical Convolutional Neural Network with HEALPix sampling for cosmological applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanaël</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Kacprzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sgier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12186</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>astro-ph]</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spherical CNNs on unstructured grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-directional geodesic neural networks via equivariant convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Poulenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spectral Networks and Deep Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Linear representations of finite groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to convolve: A generalized weight-tying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichita</forename><surname>Diaconu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D G-CNNs for pulmonary nodule detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marysia</forename><surname>Winkels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Imaging with Deep Learning (MIDL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cubenet: Equivariance to 3D rotation and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<idno type="arXiv">arXiv:1803.01588</idno>
		<title level="m">Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Truong-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04015</idno>
		<title level="m">Cormorant: Covariant molecular neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="96" to="107" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rotation equivariant CNNs for digital pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><forename type="middle">S</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ti-pooling: Transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Matrix capsules with EM routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabour</forename><surname>Sara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
