<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR &amp; AIRIA</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR &amp; AIRIA</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR &amp; AIRIA</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR &amp; AIRIA</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Skeleton</term>
					<term>Action Recognition</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dynamic skeletal data, represented as the 2D/3D coordinates of human joints, has been widely studied for human action recognition due to its high-level semantic information and environmental robustness. However, previous methods heavily rely on designing handcrafted traversal rules or graph topologies to draw dependencies between the joints, which are limited in performance and generalizability. In this work, we present a novel decoupled spatial-temporal attention network (DSTA-Net) for skeleton-based action recognition. It involves solely the attention blocks, allowing for modeling spatial-temporal dependencies between joints without the requirement of knowing their positions or mutual connections. Specifically, to meet the specific requirements of the skeletal data, three techniques are proposed for building attention blocks, namely, spatial-temporal attention decoupling, decoupled position encoding and spatial global regularization. Besides, from the data aspect, we introduce a skeletal data decoupling technique to emphasize the specific characteristics of space/time and different motion scales, resulting in a more comprehensive understanding of the human actions. To test the effectiveness of the proposed method, extensive experiments are conducted on four challenging datasets for skeleton-based gesture and action recognition, namely, SHREC, DHG, NTU-60 and NTU-120, where DSTA-Net achieves state-of-the-art performance on all of them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition has been studied for decades since it can be widely used for many applications such as human-computer interaction and abnormal behavior monitoring <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref>. Recently, skeletal data draws increasingly more attention because it contains higher-level semantic information in a small amount of data and has strong adaptability to the dynamic circumstance <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>The raw skeletal data is a sequence of frames each contains a set of points. Each point represents a joint of human body in the form of 2D/3D coordinates. Previous data-driven methods for skeleton-based action recognition rely on manual designs of traversal rules or graph topologies to transform the raw skeletal data into a meaningful form such as a point-sequence, a pseudo-image or a graph, so that they can be fed into the deep networks such as RNNs, CNNs and GCNs for feature extraction <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>. However, there is no guarantee that the hand-crafted rule is the optimal choice of modeling global dependencies of joints, which limits the performance and generalizability of previous approaches. Recently, transformer <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b4">5]</ref> has achieved big success in the NLP field, whose basic block is the self-attention mechanism. It can learn the global dependencies between the input elements with less computational complexity and better parallelizability. For skeletal data, employing the self-attention mechanism has an additional advantage that there is no requirement of knowing a intrinsic relations between the elements, thus it provides more flexibility for discovering useful patterns. Besides, since the number of joints of the human body is limited, the extra cost of applying self-attention mechanism is also relatively small. Inspired by above observations, we propose a novel decoupled spatial-temporal attention networks (DSTA-Net) for skeleton-based action recognition. It is based solely on the self-attention mechanism, without using the structure-relevant RNNs, CNNs or GCNs. However, it is not straightforward to apply a pure attention network for skeletal data as shown in following three aspects: <ref type="bibr" target="#b0">(1)</ref> The input of original self-attention mechanism is the sequential data, while the skeletal data exists in both the spatial and temporal dimensions. A naive method is simply flattening the spatial-temporal data into a single sequence like <ref type="bibr" target="#b31">[32]</ref>. However, it is not reasonable to treat the time and space equivalently because they contain totally different semantics <ref type="bibr" target="#b7">[8]</ref>. Besides, simple flattening operation increases the sequence length, which greatly increases the computation cost due to the dot-product operation of the self-attention mechanism. Instead, we propose to decouple the self-attention mechanism into the spatial attention and the temporal attention sequentially. Three strategies are specially designed to balance the independence and the interaction between the space and the time. <ref type="bibr" target="#b1">(2)</ref> There are no predefined orders or structures when feeding the skeletal joints into the attention networks. To provide unique markers for every joint, a position encoding technique is introduced. For the same reason as before, it is also decoupled into the spatial encoding and the temporal encoding. (3) It has been verified that adding proper regularization based on prior knowledge can effectively reduce the over-fitting problem and improve the model generalizability. For example, due to the translation-invariant structure of images, CNNs exploit the local-weightsharing mechanism to force the model to learn more general filters for different regions of images. As for skeletal data, each joint of the skeletons has specific physical/semantic meaning (e.g., head or hand), which is fixed for all the frames and is consistent for all the data samples. Based on this prior knowledge, a spatial global regularization is proposed to force the model to learn more general attentions for different samples. Note the regularization is not suitable for temporal dimension because there is no such semantic alignment property.</p><p>Besides, from the data aspect, the most discriminative pattern is distinct for different actions. We claim that two properties should be considered. One property is whether the action is motion relevant or motion irrelevant, which aims to choose the specific characters of space and time. For example, to classify the gestures of "waving up" versus "waving down", the global trajectories of hand is more important than hand shape, but when recognizing the gestures like "point with one finger" versus "point with two finger", the spatial pattern is more important than hand motion. Based on this observation, we propose to decouple the data into the spatial and temporal dimensions, where the spatial stream contains only the motion-irrelevant features and temporal stream contains only the motion-relevant features. By modeling these two streams separately, the model can better focus on spatial/temporal features and identity specific patterns. Finally, by fusing the two streams, it can obtain a more comprehensive understanding of the human actions. Another property is the sensibility of the motion scales. For temporal stream, the classification of some actions may rely on the motion mode of a few consecutive frames while others may rely on the overall movement trend. For example, to classify the gestures of "clapping" versus "put two hands together", the short-term motion detail is essential. But for "waving up" versus "waving down", the long-term motion trend is more important. Thus, inspired by <ref type="bibr" target="#b7">[8]</ref>, we split the temporal information into a fast stream and a slow stream based on the sampling rate. The low-frame-rate stream can capture more about global motion information and the high-frame-rate stream can focus more on the detailed movements. Similarly, the two streams are fused to improve the recognition performance.</p><p>We conduct extensive experiments on four datasets, including two hand gesture recognition datasets, i.e., SHREC and DHG, and two human action recognition datasets, i.e., NTU-60 and NTU-120. Without the need of hand-crafted traversal rules or graph topologies, our method achieves state-of-the-art performance on all these datasets, which demonstrates the effectiveness and generalizability of the proposed method.</p><p>Overall, our contributions lie in four aspects:</p><p>1. To the best of our knowledge, we are the first to propose a decoupled spatialtemporal attention networks (DSTA-Net) for skeleton-based action recognition, which is built with pure attention modules without manual designs of traversal rules or graph topologies. 2. We propose three effective techniques in building attention networks to meet the specific requirements for skeletal data, namely, spatial-temporal attention decoupling, decoupled position encoding and spatial global regularization. 3. We propose to decouple the data into four streams, namely, spatial-temporal stream, spatial stream, slow-temporal stream and fast-temporal stream, each focuses on a specific aspect of the skeleton sequence. By fusing different types of features, the model can have a more comprehensive understanding for human actions. 4. On four challenging datasets for action recognition, our method achieves state-of-the-art performance with a significant margin. DSTA-Net outperforms SOTA 2.6%/3.2% and 1.9%/2.9% on 14-class/28-class benchmarks of SHREC and DHG, respectively. It achieves 91.5%/96.4% and 86.6%/89.0% on CS/CV benchmarks of NTU-60 and NTU-120, respectively. The code will be released for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Skeleton-based action recognition has been widely studied for decades. The main-stream methods lie in three branches: (1) the RNN-based methods that formulate the skeletal data as a sequential data with a predefined traversal rules, and feed it into the RNN-based models such as the LSTM <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref>; (2) the CNN-based methods that convert the input skeletons into a pseudo-image with hand-crafted transformation rules, and model it with various successful networks used in image classification fields <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>; (3) the GCN-based methods that encode the skeletal data into a predefined spatial-temporal graph, and model it with the graph convolutional networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26]</ref>. In this work, instead of formulating the skeletal data into the images or graphs, we directly model the dependencies of joints with pure attention blocks. Our model is more concise and general, without the need of designing hand-crafted transformation rules, and it outperforms the previous methods with a significant margin. Self-attention mechanism is the basic block of transformer <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b4">5]</ref>, which is the mainstream method in the NLP field. Its input consists of a set of queries Q, keys K of dimension C and values V , which are packaged in the matrix form for fast computation. It first computes the dot products of the query with all keys, divides each by √ C, and applies a softmax function to obtain the weights on the values <ref type="bibr" target="#b30">[31]</ref>. In formulation:</p><formula xml:id="formula_0">Attention(Q, K, V ) = sof tmax( QK T √ C )<label>(1)</label></formula><p>The similar idea has also been used for many computer vision tasks such as relation modeling <ref type="bibr" target="#b21">[22]</ref>, detection <ref type="bibr" target="#b10">[11]</ref> and semantic segmentation <ref type="bibr" target="#b8">[9]</ref>. To the best of our knowledge, we are the fist to apply the pure attention networks for skeletal data and further propose several improvements to meet the specific requirements of skeletons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we first propose three techniques for building attention blocks in Sec. 3.1, Sec. 3.2 and Sec. 3.3. Then, the basic attention module and the detailed architecture of the network are introduced in Sec. 3.4 and Sec. 3.5. Finally, the data decoupling and the overall multi-stream solution is described in Sec. 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatial-temporal attention module</head><p>Original transformer is fed with the sequential data, i.e., a matrix X ∈ R N ×C , where N denotes the number of elements and C denotes the number of channels.</p><formula xml:id="formula_1">(a) (b) (c) 1 … … Dot-Product Attention Attention Map Dot-Product Attention Attention Map 1 1 Dot-Product Attention Attention Map Dot-Product Attention Dot-Product Attention Dot-Product Attention 1 … Dot-Product Attention Attention Map Dot-Product Attention … Fig. 1.</formula><p>Illustration of the three decoupling strategies. We use the spatial attention strategy as an example and the temporal attention strategy is an analogy. N and T denote the number of joints and frames, respectively.</p><p>For dynamic skeletal data, the input is a 3-order tensor X ∈ R N ×T ×C , where T denotes the number of frames. It is worth to investigate how to deal with the relationship between the time and the space. Wang et al. <ref type="bibr" target="#b31">[32]</ref> propose to ignore the difference between time and space, and regard the inputs as a sequential data X ∈ RN ×C , whereN = N T . However, the temporal dimension and the spatial dimension are totally different as introduced in Sec. 1. It is not reasonable to treat them equivalently. Besides, the computational complexity of calculating the attention map in this strategy is O(T 2 N 2 C) (using the naive matrix multiplication algorithm), which is too large. Instead, we propose to decouple the spatial and temporal dimensions, where the computational complexity is largely reduced and the performance is improved. We design three strategies for decoupling as shown in <ref type="figure">Fig 1.</ref> Using the spatial attention as an example, the first strategy <ref type="figure">(Fig 1, a)</ref> is calculating the attention maps frame by frame, and each frame uses a unique attention map:</p><formula xml:id="formula_2">A t = sof tmax(σ(X t )φ(X t ) )<label>(2)</label></formula><p>where A t ∈ R N ×N is the attention map for frame t. X t ∈ R N ×C . σ and φ are two embedding functions. denote matrix transpose. This strategy only considers the dependencies of joints in a single frame thus lacks the modeling capacity. The computational complexity of calculating spatial attention of this strategy is O(T N 2 C). For temporal attention, the attention map of joint n is A n ∈ R T ×T and the input data is X n ∈ R T ×C . Its calculation is analogical with the spatial attention. Considering both the spatial and temporal attention, the computational complexity of the first strategy for all frames is O(</p><formula xml:id="formula_3">T N 2 C + N T 2 C).</formula><p>The second strategy <ref type="figure">(Fig 1, b)</ref> is calculating the relations of two joints between all of the frames, which means both the intra-frame relations and the inter-frame relations of two joints are taken into account simultaneously. The attention map is shared over all frames. In formulation:</p><formula xml:id="formula_4">A t = sof tmax( T t T τ (σ(X t )φ(X τ ) ))<label>(3)</label></formula><p>The computational complexity of this strategy is O(</p><formula xml:id="formula_5">T 2 N 2 C + N 2 T 2 C).</formula><p>The third strategy <ref type="figure">(Fig 1, c)</ref> is a compromise, where only the joints in same frame are considered to calculate the attention map, but the obtained attention maps of all frames are averaged and shared. It is equivalent to adding a time consistency restriction for attention computation, which can somewhat reduce the overfitting problem caused by the element-wise relation modeling of the second strategy.</p><formula xml:id="formula_6">A t = sof tmax( T t (σ(X t )φ(X t ) ))<label>(4)</label></formula><p>By concatenating the frames into an N × T C matrix, the summation of matmultiplications can be efficiently implemented with one big mat-multiplication operation. The computational complexity of this strategy is O(T N 2 C + N T 2 C). as shown in ablation study 4.3, we finally use the strategy (c) in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoupled Position encoding</head><p>The skeletal joints are organized as a tensor to be fed into the neural networks. Because there are no predefined orders or structures for each element of the tensor to show its identity (e.g., joint index or frame index), we need a position encoding module to provide unique markers for every joint. Following <ref type="bibr" target="#b30">[31]</ref>, we use the sine and cosine functions with different frequencies as the encoding functions: </p><p>where p denotes the position of element and i denotes the dimension of the position encoding vector. However, different with <ref type="bibr" target="#b30">[31]</ref>, the input of skeletal data have two dimensions, i.e., space and time. One strategy for position encoding is unifying the spatial and temporal dimensions and encoding them sequentially. For example assuming there are three joints, for the first frame the position of joints is 1, 2, 3, and for the second frame it is 4, 5, 6. This strategy cannot well distinguish the same joint in different frames. Another strategy is decoupling the process into spatial position encoding and temporal position encoding. Using the spatial position encoding as an example, the joints in the same frame are encoded sequentially and the same joints in different frames have the same encoding.</p><p>In above examples, it means for the first frame the position is 1, 2, 3, and for the second frame it is also 1, 2, 3. As for the temporal position encoding, it is reversed and analogical, which means the joints in the same frame have the same encoding and the same joints in different frames are encoded sequentially. Finally, the position features are added to the input data as shown in <ref type="figure">Fig 2.</ref> In this way, each element is aligned with an unique marker to help learning the mutual relations between the joints, and the difference between space and time is also well expressed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatial global regularization</head><p>As explained in Sec. 1, each joint has a specific meaning. Based on this prior knowledge, we propose to add a spatial global regularization to force the model to learn more general attentions for different samples. In detail, a global attention map (N ×N matrix) is added to the attention map (N ×N matrix) learned by the dot-product attention mechanism introduced in Sec. 3.1. The global attention map is shared for all data samples, which represents a unified intrinsic relationship pattern of the human joints. We set it as the parameter of the network and optimize it together with the model. An α is multiplied to balance the strength of the spatial global regularization. This module is simple and light-weight, but it is effective as shown in the ablation study. Note that the regularization is only added for spatial attention computing because the temporal dimension has no such semantic alignment property. Forcing a global regularization for temporal attention is not reasonable and will harm the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Complete attention module</head><p>Because the spatial attention module and the temporal attention module are analogical, we select the spatial module as an example for detailed introduction. The complete attention module is showed in <ref type="figure">Fig 2.</ref> The procedures inside the purple rounded rectangle box illustrate the process of the single-head attention calculation. The input X ∈ R N ×T Cin is first added with the spatial position encoding. Then it is embedded with two linear mapping functions to X ∈ R N ×T Ce . C e is usually small than C out to remove the feature redundancy and reduce the computations. The attention map is calculated by the strategy (c) of <ref type="figure">Fig. 1</ref> and added with the spatial global regularization. Note that we found the Tanh is better than SoftMax when computing the attention map. We believe that it is because the output of Tanh is not restricted to positive values thus can generate negative relations and provide more flexibility. Finally the attention map is mat-multiplied with the original input to get the output features.  <ref type="figure" target="#fig_1">Fig. 3</ref> shows the overall architecture of our method. The input is a skeleton sequence with N joints, T frames and C channels. In each layer, we first regard the input as an N × T C matrix, i.e., N elements with T C channels, and feed it into the spatial attention module (introduced in <ref type="figure">Fig. 2</ref>) to model the spatial relations between the joints. Then, we transpose the output matrix and regard it as T elements each has N C channels, and feed it into the temporal attention module to model the temporal relations between the frames. There are totally L layers stacked to update features. The final output features are global-averagepooled and fed into a fully-connected layers to obtain the classification scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Overall architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Data decoupling</head><p>The action can be decoupled into two dimensions: the spatial dimension and the temporal dimension as illustrated in <ref type="figure">Fig. 4 (a, b and c)</ref>. The spatial information is the difference of two different joints that are in the same frame, which mainly contains the relative position relationship between different joints. To reduce the redundant information, we only calculate the spatial information along the human bones. The temporal information is the difference of the two joints with same spatial meaning in different frames, which mainly describes the motion trajectory of one joint along the temporal dimension. When we recognize the (a)Spatial-temporal information (b)Spatial information (c)Temporal information (d)Different frame rate <ref type="figure">Fig. 4</ref>. For simplicity, we draw two joins in two consecutive frames in a 2D coordinate system to illustrate the data decoupling. as shown in (a), P i t k denotes the joint i in frame k. Assume that joint i and joint j are the two end joints of one bone. (a) denotes the raw data, i.e., the spatial-temporal information. The orange dotted line and blue dotted line denote the decoupled spatial information and temporal information, which are showed as (b) and (c), respectively. (d) illustrates the difference between the fasttemporal information (blue arrow) and the slow-temporal information (orange arrow).</p><p>gestures like "Point with one finger" versus "Point with two finger", the spatial information is more important. However, when we recognize the gestures like "waving up" versus "waving down", the temporal information will be more essential.</p><p>Besides, for temporal stream, different actions have different sensibilities of the motion scale. For some actions such as "clapping" versus "put two hands together", the short-term motion detail is essential. But for actions like "waving up" versus "waving down", the long-term movement trend is more important. Inspired by <ref type="bibr" target="#b7">[8]</ref>, we propose to calculate the temporal motion with both the high frame-rate sampling and the low frame-rate sampling as shown in <ref type="figure">Fig. 4  (d)</ref>. The generated two streams are called as the fast-temporal stream and the slow-temporal stream, respectively.</p><p>Finally, we have four streams all together, namely, spatial-temporal stream (original data), spatial stream, fast-temporal stream and slow-temporal stream. We separately train four models with the same architecture for each of the streams. The classification scores are averaged to obtain the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To verify the generalization of the model, we use two datasets for hand gesture recognition (DHG <ref type="bibr" target="#b5">[6]</ref> and SHREC <ref type="bibr" target="#b6">[7]</ref>) and two datasets for human action recognition (NTU-60 <ref type="bibr" target="#b22">[23]</ref> and NTU-120 <ref type="bibr" target="#b13">[14]</ref>). We first perform exhaustive ablation studies on SHREC to verify the effectiveness of the proposed model components. Then, we evaluate our model on all four datasets to compare with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>DHG: DHG <ref type="bibr" target="#b5">[6]</ref> dataset contains 2800 video sequences of 14 hand gestures performed 5 times by 20 subjects. They are performed in two ways: using one finger and the whole hand. So it has two benchmarks: 14-gestures for coarse classification and 28-gestures for fine-grained classification. The 3D coordinates of 22 hand joints in real-world space is captured by the Intel Real-sense camera. It uses the leave-one-subject-out cross-validation strategy for evaluation.</p><p>SHREC: SHREC <ref type="bibr" target="#b6">[7]</ref> dataset contains 2800 gesture sequences performed 1 and 10 times by 28 participants in two ways like the DHG dataset. It splits the sequences into 1960 train sequences and 840 test sequences. The length of sample gestures ranges from 20 to 50 frames. This dataset is used for the competition of SHREC'17 in conjunction with the Euro-graphics 3DOR'2017 Workshop.</p><p>NTU-60: NTU-60 <ref type="bibr" target="#b22">[23]</ref> is a most widely used in-door-captured action recognition dataset, which contains 56,000 action clips in 60 action classes. The clips are performed by 40 volunteers and is captured by 3 KinectV2 cameras with different views. This dataset provides 25 joints for each subject in the skeleton sequences. It recommends two benchmarks: cross-subject (CS) and cross-view (CV), where the subjects and cameras used in the training/test splits are different, respectively.</p><p>NTU-120: NTU-120 <ref type="bibr" target="#b22">[23]</ref> is similar with NTU-60 but is larger. It contains 114,480 action clips in 120 action classes. The clips are performed by 106 volunteers in 32 camera setups. It recommends two benchmarks: cross-subject (CS) and cross-setup (CE), where cross-setup means using samples with odd setup IDs for training and others for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>To show the generalization of our methods, we use the same configuration for all experiments. The network is stacked using 8 DSTA blocks with 3 heads. The output channels are 64, 64, 128, 128, 256, 256, 256 and 256, respectively. The input video is randomly/uniformly sampled to 150 frames and then randomly/centrally cropped to 128 frames for training/test splits. For fast-temporal features, the sampling interval is 2. When training, the initial learning rate is 0.1 and is divided by 10 in 60 and 90 epochs. The training is ended in 120 epochs. Batch size is 32. Weight decay is 0.0005. We use the stochastic gradient descent (SGD) with Nesterov momentum (0.9) as the optimizer and the cross-entropy as the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>In this section, we investigate the effectiveness of the proposed components of the network and different data modalities. We conduct experiments on SHREC dataset. Except for the explored object, other details are set the same for fair comparison.</p><p>Network architectures We first investigate the effect of the position embedding. as shown in Tab. 1, removing the position encoding will seriously harm the performance. Decoupling the spatial and temporal dimension (DPE) is better than not (UPE). This is because the spatial and temporal dimensions actually have different properties and treat them equivalently will confuse the model.</p><p>Then we investigate the effect of the proposed spatial global regularization (SGR). By adding the SGR, the performance is improved from 94.3% to 96.3%, but if we meanwhile regularize the temporal dimension, the performance drops. This is reasonable since there are no specific meanings for temporal dimension and forced learning of a unified pattern will cause the gap between the training set an testing set.</p><p>Finally, we compare the three strategies introduced in <ref type="figure">Fig. 1</ref>. It shows that the strategy (a) obtains the lowest performance. We conjecture that it dues to the fact that it only considers the intra-frame relations and ignore the interframe relations. Modeling the inter-frame relations exhaustively (strategy b) will improve the performance and a compromise (c) obtains the best performance. It may because that the compromise strategy can somewhat reduce the overfitting problem. <ref type="table">Table 1</ref>. Ablation studies for architectures of the model on the SHREC dataset. ST-ATT-c denotes the spatial temporal attention networks with attention type c introduced in <ref type="figure">Fig 1.</ref> PE denotes position encoding. UPE/DPE denote using unified/decoupled encoding for spatial and temporal dimensions. STGR denotes spatial-temporal global regularizations for computing attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy We show the learned attention maps of different layers (layer #1 and layer #8) in <ref type="figure" target="#fig_4">Fig. 5</ref>. Other layers are showed in supplement materials. It shows that the attention maps learned in different layers are not the same because the information contained in different layers has distinct semantics. Besides, it seems the model focuses more on the relations between the tips of the fingers (T4, I4, M4, R4) and wrist, especially in the lower layers. This is intuitive since these joints are more discriminative for human to recognize gestures. On the higher layers, the information are highly aggregated and the difference between each of the joints becomes unapparent, thus the phenomenon also becomes unapparent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrist</head><p>Palm <ref type="table" target="#tab_1">T1  T2  T3  T4  I1  I2  I3  I4  M1  M2  M3  M4  R1  R2  R3  R4  L1  L2  L3  L4   Wrist   Palm   T1   T2   T3   T4   I1   I2   I3   I4   M1   M2   M3   M4   R1   R2   R3   R4   L1   L2   L3   L4</ref>   <ref type="table" target="#tab_1">T1  T2  T3  T4  I1  I2  I3  I4  M1  M2  M3  M4  R1  R2  R3  R4  L1  L2  L3  L4   Wrist   Palm   T1   T2   T3   T4   I1   I2   I3   I4</ref>  Data decoupling To show the necessity of decoupling the raw data into four streams as introduced in Sec. 3.6, we show the results of using four streams separately and the result of fusion in Tab. 2. It shows that the accuracies of decoupled streams are not as good as the raw data because some of the information is lost. However, since the four streams focus on different aspects and are complementary with each other, when fusing them together, the performance is improved significantly. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, We plot the per-class accuracies of the four streams to show the complementarity clearly. We also plot the difference of accuracies between different streams, which are represented as the dotted lines. For spatial information versus temporal information, it (orange dotted lines) shows that the network with spatial information obtains higher accuracies mainly in classes that are closely related with the shape changes such as "grab", "expand" and "pinch", and the network with temporal information obtains higher accuracies mainly in classes that are closely related with the positional changes such as "swipe", "rot" and "shake". As for different frame-rate sampling, it (red dotted lines) shows that the slow-temporal performs better for classes of "expand", "tap", etc, and the fast-temporal performs better for classes of "swipe", "rot", etc. These phenomenons verify the complementarity of the four modalities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with previous methods</head><p>We evaluate our model with state-of-the-art methods for skeleton-based action recognition on all four datasets, where our model significantly outperforms the other methods. Due to the space restriction, we only show some representative works, where more comparisons are showed in supplement materials. On SHREC/DHG datasets for skeleton-based hand gestures recognition (Tab. 3), our model brings 2.6%/1.9% and 3.2%/2.9% improvements for 14-gestures and 28-gestures benchmarks compared with the state-of-the-arts. Note that the stateof-the-art accuracies are already very high (94.4%/91.9% and 90.7%/88.0% for <ref type="table">Table 4</ref>. Recognition accuracy comparison of our method and state-of-the-art methods on NTU-60 dataset. CS and CV denote the cross-subject and cross-view benchmarks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Year CS (%) CV (%) ST-GCN <ref type="bibr" target="#b32">[33]</ref> 2018 14-gestures and 28-gestures, respectively), but our model still obtains remarkable performance. On NTU-60 dataset (Tab. 4), our model obtains 1.6% and 0.3% improvements. The performance of CV benchmark is nearly saturated. For both CS and CV benchmarks, we visualize the wrong examples and find that it is even impossible for human to recognize many examples using only the skeletal data. For example, for the two classes of reading and writing, the humans are both in a same posture (standing or sitting) and holding a book. The only difference is whether there is a pen in the hand, which cannot be captured through the skeletal data. On NTU-120 dataset (Tab. 5), our model also achieves stateof-the-art performance. Since this dataset is released recently, our method can provide a new baseline on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel decoupled spatial-temporal attention network (DSTA-Net) for skeleton-based action recognition. It is a unified framework based solely on attention mechanism, with no needs of designing hand-crafted traversal rules or graph topologies. We propose three techniques in building DSTA-Net to meet the specific requirements for skeletal data, including spatialtemporal attention decoupling, decoupled position encoding and spatial global regularization. Besides, we introduce a skeleton-decoupling method to emphasize the spatial/temporal variations and motion scales of the skeletal data, resulting in a more comprehensive understanding for human actions and gestures. To verify the effectiveness and generalizability of the DSTA-Net, extensive experiments are conducted on four large datasets for both gesture and action recognition, where the DSTA-Net achieves the state-of-the-art performance on all of them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>P E(p, 2i) = sin(p/10000 2i/Cin ) P E(p, 2i + 1) = cos(p/10000 2i/Cin )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the overall architecture of the DSTA-Net. N, T, C denote the number of joints, frames and channels, respectively. The red rounded rectangle box represents one spatial-temporal attention layer. There are totally L layers. The final output features are global-average-pooled (GAP) and fed into a fully-conected layer (FC) to make the prediction. To allow the model jointly attending to information from different representation sub-spaces, there are totally S heads for attention calculations in the module. The results of all heads are concatenated and mapped to the output space R N ×T Cout with a linear layer. Similar with the transformer, a point-wise feed-forward layer is added in the end to obtain the final output. We use the leaky ReLU as the non-linear function. There are two residual connections in the module as shown in the Fig 2 to stabilize the network training and integrate different features. Finally, all of the procedures inside the green rounded rectangle box represent one whole attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ST-Att-c w/o PE 89.4 ST-Att-c + UPE 93.2 ST-Att-c + DPE 94.5 ST-Att-c + DPE + SGR 96.3 ST-Att-c + DPE + STGR 94.6 ST-Att-a + DPE + SGR 94.6 ST-Att-b + DPE + SGR 95.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0.57 0.41 0.58 0.58 0.55 0.47 0.36 0.34 0.33 0.55 0.44 0.35 0.37 0.51 0.41 0.33 0.42 0.45 0.57 0.49 0.5 0.55 0.64 0.58 0.63 0.7 0.54 0.57 0.55 0.52 0.44 0.56 0.6 0.48 0.44 0.55 0.51 0.42 0.52 0.51 0.62 0.56 0.56 0.56 0.43 0.35 0.46 0.49 0.45 0.52 0.51 0.47 0.4 0.58 0.58 0.43 0.38 0.54 0.4 0.34 0.42 0.43 0.49 0.44 0.45 0.49 0.38 0.29 0.4 0.47 0.36 0.61 0.53 0.44 0.36 0.75 0.55 0.4 0.33 0.7 0.4 0.34 0.38 0.39 0.43 0.42 0.42 0.48 0.15 0.1 0.22 0.34 0.2 0.68 0.55 0.4 0.3 0.87 0.51 0.35 0.23 0.84 0.28 0.28 0.28 0.31 0.24 0.32 0.33 0.47 -0.01 -0.01 0.09 0.23 0.08 0.69 0.56 0.4 0.28 0.91 0.48 0.32 0.15 0.9 0.21 0.23 0.2 0.24 0.11 0.28 0.27 0.44 0.44 0.59 0.42 0.51 0.37 0.59 0.8 0.77 0.6 0.56 0.81 0.65 0.5 0.62 0.58 0.5 0.54 0.5 0.53 0.52 0.51 0.46 0.47 0.58 0.44 0.42 0.38 0.57 0.73 0.76 0.68 0.52 0.76 0.75 0.67 0.55 0.61 0.66 0.62 0.61 0.44 0.56 0.57 0.53 0.52 0.57 0.5 0.43 0.41 0.5 0.75 0.79 0.76 0.52 0.79 0.8 0.83 0.51 0.6 0.79 0.74 0.75 0.36 0.64 0.67 0.58 0.7 0.66 0.67 0.54 0.48 0.45 0.79 0.84 0.82 0.55 0.84 0.81 0.99 0.53 0.56 0.93 0.95 0.94 0.37 0.79 0.86 0.68 0.58 0.69 0.54 0.61 0.47 0.55 0.63 0.64 0.55 0.51 0.64 0.57 0.49 0.58 0.56 0.49 0.56 0.53 0.62 0.57 0.56 0.52 0.46 0.62 0.46 0.46 0.48 0.58 0.64 0.63 0.52 0.41 0.64 0.6 0.47 0.52 0.57 0.51 0.41 0.33 0.63 0.44 0.33 0.37 0.38 0.49 0.4 0.41 0.5 0.52 0.66 0.69 0.6 0.39 0.66 0.61 0.56 0.5 0.46 0.61 0.52 0.4 0.51 0.43 0.36 0.41 0.2 0.3 0.25 0.27 0.41 0.47 0.69 0.79 0.72 0.4 0.68 0.64 0.68 0.52 0.37 0.66 0.62 0.47 0.33 0.36 0.36 0.41 0.71 0.79 0.66 0.7 0.56 0.52 0.49 0.53 0.5 0.49 0.49 0.5 0.48 0.56 0.53 0.49 0.6 0.58 0.7 0.63 0.62 0.58 0.6 0.6 0.55 0.55 0.53 0.44 0.39 0.39 0.49 0.51 0.35 0.42 0.48 0.55 0.48 0.5 0.54 0.56 0.48 0.59 0.57 0.54 0.54 0.46 0.54 0.55 0.55 0.47 0.34 0.37 0.51 0.56 0.29 0.42 0.51 0.56 0.43 0.55 0.55 0.57 0.37 0.64 0.63 0.58 0.54 0.37 0.57 0.58 0.58 0.52 0.27 0.34 0.49 0.51 0.24 0.41 0.49 0.5 0.35 0.54 0.52 0.54 0.33 0.68 0.66 0.61 0.89 0.9 0.82 0.82 0.68 0.44 0.3 0.37 0.43 0.45 0.31 0.4 0.47 0.52 0.49 0.48 0.63 0.62 0.82 0.71 0.69 0.65 0.71 0.59 0.7 0.69 0.62 0.54 0.27 0.24 0.35 0.54 0.22 0.34 0.38 0.58 0.33 0.42 0.44 0.5 0.52 0.56 0.5 0.55 0.76 0.56 0.76 0.8 0.64 0.56 0.35 0.28 0.37 0.59 0.32 0.41 0.44 0.63 0.31 0.47 0.49 0.54 0.48 0.62 0.56 0.56 0.67 0.46 0.7 0.76 0.6 0.59 0.34 0.28 0.38 0.61 0.3 0.43 0.47 0.64 0.23 0.46 0.49 0.56 0.38 0.61 0.56 0.54 Layer_1 Wrist Palm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Layer_8Fig. 5 .</head><label>5</label><figDesc>0.37 0.3 0.37 0.42 0.63 0.55 0.5 0.51 0.5 0.54 0.51 0.5 0.48 0.5 0.48 0.45 0.46 0.35 0.44 0.45 0.47 0.36 0.26 0.36 0.5 0.44 0.71 0.61 0.56 0.57 0.57 0.59 0.55 0.56 0.54 0.62 0.53 0.54 0.57 0.39 0.55 0.55 0.55 0.26 0.32 0.21 0.31 0.39 0.55 0.49 0.44 0.45 0.42 0.47 0.45 0.45 0.41 0.44 0.42 0.38 0.38 0.32 0.36 0.37 0.39 0.27 0.28 0.25 0.11 0.27 0.4 0.34 0.3 0.31 0.2 0.34 0.31 0.32 0.19 0.33 0.31 0.21 0.21 0.3 0.17 0.19 0.22 0.54 0.54 0.58 0.66 0.48 0.86 0.86 0.81 0.82 0.78 0.85 0.8 0.81 0.75 0.64 0.77 0.73 0.73 0.4 0.74 0.74 0.74 0.51 0.44 0.46 0.37 0.46 0.22 0.2 0.18 0.23 -0.01 0.21 0.18 0.26 0.07 0.45 0.28 0.21 0.22 0.46 0.21 0.2 0.23 0.59 0.54 0.56 0.43 0.59 0.39 0.16 0.13 0.25 0.26 0.17 0.22 0.32 0.27 0.37 0.37 0.38 0.35 0.54 0.38 0.38 0.37 0.6 0.53 0.57 0.44 0.7 0.43 0.27 0.24 0.37 0.38 0.28 0.35 0.45 0.38 0.41 0.47 0.47 0.48 0.58 0.47 0.48 0.47 0.34 0.35 0.33 0.23 0.5 0.29 0.19 0.23 0.28 0.37 0.2 0.25 0.34 0.37 0.33 0.31 0.31 0.38 0.45 0.31 0.33 0.32 0.79 0.68 0.79 0.67 0.81 0.55 0.69 0.69 0.75 0.49 0.67 0.65 0.71 0.54 0.82 0.73 0.7 0.71 0.67 0.65 0.63 0.64 0.58 0.52 0.55 0.41 0.6 0.4 0.16 0.14 0.27 0.27 0.13 0.24 0.35 0.27 0.35 0.39 0.38 0.35 0.53 0.39 0.37 0.37 0.49 0.44 0.48 0.33 0.57 0.34 0.22 0.22 0.27 0.26 0.24 0.19 0.3 0.24 0.38 0.3 0.3 0.32 0.48 0.33 0.32 0.31 0.34 0.34 0.33 0.25 0.44 0.31 0.27 0.29 0.32 0.31 0.29 0.24 0.19 0.25 0.36 0.21 0.21 0.24 0.38 0.22 0.23 0.23 0.87 0.71 0.84 0.74 0.75 0.68 0.75 0.77 0.81 0.55 0.74 0.72 0.75 0.52 0.92 0.74 0.61 0.66 0.69 0.57 0.54 0.6 0.87 0.98 0.85 0.77 0.82 0.97 0.9 0.9 0.96 0.9 0.89 0.95 0.99 0.88 0.76 0.98 0.95 0.97 0.87 0.96 0.95 0.95 0.31 0.31 0.31 0.2 0.46 0.34 0.31 0.3 0.29 0.28 0.31 0.22 0.21 0.2 0.38 0.15 0.16 0.19 0.38 0.19 0.2 0.2 0.38 0.35 0.37 0.21 0.47 0.3 0.34 0.33 0.32 0.25 0.33 0.27 0.24 0.14 0.42 0.23 0.15 0.19 0.41 0.16 0.17 0.17 0.3 0.28 0.29 0.17 0.37 0.24 0.23 0.25 0.3 0.32 0.23 0.21 0.2 0.22 0.35 0.17 0.13 0.15 0.35 0.12 0.12 0.13 0.38 0.47 0.38 0.47 0.42 0.65 0.58 0.54 0.57 0.55 0.57 0.54 0.58 0.54 0.63 0.54 0.54 0.57 0.33 0.55 0.55 0.57 0.39 0.31 0.34 0.22 0.4 0.36 0.32 0.3 0.31 0.23 0.32 0.28 0.28 0.16 0.41 0.24 0.17 0.2 0.4 0.09 0.12 0.15 0.41 0.34 0.37 0.24 0.43 0.38 0.36 0.35 0.36 0.27 0.36 0.34 0.32 0.2 0.44 0.3 0.22 0.25 0.44 0.17 0.16 0.19 0.31 0.23 0.27 0.19 0.28 0.29 0.25 0.22 0.25 0.2 0.24 0.22 0.21 0.15 0.32 0.18 0.11 0.13 0.34 0.08 0.06 0.05 Examples of the learned attention maps for different layers. T, I, M, R and L denote thumb, index finger, middle finger, ring finger and little finger, respectively. As for articulation, T1 denotes the base of the thumb and T4 denote the tip of the thumb.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>c lo c k w is e r o t c / c lo c k w is e s w ip e r ig h t s w ip e le f t s w ip e u p s w ip e d o w n s w ip e x s w ip e + s w ip e v s h a Per-class accuracies for different modalities on SHREC-14 dataset. The dotted lines shows the difference between two modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Illustration of the attention module. We show the spatial attention module as an example. The temporal attention module is an analogy. The purple rounded rectangle box represents a single-head self-attention module. There are totally S self-attention modules, whose output are concatenated and fed into two linear layers to obtain the output. LReLU represents the leaky ReLU<ref type="bibr" target="#b17">[18]</ref>.</figDesc><table><row><cell>× in Position Encoding</cell><cell></cell><cell></cell><cell></cell><cell>Spatial Global</cell><cell>x S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linear Linear</cell><cell>× e</cell><cell>Mat Mult.</cell><cell>Tanh</cell><cell>Attention Map N x N Regularization Mat Mult. ×α N x N</cell><cell>Multi-Head Concat.</cell><cell>×</cell><cell>e</cell><cell>Linear</cell><cell>×</cell><cell>+ BN LReLU + Linear</cell><cell>LReLU</cell></row><row><cell>×</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Residual (opt.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies for feature fusion on the SHREC dataset. Spatial-temporal denotes the raw data, i.e., the joint coordinates. Other types of features are introduced in Sec. 3.6.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>spatial-temporal</cell><cell>96.3</cell></row><row><cell>spatial</cell><cell>95.1</cell></row><row><cell>fast-temporal</cell><cell>94.5</cell></row><row><cell>slow-temporal</cell><cell>93.7</cell></row><row><cell>Fusion</cell><cell>97.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Recognition accuracy comparison of our method and state-of-the-art methods on SHREC dataset and DHG dataset.</figDesc><table><row><cell>Method</cell><cell>Year</cell><cell cols="4">SHREC 14 gestures 28 gestures 14 gestures 28 gestures DHG</cell></row><row><cell>ST-GCN [33]</cell><cell>2018</cell><cell>92.7</cell><cell>87.7</cell><cell>91.2</cell><cell>87.1</cell></row><row><cell>STA-Res-TCN [10]</cell><cell>2018</cell><cell>93.6</cell><cell>90.7</cell><cell>89.2</cell><cell>85.0</cell></row><row><cell cols="2">ST-TS-HGR-NET [19] 2019</cell><cell>94.3</cell><cell>89.4</cell><cell>87.3</cell><cell>83.4</cell></row><row><cell>DG-STA. [4]</cell><cell>2019</cell><cell>94.4</cell><cell>90.7</cell><cell>91.9</cell><cell>88.0</cell></row><row><cell>DSTA-Net(ours)</cell><cell>-</cell><cell>97.0</cell><cell>93.9</cell><cell>93.8</cell><cell>90.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Recognition accuracy comparison of our method and state-of-the-art methods on NTU-120 dataset. CS and CE denote the cross-subject and cross-setup benchmarks, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>81.5</cell><cell>88.3</cell></row><row><cell>SRN+TSL [29]</cell><cell>2018</cell><cell>84.8</cell><cell>92.4</cell></row><row><cell>2s-AGCN [26]</cell><cell>2019</cell><cell>88.5</cell><cell>95.1</cell></row><row><cell>DGNN [25]</cell><cell>2019</cell><cell>89.9</cell><cell>96.1</cell></row><row><cell>NAS [20]</cell><cell>2020</cell><cell>89.4</cell><cell>95.7</cell></row><row><cell>DSTA-Net(ours)</cell><cell>-</cell><cell>91.5</cell><cell>96.4</cell></row><row><cell>Methods</cell><cell></cell><cell cols="3">Year CS (%) CE (%)</cell></row><row><cell cols="3">Two-Stream Attention LSTM [15] 2017</cell><cell>61.2</cell><cell>63.3</cell></row><row><cell cols="2">Body Pose Evolution Map [17]</cell><cell>2018</cell><cell>64.6</cell><cell>66.9</cell></row><row><cell>SkeletonMotion [1]</cell><cell></cell><cell>2019</cell><cell>67.7</cell><cell>66.9</cell></row><row><cell>DSTA-Net(ours)</cell><cell></cell><cell>-</cell><cell>86.6</cell><cell>89.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SkeleMotion: A New Representation of Skeleton Joint Sequences based on Motion Information for 3d Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Skeleton-Based Action Recognition with Gated Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3247" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
	<note>Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Construct Dynamic Graphs for Hand Gesture Recognition via Spatial-Temporal Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<title level="m">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Skeleton-Based Dynamic Hand Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>De Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1206" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SHREC&apos;17 Track: 3d Hand Gesture Recognition Using a Depth and Skeletal Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>De Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<editor>Pratikakis, I., Dupont, F., Ovsjanikov, M.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial-temporal attention res-TCN for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relation Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skeleton-based Action Recognition with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building A longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<title level="m">NTU RGB+D 120: A Large-Scale Benchmark for 3d Human Activity Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing Human Actions as the Evolution of Pose Estimation Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML. vol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural network based on SPD manifold learning for skeleton-based hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lzoray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bougleux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Spatio-Temporal Representation with Pseudo-3d Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
	<note>NTU RGB+D: A Large Scale Dataset for 3d Human Activity Analysis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Action Recognition via Pose-Based Graph Convolutional Networks with Intermediate Dense Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12509</idno>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition With Directed Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gesture Recognition using Spatiotemporal Deformable Convolutional Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hanqing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-Local Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition From Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
