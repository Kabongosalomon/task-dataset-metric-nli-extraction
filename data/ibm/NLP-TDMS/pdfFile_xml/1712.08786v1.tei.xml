<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stat Merging K-means with hierarchical clustering for identifying general-shaped groups</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">D</forename><surname>Peterson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><forename type="middle">P</forename><surname>Ghosh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjan</forename><surname>Maitra</surname></persName>
						</author>
						<title level="a" type="main">Stat Merging K-means with hierarchical clustering for identifying general-shaped groups</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received 16 November 2017; Accepted 28 November 2017</note>
					<note>The ISI&apos;s Journal for the Rapid Dissemination of Statistics Research</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>K-means algorithm</term>
					<term>hierarchical clustering</term>
					<term>single linkage</term>
					<term>complete linkage</term>
					<term>distance measure</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clustering partitions a dataset such that observations placed together in a group are similar but different from those in other groups. Hierarchical and K-means clustering are two approaches but have different strengths and weaknesses. For instance, hierarchical clustering identifies groups in a tree-like structure but suffers from computational complexity in large datasets while K-means clustering is efficient but designed to identify homogeneous spherically-shaped clusters. We present a hybrid non-parametric clustering approach that amalgamates the two methods to identify general-shaped clusters and that can be applied to larger datasets. Specifically, we first partition the dataset into spherical groups using K-means. We next merge these groups using hierarchical methods with a data-driven distance measure as a stopping criterion. Our proposal has the potential to reveal groups with general shapes and structure in a dataset. We demonstrate good performance on several simulated and real datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering partitions a dataset into subsets called clusters without any prior knowledge of group assignment. The general objective is that observations placed in the same cluster are similar in some sense while being different to those in other groups. The substantial body of literature <ref type="bibr" target="#b8">(Everitt et al., 2001;</ref><ref type="bibr" target="#b11">Fraley &amp; Raftery, 2002;</ref><ref type="bibr" target="#b13">Hartigan, 1985;</ref><ref type="bibr" target="#b18">Kaufman &amp; Rousseuw, 1990;</ref><ref type="bibr" target="#b19">Kettenring, 2006;</ref><ref type="bibr" target="#b32">Melnykov &amp; Maitra, 2011;</ref><ref type="bibr" target="#b30">McLachlan &amp; Basford, 1988;</ref><ref type="bibr" target="#b34">Murtagh, 1985;</ref><ref type="bibr" target="#b37">Ramey, 1985)</ref> dedicated to the topic reflects the difficulty and diversity of clustering applications. Most unsupervised clustering techniques are broadly hierarchical or partition-optimization-based. Traditionally, hierarchical algorithms provide a tree-like structure for demarcating groups, with the property that all observations in a group at some branch are also in the same group higher up the tree. Hierarchical algorithms may be agglomerative (clustermerging) or divisive (cluster-breaking). Agglomerative algorithms successively merge smaller clusters together whereas divisive algorithms successively break larger clusters apart. Most hierarchical clustering methods use some dissimilarity measure between groups to decide whether to merge (or split) groups. The result can be represented as a dendrogram  We illustrate some shortcomings of these algorithms through the Bullseye dataset of <ref type="bibr" target="#b38">Stuetzle &amp; Nugent (2010)</ref> which has 400 observations from a spherical cluster surrounded by a ring of observations (which form the second group). <ref type="figure" target="#fig_1">Figure 1</ref>(a-b) shows the clustering using 2-and 6-means. In addition, <ref type="figure" target="#fig_1">Figure 1(c)</ref> shows the grouping based on hierarchical clustering with single linkage and K = 2. Neither approach clusters into their true groupings. Although <ref type="figure" target="#fig_1">Figure 1(b)</ref> captures the center group, we required 5 groups to create the outer ring. One possibility of improving this solution is to merge these groups using some objective mechanism and we will explore this approach in this paper.</p><p>The idea of merging clusters is not new in the literature. <ref type="bibr" target="#b12">Fred &amp; Jain (2005)</ref> introduced evidence accumulation clustering (EAC) for combining the results from multiple applications of K-means. The idea behind EAC is that each partition gives independent evidence on the organization of the data. The authors proposed independent runs of Kmeans on the dataset and created a similarity (frequency) matrix between all pairs of data points with the (i , j)th entry representing the number of times the i th and jth observations were placed in the same group. The final data partition is obtained by applying a hierarchical agglomerative clustering algorithm using this similarity matrix. The motivation here is that observations that are together in the majority of partitions should also be so in the final chosen partition. This procedure is novel in that it chooses among several different partitions but it is computationally expensive since it involves performing either single linkage or average linkage on an n × n distance matrix, where n is the number of observations. <ref type="bibr" target="#b38">Stuetzle &amp; Nugent (2010)</ref> adopt a nonparametric approach to clustering based on the premise that groups correspond to modes of the density. <ref type="bibr" target="#b38">Stuetzle &amp; Nugent (2010)</ref> find the modes within a dataset and assign observations to the "domain of attraction" of a mode. The collection of high density modes is used to create a hierarchical structure where dissimilarity between modes is based on the lowest density observed between any pair of groups. <ref type="bibr" target="#b6">Baudry et al. (2010)</ref> propose a cluster merging method using a model-based clustering approach. They propose first selecting the total number of Gaussian mixtures components, K 0 , using BIC and then combining them hierarchically. This yields a unique soft clustering for each K less than K 0 . Further refinements to this method were provided by the DEMP <ref type="bibr" target="#b15">(Hennig, 2010)</ref> and DEMP+ <ref type="bibr" target="#b31">(Melnykov, 2016)</ref> algorithms. However, model-based clustering is computationally slower and typically more difficult to apply on to larger datasets.</p><p>In this paper, we propose a K-means hierarchical (K − mH) cluster merging algorithm which combines the computational benefits of K-means with agglomerative hierarchical clustering. The general methodology and our algorithm are detailed in Section 2. We present several examples of datasets with clusters of complicated/general shapes in Section 3 to illustrate and evaluate our algorithm. We end with a short discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>Let S = {X 1 , X 2 , . . . , X n } be a dataset of n p-dimensional observations that are presumed to be in a partition P comprising defined categories C 1 , C 2 , . . . , C K according to some similarity measure between observations. Suppose we have N such partitions of a dataset S where Ψ = {P 1 , P 2 , . . . , P N } is the set of the N partitions. Then we define P i = {C i 1 , C i 2 , . . . , C i K i } as a candidate partition where C i j is cluster j of partition P i , |C i j | is the number of observations in C i j , K i is the number of clusters in partition P i and K i j=1 |C i j | = n for all i . Then the goal is to find, among the N partitions in Ψ, the optimal partition P * that ideally provides a close match to the true partition. Our objective in this paper is to provide methodology to identify the partitions P i and the optimal P * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Background and Preliminaries</head><p>The development of our algorithm borrows ideas from K-means and hierarchical clustering, so we revisit them briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">K-means:</head><p>The K-means algorithm starts with K 0 p-dimensional seeds {µ (0) k ; 1 ≤ k ≤ K 0 } and then iterates between cluster assignments and mean updates till convergence. Therefore, at the i th step, we update our partitions to be</p><formula xml:id="formula_0">C (i) k = {X j : X j − µ (i) k = min 1≤l≤K X j − µ (l) k j = 1, . . . , n}, for k = 1, 2, . . . , K 0 , with x = √ x x.</formula><p>These updates are followed by recalculated cluster means, with µ (i+1)</p><formula xml:id="formula_1">k = j∈C (i) k X j /|C (i) k |.</formula><p>The algorithm continues until there are no further changes in {C (i) k : k = 1, . . . , K 0 } (or, equivalently, in the µ (i) k s).</p><p>Initialization: Initialization can greatly impact performance of K-means <ref type="bibr" target="#b22">(Maitra, 2009</ref>) so we adopt MacQueen (1967)'s suggestion that samples K distinct observations from the dataset as initial seeds and runs the algorithm to convergence. We run this procedure I times, with the converged solution having the smallest within-group sum-ofsquares chosen as our K-groups partition. This approach is the default setting of the kmeans() function in R (R Core Team, 2017), with the number of initializations set by the nstart argument.</p><p>Choosing K 0 : Many methods (for example, <ref type="bibr" target="#b25">Marriott, 1971;</ref><ref type="bibr" target="#b40">Tibshirani et al., 2003;</ref><ref type="bibr" target="#b26">McLachlan, 1987;</ref><ref type="bibr" target="#b39">Sugar &amp; James, 2003;</ref><ref type="bibr" target="#b23">Maitra et al., 2012)</ref> exist for choosing K 0 . Here we discuss the <ref type="bibr" target="#b20">Krzanowski &amp; Lai (1988)</ref> criterion which uses the trace of the pooled within-group variance-covariance matrix, which we denote as W g for a K-groups partition. Following <ref type="bibr" target="#b20">Krzanowski &amp; Lai (1988)</ref>, tr ace(W K ) should decrease dramatically as K increases provided that K &lt;K, whereK is the true number of spherical groups, but that this decrease should slow down once K ≥K. Based on this rationale, and defining Di f f (K) = (K − 1) 2/p tr ace(W K−1 ) − K 2/p tr ace(W K ), the number of homogeneous spherically-dispersed groups K 0 can be obtained as follows:</p><formula xml:id="formula_2">Let C K = |Di f f (K)/Di f f (K + 1)| and K 1 , K 2 , . . . , K l be such that C K1 ≥ C K2 ≥, . . . , ≥ C K l . Then choose K 0 = K 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Agglomerative Hierarchical clustering:</head><p>Here, we successively merge current groups assuming a distance d(A, B) between any two groups A and B and a mechanism (or linkage) to recalculate the distances when groups are merged. Examples of linkages are single where d(A, B) = min{ x − y : x ∈ A, y ∈ B} or average with d(A, B) = x∈A y ∈B x − y /(|A||B|). The algorithm initially places every observation in its own group, that is, by setting C j (0) = X j for all j = 1, 2, . . . , n. Then, we successively merge clusters at each stage, so that at the i th stage, we have n − i clusters, with (n − i − 2) many of those groups unchanged from the previous stage. That is, we haveC</p><formula xml:id="formula_3">(i) j ≡C (i−1) j for all j ∈ {1, . . . , n − i } \ (k.l) where k, l are such that k &lt; l and d(C i−1 k ,C i−1 l ) = min 1≤m&lt;q≤n−i+1 d(C m (i−1) ,C (i−1) q ). SetC (i) k =C (i−1) k ∪C (i−1) l and if l &lt; n − i + 1 thenC (i) l =C (i−1) n−i+1 . Set i = i + 1.</formula><p>The merging continues until the entire hierarchy has been built, or a hierarchy with a pre-specified number of groups K • have been obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The K-means hierarchical (K-mH) cluster merging algorithm</head><p>Our proposed algorithm removes scatter and then creates multiple partitions, each formed by combining K-means and hierarchical clustering. The algorithm has the following steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Removing scatter from the dataset: The algorithm first removes scatter from the dataset from consideration. 2. Finding a partition: Our algorithm has two phases. The first focuses on finding a (potentially) large number (K 0 ) of homogeneous spherical groups while the next merges these groups according to some criterion. We call these phases the K-means and hierarchical phases. The exact details of these phases are as follows:</p><p>(a) The K-means phase: For a given K 0 and initialization, the K-means phase uses its namesake algorithm with multiple (m) initializations to identify K 0 homogeneous spherically-distributed groups. This phase yields K 0 groups {C 1 , C 2 , . . . , C K0 } with means µ 1 , µ 2 , . . . , µ K0 . Each obtained cluster C k is now considered to be one entity. Therefore, we now have K 0 entities labeled as C 1 , C 2 , . . . , C K0 for consideration. (b) Hierarchical phase: For given K * and distance d(·, ·), we successively merge the K-means groups as follows: </p><formula xml:id="formula_4">i. Set i * = 1 and d * 1 = 1. DefineC j (1) = C j for all j. ii. For j ∈ 1...(K 0 − i * )C (i * +1) j =C (i * ) j . Find k, l such that k &lt; l and d(C i * k ,C i * l ) = min 1≤m&lt;q≤(K0−i * +1) d(C i * m ,C i * q ). SetC (i * +1) k =C (i * ) k ∪C (i * ) l and if l &lt; K 0 − i * + 1 thenC (i * +1) l = C (i * ) K0−i * +1 , define d * i * = d(C i * k ,C i * l ). Set i * = i * + 1. iii. If i * = K 0 or i * = K 0 − K * + 1 terminate, else return to Step 2(b</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Forming multiple partitions and choosing the optimal P * : Repeat Step 2 N = ML times with M different K 0 s and L different K * s to form multiple partitions. Determine the optimal hierarchical partition P * .</p><p>Our outlined algorithm has several aspects that need clarification. We do this next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Scatter Removal:</head><p>Outliers or scatter can greatly influence clustering performance <ref type="bibr" target="#b24">(Maitra &amp; Ramler, 2009)</ref>. Although many methods <ref type="bibr" target="#b7">(Byers &amp; Raftery, 1998;</ref><ref type="bibr" target="#b41">Tseng &amp; Wong, 2005;</ref><ref type="bibr" target="#b24">Maitra &amp; Ramler, 2009</ref>) exist, we adopt the following straightforward approach to eliminating scatter. We use K-means with the largest of our candidate group sizes (G) and multiple initializations (K √ np) to obtain a G-means partition. Observations in any of the G groups that have less than 0.1% of the size of the dataset are labeled as scatter and eliminated from further consideration. This leaves us with n * observations X 1 , X 2 , . . . , X n * (say) which we proceed with clustering using K − mH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Distance between entities: For the hierarchical phase of</head><p>Step 2, we calculate the distance between two clusters obtained from the K-means step by assuming (non-homogeneous) spherically-dispersed Gaussian-distributed groups in the dataset. Specifically, we let X 1 , X 2 , . . . , X n * be independent p-variate observations with</p><formula xml:id="formula_5">X i ∼ N p (µ ζ i , σ 2 ζ i I), where ζ i ∈ {1, 2, . . . , K} for i = 1, 2, . . . , n * .</formula><p>Here we assume that µ k 's are all distinct and that n k is the number of observations in cluster k. Then the density for the X i 's is given by</p><formula xml:id="formula_6">f (X) = K k=1 I(X ∈ C k )φ(X; µ k , σ 2 k I), where C k is a cluster indexed by the N p (µ k , σ 2 k I) density and I(X ∈ C k ) is an indicator function specifying whether observation X belongs to the kth group having a p-dimensional multivariate normal density φ(X; µ k , σ 2 k I) ∝ σ −p k exp − 1 2σ 2 k (X − µ k ) (X − µ k ) , k = 1, . . . , K. Define the distance measure D k (X i ) = (X i − µ k ) (X i − µ k ) σ 2 k (1) and the variable Y j,l (X) = D j (X) − D l (X), where X ∈ C l ,<label>(2)</label></formula><p>and Y l,j (X) similarly. Using the spherically-dispersed Gaussian models formulated above, Y j,l (X) is a random variable which represents the difference in squared distances of X ∈ C l to the center of C j and to the center of C l . Then</p><formula xml:id="formula_7">p j l = Pr[Y j,l (X) &lt; 0]</formula><p>is the probability that an observation from C l is classified into C j and is calculated as follows.</p><formula xml:id="formula_8">Theorem 1 Let X ∼ N p (µ l , Σ l ), with Σ l a positive-definite matrix. Further, let Y j,l (X) = D j (X) − D l (X), where D k (X) = (X − µ k ) Σ −1 k (X − µ k ) for k ∈ {j, l}. Let λ 1 , λ 2 , . . . , λ p be the eigenvalues of Σ j|l ≡ Σ 1 2 l Σ −1 j Σ 1 2 l with corresponding eigenvectors γ 1 , γ 2 , ...γ p . Then Y j,l (X) is distributed as p i=1 I(λ i = 1) (λ i − 1)U i − λ i δ 2 i /(λ i − 1) + p i=1 I(λ i = 1)δ i (2Z i + δ i ),</formula><p>where U i s are independent non-central χ 2 random variables with one degree of freedom and non-centrality parameter λ 2</p><formula xml:id="formula_9">i δ 2 i /(λ i − 1) 2 with δ i = γ i Σ − 1 2 l (µ l − µ j ) for i ∈ {1, 2, ..., p} ∩ {i : λ l = 1}, independent of Z i 's, which are independent standard normal random variables, for i ∈ {1, 2, ..., p} ∩ {i : λ i =</formula><p>Stat 2017, 00 1-16</p><formula xml:id="formula_10">Proof Let ξ ∼ N p (0, I). Since X d = Σ 1 2 l ξ + µ l , we have Y j,l (X) = X (Σ −1 j − Σ −1 l )X + 2X (Σ −1 l µ l − Σ −1 j µ j ) + µ j Σ −1 j µ j − µ l Σ −1 l µ l d = (Σ 1 2 l ξ + µ l ) (Σ −1 j − Σ −1 l )(Σ 1 2 l ξ + µ l ) + 2(Σ 1 2 l ξ + µ l ) (Σ −1 l µ l − Σ −1 j µ j ) + µ j Σ −1 j µ j − µ l Σ −1 l µ l = ξ (Σ j|l − I)ξ + 2ξ Σ 1 2 l Σ −1 j (µ l − µ j ) + (µ l − µ j ) (Σ −1 j )(µ l − µ j )<label>(3)</label></formula><p>where</p><formula xml:id="formula_11">Σ j|l = Σ 1 2 l Σ −1 j Σ 1 2 l .</formula><p>Let the spectral decomposition of Σ j|l be given by Σ j|l = Γ j|l Λ j|l Γ j|l , where Λ j|l is a diagonal matrix containing the eigenvalues λ 1 , λ 2 , . . . λ p of Σ j|l , and Γ j|l is an orthogonal matrix containing the eigenvectors</p><formula xml:id="formula_12">γ 1 , γ 2 , . . . , γ p of Σ j|l . Since Z ≡ Γ j|l ξ ∼ N p (0, I) as well, we get from (3) that Y j,l (X) d = ξ (Γ j|l Λ j|l Γ j|l − Γ j|l Γ j|l )ξ + 2ξ (Γ j|l Λ j|l Γ j|l Σ − 1 2 1 )(µ l − µ j ) + (µ l − µ j ) (Σ − 1 2 1 Γ j|l Λ j|l Γ j|l Σ − 1 2 l )(µ l − µ j ) = (Γ j|l ξ) (Λ j|l − I)(Γ j|l ξ) + 2(Γ j|l ξ) (Λ j|l Γ j|l Σ − 1 2 l )(µ l − µ j ) + (µ l − µ j ) (Σ − 1 2 l Γ j|l Λ j|l Γ j|l Σ − 1 2 l )(µ l − µ j ) d = p i=1 (λ i − 1)Z i 2 + 2λ i δ i Z i + λ i δ 2 i ,<label>(4)</label></formula><p>where δ i , i = 1, 2, . . . , p are as in the statement of the theorem. We can simplify (4) further based on the values of</p><formula xml:id="formula_13">λ i : If λ i &gt; 1: (λ i − 1)Z 2 i + 2λ i δ i Z i + λ i δ 2 i = ( √ λ i − 1Z i + λ i δ i / √ λ i − 1) 2 − λ i δ 2 i /(λ i − 1), while for λ i &lt; 1: (λ i − 1)Z i 2 + 2λ i δ i Z i + λ i δ 2 i = −( √ 1 − λ i Z i − λ i δ i / √ 1 − λ i ) 2 − λ i δ 2 i /(λ i − 1). In both cases, (λ i − 1)Z i 2 + 2λ i δ i Z i + λ i δ 2 i is distributed as a (λ i − 1)χ 2 l,λ 2 i δ 2 i /(λ i −1) 2 -random variable shifted by −λ i δ 2 i /(λ i − 1). When λ i = 1, (λ i − 1)Z 2 i + 2λ i δ i Z i + λ i δ 2 i = 2δ i Z i + δ 2 i .</formula><p>The theorem follows from some further minor rearrangement of terms.</p><formula xml:id="formula_14">Corollary 1 Let X ∼ N p (µ l , σ 2 l I). Define D k (X) as in (1). If σ l = σ j , we have Y j,l (X) ∼ N( µ j − µ l 2 /σ 2 l , 4 µ j − µ l 2 /σ 2 l ), otherwise Y j,l (X) ∼ (σ 2 l /σ 2 j − 1)χ 2 p; µ l −µ j 2 /(σ 2 j −σ 2 l ) 2 − µ l − µ j 2 /(σ 2 l − σ 2 j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>Here, λ i ≡ σ 2 l /σ 2 j , γ i is the i th unit vector, and</p><formula xml:id="formula_15">p i=1 δ 2 i = µ l − µ j 2 /σ 2 l .</formula><p>Also, the sum of p independent χ 2 1;τ 2 i random variables has the same distribution as a χ 2 p; p i=1 τ 2 i random variable. The proof follows from Theorem 1.</p><p>Corollary 1 provides an easy calculation for p j l and p l j . Note, however, that for large δ (and/or p) the χ 2 p;δ cumulative distribution function is not evaluated accurately so we approximate this quantity by the corresponding cumulative distribution function of the N(p + δ, 2(p + 2δ)) random variable (for details, see <ref type="bibr" target="#b33">Muirhead, 2005</ref>, pages 22-24 and problem 1.8). The net result is that we have approximate but very speedy and accurate calculations. This is important because our hierarchical phase uses the distance measure between groups C j and C l that we define to be</p><formula xml:id="formula_16">d(C j , C l ) = 1 − (p j l + p l j )/2.<label>(5)</label></formula><p>We now adapt this distance measure to the initial and iterative parts of the hierarchical phase. At the beginning of the hierarchical phase (equivalently, the conclusion of the K-means phase), we have K 0 entities with labels C 1 , . . . , C K0 . For 1 ≤ k ≤ K 0 , we already have theμ k s while the covariance matrix (σ 2 k I) is estimated by setting σ 2 k as the trace of the variance-covariance matrix of C k scaled by p. For subsequent stages, (5) is updated by replacing the distance between an entity (say, C l ) and a merged entity (say,</p><formula xml:id="formula_17">C j ∪ C k ) as d(C l , C j ∪ C k ) = min{d(C l , C j ), d(C l , C k )}.</formula><p>A convenient aspect of this strategy is that off-the-shelf hierarchical clustering software (for example, the hclust function in R) with single linkage can be used to implement the hierarchical phases of our K-mH algorithm  <ref type="bibr" target="#b0">3</ref>. Forming N partitions and choosing P * :</p><p>Step 2 of the K − mH algorithm produces one partition starting with K 0 entities ending with K * clusters. <ref type="bibr">Step 3 runs</ref> Step 2 N = ML times, where M is the number of K 0 s and L is the number of K * s used. We discuss choosing K 0 and K * next.</p><p>Choosing candidate K 0 : Our proposal for K 0 involves chooses a range of values {k 1 , k 2 , . . . , k m }, m ≥ M for which we calculate C k1 , C k2 , . . . , C km using <ref type="bibr" target="#b20">Krzanowski &amp; Lai (1988)</ref>'s suggestions of Section 2.1.1. We sort these values to</p><formula xml:id="formula_18">get C g1 ≥ C g2 ≥, . . . , ≥ C gm , where the set {g 1 , g 2 , . . . , g m } = {k 1 , k 2 , . . . , k m }.</formula><p>However, instead of setting K 0 ≡ g 1 as recommended by <ref type="bibr" target="#b20">Krzanowski &amp; Lai (1988)</ref>, we propose running Step 2 of our algorithm for each</p><formula xml:id="formula_19">K 0 ≡ K (i) o , where K (1) 0 = g 1 , K (2) 0 = g 2 , . . . , K (M) 0 = g M , that</formula><p>is, for the numbers of clusters corresponding to the M highest C g j s. So we run the K-means phase M times with K 0 = K (i)</p><formula xml:id="formula_20">0 for i = 1, 2, . . . , M, with K (1) 0 = g 1 , K (2) 0 = g 2 , . . . , K (M) 0 = g M .</formula><p>For each of these runs, we set K * ≡ K (i) * in the hierarchical phase and in the manner described next.</p><p>Choosing candidate K * : For each value of K (i) 0 , we use K * if the number of desired general-shaped clusters is known and then we set L = 1. When K * is unknown, we obtain a range of K * s by defining change-points</p><formula xml:id="formula_21">(CP s) as CP k = d * k+1 − d * k (for k = 1, . . . , K (i) 0 ) where d * 1 ≤ d * 2 ≤ . . . ≤ d * K (i) 0 are calculated during Step 2b of the algorithm. We sort these CP -values to get CP q1 ≥ CP q2 ≥, . . . , ≥ CP q K (i) 0 −1 , where the set {q 1 , q 2 , . . . , q K (i) 0 −1 } is some appropriate permutation of the set {2, 3, . . . , K (i) o }. We consider the first L of these values. That is, we define k i,1 = q 1 , k i,2 = q 2 , . . . , k i,L = q L as in Section 2.2.3 for when we have K 0 = K (i) 0 . Then for each K (i) 0 we obtain L partitions using K * = k i,j for j ∈ {1, 2, . . . , L}.</formula><p>Thus, we arrive at N = ML partitions {P 1 , P 2 , . . . , P N } for all combinations of K 0 and K * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.4.</head><p>Visualizing partitions and choosing optimal K * : We extend <ref type="bibr" target="#b12">Fred &amp; Jain (2005)</ref>'s ideas to visualize the stability and variability in our partitions. Consider the n × n similarity matrix Ψ with (i , j)th entry ψ ij = n i,j /N, where n ij is the number of times that the i th and jth observations are in the same cluster across the N partitions obtained from Section 2.2. <ref type="bibr" target="#b0">3</ref>. We display Ψ via a clustered heatmap. The heatmap provides indication into both the structure and stability of the clustering. We can use this heatmap to decide on K * by determining all partitions which remain after thresholding below ψ ij = 0.5. We use two alternative choices in forming these partitions. In the first case, if the offdiagonal ψ ij s are generally small or uncertain (i.e. their mean is small or their coefficient of variation is high), we use single-linkage otherwise we use complete linkage. As with <ref type="bibr" target="#b12">Fred &amp; Jain (2005)</ref>, heatmaps create very large files for large n so we then use a random sample of the observations. We replicate this process B times to assess the variability in K * .</p><p>Final partition: With K * known or determined through the methods of Section 2.2.4, we have L = 1 as per Section 2.2. <ref type="bibr" target="#b0">3</ref>. Then, with the N = M partitions, we pick the clustering that is most similar to the other N − 1 partitions. This is operationally implemented by defining the N × N matrix W where W i,j = R i,j , where R i,j is the value for the Adjusted Rand Index <ref type="bibr" target="#b16">(Hubert &amp; Arabie, 1985)</ref> between partitions P i and P j . Define the objective function: W i = j W i,j /N. Then, we choose P * to be the partition that best matches Ψ in the sense of maximizing the objective function. Thus, P * = {P i :W i = max 1&lt;j&lt;NW j }is our choice for the final clustering and represents the partition that is most similar to all the other candidate partitions.</p><p>In this section, we have developed an algorithm that combines elements of K-means and hierarchical clustering to identify general-shaped clusters. All steps in our algorithm are easily implemented using existing software libraries and</p><p>Stat 2017, 00 1-16 functions in R (R Core Team, 2017) and other programming languages. We next evaluate performance of our algorithm on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Performance Evaluations</head><p>We now evaluate K-mH on simulated and real datasets to highlight the strengths and weaknesses of our methodology. We compare K-mH to the EAC (FJ) of <ref type="bibr" target="#b12">Fred &amp; Jain (2005)</ref> (FJ), cluster merging (CM) of <ref type="bibr" target="#b6">Baudry et al. (2010)</ref>, generalized single linkage with nearest-neighbor density estimate (GSL-NN) <ref type="bibr" target="#b38">(Stuetzle &amp; Nugent, 2010)</ref>, DEMP <ref type="bibr" target="#b15">(Hennig, 2010)</ref> and DEMP+ <ref type="bibr" target="#b31">(Melnykov, 2016)</ref>. We used R (R Core Team, 2017) for all methods except for CM which used Matlab code provided in the supplemental material of <ref type="bibr" target="#b6">Baudry et al. (2010)</ref>. For CM, we used the "elbow rule" on the plot of entropy variation against K to determine K <ref type="bibr" target="#b6">(Baudry et al., 2010)</ref> while for GSL-NN, we used the procedure in Section 7 of <ref type="bibr" target="#b38">Stuetzle &amp; Nugent (2010)</ref>. For FJ, er used the method in Section <ref type="bibr">3.3 of Fred &amp; Jain (2005)</ref>. Our K-mH algorithm used M = min{10, √ np/10 } (where x is the smallest integer less than or equal to x), L = 3 (before estimating K * ), B = 100 and G = √ n . In all cases, we used R <ref type="bibr" target="#b16">(Hubert &amp; Arabie, 1985)</ref> calculated between the true and estimated partitions to quanitify performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Two-dimensional Examples</head><p>We first illustrate and evaluate performance on many two-dimensional examples found in the literature.    <ref type="figure" target="#fig_3">Figure 2b</ref> displays the heatmap obtained as part of K-mH. Two large clustered blocks are indicated with uncertainty over whether the upper right block should be partitioned further. (It is this partitioning that DEMP and CM go for.) Therefore, the heatmap displays the uncertainty and structure in the partitioning, but the K-mH algorithm chooses two groups.   Revisiting the Bullseye dataset of <ref type="figure" target="#fig_1">Figure 1</ref>, we find that FJ, GSL-NN and K-mH produce good partitions <ref type="figure" target="#fig_6">(Figure 3a</ref>) with R ≥ 0.99 but DEMP, DEMP+ and CM perform poorly with the outer ring broken into several further groups. The heatmap <ref type="figure" target="#fig_6">(Figure 3b</ref>) indicates a lot of uncertainty but the methodology of Section 2.2.4 suggests two groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">The Banana-spheres dataset:</head><p>This dataset has two separated banana-shaped half rings of 250 observations each that are surrounded by a third group in the shape of a full ring of 1500 observations. The observations in each group were simulated using pseudo-random realizations from different bivariate normal distributions with means that followed the central path of each shape. An additional 15 outlying observations from each cluster were added to provide a dataset of 3015 observations.  3.1. <ref type="bibr" target="#b0">3</ref>. The SCX Dataset: This dataset has a variety of cluster shapes and sizes, with three separated C-shaped groups rotated at different angles, a large S-shaped group and four small X-shaped groups. Twenty outlying observations are added to the clusters for a total of 3420 observations. Here, K-mH partitioning ( <ref type="figure" target="#fig_11">Figure 5</ref>) is near-perfect (with two observations misclassified as scatter and not displayed in the dataset) while FJ is the next best performer. CM, DEMP and DEMP+ perform similarly, but GSL-NN finds 7 groups (R = 0.53) clusters, with the S and 4 crosses all placed in one group and the two lower C's split into 2 and three groups, respectively. The heatmap indicates uncertainty with 4 large groups with further definition and K * not easily identified. This uncertainty is reflected in the estimated K * s which were 7, 8, 9, and 10, with frequency of occurrence 28, 48, 22, and 2% of the time, respectively. The median estimated K * = 8 yields the perfect solution of <ref type="figure" target="#fig_11">Figure 5a</ref>. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q  </p><formula xml:id="formula_22">q q q q qq q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q qq q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq qq q q q q q q q qq q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qqq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq</formula><formula xml:id="formula_23">(b) R = 0.89, K = 8 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q qq q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q qq q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q qq q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q qq q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q qq q q q q q qq q q q q</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">The Cigarette-Bullseye dataset</head><p>We have another complex-structured dataset with 3 concentric ringed groups, 2 long groups above 2 small spherical ones and 1 group that is actually a superset of 2 overlapping Gaussian groups. K-mH and FJ perform similarly while CM finds 6 clusters but R = 0.99 because the smaller groups are the ones not identified clearly. GSL-NN also underestimates the number of groups to be 6, with R = 0.78. Both DEMP (R = 0.62) and DEMP+ (R = 0.64) exhibit poorer performance. The heatmap has similar characteristics as SCX, with 3-4 large groups but no clear choice for K * beyond that even though there are suggestions of sub-groups within each of the large groups. However, estimates of K * were 8 (50% of the time), 9 (42%) and 10 (8% of the time). The median K * = 8 yields the perfect K-mH solution of <ref type="figure" target="#fig_14">Figure 6a</ref> while K * = 9 breaks the leftmost long cluster further into two groups, yielding a similar partitioning as FJ <ref type="figure" target="#fig_14">(Figure 6c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Higher-Dimensional Datasets</head><p>We next present performance evaluations on three higher-dimensional datasets often used in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Olive Oils:</head><p>This dataset <ref type="bibr" target="#b10">(Forina &amp; Tiscornia, 1982;</ref><ref type="bibr" target="#b9">Forina et al., 1983)</ref> has measurements on 8 chemical components for 572 samples of olive oil taken from 9 different areas in Italy which are from three regions: q qq q q q qq q qq q q q qqq qq q qq q q q q q qq q qq qq qq q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q qq q q q q q q q q q qq q q q q qq q q q q q q q q q qq q q q q qq q qqq q qq q q q q qq q q q q q q qq q q q q qq q q q qq q qq q q qq q q qq q qq q q qq q q q q qq q q q q qq qq q q q q q q q q q q qq q q q qq q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q qq q q q q q q q q qq q q q qq q q qqq q q q q qq qq q q q q q qq qq q q q qq q q q q q q q q q qq q qq q q q q qq qqq q qq qq q q q q qq q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q qq q q q q q q qq q q qq q q qq q q q q q q q qqq q q qq q q q qq q q q qq q q q qq qq qq qq qqq q q q q qqq qq qq qqq qq q q q q q q q q q q q qq q q q q q q q q qq qq q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q qq q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q qq q q q q qq q q qq q q q q qq q q q q q q q q q q q q q qq q q q q q qq q q qq q qq qq q q q q q qq q q q q q qq qqq q q q qq q q qqq q q q q qq q q q q q q q q q q qq q q q q q q q q q q q q q qq q q q q q q q qq q q q q q q q q q q q q q q q q q q qq qq q q q q q q q q q q q q q q q q qq q q q q q q q q q q q qq q q q q q q q q q q q qq q q q q q q q q q q q q q qq qq q q q q qq q q q q qq q q q qqq q q q q q q q qq q q qq q qq q q q q qq q q q q qq q (c) R = 0.96, K = 9  and Northern and Southern Italy. For this dataset, GSL-NN is the only method that identifies 9 groups (R = 0.61) while FJ identifies 8 groups (R = 0.54). CM (R = 0.75), DEMP (R = 0.82)and DEMP+ (R = 0.85) are the best (R = 0.75) performers even though they identify only 7 groups. The visualization step of the K-mH algorithm on the other hand largely identifies 8 kinds of olive oils (88% of the time) and also 7 (2%) and 9 (12%) kinds of olive oils. The median estimated K * = 11 yields a partitioning with R = 0.67. A closer look at the K-mH partitions reveals that oils from the southern areas of Calabria, Sicily and South Apulia are mainly grouped together in our second cluster while the remaining southern area of North-Apulia primarily populates our ninth cluster. Coastal and Inland Sardinian olive oils are identified very well by our groupings. Our partitioning aligns very well with the three regions with our groups 3, 4, 5, 10 and 11 (with the exception of one oil) all exclusively from the north, groups 7 and 8 from Sardinia and groups 1, 2, 6 and 9 exclusively from the south. The near-perfect embedding of our groups within the three regions indicates that the nine areas drawn using political geography may not distinguish the different kinds of olive oils as well as a different characterization using a different set of sub-regions that are based on physical geography.  <ref type="bibr" target="#b38">Stuetzle &amp; Nugent (2010)</ref> report that GSL-NN "vaguely" finds 9 groups (R = 0.64) but that their 10-groups solution is worse (R = 0.54). We normalized the measurements for each digit to have zero mean and unit variance so that the Euclidean distance between any two observations is negatively but affinely related to the correlation between them. We reduced dimensions by principal components analysis and used the projection of the observations into the space spanned by the first 54 principal components which explain at least 90% of the variation in the data. This dataset is perhaps too cumbersome for CM, DEMP and DEMP+ while FJ finds 6 groups but the assignment is not very far from random (R = 0.05). The K-mH heatmap <ref type="figure">(Figure 8)</ref>  in Section 3.2.2, may suggest that the 16 attributes used to characterize the samples may have focused more on some features of the handwriting of digits.</p><p>The performances of K-mH, FJ, CM, DEMP and DEMP+ for all cases are summarized in <ref type="table" target="#tab_4">Table 1</ref> and shows that K-mH is always among the top performers. This happens with very complicated as well as simpler structures. Even when performance is not outstanding, as happened with higher-dimensional real-life datasets, K-mH is still a top performer, often producing results that are interpretable. FJ is also a good performer in the two-dimensional examples but this performance degraded more in higher dimensions than with K-mH. DEMP ad DEMP+ was a good performer only on the Olive Oils dataset where it performed very well despite underestimating the number of groups by 2. Our algorithm was also able to handle computations for the larger handwritten digits dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>In this paper we propose a new K-means hierarchical clustering algorithm that builds on the idea of <ref type="bibr" target="#b12">Fred &amp; Jain (2005)</ref> that different clusterings of a dataset each provide different discrete evidence of a grouping. We compare several different clusterings of the data and choose the final grouping that is most similar to the proposed partitions. Our algorithm is among the top performing methods for both simulated datasets with complicated shapes as well as several real datasets. We also present an automated clustering approach for finding the optimal parition and number of groups that is shown to perform well. In addition, we use a graphical method introduced in Fred &amp; Jain <ref type="formula" target="#formula_6">(2005)</ref> that we use to investigate uncertainty and structural stability of the clustering and to determine the correct number of groups. Our K-mH algorithm is computationally efficient for larger datasets in comparison to several other cluster merging algorithms. Indeed, the main computational cost is that of performing K-means for different K, which can be expensive given the number of initializing runs for each K. Further, it is very easily coded: simple R functions doing the same are available on request.</p><p>There are several directions for future work. One possibility is to compare other distance measures in the hierarchical</p><p>Stat 2017, 00 1-16 step of the K-mH algorithm. It may be worthwhile to further use other different distance measures as candidate partitions when choosing the optimal partition P * . Another aspect worthy of investigation would be to explore additional ways for determining K * . It is worth noting in this context that the hierarchical map for visualizing structural stability can be a memory-intensive operation. Thus, we see that while we have put forward a promising algorithm, issues meriting further attention remain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>HC, K = 2, single linkage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Partition (using both color and symbol) of the Bullseye dataset with (a) 2-means, (b) 6-means and (c) single linkage hierarchical clustering with 2 groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>(a) K-mH partitioning of the Banana-clump dataset and (b) heatmap illustrating clustering uncertainty and stability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 1</head><label>1</label><figDesc>.1. Smaller-sized Datasets: The Banana-clump dataset (Figure 2a) of Stuetzle &amp; Nugent (2010) has 200 observations. FJ, DEMP+, GSL-NN and K-mH all reproduce the original partitioning but DEMP and the "elbow" approach of CM suggest three groups with the banana essentially halved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>(a) K-mH partitioning of the Banana-clump dataset and (b) heatmap illustrating clustering uncertainty and stability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figures 4a-c display the top three performers. K-mH chooses three groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Top three partitionings of the Bananas-sphere dataset using (a) K-mH (b) FJ and (c) GSL-NN. Captions indicate estimated number of groups and R between estimated and true groupings. (d) K-mH heatmap for stability of groupings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 .</head><label>5</label><figDesc>Top three performers for SCX: (a) K-mH (b) FJ and (c) CM and (d) the K-mH heatmap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>. Copyright c 2017 John Wiley &amp; Sons, Ltd. 10 Stat 2017, 00 1-16 Prepared using staauth.cls R = 0.99, K = 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 .</head><label>6</label><figDesc>Top three performers on the Cigarette-Bullseye dataset: (a) K-mH, (b) CM, (c) FJ and (d) the K-mH heatmap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 .</head><label>7</label><figDesc>The K-mH heatmap and the results by region and area obtained from K-mH clustering of the Olive Oils dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9 .</head><label>9</label><figDesc>The K-mH heatmap and the results by digit obtained from K-mH clustering of the Handwritten Digits dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. .   2017, 00 1-163.2.2. Zipcode  Images: The zipcode images dataset made available by<ref type="bibr" target="#b38">Stuetzle &amp; Nugent (2010)</ref> has been used in machine learning to evaluate clustering and classification algorithms and consists of 2000 16 × 16 images of handwritten Hindu-Arabic numerals. Thus, p = 256 here.</figDesc><table><row><cell>Stat</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Performance in terms of R (first row of each block) and estimated number of groupsK (second row of each block) for all datasets used in the experiment. A "-" indicates that the algorithm failed to converge or returned an error message.</figDesc><table><row><cell>Name</cell><cell cols="2">Dataset</cell><cell>(N, p, K)</cell><cell>Measure</cell><cell>FJ</cell><cell cols="4">Method CM GSL-NN DEMP DEMP+ K-mH</cell></row><row><cell cols="2">Banana-Clump</cell><cell></cell><cell>(200,2,2)</cell><cell>R K</cell><cell cols="2">1.0 0.78 2 3</cell><cell>1.0 2</cell><cell>0.77 3</cell><cell>1.0 2</cell><cell>1.0 2</cell></row><row><cell>Bullseye</cell><cell></cell><cell></cell><cell>(400,2,2)</cell><cell>R K</cell><cell cols="2">0.99 0.53 2 5</cell><cell>0.74 2</cell><cell>0.21 7</cell><cell>0.31 6</cell><cell>0.99 2</cell></row><row><cell cols="2">Banana-Spheres</cell><cell></cell><cell>(3015,2,3)</cell><cell>R K</cell><cell cols="2">0.95 0.53 5 11</cell><cell>0.74 2</cell><cell>0.29 18</cell><cell>0.45 13</cell><cell>0.99 3</cell></row><row><cell>SCX</cell><cell></cell><cell></cell><cell>(3420,2,8)</cell><cell>R K</cell><cell cols="2">0.89 0.78 8 12</cell><cell>0.53 7</cell><cell>0.77 12</cell><cell>0.78 12</cell><cell>1.0 8</cell></row><row><cell cols="2">Cigarette-Bullseye</cell><cell></cell><cell>(3025,2,8)</cell><cell>R K</cell><cell cols="2">0.96 0.99 9 6</cell><cell>0.78 6</cell><cell>0.62 11</cell><cell>0.64 10</cell><cell>1.0 8</cell></row><row><cell cols="2">Olive Oils</cell><cell></cell><cell>(572,8,9)</cell><cell>R K</cell><cell cols="2">0.54 0.75 8 7</cell><cell>0.61 9</cell><cell>0.82 7</cell><cell>0.85 7</cell><cell>0.67 11</cell></row><row><cell cols="2">Zipcode Digits</cell><cell cols="2">(2000,256,10)</cell><cell>R K</cell><cell>0.05 8</cell><cell>--</cell><cell>0.64 9</cell><cell>--</cell><cell>--</cell><cell>0.54 26</cell></row><row><cell cols="4">Handwritten Digits (10992,16,10)</cell><cell>R K</cell><cell>0.10 10</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>0.64 24</cell></row><row><cell cols="5">Number of cases where a competitor performs better</cell><cell>6</cell><cell>8</cell><cell>7</cell><cell>8</cell><cell>6</cell><cell>2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Copyright c 2017 John Wiley &amp; Sons, Ltd. Prepared using staauth.cls</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was supported, in part, by National Science Foundation (NSF) grants DMS-0707069, DMS-CAREER-0437555 and by the National Institutes of Health grant R21EB0126212. The content of this paper however is solely the responsibility of the authors and does not represent the official views of the NSF or the NIH.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of clarity in the number of groups with K * chosen at between 29 and 30 most of the time. The median K * = 30 yields the grouping (R = 0.54) of <ref type="figure">Figure 8</ref>. Inspection indicates five main types of handwritten digits for 0 and 2, four kinds for 6, three kinds of 4 and 9, two major kinds of 3, 5 and 7 and one major kind for each of 1 and 8. Our groups correspond very reasonably to handwriting styles for digits and are very interpretable.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handwritten Digits: The Handwritten Digits dataset</title>
		<idno>2.3</idno>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Alimoglu</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alimoglu &amp;amp; Alpaydin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">but not with a simpler digit like 1, which, in the light of our findings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Combining Multiple Classifiers for Pen-Based Handwritten Digit Recognition, Master&apos;s thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alimoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>Institute of Graduate Studies in Science and Engineering, Bogazici University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Methods of combining multiple classifiers based on different representations for pen-based handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alimoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>&amp;amp; Alpaydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Turkish Artificial Intelligence and Artificial Neural Networks Symposium (TAINN 96)</title>
		<meeting>the Fifth Turkish Artificial Intelligence and Artificial Neural Networks Symposium (TAINN 96)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . .</forename></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining mixture components for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Baudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gottardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="332" to="353" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nearest neighbor clutter removal for estimating features in spatial point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S &amp;amp;</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leesem</surname></persName>
		</author>
		<title level="m">Cluster Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Hodder Arnold</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>4th ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification of olive oils from their fatty acid composition,&apos; in Food Research and Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Armanino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tiscornia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Science Publishers</title>
		<imprint>
			<biblScope unit="page" from="189" to="214" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pattern recognition methods in the prediction of italian olive oil origin by their fatty acid content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M &amp;amp;</forename><surname>Forina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tiscornia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annali di Chimica</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-based clustering, discriminant analysis, and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C &amp;amp;</forename><surname>Fraley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="611" to="631" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings using evidence accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al &amp;amp;</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="835" to="850" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical theory in clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="63" to="76" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ja</forename><forename type="middle">&amp;amp;</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Methods for merging Gaussian mixture components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11634-010-0058-3</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Data Analysis and Classification</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>&amp;amp; Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Applied Multivate Statical Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ra &amp;amp; Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
	<note>6 edn</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Finding Groups in Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L &amp;amp;</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rousseuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>John Wiley and Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The practice of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kettenring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A criterion for determining the number of groups in a data set using sum-of-squares clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wj &amp;amp;</forename><surname>Krzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some methods of classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Initializing partition-optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maitra</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCBB.2007.70244</idno>
		<ptr target="http://doi.ieeecomputersociety.org/10.1109/TCBB.2007.70244" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bootstrapping for significance of compact clusters in multi-dimensional datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melnykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lahiri</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2011.646935</idno>
		<ptr target="http://dx.doi.org/10.1080/01621459.2011.646935" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">497</biblScope>
			<biblScope unit="page" from="378" to="392" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustering in the presence of scatter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R &amp;amp;</forename><surname>Maitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="341" to="352" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Practical problems in a method of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Marriott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="501" to="514" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On bootstrapping the likelihood ratio test statistic for the number of components in a normal mixture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="318" to="324" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G &amp;amp;</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
		<title level="m">Finite Mixture Models</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons, Inc</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . .</forename></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="page" from="0" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ke</surname></persName>
		</author>
		<title level="m">Mixture Models: Inference and Applications to Clustering, Marcel Dekker</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Merging mixture components for clustering through pairwise overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Melnykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="90" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CARP: Software for fishing out good clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V &amp;amp;</forename><surname>Melnykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="69" to="73" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Muirhead</surname></persName>
		</author>
		<title level="m">Aspects of Multivariate Statistical Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>2 edn</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-dimensional clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hettich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cl &amp;amp; Merz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R Core Team</surname></persName>
		</author>
		<title level="m">R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonparametric clustering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Ramey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Statistical Science</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1985" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="318" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A generalized single linkage method for estimating the cluster tree of a density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W &amp;amp;</forename><surname>Stuetzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nugent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCGS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="397" to="418" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Finding the number of clusters in a dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ca &amp;amp;</forename><surname>Sugar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="issue">463</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Estimating the number of clusters in a dataset via the gap statistic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walther</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="411" to="423" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tight clustering: A resampling-based approach for identifying stable and tight patterns in data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gc &amp;amp;</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="10" to="16" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
