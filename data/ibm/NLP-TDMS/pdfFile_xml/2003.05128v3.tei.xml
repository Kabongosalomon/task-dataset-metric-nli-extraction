<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cars Can&apos;t Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">A&amp;B Center</orgName>
								<orgName type="institution">LG Electronics</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Lawrence Livermore National Laboratory</orgName>
								<address>
									<settlement>Livermore</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cars Can&apos;t Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper exploits the intrinsic features of urban-scene images and proposes a general add-on module, called height-driven attention networks (HANet), for improving semantic segmentation for urban-scene images. It emphasizes informative features or classes selectively according to the vertical position of a pixel. The pixel-wise class distributions are significantly different from each other among horizontally segmented sections in the urban-scene images. Likewise, urban-scene images have their own distinct characteristics, but most semantic segmentation networks do not reflect such unique attributes in the architecture. The proposed network architecture incorporates the capability exploiting the attributes to handle the urbanscene dataset effectively. We validate the consistent performance (mIoU) increase of various semantic segmentation models on two datasets when HANet is adopted. This extensive quantitative analysis demonstrates that adding our module to existing models is easy and cost-effective. Our method achieves a new state-of-the-art performance on the Cityscapes benchmark with a large margin among ResNet-101 based segmentation models. Also, we show that the proposed model is coherent with the facts observed in the urban scene by visualizing and interpreting the attention map. Our code and trained models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic image segmentation, a fundamental task in computer vision, is employed for basic urban-scene understanding in autonomous driving. Fully convolutional networks (FCNs) <ref type="bibr" target="#b27">[28]</ref> are seminal work that adopts deep convolutional neural networks (CNNs) in semantic segmentation, by replacing fully connected layers with convolutional ones at the last stage in typical CNN architectures. Other advanced techniques, such as skip-connections in an encoder-decoder architecture <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b7">8]</ref>, an atrous convolu-tion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46]</ref>, an atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b6">[7]</ref>, and a pyramid pooling module <ref type="bibr" target="#b50">[51]</ref>, have further improved the FCN-based architecture in terms of semantic segmentation performance. They have proven to be successful in diverse semantic segmentation benchmarks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> including urban-scene datasets.</p><p>Yet, urban-scene images have their own distinct nature related to perspective geometry <ref type="bibr" target="#b22">[23]</ref> and positional patterns <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b9">10]</ref>. Due to the fact that the urban-scene images are captured by the cameras mounted on the front side of a car, the urban-scene datasets consist only of roaddriving pictures. This leads to the possibility of incorporating common structural priors depending on a spatial position, markedly in a vertical position. To verify this characteristic, we present the class distribution of an urban-scene dataset across vertical positions in <ref type="figure">Fig. 1</ref>. Although the pixels of the few classes are dominant in an entire region of an image ( <ref type="figure">Fig. 1(a)</ref>), the class distribution has significant dependency on a vertical position. That is, a lower part of an image is mainly composed of road, while the middle part contains various kinds of relatively small objects. In the upper part, buildings, vegetation, and sky are principal objects as shown in <ref type="figure">Fig. 1(b)</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows the probabilities with respect to the dominant top-5 classes: road, building, vegetation, car, and sidewalk. The class distribution is extremely imbalanced that the dominant classes take over 88% of the entire dataset. As mentioned above, the class distribution is completely different if the image is divided into three regions: upper, middle, and lower parts. For instance, the probability of the road class p road is 36.9% on average given an entire image, but this chance drops dramatically to 0.006% for an upper region, while jumps to 87.9% if a lower region is considered.</p><p>Also, we analyzed this observation using entropy, a measure of uncertainty. The entropy of the probability distribution X of a pixel over 19 classes in the Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref> is computed as  <ref type="figure">Figure 1</ref>: Motivation of our approach, the pixel-wise class distributions. All numbers are average values obtained from the entire training set of the Cityscapes dataset with its pixel-level class labels <ref type="bibr" target="#b11">[12]</ref>. Note that there exists a total of 2048K pixels per image, and the y-axis is in log-scale. (a) Each bar represents the average number of pixels assigned to each class contained in a single image. For example, on average, about 685K pixels per image are assigned to the road class. (b) Each part of an image divided into three horizontal sections has a significantly different class distribution from each other. For example, the upper region has just 38 pixels of the road class, while the lower region has 480K pixels of it.</p><p>H(X) = H(p road , p building , . . . , p motorcycle )</p><formula xml:id="formula_0">= − i p i log p i ,<label>(1)</label></formula><p>where p i denotes the probability that an arbitrary pixel is assigned to the i-th class. the conditional entropy H(X image) of X given an image, computed by Eq. (1), is 1.84. On the other hand, the average conditional entropy of X given each of the three regions as H(X upper), H(X middle), and H(X lower), is 1.26 as shown in <ref type="table" target="#tab_1">Table 1</ref>. As a result, one can see the uncertainty is reduced if we divide an image into several parts horizontally. Based on this analysis, if we can identify the part of an image to which a given arbitrary pixel belongs, it will be helpful for pixel-level classification in semantic segmentation. Motivated by these observations, we propose a novel height-driven attention networks (HANet) as a general addon module to semantic segmentation for urban-scene images. Given an input feature map, HANet extracts "heightwise contextual information", which represents the context of each horizontally divided part, and then predicts which features or classes are more important than the others within each horizontal part from the height-wise contextual information. The models adopting HANet consis-  tently outperform baseline models. Importantly, our proposed module can be added to any CNN-based backbone networks with negligible cost increase. To verify the effectiveness of HANet, we conduct extensive experiments with various backbone networks such as ShuffleNetV2 <ref type="bibr" target="#b28">[29]</ref>, MobileNetV2 <ref type="bibr" target="#b36">[37]</ref>, ResNet-50 <ref type="bibr" target="#b17">[18]</ref>, and ResNet-101 <ref type="bibr" target="#b17">[18]</ref>. We also focus on lightweight backbone networks where a lightweight HANet is more effective. The main contributions of this paper include:</p><p>• We propose a novel lightweight add-on module, called HANet, which can be easily added to existing models and improves the performance by scaling the activation of channels according to the vertical position of a pixel. We show the effectiveness and wide applicability of our method through extensive experiments by applying on various backbone networks and two different datasets.</p><p>• By adding HANet to the baseline, with negligible computational and memory overhead, we achieve a new stateof-the-art performance on the Cityscapes benchmark with a large margin among ResNet-101 based models.</p><p>• We visualize and interpret the attention weights on individual channels to experimentally confirm our intuition and rationale that height position is crucial to improve the segmentation performance on urban scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Model architectures for semantic segmentation. Maintaining the resolution of a feature map while capturing highlevel semantic features is essential in achieving high performance of semantic segmentation. Typically, high-level features are extracted by stacking multiple convolutions and spatial pooling layers, but the resolution gets coarser in the process. Several studies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref> address this limitation by leveraging deconvolution for learnable upsampling from low-resolution features. Skip-connections overcome the limitation by recovering the object boundaries in a decoder layer through leveraging high-resolution features existing earlier in the encoder layer <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8]</ref>. Another prevalent method is atrous convolution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46]</ref>, which increases the receptive field size without increasing the number of parameters, and it is widely adopted in recent semantic segmentation networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>. Additionally, ASPP <ref type="bibr" target="#b6">[7]</ref> and pyramid pooling modules <ref type="bibr" target="#b50">[51]</ref> address such challenges caused by diverse scales of objects. More recently, long-range dependency is captured <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16]</ref> to improve the performance especially by extending self-attention mechanism <ref type="bibr" target="#b38">[39]</ref>. Focusing on boundary information is another approach in semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>. Recent work imposes separate modules for targeting boundary processing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref> or boundary driven adaptive downsampling <ref type="bibr" target="#b29">[30]</ref>. Also, capturing contextual information is widely exploited. ACFNet <ref type="bibr" target="#b47">[48]</ref> uses class center to gather features of pixels in each class as categorical context, while CFNet <ref type="bibr" target="#b49">[50]</ref> learns the distribution of co-occurrent features and captures co-occurrent context to relook before making predictions. CiSS-Net <ref type="bibr" target="#b51">[52]</ref> adopts reinforcement learning to explore the context information in predicted segmentation maps, not having any supervision. Exploitation of urban-scene image. In the field of semantic segmentation, several studies exploit the characteristics of the urban-scene images. In general, the scale of objects significantly vary in the urban-scene images. FoveaNet <ref type="bibr" target="#b22">[23]</ref> localizes a "fovea region", where the smallscale objects are crowded, and performs scale normalization to address heterogeneous object scales. DenseASPP <ref type="bibr" target="#b42">[43]</ref> adopts densely connected ASPP, which connects multiple atrous convolutional layers <ref type="bibr" target="#b19">[20]</ref> to address large-scale changes of the objects. Another recent approach <ref type="bibr" target="#b52">[53]</ref> exploits the fact that the urban-scene images have continuous video frame sequences and proposes the data augmentation technique based on a video prediction model to create future frames and their labels.</p><p>Recent approaches in the field of domain adaptation propose the method to leverage the properties of the urbanscene images. A class-balanced self-training with spatial priors <ref type="bibr" target="#b54">[55]</ref> generates pseudo-labels for unlabeled target data by exploiting which classes appear frequently at a particular position in an image for unsupervised domain adaptation. Another approach <ref type="bibr" target="#b9">[10]</ref> divides an urban-scene image into several spatial regions and conducts domain adaptation on the pixel-level features from the same spatial region. Also, the correlation between depth information and semantic is exploited to gain additional information from synthetic data for urban-scene domain adaptation <ref type="bibr" target="#b8">[9]</ref>. Channel-wise attention. Our proposed method, HANet, has strong connections to a channel-wise attention approach, which exploits the inter-channel relationship of fea-tures and scales the feature map according to the importance of each channel. Squeeze-and-excitation networks (SENets) <ref type="bibr" target="#b18">[19]</ref> capture the global context of the entire image using global average pooling and predict per-channel scaling factors to extract informative features for an image classification task. This mechanism is widely adopted in subsequent studies <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b21">22]</ref> for image classification and semantic segmentation tasks. Inspired from ParseNet <ref type="bibr" target="#b26">[27]</ref>, which shows the impact of a global context of an entire image in semantic segmentation, previous work <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b21">22]</ref> for semantic segmentation exploits the global context of the entire image to generate channel-wise attention. However, the urban-scene datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b3">4]</ref> consist only of roaddriving pictures, which means that the images share similar class statistics. Therefore, the global context should be relatively similar among urban-scene images. As a result, the global context of the entire image cannot present distinct information of each image to help per-pixel classification in urban-scene images. This explains why the previous work related to channel-wise attention for semantic segmentation mainly focuses on the generic scene datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Urban-scene images generally involve common structural priors depending on a spatial position. Each row of an image has significantly different statistics in terms of a category distribution. In this sense, individually capturing the height-wise contextual information, which represents the global context of each row can be used to estimate how channels should be weighted during pixel-level classification for urban-scene segmentation. Therefore, we propose HANet which aims to i) extract the height-wise contextual information and ii) compute height-driven attention weights to represent the importance of features (at intermediate layers) or classes (at last layer) for each row using the context. In this section, we first describe HANet as a general addon module and then present the semantic segmentation networks incorporating several HANet at different layers specialized for urban-scene segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Height-driven Attention Networks (HANet)</head><p>HANet generates per-channel scaling factors for each individual row from its height-wise contextual information as its architecture illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Let X ∈ R C ×H ×W and X h ∈ R C h ×H h ×W h denote the lower-and higher-level feature maps in semantic segmentation networks, where C is the number of channels, H and W are the spatial dimensions of the input tensor, height and width, respectively. Given the lower-level feature map X , F HANet generates a channel-wise attention map A ∈ R C h ×H h made up of height-wise per-channel scaling factors and fitted to the channel and height dimensions of the higher-level feature map X h . This is done in a series of steps: width-wise pooling ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>), interpolation for coarse attention ( <ref type="figure" target="#fig_1">Fig. 2(b,d)</ref>), and computing height-driven attention map ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>). Moreover, adding positional encoding is included in the process ( <ref type="figure" target="#fig_1">Fig. 2(e)</ref>).</p><p>After computing the attention map, the given higherlevel feature map X h can be transformed into a new rep-resentationX h , acquired by an element-wise multiplication of A and X h . Note that single per-channel scaling vector is derived for each individual row or for each set of multiple consecutive rows, so the vector is copied along with the horizontal direction, which is formulated as</p><formula xml:id="formula_1">X h = F HANet (X ) X h = A X h .<label>(2)</label></formula><p>Width-wise pooling ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>). In order to obtain a channel-wise attention map, we firstly extract height-wise contextual information from each row by aggregating the C × H × W input representation X into a C × H × 1 matrix Z using a width-wise pooling operation G pool , i.e.,</p><formula xml:id="formula_2">Z = G pool (X ) .<label>(3)</label></formula><p>There are two typical pooling methods, average pooling and max pooling, to squeeze the spatial dimension. The choice between max pooling and average pooling for the widthwise pooling operation is a hyper-parameter and is empirically set to average pooling. Formally, the h-th row vector of Z is computed as</p><formula xml:id="formula_3">Z :,h = [ 1 W W i=1 X 1,h,i ; . . . ; 1 W W i=1 X C,h,i ].<label>(4)</label></formula><p>Interpolation for coarse attention ( <ref type="figure" target="#fig_1">Fig. 2(b,d)</ref>). After the pooling operation, the model generates a matrix Z ∈ R C ×H . However, not all the rows of matrix Z may be necessary for computing an effective attention map. As illustrated in <ref type="figure">Fig. 1(b)</ref>, class distributions for each part highly differ from each other, even if we divide the entire area into just three parts. Therefore, we interpolate C × H matrix Z into C ×Ĥ matrixẐ by downsampling it <ref type="figure" target="#fig_1">(Fig. 2(b)</ref>).Ĥ is a hyper-parameter and is empirically set to 16. Since the attention map, constructed from downsampled representations, is also coarse, the attention map is converted to have the equivalent height dimension with the given higher-level feature map X h via upsampling ( <ref type="figure" target="#fig_1">Fig. 2(d)</ref>).</p><p>Computation of height-driven attention map ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>).</p><p>A height-driven channel-wise attention map A is obtained by convolutional layers that take width-wise pooled and interpolated feature mapẐ as input. Recent work that utilized a channel-wise attention in classification and semantic segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref> adopts fully connected layers rather than convolutional layers since they generate a channel-wise attention for an entire image. However, we adopt convolutional layers to let the relationship between adjacent rows be considered while estimating the attention map since each row is related to its adjacent rows. The attention map A indicates which channels are critical at each individual row. There may exist multiple informative features at each row in the intermediate layer; in the last layer, each row can be associated with multiple labels (e.g., road, car, sidewalk, etc.). To allow these multiple features and labels, a sigmoid function is used in computing the attention map, not a softmax function. These operations consisting of N convolutional layers can be written as</p><formula xml:id="formula_4">A = G up σ G N Conv · · · δ G 1 Conv Ẑ ,<label>(5)</label></formula><p>where σ is a sigmoid function, δ is a ReLU activation, and G i Conv denotes i-th one-dimensional convolutional layer. We empirically adopt three convolutional layers: the first one</p><formula xml:id="formula_5">G 1 Conv Ẑ = Q 1 ∈ R C r ×Ĥ for channel reduction, the sec- ond one G 2 Conv δ(Q 1 ) = Q 2 ∈ R 2· C r ×Ĥ ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and the last one G 3</head><p>Conv δ(Q 2 ) =Â ∈ R C h ×Ĥ for generating an attention map. The reduction ratio r reduces the parameter overhead of HANet as well as gives a potential regularization effect. An analysis on the effect of various reduction ratio as a hyper-parameter will be presented in Section 4.2.2. Incorporating positional encoding ( <ref type="figure" target="#fig_1">Fig. 2(e)</ref>). When humans recognize a driving scene, they have prior knowledge on the vertical position of particular objects (e.g., road and sky appear in the lower and upper part, respectively). Inspired by this observation, we add the sinusoidal positional encodings <ref type="bibr" target="#b38">[39]</ref> to the intermediate feature map Q i at the ith layer in the HANet. A hyper-parameter i is analyzed in the supplementary material. For injecting positional encodings, we follow the strategy proposed in Transformer <ref type="bibr" target="#b38">[39]</ref>. The dimension of the positional encodings is same as the channel dimension C of the intermediate feature map Q i . The positional encodings are defined as</p><formula xml:id="formula_6">P E (p,2i) = sin p/100 2i/C P E (p,2i+1) = cos p/100 2i/C ,</formula><p>where p denotes the vertical position index in the entire image ranging from zero toĤ − 1 of coarse attention, and i is the dimension. The number of the vertical position is set toĤ as the number of rows in coarse attention. The new representationQ incorporating positional encodings is for-</p><formula xml:id="formula_7">mulated asQ = Q ⊕ P E,<label>(6)</label></formula><p>where ⊕ is an element-wise sum.</p><p>Height positions are randomly jittered by up to two positions to generalize over different camera location from various datasets to prevent an inordinately tight position-object coupling. Additionally, we experiment with using learnable positional embeddings <ref type="bibr" target="#b16">[17]</ref> to find the best way to incorporate positional information in the supplementary material.</p><p>Meanwhile, CoordConv <ref type="bibr" target="#b25">[26]</ref> proposed to embed height and width coordinates in the intermediate features for various vision tasks: extra channels containing hard-coded coordinates (e.g., height, width, and optional r) are concatenated channel-wise to the input representation, and then a standard convolutional layer is applied. Unlike this model, HANet exploits positional information of height to obtain attention values, C × H × 1, which is used for gating the output representations of main networks. Therefore, HANet differs significantly from CoordConv in terms of how to exploit the positional information. We experimentally compare ours with CoordConv in Section 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segmentation Networks based on HANet</head><p>We adopt DeepLabv3+ <ref type="bibr" target="#b5">[6]</ref> as a baseline for semantic segmentation. DeepLabv3+ has an encoder-decoder architecture with ASPP that employs various dilation rates. We add HANet to the segmentation networks at five different layers <ref type="figure" target="#fig_2">(Fig. 3)</ref> after the point where high-level representation is encoded from backbone networks. This is because the higher-level feature has a stronger correlation with the vertical position. We conduct an ablation study to see the performance gain from adding HANet at different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with Other Attention Strategies</head><p>Self-attention-based approaches like DANet <ref type="bibr" target="#b15">[16]</ref>, obtain attention values, (H × W )×(H × W ), C × C, from the semantic interdependencies in spatial and channel dimensions, respectively. However, HANet does not consider  the interdependencies among the dimensions. HANet includes one-dimensional convolutional layers being a separate branch as a modular design ( <ref type="figure" target="#fig_1">Fig. 2(a)-(d)</ref>), which specifically consider the structural property in urban-scene data. In this manner, HANet derives attention values, C × H × 1, to gate activation at the horizontal section of a feature map output in the main networks, considering vertical position. HANet is significantly more lightweight than selfattention-based approaches that considers the relationship between every pair in each dimension. Meanwhile, channelwise attention approaches such as SENet <ref type="bibr" target="#b18">[19]</ref> generates attention values, C × 1 × 1, only at an image level. This is not ideal for urban-scene segmentation as most of the images shares similar circumstances and context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first describe the implementation details of HANet. Then, we experimentally demonstrate the effectiveness and wide applicability of our proposed methods by extensive quantitative analysis including ablation studies. We evaluate HANet on two different urban-scene datasets including Cityscapes <ref type="bibr" target="#b11">[12]</ref> and BDD100K <ref type="bibr" target="#b44">[45]</ref>. Furthermore, we visualize and analyze the attention map generated from HANet. For all the quantitative experiments, we measure the segmentation performance in terms of mean Intersection over Union (mIoU) metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Base segmentation architecture Our network architecture for semantic segmentation is based on DeepLabv3+ <ref type="bibr" target="#b7">[8]</ref>. We adopt various backbone networks including Shuf-fleNetV2 <ref type="bibr" target="#b28">[29]</ref>, MobileNetV2 <ref type="bibr" target="#b36">[37]</ref>, and ResNet <ref type="bibr" target="#b17">[18]</ref> to verify wide applicability of HANet. Note that HANet can be easily inserted on top of various backbone networks. The adopted backbone networks are pretrained on Ima-geNet <ref type="bibr" target="#b35">[36]</ref>   Norm (batch statistics synchronized across multiple GPUs) publicly included in PyTorch v1.1 and by replacing a single 7×7 convolution by three 3×3 convolutions in the first layer of ResNet-101. we also adopted an auxiliary crossentropy loss in the intermediate feature map and class uniform sampling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53]</ref> to handle class imbalance problems. As a result, our baseline achieves the mIoU of 79.25% on the Cityscapes validation set, which surpasses the other baseline models based on DeepLabv3+ architecture with ResNet-101 of previous work. Training protocol. We employ SGD optimizer with initial learning rate of 1e-2 and momentum of 0.9. The weight decays are 5e-4 and 1e-4 for main networks and HANet, respectively. The learning rate scheduling follows the polynomial learning rate policy <ref type="bibr" target="#b26">[27]</ref>. The initial learning rate is multiplied by 1− iteration max iteration power , where power is 0.9.</p><p>To avoid overfitting, typical data augmentations in semantic image segmentation models are used, including random horizontally flipping, random scaling in the range of [0.5,2], gaussian blur, color jittering, and random cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cityscapes</head><p>The Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref> is a large-scale urban-scene dataset, holding high-quality pixel-level annotations of 5K images and 20K coarsely annotated images. Finely annotated images consist of 2,975 train images, 500 validation images, and 1,525 test images. The annotations of test images are withheld for benchmarks. The resolution of each image is 2048×1024, and 19 semantic labels are defined. In all the experiments on Cityscapes validation set, we train our models using finely annotated training set for 40K iterations with a total batch size of 8 and a crop size of 768×768.   <ref type="bibr" target="#b28">[29]</ref>, MobileNetV2 <ref type="bibr" target="#b36">[37]</ref>, ResNet-50, and ResNet-101 <ref type="bibr" target="#b17">[18]</ref>. Models with HANet consistently outperform baseline models with significant increases on MobileNetV2 and ResNet-101. Moreover, the model parameters and complexity results indicate that the cost of adding HANet is practically negligible. From <ref type="figure" target="#fig_3">Fig. 4</ref>, we can see clearly that adding HANet (blue arrow) is worth more than it costs in FLOPs, compared to improving the model through changing output stride (red arrow). Therefore, HANet has a great advantage of not only an effective way of improving semantic segmentation accuracy, but also lightweight algorithm design for practical usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation studies</head><p>For ablation studies, we use ResNet-101 backbone with output stride of 16 and evaluate on the Cityscapes validation set. <ref type="table" target="#tab_6">Table 3</ref> shows the performance gain when HANet is added to the multiple layers and incorporates the positional encodings. Additionally, we conduct experiments by changing channel reduction ratio r and pooling method. When we add HANet including positional encodings at multiple layers from L1 to L4 <ref type="figure" target="#fig_2">(Fig. 3</ref>) and the reduction ratio is set to 32, the mIoU significantly increases from 77.80 to 79.31. To compare with CoordConv <ref type="bibr" target="#b25">[26]</ref>, we conduct experiments by replacing standard convolutional layers after the backbone with CoordConv.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Efficacy at segmented regions</head><p>As mentioned in Section 1, the average entropy decreases as we divide the image into multiple horizontal subsections. This implies the performance improvement in the entire region of an image. Besides, the entropy of the upper and lower regions have low entropy compared to the middle region. In this respect, we expect the performance increase arising from HANet would be larger in the upper and lower regions than that in the middle or entire region. Indeed, the performance significantly rises on the upper and lower regions as in <ref type="table" target="#tab_10">Table 5</ref>.   <ref type="bibr" target="#b40">[41]</ref>. We compare our best models based on ResNet-101 and ResNext-101 with other recent models on the Cityscapes test set ( <ref type="table" target="#tab_12">Table 6</ref>). Our models achieve a new state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">BDD100K</head><p>Berkeley Deep Drive dataset (BDD100K) <ref type="bibr" target="#b44">[45]</ref> is a largescale diverse driving video database. It includes a semantic image segmentation dataset, consisting of 7,000 training and 1,000 validation images with a resolution of 1280×720. It is a challenging dataset including images of various driving circumstance such as day, night, and diverse weather conditions. <ref type="table">Table 7</ref> shows the superior results of HANet in BDD100K. The training strategy of BDD100K is similar to Cityscapes, but we change the crop size into 608×608 and train for 60K iterations with a total batch size of 16. These adjustments simply comes from smaller image size and larger dataset size compared to Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Analysis</head><p>Attention map visualization. We visualize the attention weights to analyze the behavior of the proposed HANet. The attention visualization highlights those channels emphasized at a different vertical position. Through the visualization, we can find out interesting clues to validate our   <ref type="table">Table 7</ref>: Comparison of mIoU between the baseline and HANet on BDD100K validation set. The output stride is set to 16 and the crop size is 608×608.</p><p>observations and methods. <ref type="figure" target="#fig_4">Fig. 5</ref> clearly shows that HANet assigns a different amount of attention to a different vertical position, indicating that the model properly learns structural priors with respect to the height in urban-scene data.  <ref type="figure" target="#fig_5">Fig. 6</ref> visualizes the attention map from HANet L2, which computes the attention weights for ASPP layer. In ASPP layer, the channels are obtained from convolution filters that have multi-scale receptive fields and grouped for each scale. Therefore, we can interpret the HANet L2 attention map by each group, with the sequence of channels remaining unchanged. Colored boxes give us insights of our method and the urban-scene images. The green boxes in <ref type="figure" target="#fig_5">Fig. 6</ref> shows the low-focused channels of the middle region of images, while blue box indicates the channels which are relatively more focused. That is, the channels obtained from the small receptive field are weighted in the middle region. Since the middle region is where small objects are crowded as pointed in the left figure in <ref type="figure" target="#fig_5">Fig. 6</ref>, small receptive fields are effective for this region and vice versa. In this manner, we verify that HANet properly learns and captures the intrinsic features of urban scene. <ref type="figure" target="#fig_6">Fig. 7</ref> illustrates that the distribution of the attention map (right figure) from HANet at the last layer, which is following the actual height-wise class distribution (left figure) obtained from the Cityscapes training images. Each class gives a different weight according to the vertical position, meaning that the model actually uses vertical positions in the image. This information corresponds to the observation we introduced through the class distribution analysis in <ref type="figure">Fig. 1</ref> and <ref type="table" target="#tab_1">Table 1</ref>. For instance, road (class 0) appears only in the middle and the lower regions, while sky (class 10) is mainly emphasized in the upper rows.</p><p>Visualization of semantic segmentation results are shown in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed HANet for urban-scene segmentation as an effective and wide applicable add-on module. Our method exploits the spatial priors existing in urban-scene images to construct a cost-effective architecture. We demonstrated the performance increase by adding our method to the baseline model with negligible cost increase. Moreover, we visualized and analyzed the attention maps to show the validity of our initial hypothesis that exploiting vertical positional information helps for urbanscene semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>This section complements our paper by presenting additional information, experimental results and visualizations. First, we provide further comparison results with other state-of-the-arts in Section A.1. In Section A.2, we conduct experiments to find the best way to incorporate positional information. We then describe the architecture details of the baseline and HANet in Section A.3. In Section A.4, we compare height-wise and width-wise class distributions. Finally, we conduct quantitative and qualitative comparisons between ours and the baseline model in Section A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Additional comparisons with other models</head><p>We compare the best performance of our model with other state-of-the-arts on the Cityscapes validation set.  A.2. Positional encoding and embedding.</p><p>In the NLP domains, there exist different approaches to inject positional information of each token in the input sequence. Positional encoding using sinusoidal values <ref type="bibr" target="#b38">[39]</ref> and learned positional embeddings <ref type="bibr" target="#b16">[17]</ref> have been shown to produce comparable performances <ref type="bibr" target="#b38">[39]</ref>. We conduct experiments to find the best way to incorporate positional information. It turns out that the best way is to put sinusoidal positional encoding into the second convolutional layer of HANet (  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Further implementation details</head><p>We implement our methods based on the opensource implementations of NVIDIA semantic segmentation  model <ref type="bibr" target="#b52">[53]</ref>. HANet consists of three convolutional layers incorporating dropout and batch normalization. To extract the height-wise contextual information from each row, we empirically adopt average pooling.</p><p>HANet architecture <ref type="figure" target="#fig_8">Fig. 8</ref> shows detailed architecture of HANet. Width-wise pooling and interpolation for coarse attention are implemented using two-dimensional adaptive average pooling operation 2 in Pytorch. Afterwards, a dropout layer and three one-dimensional convolutional layers are applied. Blue values in <ref type="figure" target="#fig_1">Fig. 8, 16 and 32</ref>, are respectively the height of coarse attention and the channel reduction ratio r, which are our hyper-parameters. All the hyperparameters can be found in our code.</p><p>Baseline architecture <ref type="figure">Fig. 9</ref> shows detailed architecture of the baseline model, which is based on DeepLabv3+. As an encoder-decoder architecture, low-level features obtained from ResNet stage 1 are concatenated to high-level features via skip-connection. An auxiliary loss proposed in PSPNet <ref type="bibr" target="#b21">[22]</ref> is applied to facilitate the learning process of deep networks. To adopt the auxiliary loss, additional convolutional layers are added after ResNet stage 3 as an auxiliary branch. The loss for this auxiliary branch has a weight of 0.4. The output stride is set to 8 as shown in blue color; this can be set differently, e.g., <ref type="bibr" target="#b15">16</ref>.  2D Dropout ( =0.1) <ref type="figure">Figure 9</ref>: Detailed architecture of the baseline model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Height-and width-wise class distribution</head><p>As shown in <ref type="figure" target="#fig_9">Fig. 10</ref>, the width-wise class distributions are relatively similar across columns than the height-wise ones are, so it would be relatively difficult to extract distinct information with respect to the horizontal position of an image. Also, empirically, no meaningful performance increase has been observed when using the attention networks exploiting a width-wise class distribution.</p><p>This clear pattern corroborates the rationale behind the idea of HANet that extracts and incorporates height-wise contextual information rather than the width-wise one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Per-class IoU and segmentation maps</head><p>We present per-class IoU and segmentation maps to analyze HANet qualitatively and quantitatively.</p><p>Comparison to baseline. <ref type="table" target="#tab_1">Table 10</ref> shows the per-class IoU and mIoU results to compare the baseline and our methods in detail. Compared to the baseline, all the classes show similar or improved results; up to 7.3% IoU increase is observed. Qualitatively, ours can properly distinguish individual objects, even between the classes much alike to each other ( <ref type="figure" target="#fig_1">Figs. 11 and 12</ref>). From our result in <ref type="figure" target="#fig_10">Fig. 11(a)</ref> and <ref type="figure" target="#fig_1">Fig. 12(a)</ref>-(c), one can see that the train, trucks, or cars in a far, crowded region are properly predicted by ours, even if similar vehicles are found nearby. Also, vegetation is accurately distinguished from terrain in ours, compared to the baseline ( <ref type="figure" target="#fig_10">Fig. 11(b)</ref>). Another interesting examples are found in <ref type="figure" target="#fig_1">Fig. 12</ref>(e)-(f); the poles are connected fully in ours but dotted or missed in the baseline. We conjecture that HANet helps to distinguish confusing classes by properly gating the activation maps using height-wise contextual information based on their vertical positions. To summarize, compared to the baseline, our method generally forms a clear boundary of a object while avoiding its unnecessary fragmentation into multiple pieces.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2003.05128v3 [cs.CV] 7 Apr 2020 upper middle lower image Average number of pixels assigned to each class in single image Average number of pixels assigned to each class in each of upper, middle, and lower part</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of our proposed HANet. Each operation op is notated as Gop, and feature maps are in bold-X : lower-level feature map, Z: width-wise pooled X ,Ẑ: down-sampled Z, Q n : n-th intermediate feature map of 1D convolution layers,Â: down-sampled attention map, A: final attention map, X h : higher-level feature map,X h : transformed new feature map. Details can be found in Section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Semantic segmentation networks incorporating HANet in five different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of the performance and complexity among the baseline and HANet on the various backbone networks. x-axis denotes teraFLOPs and y-axis denotes mIoU. The circle size denotes the number of model parameters. The texts in the colored circle indicate backbone networks, output stride, and whether HANet is adopted to the baseline. S, M, R50, and R101 denote Shuf-fleNetV2, MobileNetV2, ResNet-50, and -101, respectively. (e.g., S-16: Baseline, ShuffleNetV2, and output stride 16)4.2.1 Effectiveness of the HANet components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of attention map from HANet at different layers. x-axis denotes the channels, and y-axis denotes the height of the feature map, showing which channels are weighted at different vertical position. The channels focused by each height are clearly different. To better visualize, the channels are clustered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>(left) The color in the heatmap denotes an average number of pixels that make up an object. Relatively small objects are crowded in the middle regions, while large objects exist in the lower region. (right) Visualization of attention map from HANet at the second layer.Unlike Fig. 5, the sequence of channels remains unchanged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Height-wise class distributions and attention map visualization (L5). The number ranging from 0 to 18 indicates a different class. The darker it is colored, the higher probability (more pixels) assigned to a particular class. The attention visualization follows the patterns in the real class distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Detailed architecture of HANet. p denotes the dropout probability, and K denotes the kernel size of each one-dimensional convolution layer. BN denotes a batch normalization layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of a height-wise and a width-wise class distributions. A darker color indicates a higher probability (more pixels) assigned to a particular class (from 0 to 18). The heightwise class distributions show distinct patterns across vertical positions while it is not the case for width-wise ones. Normalized distributions of each class are presented on the right column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Comparison of predicted segmentation maps: (a) truck, bus, and car. (b) vegetation and terrain. (c) motorcycle and bicycle. (d) fence and vegetation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Comparison of predicted segmentation maps: (a) truck and car. (b) car and truck. (c) bus and car. (d) building and fence. (e) sky and building; pole and building. (f) pole and building.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison of the probability distributions (%) of pixels being assigned to each class when an entire image is separated on upper, middle, and lower regions of the Cityscapes dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>for all the experiments, unless otherwise noted. Stronger baseline To strictly verify the effectiveness of HANet, we improved the performance of the DeepLabv3+ baseline adopting ResNet-101, by integrating SyncBatch-</figDesc><table><row><cell>Backbone</cell><cell cols="4">OS Models Params GFLOPs mIoU(%)</cell></row><row><cell>ShuffleNet</cell><cell>32</cell><cell>Baseline 12.6M +HANet 14.9M</cell><cell>64.34 64.39</cell><cell>70.27 71.30</cell></row><row><cell>V2 (1×) [29]</cell><cell>16</cell><cell>Baseline 12.6M +HANet 13.7M</cell><cell>117.09 117.14</cell><cell>70.85 71.52</cell></row><row><cell>MobileNet</cell><cell>16</cell><cell>Baseline 14.8M +HANet 16.1M</cell><cell>142.74 142.80</cell><cell>73.93 74.96</cell></row><row><cell>V2 [29]</cell><cell>8</cell><cell>Baseline 14.8M +HANet 15.4M</cell><cell>428.70 428.82</cell><cell>73.40 74.70</cell></row><row><cell>ResNet-50</cell><cell>16</cell><cell>Baseline 45.1M +HANet 47.6M</cell><cell>553.74 553.85</cell><cell>76.84 77.78</cell></row><row><cell>[18]</cell><cell>8</cell><cell cols="2">Baseline 45.1M 1460.56 +HANet 46.3M 1460.76</cell><cell>77.76 78.71</cell></row><row><cell>ResNet-101</cell><cell>16</cell><cell>Baseline 64.2M +HANet 65.4M</cell><cell>765.53 765.63</cell><cell>77.80 79.31</cell></row><row><cell>[18]</cell><cell>8</cell><cell cols="2">Baseline 64.2M 2137.82 +HANet 65.4M 2138.02</cell><cell>79.25 80.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison of mIoU, the number of model parameters and FLOPs between the baseline and HANet on Cityscapes valida- tion set according to various backbone networks and output stride. Adding HANet to the baseline consistently increase the mIoU with minimal cost increase.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table /><note>the effect of adopting HANet through per- formance increase (mIoU) according to the number of pa- rameters and FLOPs which indicate model size and com- plexity, respectively. To demonstrate the wide applicability of HANet, various backbones are examined including Shuf- fleNetV2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>shows that HANet outperforms the baseline with CoordConv.</figDesc><table><row><cell>1</cell><cell>2</cell><cell>Layers 3</cell><cell>4</cell><cell>5</cell><cell>Positional encoding Ratio r</cell><cell>Pooling method</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell cols="2">average 78.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell cols="2">average 78.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell cols="2">average 78.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell cols="2">average 79.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell cols="2">average 78.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell cols="2">average 79.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell cols="2">average 79.08</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell cols="2">average 78.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell>max</cell><cell>78.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Baseline</cell><cell></cell><cell>77.80</cell></row><row><cell></cell><cell cols="6">Baseline + CoordConv [26] (Height + Width)</cell><cell>78.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies and hyper-parameter impacts with regard to the HANet injected layers, using positional encodings or not, and channel reduction ratio. ResNet-101, output stride 16 on Cityscapes validation set.</figDesc><table><row><cell>4.2.3 Inference techniques.</cell><cell></cell><cell></cell></row><row><cell cols="3">For further performance improvements, we adopt fre-</cell></row><row><cell cols="3">quently used techniques such as left-right flipped, multi-</cell></row><row><cell cols="3">scale (with scales={0.5, 1.0, 2.0}) and sliding inference. In</cell></row><row><cell cols="3">such manner, our best model achieves 82.05% mIoU on the</cell></row><row><cell cols="2">Cityscapes validation set as in Table 4.</cell><cell></cell></row><row><cell>Inference techniques</cell><cell>Baseline</cell><cell>+HANet</cell></row><row><cell>None</cell><cell>79.25</cell><cell>80.29</cell></row><row><cell>Multiscale, Sliding, and Flipping</cell><cell>81.14</cell><cell>82.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>mIoU(%) comparison with respect to inference tech- niques. ResNet-101, output stride 8 on Cityscapes validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>mIoU(%) comparison to baseline on each part of image divided into four horizontal sections. ResNet-101, output stride 8 on Cityscapes validation set.</figDesc><table><row><cell>4.2.5 Comparison to other state-of-the-art models</cell></row><row><cell>To compare with other state-of-the-arts, we train our mod-</cell></row><row><cell>els using finely annotated training and validation set for</cell></row><row><cell>90K iterations. In case of adopting ResNext-101 [42] back-</cell></row><row><cell>bone, coarsely annotated images are additionally used and</cell></row><row><cell>the model is pretrained on Mapillary [32]. The crop size and</cell></row><row><cell>batch size are changed into 864×864 and 12, respectively.</cell></row><row><cell>The inference techniques from Section 4.2.3 are used, but</cell></row><row><cell>we do not adopt any other techniques such as online boot-</cell></row><row><cell>strapping for hard training pixels</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Backbone</cell><cell>OS</cell><cell>Baseline</cell><cell>+HANet</cell></row><row><cell>MobileNetV2</cell><cell>16</cell><cell>58.91%</cell><cell>60.05%</cell></row><row><cell>ResNet-101</cell><cell>16</cell><cell>63.75%</cell><cell>64.56%</cell></row><row><cell>ResNet-101</cell><cell>8</cell><cell>64.84%</cell><cell>65.60%</cell></row></table><note>Comparison of mIoU and per-class IoU with other state-of-the-art models on Cityscapes test set.† denotes training including Cityscapes coarsely annotated images.‡ denotes training with Mapillary pretrained model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Comparisons against the best performances reported in the published papers of other state-of-the-art models on the Cityscapes validation set. The models based on ResNet-101 are compared.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9</head><label>9</label><figDesc>).</figDesc><table><row><cell></cell><cell cols="2">Injected layer</cell></row><row><cell>Methods</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1st</cell><cell>2nd</cell></row><row><cell>Sinusoidal encoding</cell><cell>79.61%</cell><cell>80.29%</cell></row><row><cell>Learnable embedding (from scratch)</cell><cell cols="2">79.95% 79.60%</cell></row><row><cell>Learned embedding (from pretrained)</cell><cell cols="2">79.61% 79.30%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Performances comparison with respect to the layers and methods of positional encoding. Note that HANet consists of three convolutional layers. ResNet-101, output stride 8 on Cityscapes validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Model mIoU road swalk build. wall fence pole tligh. tsign veg terr. sky pers. rider car truck bus train mcyc bcyc Baseline 81.14 98.5 87.3 93.6 66.1 64.4 68.7 74.0 82.0 93.2 65.6 95.2 84.3 66.0 95.7 80.6 92.8 85.0 68.9 80.0 +HANet 82.05 98.6 87.7 93.7 66.7 66.2 68.7 74.4 81.9 93.3 67.7 95.3 84.5 66.9 96.1 87.9 92.7 86.0 70.7 80.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Performance comparison of our methods against the baseline in terms of per-class IoU and mIoU measures. Inference techniques such as sliding, multi-scale, and flipping are applied. ResNet-101, output stride 8 on the Cityscapes validation set.</figDesc><table><row><cell></cell><cell>Input Conv.</cell><cell></cell></row><row><cell></cell><cell>ResNet stage 1</cell><cell></cell></row><row><cell>Low-level features</cell><cell>ResNet stage 2</cell><cell></cell></row><row><cell></cell><cell>ResNet stage 3</cell><cell></cell></row><row><cell>Conv., BN, ReLU</cell><cell>ResNet stage 4</cell><cell>Conv., BN, ReLU</cell></row><row><cell></cell><cell>ASPP</cell><cell></cell></row><row><cell></cell><cell>Conv., BN, ReLU</cell><cell>Conv.</cell></row><row><cell></cell><cell>Conv., BN, ReLU</cell><cell>Cross entropy loss</cell></row><row><cell></cell><cell>Conv., BN, ReLU</cell><cell></cell></row><row><cell></cell><cell>Conv.</cell><cell></cell></row><row><cell></cell><cell>Cross entropy loss</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/shachoi/HANet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pytorch.org/docs/stable/nn.html#torch. nn.AdaptiveMaxPool2d</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Highfor-low and low-for-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4545" to="4554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Magnenat Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.1" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Foveanet: Perspective-aware urban scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<editor>CoRR. Citeseer</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient segmentation: Learning downsampling near semantic boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyam</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2131" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Context-reinforced semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4046" to="4055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
