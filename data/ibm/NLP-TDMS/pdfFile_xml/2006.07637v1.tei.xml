<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SE-MELGAN -SPEAKER AGNOSTIC RAPID SPEECH ENHANCEMENT A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-16">June 16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka</forename><surname>Chkhetiani</surname></persName>
							<email>lchkhetiani@systemcorp.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levan</forename><surname>Bezhanidze</surname></persName>
							<email>lbezhanidze@systemcorp.ai</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Machine Intelligence SYSTEM CORP. Tbilisi</orgName>
								<address>
									<postCode>0160</postCode>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Machine Intelligence SYSTEM CORP. Tbilisi</orgName>
								<address>
									<postCode>0160</postCode>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SE-MELGAN -SPEAKER AGNOSTIC RAPID SPEECH ENHANCEMENT A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-16">June 16, 2020</date>
						</imprint>
					</monogr>
					<note>*The author occupies undergraduate studies for now</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>MelGAN · Multi-Band Melgan · GAN · SEGAN</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancement in Generative Adversarial Networks in speech synthesis domain <ref type="bibr" target="#b2">[3]</ref>, [2] have shown, that it's possible to train GANs [8] in a reliable manner for high quality coherent waveform generation from mel-spectograms. We propose that it is possible to transfer the MelGAN's [3] robustness in learning speech features to speech enhancement and noise reduction domain without any model modification tasks. Our proposed method generalizes over multi-speaker speech dataset and is able to robustly handle unseen background noises during the inference. Also, we show that by increasing the batch size for this particular approach not only yields better speech results, but generalizes over multi-speaker dataset easily and leads to faster convergence. Additionally, it outperforms previous state of the art GAN approach for speech enhancement SEGAN [5] in two domains: 1. quality ; 2. speed. Proposed method runs at more than 100x faster than realtime on GPU and more than 2x faster than real time on CPU without any hardware optimization tasks, right at the speed of MelGAN [3].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advancement of deep learning have pushed the frontiers of many fields. Performance and speed of speech synthesis methods have been increasing rapidly for the last few years. The revolutionary autoregressive WaveNet <ref type="bibr" target="#b3">[4]</ref> have been replaced with non-autoregressive, fully-parallel Parallel WaveNet <ref type="bibr" target="#b10">[11]</ref>, trained along with WaveNet <ref type="bibr" target="#b3">[4]</ref> with Probability Density Distillation objective for realtime speech synthesis. And lately proposed methods MelGAN <ref type="bibr" target="#b2">[3]</ref> and Multi-Band Melgan <ref type="bibr" target="#b1">[2]</ref> show that it is possible to handle mel-spectogram to raw waveform conversion at insanely high speeds, with comparable quality to previous state of the art methods.</p><p>After seeing the newest advancements in the speech synthesis domain, we came up with a logical question: Why cannot we use the robustness of speech vocoders in learning the speech features to handle noise reduction and speech enhancement problems? From one point of view, if a network is able to catch important features in mel-spectograms for high quality waveform generation -we can initially assume that they should be able to differentiate important speech features from noise in case they are properly trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head><p>Some of the most distinguished researches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref> have shown different methodologies for main speaker isolation from background or parallel noise. Face landmark-based speech enhancement technique proposed in <ref type="bibr" target="#b9">[10]</ref> uses arXiv:2006.07637v1 [eess.AS] 13 Jun 2020 independent face landmark extractor <ref type="bibr" target="#b11">[12]</ref> model output as an input for LSTM-based networks to generate time-frequency masks, which later on are applied to mixed-speech spectrogram. However, for real-life situations there is rarely a case when we have main and background speech inputs on top of their frontal face video streams.</p><p>Additionally, DLIB <ref type="bibr" target="#b11">[12]</ref> framework is perfect for research purposes, but it suffers from landmark-lagging in real-time inference, along with the problem of ground-truth coordinates mismatch. And, there is no room for non-human background noise removal for this time.</p><p>However, VoiceFilter <ref type="bibr" target="#b10">[11]</ref> uses targeted speaker's reference audio as a reference vector. Nevertheless the results are promising and in fact, the network significantly reduces Word Error Rate on noisy validation data in speech recognition setting, the inference time hardly reaches real-time factor of 0.5x on expensive NVIDIA Tesla V100 GPU.</p><p>On the other hand, SEGAN <ref type="bibr" target="#b4">[5]</ref> promisingly laverages the background or parallel noise removal issue, but suffers from prominent speed problem, and is not able to nearly reach real-time factor even on expensive hardware.</p><p>Our aim is to find equilibrium, a network that is able to learn important speech features for isolation, and satisfy the need of excellent real-time inference factor on cheap hardware. The network should be lightweight and able to perform computations even locally with constrained memory budget in case the solution is served in a distributed manner, without additional hardware optimization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>One of the latest most exciting researches in mel-specrogram to raw audio conversion domain is non-autoregressive, fully feed-forward convolutional neural network MelGAN <ref type="bibr" target="#b2">[3]</ref>. It is important to note, that the network consists of 4.26 million parameters, which is almost 21x less than the fastest available vocoder on the market WaveGlow <ref type="bibr" target="#b12">[13]</ref>. Also a notable performance metric behind the <ref type="bibr" target="#b2">[3]</ref> network is that while satisfying the inference speed and lightness requirement, the results are comparable to other heavy-weight networks with 3.09 MOS (Mean Opinion Score). In our approach for speech enhancement we use the original version of proposed MelGAN architecture in <ref type="bibr" target="#b2">[3]</ref>, but treat the training process as optimization of decompressing the reasonable speech features from mel-spectogram, rather than conversion of mel to raw audio in a straightforward manner.</p><p>Our assumption is that as long as MelGAN <ref type="bibr" target="#b2">[3]</ref> is almost perfectly able to detect and learn speech features in its receptive field, we should orient on training the model specifically for speech enhancement task, rather than start searching for perfect network modifications such as: increasing the receptive field, adding residual blocks for searching deeper features, etc. We use L2 loss as described in official paper, decrease the LeakyRelu activation slope from 0.2 to 0.01 and restrict the mel max-frequency to 8000hz. As long as most of the vocoders are able to learn speech features in restricted 0-8kHz frequency area, we assume that by increasing it model will end up tackling with more unnecessary noisy features that will lead to not-generalization and quality loss.</p><p>We keep mel channels to 80, increase segment length to 16K. Filter, hop and window lengths are 1024, 256 and 1024 respectively, whereas sampling rate is set to 22050.</p><p>Additionally, it's important to keep the input audio preprocessing in a discrete time, for which STFT (Short-Time Fourier Transform) is used. By using convolutional STFT with 1D convolutions, the preprocessing time could be further reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>For synthetic dataset generation we use LibriSpeech's <ref type="bibr" target="#b8">[9]</ref> 360 hours version of multi-speaker speech data, along with MS-SNSD <ref type="bibr" target="#b5">[6]</ref> and ESC-50 <ref type="bibr" target="#b6">[7]</ref> noises dataset.</p><p>Objective for data generation is to increase the original data by 4x. Firstly, we randomly select clean speech audio sample from LibriSpeech <ref type="bibr" target="#b8">[9]</ref> dataset. Secondly, we randomly select noise audio sample from combination of above-mentioned noisy datasets, and try to align them with clean speech. In case the aligned noise is shorter than speech, that happens most of the time, we cut most noisy parts iteratively and concatenate them so the final noise is exactly the same length as clean speech. The same is done in case the noise sample is longer than clean speech sample. Finally, we stack them together and perform short time fourier transformation with 1D Convolutions. After tweaking the sample generated outputs, we have seen that most of the noisy speeches were non-audible. Thus we decrease the noisy sample volume by 30 percent, so that the final combination has audible speech in it.</p><p>Additionally, half of the clean dataset is used without augmentation. We assume that having a part of clean speech in training data will help model focus on important features of speech and easily converge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ESC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>The training procedure is done on a server with two NVIDIA Tesla V100 GPUs with total of 32GB of VRAM. High-VRAM support gives us ability to handle bigger batch sizes, that as mentioned above, leads the model to converge faster and generalize well. We use batch size of 128 for training. As an optimizer, Adam with betas 0.5, 0.9 is used.</p><p>Initially, we train the network for 1.5 Million steps with learning rate 1e-4, and decrease it to 1e-5 for further training for total of 3 Million steps.</p><p>During experimentation, we found out that decreasing learning rate in the middle of training also helps the network to perform better and generalize on unseen noise samples and speakers well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Output samples during manual validation process were chosen randomly. In the noise-mixed samples we found several ones with multi or single speaker background voice over clean speech sample with noticeably high volume. In contrary with our initial idea, we saw that the network was able to not only differentiate speaker from random background noises but it started to choose main speaker over high-volume background speech and perfectly isolate it. In harsh terms, network generalized over reasonable speech features to isolate them, rather than on noises to take them away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a novel approach for training MelGAN <ref type="bibr" target="#b2">[3]</ref> for speech enhancement domain by using almost exactly the same network architecture as proposed in <ref type="bibr" target="#b2">[3]</ref>. Results show, that nevertheless the network is designed specifically for raw waveform synthesis task, by carefully adopting it to speech enhancement issue we can yield comparable results to other state-of-the-art approaches while maintaining insanely fast inference speed. Perhaps there's a much bigger room for adoption of SOTA networks to other domains than it was previously thought.</p><p>The implementation for non-commercial use will be available at: https://github.com/systemcorp-ai/SE-MelGAN shortly.</p><p>For audio samples, please visit: https://systemcorp-ai.github.io</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Proposed MelGAN architecture in<ref type="bibr" target="#b2">[3]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>-50<ref type="bibr" target="#b6">[7]</ref> environmental noises dataset is as in the following table. Additionally, airport announcement, vacuum cleaner and neighbor speaking noise samples are used from MS-SNSD<ref type="bibr" target="#b5">[6]</ref> dataset.</figDesc><table><row><cell cols="5">Animals Natural soundscapes, water sounds Human, non-speech sounds Interior/domestic sounds Exterior/urban noises</cell></row><row><cell>Dog</cell><cell>Rain</cell><cell>Crying baby</cell><cell>Door knock</cell><cell>Helicopter</cell></row><row><cell>Rooster</cell><cell>Sea waves</cell><cell>Sneezing</cell><cell>Mouse click</cell><cell>Chainsaw</cell></row><row><cell>Pig</cell><cell>Crackling fire</cell><cell>Clapping</cell><cell>Keyboard typing</cell><cell>Siren</cell></row><row><cell>Cow</cell><cell>Crickets</cell><cell>Breathing</cell><cell>Door, wood creaks</cell><cell>Car horn</cell></row><row><cell>Frog</cell><cell>Chirping birds</cell><cell>Coughing</cell><cell>Can opening</cell><cell>Engine</cell></row><row><cell>Cat</cell><cell>Water drops</cell><cell>Footsteps</cell><cell>Washing machine</cell><cell>Train</cell></row><row><cell>Hen</cell><cell>Wind</cell><cell>Laughing</cell><cell>Vacuum cleaner</cell><cell>Church bells</cell></row><row><cell>Insects</cell><cell>Pouring water</cell><cell>Brushing teeth</cell><cell>Clock alarm</cell><cell>Airplane</cell></row><row><cell>Sheep</cell><cell>Toilet flush</cell><cell>Snoring</cell><cell>Clock tick</cell><cell>Fireworks</cell></row><row><cell>Crow</cell><cell>Thunderstorm</cell><cell>Drinking, sipping</cell><cell>Glass breaking</cell><cell>Hand saw</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>The authors would like to thank SYSTEM CORP. for funding and Seung-Won Park for amazing insights in their MelGAN <ref type="bibr" target="#b2">[3]</ref> implementation repository, that gave us a very valuable help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov</pubPlace>
		</imprint>
	</monogr>
	<note>and Demis Hassabis. Parallel wavenet: Fast high-fidelity speech synthesis. CoRR, abs/1711.10433</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-band melgan: Faster waveform generation for high-quality text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Thibault De Boissiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Zhen</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>De Brebisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melgan</surname></persName>
		</author>
		<title level="m">Generative adversarial networks for conditional waveform synthesis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">SEGAN: speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
		</author>
		<idno>abs/1703.09452</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A scalable noisy speech dataset and online subjective test framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1816" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Librispeech: An asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Face landmark-based speaker-independent audio-visual speech enhancement in multi-talker environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Pasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Tikhanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Bergamaschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Fadiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Badino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tacotron: Towards end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
