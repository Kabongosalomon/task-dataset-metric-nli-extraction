<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A DYNAMIC REDUCTION NETWORK FOR POINT CLOUDS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-19">March 19, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsey</forename><surname>Gray</surname></persName>
							<email>lagray@fnal.gov</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Fermi National Accelerator Laboratory Batavia</orgName>
								<address>
									<postCode>60510</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Klijnsma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Fermi National Accelerator Laboratory Batavia</orgName>
								<address>
									<postCode>60510</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamik</forename><surname>Ghosh</surname></persName>
							<email>shamik.ghosh@cern.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Saha Institute of Nuclear Physics Kolkata</orgName>
								<address>
									<region>West Bengal</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A DYNAMIC REDUCTION NETWORK FOR POINT CLOUDS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-19">March 19, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Machine Learning · Geometric Deep Learning · Graph Clustering · Classification · Regression · High Energy Physics</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classifying whole images is a classic problem in machine learning, and graph neural networks are a powerful methodology to learn highly irregular geometries. It is often the case that certain parts of a point cloud are more important than others when determining overall classification. On graph structures this started by pooling information at the end of convolutional filters, and has evolved to a variety of staged pooling techniques on static graphs. In this paper, a dynamic graph formulation of pooling is introduced that removes the need for predetermined graph structure. It achieves this by dynamically learning the most important relationships between data via an intermediate clustering.</p><p>The network architecture yields interesting results considering representation size and efficiency. It also adapts easily to a large number of tasks from image classification to energy regression in high energy particle physics. 12</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pooling has been an essential component of modern machine learning, allowing pertinent local information to be propagated to global intermediate feature sets or final discriminators. The shape of the pooling operation is typically determined by hand, setting the size of a convolutional filter and the number of pooling steps before an output layer. This process is difficult to optimize for graph neural networks <ref type="bibr" target="#b0">[1]</ref>, since neighbourhoods of nodes may vary in size and meaning depending on the problem at hand. In the area of message passing neural networks <ref type="bibr" target="#b1">[2]</ref> there are recent advancements in learned pooling techniques on graphs <ref type="bibr" target="#b2">[3]</ref>, and there is small but steady progress on using pooling to alter input graph structures. This text describes a new pooling architecture using dynamic graph convolutions <ref type="bibr" target="#b3">[4]</ref> and clustering algorithms to learn an optimized representation and corresponding graph for pooling. The model used in the following text is implemented in Pytorch Geometric <ref type="bibr" target="#b4">[5]</ref> (PyG).</p><p>This architecture was derived in the context of hadron 3 energy regression in High Energy Physics (HEP), where graph neural networks are beginning to solve difficult clustering problems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> in novel ways. The objective of that problem is to determine the original energy of a particle incident upon a device called a "sampling calorimeter." Within the calorimeter, which is made of dense material like lead or steel interspersed with lighter material, incident particles above a threshold energy will produce pairs of particles by nuclear interaction, creating a "shower" of particles. At a number of fixed depths within the calorimeter, it records scintillation or ionization signals at fixed depths as a proxy for the number of produced particles. These estimates of multiplicity can be used to infer the originating particle's energy. The energy deposition patterns of hadrons are known to have a high degree of local fluctuations in particle multiplicity during the shower's evolution. This means that throughout the shower there are randomly located regions that require different treatment from more homogeneous ones, and so there exists an optimal dynamic cluster for each hadron shower's data to best estimate the energy.</p><p>A human-designed algorithm called "software compensation" <ref type="bibr" target="#b7">[8]</ref> has been developed to solve this problem as well. It is already based in the principle of reducing an objective function to generate learned weights to determine shower energies. However, there is a significant amount of specific tuning that needs to be done to make the algorithm function for varying detector designs, and its domain of applicability remains well within HEP. Using the technique described in this paper, the entire algorithm is now learned, rather than a specific part of a correction, and the algorithm can dynamically adapt to the topology of a given hadron shower. Using a machine learning algorithm for this task mitigates the need for manual specialization, and affords the possibility to investigate the applications of technique on rather different tasks from calorimetry and with more widely available datasets. Benchmarks for various estimation and classification tasks will be demonstrated in the more classic machine learning tasks.</p><p>The advantages of this dynamic reduction architecture are:</p><p>• Representation spaces with good performance are very small.</p><p>• No prior graph structure is necessary, and if one is provided it can be altered by the pooling layers, since the graph pooling structure is learned.</p><p>• Without a prior graph structure, data for training and inference need very little preprocessing beyond normalization and stacking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The architecture proposed here is similar in outcome to the techniques proposed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, but radically different in implementation. Our focus is on learning latent representations that optimize the pooling performance of an unsupervised clustering algorithm. In particular, previous works in graph learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> demonstrate that controlling the behavior of an unsupervised algorithm can help in learning concise representations quickly. However, aspects of the original structure of the data were kept and only messages to pass in that structure were generated. This text expands on both previous aspects by combining it with dynamic graph convolutions <ref type="bibr" target="#b3">[4]</ref> and controlling a clustering algorithm in the latent space to produce a dynamically learned optimized pooling. All clustering algorithms can be treated as an indexing of nodes, so any clustering algorithm can be used in the latent space to make this demonstration. The unsupervised clustering algorithm is treated as a black box, and the supervised task is to optimize its performance for the task at hand. Using a dynamic graph convolutional approach, it is possible to learn the parameters that maximize the impact of the clustering on reducing information for further processing, and hence this network is called a dynamic reduction network (DRN). We have chosen to use a readily available greedy popularity based clustering algorithm <ref type="bibr" target="#b11">[12]</ref> as an initial demonstrator. Other clustering algorithms will be tested and compared once their GPU implementations are made available in PyG in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic Reduction Network</head><p>The default model used for the MNIST superpixels <ref type="bibr" target="#b9">[10]</ref> classification task in this paper is composed as follows:</p><p>1. The input data are normalized by fixed values such that the input data largely occupy the range [0, 1], outliers are allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A multilayer perceptron (MLP) encoding the normalized input data to a latent space of dimension N is applied to all input nodes. The default depth is 3 layers, with an intermediate layer half the size of the final output.</p><p>3. The latent space data are processed by a dynamic graph convolution layer, i.e. neighbours are found in the latent space rather than the original representation. The internal messages are created using a MLP with three layers, starting from 2N, 1.5N, and outputing a message of width N. The update function can be summation, maximum, or average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>The resulting latent nearest neighbours graph is then weighted by distance and clustered using a greedy clustering algorithm, pooling the node features by taking the maximum of the clustered data.</p><p>5. The reduced latent data are passed through another dynamic convolutional filter, and clustered again with the same algorithm.</p><p>6. The results of the second learned pooling step are then globally max pooled and passed through an MLP decoder to produce the output logits.</p><p>This process is summarized in <ref type="figure" target="#fig_0">Figure 1</ref>. Alterations to this model used for various test are described later in the text. The depth of the MLPs in the various encoding, message passing, and decoding steps are parameterized. The number of nearest neighbours, k, is a hyper-parameter of the model and may require tuning to a given task, depending on the relational data that is needed to make a prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This model was tested on MNIST "superpixels" dataset with 75 superpixels 4 . The superpixels dataset is a downsampled and aggregated form of the full MNIST dataset. Previous graph models are shown in Refs. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, A result of a scan in hidden-dimension (the "width") of the network is shown in 2. During the training and evaluation both the original graph structure and all pixels that are not filled are dropped, the data are "zero-suppressed". Each superpixel has a pair of coordinates defining a centroid, and an intensity. The performance using a width of 20 channels with k = 4 is a factor of two better on 75 superpixels than the reference models in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and approaches the performance of models trained on full MNIST at a width of 256 channels. The data passed to the model consists only of the centroid x and y, and the gray-scale of that pixel. In <ref type="figure">Figure 2</ref> each DRN is trained for 400 epochs using an nVidia Tesla V100 with the AdamW optimizer <ref type="bibr" target="#b12">[13]</ref> using one-cycle cosine annealing (on the learning rate only) with a starting value of 0.001. The weight decay is held constant at a value of 0.001. Best performance is often achieved quite early, so further tuning of model training and optimization is possible, and volatile GPU usage is low. There is significant room for improving the training and evaluation time performance of this model.  <ref type="figure">Figure 2</ref>: Performance for a scan in dynamic reduction network width of the architecture described above on the MNIST "superpixels" dataset <ref type="bibr" target="#b9">[10]</ref> with 75 superpixels, and two established models. DRN&lt;N&gt; means a dynamic reduction network with hidden dimension N is used. Comparison to similar models on the superpixels dataset are made. All superpixel graph data has been dropped and the network is allowed to learn an entirely new representation based on the pixel data. For all DRN models, k = 4. The performance at a hidden dimension size of 20 is particularly interesting given the modest gains in performance from wider networks. '-' indicates unknown data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>The superpixels dataset is known to yield poor performance in CNN-based networks, and graph networks have been proposed in previous work to improve upon this. The performance demonstrated in <ref type="figure">Figure 2</ref> shows clearly that further improvements on very under-sampled data are achievable and this model sets new performance benchmarks on these datasets, especially in terms of model size.</p><p>This text introduces the Dynamic Reduction Network as a new tool in High Energy Particle physics for processing sampled data from a highly varying multi-dimensional image. This is accomplished by designing a network that can learn effective pooling strategies by manipulating an unsupervised algorithm in a high-dimensional latent space. To demonstrate the efficacy of this network, a performance benchmark on an undersampled MNIST dataset indicates that this new architecture outperforms previous graph based architectures, even for very small numbers of parameters. This outcome suggests a powerful new technique for approaching classification and regression problems in both computer vision and high energy physics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The basic workflow of the Dynamic Reduction Network, details given below.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This preprint will be updated with more studies on different open datasets.<ref type="bibr" target="#b1">2</ref> The code needed to reproduce the results in this text is included in the arxiv submission file.<ref type="bibr" target="#b2">3</ref> Particles that are bound together by the strong force.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This model was developed using private data of the CMS Collaboration which cannot be published here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This research was supported in part by the Office of Science, Office of High Energy Physics, of the US Department of Energy under Contract No. DE-AC02-07CH11359 through FNAL LDRD-2019-017.</p><p>Many thanks to Matthias Fey and Song Han for quick consultation on this model. Thanks as well to Nhan Tran and Salvatore Rappoccio for proofreading assistance.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Edge contraction pooling for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Diehl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning representations of irregular particle-detector geometry with distance-weighted graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Shah Rukh Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Kieseler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Iiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal C</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">608</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph neural networks for particle reconstruction in high energy physics detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsey</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Proceedings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Software compensation in particle flow reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Huong Lan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Sefkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal C</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<title level="m">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A GPU Algorithm for Greedy Graph Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><forename type="middle">H</forename><surname>Fagginger Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bisseling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="108" to="119" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
