<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inter-Region Affinity Distillation for Road Marking Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
							<email>liuchunxiao@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
							<email>twhui@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>3ccloy@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Inter-Region Affinity Distillation for Road Marking Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of distilling knowledge from a large deep teacher network to a much smaller student network for the task of road marking segmentation. In this work, we explore a novel knowledge distillation (KD) approach that can transfer 'knowledge' on scene structure more effectively from a teacher to a student model. Our method is known as Inter-Region Affinity KD (IntRA-KD). It decomposes a given road scene image into different regions and represents each region as a node in a graph. An inter-region affinity graph is then formed by establishing pairwise relationships between nodes based on their similarity in feature distribution. To learn structural knowledge from the teacher network, the student is required to match the graph generated by the teacher. The proposed method shows promising results on three large-scale road marking segmentation benchmarks, i.e., ApolloScape, CU-Lane and LLAMAS, by taking various lightweight models as students and ResNet-101 as the teacher. IntRA-KD consistently brings higher performance gains on all lightweight models, compared to previous distillation methods. Our code is available at https://github.com/ cardwing/Codes-for-IntRA-KD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Road marking segmentation serves various purposes in autonomous driving, e.g., providing cues for vehicle navigation or extracting basic road elements and lanes for constructing high-definition maps <ref type="bibr" target="#b6">[7]</ref>. Training a deep network for road marking segmentation is known to be challenging due to various reasons <ref type="bibr" target="#b7">[8]</ref>, including tiny road elements, poor lighting conditions and occlusions caused by vehicles. The training difficulty is further compounded by the nature of segmentation labels available for training, which are usually sparse (e.g., very thin and long lane marking against a large background), hence affecting the capability of a net- †: Corresponding author. work in learning the spatial structure of a road scene <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>The aforementioned challenges become especially crippling when one is required to train a small model for road marking segmentation. This requirement is not uncommon considering that small models are usually deployed on vehicles with limited computational resources. Knowledge distillation (KD) <ref type="bibr" target="#b5">[6]</ref> offers an appealing way to facilitate the training of a small student model by transferring knowledge from a trained teacher model of larger capacity. Various KD methods have been proposed in the past, e.g., with knowledge transferred through softened class scores <ref type="bibr" target="#b5">[6]</ref>, feature maps matching <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> or spatial attention maps matching <ref type="bibr" target="#b26">[27]</ref>.</p><p>While existing KD methods are shown effective in many classification tasks, we found that they still fall short in transferring knowledge of scene structure for the task of road marking segmentation. Specifically, a road scene typically exhibits consistent configuration, i.e., road elements are orderly distributed in a scene. The structural relationship is crucial to providing the necessary constraint or regularization, especially for small networks, to combat against the sparsity of supervision. However, such structural relationship is rarely exploited in previous distillation methods. The lack of structural awareness makes small models struggle in differentiating visually similar but functionally different road markings.</p><p>In this paper, we wish to enhance the structural awareness of a student model by exploring a more effective way to transfer the scene structure prior encoded in a teacher to a student. Our investigation is based on the premise that a teacher model should have a better capability in learning discriminative features and capturing contextual information due to its larger capacity in comparison to the student model. Feature distribution relationships encoded by the teacher on different parts of a deep feature map could reveal rich structural connections between different scene regions, e.g., the lane region should look different from the zebra crossing. Such priors can offer a strong constraint to regularize the learning of the student network.</p><p>Our method is known as Inter-Region Affinity Knowledge Distillation (IntRA-KD). As the name implies, knowl- edge on scene structure is represented as inter-region affinity graphs, as shown as <ref type="figure" target="#fig_0">Fig. 1</ref>. Each region is a part of a deep feature map, while each node in the graph denotes the feature distribution statistics of each region. Each pair of nodes are connected by an edge representing their similarity in terms of feature distribution. Given the same input image, the student network and the teacher network will both produce their corresponding affinity graph. Through graph matching, a distillation loss on graph consistency is generated to update the student network.</p><p>This novel notion of inter-region affinity knowledge distillation is appealing in its simplicity and generality. The method is applicable to various road marking scenarios with an arbitrary number of road element classes. It can also work together with other knowledge distillation methods. It can even be applied on more general segmentation tasks (e.g., Cityscapes <ref type="bibr" target="#b2">[3]</ref>). We present an effective and efficient way of building inter-region affinity graphs, including a method to obtain regions from deep feature maps and a new moment pooling operator to derive feature distribution statistics from these regions. Extensive experiments on three popular datasets (ApolloScape <ref type="bibr" target="#b10">[11]</ref>, CULane <ref type="bibr" target="#b13">[14]</ref> and LLAMAS <ref type="bibr" target="#b0">[1]</ref>) show that IntRA-KD consistently outperforms other KD methods, e.g., probability map distillation <ref type="bibr" target="#b5">[6]</ref> and attention map distillation <ref type="bibr" target="#b26">[27]</ref>. It generalizes well to various student architectures, e.g., ERFNet <ref type="bibr" target="#b19">[20]</ref>, ENet <ref type="bibr" target="#b15">[16]</ref> and ResNet-18 <ref type="bibr" target="#b4">[5]</ref>. Notably, with IntRA-KD, ERFNet achieves compelling performance in all benchmarks with 21× fewer parameters (2.49 M v.s. 52.53 M) and runs 16× faster (10.2 ms v.s. 171.2 ms) compared to ResNet-101 model. Encouraging results are also observed on Cityscapes <ref type="bibr" target="#b2">[3]</ref>. Due to space limit, we include the results in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Road marking segmentation. Road marking segmentation is conventionally handled using hand-crafted features to obtain road marking segments. Then, a classification network is employed to classify the category of each segment <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. These approaches have many drawbacks, e.g., require sophisticated feature engineering process and only work well in simple highway scenarios.</p><p>The emergence of deep learning has avoided manual feature design through learning features in an end-to-end manner. These approaches usually adopt the dense prediction formulation, i.e., assign each pixel a category label <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>. For example, Wang et al. <ref type="bibr" target="#b23">[24]</ref> exploit deep neural networks to map an input image to a segmentation map. Since large models usually demand huge memory storage and have slow inference speed, many lightweight models, e.g., ERFNet <ref type="bibr" target="#b19">[20]</ref>, are leveraged to fulfil the requirement of fast inference and small storage <ref type="bibr" target="#b7">[8]</ref>. However, due to the limited model size, these small networks perform poorly in road marking segmentation. A common observation is that such small models do not have enough capacity to capture sufficient contextual knowledge given the sparse supervision signals <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>. Several schemes have been proposed to relieve the sparsity problem. For instance, Hou et al. <ref type="bibr" target="#b7">[8]</ref> reinforce the learning of contextual knowledge through self knowledge distillation, i.e., using deep-layer attention maps to guide the learning of shallower layers. SCNN <ref type="bibr" target="#b13">[14]</ref> resolves this problem through message passing between deep feature layers. Zhang et al. <ref type="bibr" target="#b28">[29]</ref> propose a framework to perform lane area segmentation and lane boundary detection simultaneously. The aforementioned methods do not take structural relationship between different areas into account and they do not consider knowledge distillation from teacher networks. Knowledge distillation. Knowledge distillation was originally introduced by [6] to transfer knowledge from a teacher model to a compact student model. The distilled knowledge can be in diverse forms, e.g., softened output logits <ref type="bibr" target="#b5">[6]</ref>, intermediate feature maps <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref> or pairwise similarity maps between neighbouring layers <ref type="bibr" target="#b25">[26]</ref>. There is another line of work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> that uses self-derived knowledge to reinforce the representation learning of the network itself, without the supervision of a large teacher model. Recent studies have expanded knowledge distillation from one sample to several samples <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>. For instance, Park et al. <ref type="bibr" target="#b14">[15]</ref> transfer mutual relations between a batch of data samples in the distillation process. Tung et al. <ref type="bibr" target="#b22">[23]</ref> take the similarity scores of features of different samples as distillation targets. The aforementioned approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> do not consider the structural relationship between different areas in one sample. On the contrary, the proposed IntRA-KD takes inter-region relationship into account, which is new in knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Road marking segmentation is commonly formulated as a semantic segmentation task <ref type="bibr" target="#b23">[24]</ref>. More specifically, given an input image X ∈ R h×w×3 , the objective is to assign a label l ∈ {0, . . . , n−1} to each pixel (i, j) of X, comprising the segmentation map O. Here, h and w are the height and width of the input image, n is the number of classes and class 0 denotes the background. The objective is to learn a mapping F: X → O. Contemporary algorithms use CNN as F for end-to-end prediction.</p><p>Since autonomous vehicles have limited computational resources and demand real-time performance, lightweight models are adopted to fulfil the aforementioned requirements. On account of limited parameter size as well as insufficient guidance due to sparse supervision signals, these small models usually fail in the challenging road marking segmentation task. Knowledge distillation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13</ref>] is a common approach to improving the performance of small models by means of distilling knowledge from large models. There are two networks in knowledge distillation, one is called the student and the other is called the teacher. The purpose of knowledge distillation is to transfer dark knowledge from the large, cumbersome teacher model to the small, compact student model. The dark knowledge can take on many forms, e.g., output logits and intermediate layer activations. There exist previous distillation methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> that exploit the relationship between a batch of samples. These approaches, however, do not take into account the structural relationship between different areas within a sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Unlike existing KD approaches, IntRA-KD considers intrinsic structural knowledge within each sample as a form of knowledge for distillation. Specifically, we consider each input sample to have n road marking classes including the background class. We treat each class map as a region. In practice, the number of classes/regions co-existing in a sample can be fewer than n. Given the same input, an inter-region affinity graph G S for the student network and an inter-region affinity graph G T for the teacher are constructed. Here, an affinity graph is defined as</p><formula xml:id="formula_0">G = µ, C ,<label>(1)</label></formula><p>where µ is a set of nodes, each of which represents feature distribution statistics of each region. Each pair of nodes are connected by an edge C that denotes the similarity between two nodes in terms of feature distribution. The overall pipeline of our IntRA-KD is shown in <ref type="figure">Fig. 2</ref>. The framework is composed of three main components: 1) Generation of areas of interest (AOI) -to extract regions representing the spatial extent for each node in the graphs.</p><p>2) AOI-grounded moment pooling -to quantify the statistics of feature distribution of each region. 3) Inter-region affinity distillation -to construct the interregion affinity graph and distill structural knowledge from the teacher to the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inter-Region Affinity Knowledge Distillation</head><p>Generation of AOI. The first step of IntRA-KD is to extract regions from a given image to represent the spatial extent of each class. The output of this step is n AOI maps constituting a set M ∈ R h×w×n , where h is the height, w is the width, and n is the number of classes. Each mask map is binary -'1' represents the spatial extent of a particular class, e.g., left lane, while '0' represents other classes and background.</p><p>A straightforward solution is to use the ground-truth labels as AOI. However, ground-truth labels only consider the road markings but neglect the surrounding areas around the road markings. We empirically found that naïve distillation in ground-truth areas is ineffective for the transfer of contextual information from a teacher to a student model.</p><p>To include a larger area, we use a transformation operation to generate AOI from the ground-truth labels. Unlike labels that only contain road markings, areas obtained after the operation also take into account the surrounding areas of road markings. An illustration of AOI generation is shown in <ref type="figure" target="#fig_5">Fig. 3</ref>. Suppose we have n binary ground-truth label maps comprising a set L ∈ R h×w×n . For each class label map L ∈ R h×w , we smooth the label map with an average kernel φ and obtain an AOI map for the corresponding class as M = 1 (φ(L) &gt; 0), where 1(.) is an indicator function and M ∈ R h×w has the same size as L. Repeating these steps for all n ground-truth label maps provides us n AOI maps. Note that AOI maps can also be obtained by image morphological operations. AOI-grounded moment pooling. Suppose the feature maps of a network are represented as F ∈ R h f ×w f ×c , where h f , w f and c denote the height, width and channel of the feature map, respectively. Once we obtain the AOI maps M, we can use them as masks to extract AOI features from F for each class region. The obtained AOI features can then be used to compute the inter-region affinity. For effective affinity computation, we regard AOI features of each region as a distribution. Affinity can then be defined as the similarity between two feature distributions.</p><p>Moments have been widely-used in many studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>. Inspired by these prior studies, we calculate moment statistics of a distribution and use them for affinity computation. In particular, we extract the first moment µ 1 , second moment µ 2 and third moment µ 3 as the high-level statistics of a distribution. The moments of features have explicit meanings, i.e., the first moment represents the mean of the distribution, the second moment (variance) and the      <ref type="figure">Figure 2</ref>. The pipeline of IntRA-KD. There are two networks in our approach, one serves as the student and the other serves as the teacher. Given an input image, the student is required to mimic the inter-region affinity graph of the trained teacher model at selected layers. Labels are pre-processed by a smoothing operation to obtain the areas of interest (AOI). AOI maps, shown as an integrated map here, provide the masks to extract features corresponding to each class region. Moment pooling is performed to compute the statistics of feature distribution for each region. This is followed by the construction of an inter-region affinity graph that captures the similarity of feature distribution between different regions. The inter-region affinity graph is composed of three sub-graphs, i.e., the mean graph, the variance graph and the skewness graph. third moment (skewness) describe the shape of that distribution. We empirically found that using higher-order moments brings marginal performance gains while requiring heavier computation cost.</p><formula xml:id="formula_1">P z C G u Z l a Q k = " &gt; A A A B 8 X i c b V B N S w M x F H x b v 2 r 9 q n r 0 E i x C T 2 W 3 C n o s e P E i V L C t 2 J a S T b N t a D a 7 J G + F s v R f e P G g i F f / j T f / j d l 2 D 9 o 6 E B h m 3 i P z x o + l M O i 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t U 2 U a M Z b L J K R f v C p 4 V I o 3 k K B k j / E m t P Q l 7 z j T 6 4 z v / P E t R G R u s d p z P s h H S k R C E b R S o + 9 k O L Y D 9 L b 2 a B c c W v u H G S V e D m p Q I 7 m o P z V G 0 Y s C b l C J q k x X c + N s Z 9 S j Y J J P i v 1 E s N j y i Z 0 x L u W K h p y 0 0 / n i W f k z C p D E k T a P o V k r v 7 e S G l o z D T 0 7 W S W 0 C x 7 m f i f 1 0 0 w u O q n Q s U J c s U W H w W J J B i R 7 H w y F J o z l F N L K N P C Z i V s T D V l a E</formula><formula xml:id="formula_2">N O E + x E d K R E K R t F K j / 2 I 4 j g I s + 5 s U K 6 4 N X c B s k 6 8 n F Q g R 3 N Q / u o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P S v 3 U 8 I S y C R 3 x n q W K R t z 4 2 S L x j F x Y Z U j C W N u n k C z U 3 x s Z j Y y Z R o G d n C c 0 q 9 5 c / M / r p R j e + J l Q S Y p c s e V H Y S o J x m R + P h k K z R n K q S W U a W G z E j a m m j K 0 J Z V s C d 7 q y e u k X a 9 5 l 7 X 6 / V W l U c 3 r K M I Z n E M V P L i G B t x B E 1 r A Q M E z v M K b Y 5 w X</formula><formula xml:id="formula_3">= " &gt; A A A B 8 X i c b V B N S w M x F H x b v 2 r 9 q n r 0 E i x C T 2 W 3 C n o s e P F m B d u K b S n Z N N u G Z r N L 8 l Y o S / + F F w + K e P X f e P P f m G 3 3 o K 0 D g W H m P T J v / F g K g 6 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R 2 0 S J Z r z F I h n p B 5 8 a L o X i L R Q o + U O s O Q 1 9 y T v + 5 D r z O 0 9 c G x G p e 5 z G v B / S k R K B Y B S t 9 N g L K Y 7 9 I L 2 d D c o V t + b O Q V a J l 5 M K 5 G g O y l + 9 Y c S S k C t k k h r T 9 d w Y + y n V K J j k s 1 I v M T y m b E J H v G u p o i E 3 / X S e e E b O r D I k Q a T t U 0 j m 6 u + N l I b G T E P f T m Y J z b K X i f 9 5 3 Q S D q 3 4 q V J w g V 2 z x U Z B I g h H J z i d D o T l D O b W E M i 1 s V s L G V F O G t q S S L c F b P n m V t O</formula><p>To compute µ 1 (k), µ 2 (k) and µ 3 (k) of class k, we introduce the moment pooling operation to process the AOI features.</p><formula xml:id="formula_4">µ 1 (k) = 1 |M(:, :, k)| h f i=1 w f j=1 M(i, j, k)F(i, j), µ 2 (k) = 1 |M(:, :, k)| h f i=1 w f j=1 (M(i, j, k)F(i, j) − µ 1 (k)) 2 , µ 3 (k) = 1 |M(:, :, k)| h f i=1 w f j=1 M(i, j, k)F(i, j) − µ 1 (k) µ 2 (k) 3 ,<label>(2)</label></formula><p>...  An illustration of the process of moment pooling is depicted in <ref type="figure" target="#fig_7">Fig. 4</ref>. The moment pooling operation has the following properties. First, it can process areas with arbitrary shapes and sizes, which can be seen as an extension of the conventional average pooling. Second, the moment vectors obtained by the moment pooling operation can faithfully reflect the feature distribution of a particular region, and yet, the vectors are in a very low-dimension, thus facilitating efficient affinity computation in the subsequent step.</p><p>Inter-region affinity distillation. Since output feature  <ref type="figure">Figure 5</ref>. Visualization of the affinity graph generated by different methods. We represent edges in an affinity graph by inverse similarity score. The number next to the method's name is F1measure.</p><p>maps of the teacher model and those of the student model may have different dimensions, performing matching of each pair of moment vectors would require extra parameters or operations to guarantee dimension consistency. Instead, we compute the cosine similarity of the moment vectors of class k 1 and class k 2 , i.e.,</p><formula xml:id="formula_5">C(k 1 , k 2 , r) = µ r (k 1 ) T µ r (k 2 ) µ r (k 1 ) 2 µ r (k 2 ) 2 , r ∈ {1, 2, 3}. (3)</formula><p>The similarity score captures the similarity of each pair of classes and it is taken as the high-level knowledge to be learned by the student model. The moment vectors µ and the similarity scores C constitute the nodes and the edges of the affinity graph G = µ, C , respectively (see <ref type="figure">Fig. 2</ref>). The inter-region affinity distillation loss is given as follows:</p><formula xml:id="formula_6">L m (C S , C T ) = 1 3n 2 3 r=1 n k1=1 n k2=1 C S (k 1 , k 2 , r) − C T (k 1 , k 2 , r) 2 2 .<label>(4)</label></formula><p>The introduced affinity distillation is robust to the network differences between the teacher and student models since the distillation is only related to the number of classes and is irrelevant to the specific dimension of feature maps. In addition, the affinity knowledge is comprehensive since it gathers information from AOI features from both the foreground and background areas. Finally, in comparison to previous distillation methods <ref type="bibr" target="#b5">[6]</ref> that use probability maps as distillation targets, the affinity graph is more memoryefficient since it reduces the size of the distillation targets from h × w × n to n 2 where n is usually thousands of times smaller than h × w.</p><p>From <ref type="figure">Fig. 5</ref>, we can see that IntRA-KD not only improves the predictions of ERFNet, but also causes a closer feature structure between the student model and the ResNet-101 teacher model. This is reflected by the very similar structure between the affinity graphs of ERFNet and ApolloScape CULane LLAMAS <ref type="figure">Figure 6</ref>. Typical video frames of ApolloScape, CULane and LLAMAS datasets.</p><p>ResNet-101. It is interesting to see that those spatially close and visually similar road markings are pulled closer and those spatially distant and visually different markings are pulled apart in the feature space using IntRA-KD . An example is shown in <ref type="figure">Fig. 5</ref>, illustrating the effectiveness of IntRA-KD in transferring structural knowledge from the teacher model to the student model. We show in the experiment section that such transfers are essential to improve the performance of student models. Adding IntRA-KD to training. The final loss is composed of three terms, i.e., the cross entropy loss, the inter-region affinity distillation loss and the attention map distillation loss. The attention map distillation loss is optional in our framework but it is useful to complement the region-level knowledge. The final loss is written as</p><formula xml:id="formula_7">L = L seg (O, L) + α 1 L m (C S , C T ) + α 2 L a (A S , A T ).<label>(5)</label></formula><p>Here, α 1 and α 2 are used to balance the effect of different distillation losses on the main task loss L seg . Different from the mimicking of feature maps F ∈ R h f ×w f ×c , which demand huge memory resources and are hard to learn, attention maps A ∈ R h f ×w f are more memory-friendly and easier to mimic since only several important areas are needed to learn. The attention map distillation loss is given as follows:</p><formula xml:id="formula_8">L a (A S , A T ) = h f i=1 w f j=1 A S (i, j) − A T (i, j) 2 2 . (6)</formula><p>We follow <ref type="bibr" target="#b26">[27]</ref> to derive attention maps from feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We conduct experiments on three datasets, namely ApolloScape <ref type="bibr" target="#b10">[11]</ref>, CULane <ref type="bibr" target="#b13">[14]</ref> and LLAMAS <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure">Figure 6</ref> shows a selected video frame from each of the three datasets. These three datasets are challenging due to poor light conditions, occlusions and the presence of many tiny road markings. Note that CULane and LLAMAS only label lanes according to their relative positions to the ego vehicle while ApolloScape labels every road marking on the road according to their functions. Hence, ApolloScape has much more classes and it is more challenging compared with the other two datasets. Apart from the public result <ref type="bibr" target="#b10">[11]</ref>, we also reproduce the most related and stateof-the-art methods (e.g., ResNet-50 and UNet-ResNet-34) on ApolloScape for comparison. As to LLAMAS dataset, since the official submission server is not established, the evaluation on the original testing set is impossible. Hence, we split the original validation set into two parts, i.e., one is used for validation and the other is used for testing. <ref type="table" target="#tab_1">Table 1</ref> summarizes the details and train/val/test partitions of the datasets. Evaluation metrics. We use different metrics on each dataset following the guidelines of the benchmark and practices of existing studies. 1) ApolloScape. We use the official metric, i.e., mean intersection-over-union (mIoU) as evaluation criterion <ref type="bibr" target="#b10">[11]</ref>.</p><p>2) CULane. Following <ref type="bibr" target="#b13">[14]</ref>, we use F 1 -measure as the evaluation metric, which is defined as:</p><formula xml:id="formula_9">F 1 = 2×P recision×Recall P recision+Recall , where Precision = T P</formula><p>T P +F P and Recall = T P T P +F N . 3) LLAMAS. We use mean average precision (mAP) to evaluate the performance of different algorithms <ref type="bibr" target="#b0">[1]</ref>. Implementation details. Since there is no road marking in the upper areas of the input image, we remove the upper part of the original image during both training and testing phases. The size of the processed image is 3384 × 1010 for ApolloScape, 1640 × 350 for CULane, and 1276 × 384 for LLAMAS. To save memory usage, we further resize the processed image to 1692 × 505, 976 × 208 and 960 × 288, respectively. We use SGD <ref type="bibr" target="#b1">[2]</ref> to train our models and the learning rate is set as 0.01. Batch size is set as 12 for CULane and LLAMAS, and 8 for ApolloScape. The total number of training episodes is set as 80K for CULane and LLAMAS, and 180K for ApolloScape since ApolloScape is more challenging. The cross entropy loss of background pixels is multiplied by 0.4 for CULane and LLAMAS, and 0.05 for ApolloScape since class imbalance is more severe in ApolloScape. For the teacher model, i.e., ResNet-101, we add the pyramid pooling module <ref type="bibr" target="#b29">[30]</ref> to obtain both local and global context information. α 1 and α 2 are both set as 0.1 and the size of the averaging kernel for obtaining AOI maps is set as 5 × 5. Our results are not sensitive to the kernel size.</p><p>In our experiments, we use either ERFNet <ref type="bibr" target="#b19">[20]</ref>, ENet <ref type="bibr" target="#b15">[16]</ref> or ResNet-18 <ref type="bibr" target="#b4">[5]</ref> as the student and ResNet-101 as the teacher. While we choose ERFNet to report most of our ablation studies in this paper, we also report overall re-  <ref type="bibr" target="#b15">[16]</ref> 39.8 ResNet-50 <ref type="bibr" target="#b4">[5]</ref> 41.3 UNet-ResNet-34 <ref type="bibr" target="#b20">[21]</ref> 42.4 Teacher</p><p>ResNet-101 <ref type="bibr" target="#b4">[5]</ref> 46.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student</head><p>ERFNet <ref type="bibr" target="#b19">[20]</ref> 40.4</p><p>Self distillation ERFNet-DKS <ref type="bibr" target="#b21">[22]</ref> 40.8 ERFNet-SAD <ref type="bibr" target="#b7">[8]</ref> 40.9</p><p>Teacher-student distillation ERFNet-KD <ref type="bibr" target="#b5">[6]</ref> 40.7 ERFNet-SKD <ref type="bibr" target="#b12">[13]</ref> 40.9 ERFNet-PS-N <ref type="bibr" target="#b25">[26]</ref> 40.6 ERFNet-IRG <ref type="bibr" target="#b11">[12]</ref> 41.0 ERFNet-BiFPN <ref type="bibr" target="#b30">[31]</ref> 41.6 ERFNet-IntRA-KD (ours) 43.2 sults of ENet <ref type="bibr" target="#b15">[16]</ref> and ResNet-18 <ref type="bibr" target="#b4">[5]</ref>. Detailed results are provided in the supplementary material. We extract both high-level features and middle-level features from ResNet-101 as distillation targets. Specifically, we let the features of block 2 and block 3 of ERFNet to mimic those of block 3 and block 5 of ResNet-101, respectively. Baseline distillation algorithms. In addition to the stateof-the-art algorithms in each benchmark, we also compare the proposed IntRA-KD with contemporary knowledge distillation algorithms, i.e., KD <ref type="bibr" target="#b5">[6]</ref>, SKD <ref type="bibr" target="#b12">[13]</ref>, PS-N <ref type="bibr" target="#b25">[26]</ref>, IRG <ref type="bibr" target="#b11">[12]</ref> and BiFPN <ref type="bibr" target="#b30">[31]</ref>. Here, KD denotes probability map distillation; SKD employs both probability map distillation and pairwise similarity map distillation; PS-N takes the pairwise similarity map of neighbouring layers as knowledge; IRG uses the instance features, instance relationship and inter-layer transformation of three consecutive frames for distillation, and BiFPN uses attention maps of neighbouring layers as distillation targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Tables 2-4 summarize the performance of our method, i.e., ERFNet-IntRA-KD , against state-of-the-art algorithms on the testing set of ApolloScape <ref type="bibr" target="#b10">[11]</ref>, CULane <ref type="bibr" target="#b13">[14]</ref> and LLAMAS <ref type="bibr" target="#b0">[1]</ref>. We also report the runtime and parameter size of different models in <ref type="table" target="#tab_3">Table 3</ref>. The runtime is recorded using a single GPU (GeForce GTX TITAN X Maxwell) and averages across 100 samples. ERFNet-IntRA-KD outperforms all baselines and previous distillation methods in all three benchmarks. Note that ERFNet-IntRA-KD has 21 × fewer parameters and runs 16 × faster compared with ResNet-101 on CULane testing set; the appealing performance strongly suggests the effectiveness of IntRA-KD.</p><p>We also apply IntRA-KD to ENet and ResNet-18, and  <ref type="bibr" target="#b12">[13]</ref> 70.7 ERFNet-PS-N <ref type="bibr" target="#b25">[26]</ref> 70.6 ERFNet-IRG <ref type="bibr" target="#b11">[12]</ref> 70.7 ERFNet-BiFPN <ref type="bibr" target="#b30">[31]</ref> 71.4 ERFNet-IntRA-KD (ours) 72.4  <ref type="bibr" target="#b13">[14]</ref> 0.597 ResNet-50 <ref type="bibr" target="#b4">[5]</ref> 0.578 UNet-ResNet-34 <ref type="bibr" target="#b20">[21]</ref> 0.592 Teacher ResNet-101 <ref type="bibr" target="#b4">[5]</ref> 0.607 Student ERFNet <ref type="bibr" target="#b19">[20]</ref> 0.570</p><p>Self distillation ERFNet-DKS <ref type="bibr" target="#b21">[22]</ref> 0.573 ERFNet-SAD <ref type="bibr" target="#b7">[8]</ref> 0.575</p><p>Teacher-student distillation ERFNet-KD <ref type="bibr" target="#b5">[6]</ref> 0.572 ERFNet-SKD <ref type="bibr" target="#b12">[13]</ref> 0.576 ERFNet-PS-N <ref type="bibr" target="#b25">[26]</ref> 0.575 ERFNet-IRG <ref type="bibr" target="#b11">[12]</ref> 0.576 ERFNet-BiFPN <ref type="bibr" target="#b30">[31]</ref> 0.583 ERFNet-IntRA-KD (ours) 0.598 find that IntRA-KD can equivalently bring more performance gains to the backbone models than the state-of-theart BiFPN <ref type="bibr" target="#b30">[31]</ref> on ApolloScape dataset <ref type="figure" target="#fig_9">(Fig. 7)</ref>. Note that BiFPN is a competitive algorithm in all benchmarks. Encouraging results are also observed on CULane and LLA-MAS when applying IntRA-KD to ENet and ResNet-18. Due to space limit, we report detailed performance of applying different distillation algorithms to ENet and ResNet-18 in the supplementary material. The effectiveness of our IntRA-KD on different backbone models validates the good generalization ability of our method.</p><p>We also show some qualitative results of our IntRA-KD and BiFPN <ref type="bibr" target="#b30">[31]</ref> (the most competitive baseline) on three benchmarks. As shown in (a) and (c) of <ref type="figure" target="#fig_10">Fig. 8</ref>, IntRA-KD helps ERFNet predict more accurately on both long and thin road markings. As to other challenging scenarios of crowded roads and poor light conditions, ERFNet and ERFNet-BiFPN either predict lanes inaccurately or miss the predictions. By contrast, predictions yielded by ERFNet-IntRA-KD are more complete and accurate.</p><p>Apart from model predictions, we also show the deep   feature embeddings of different methods. As can be seen from <ref type="figure">Fig. 9</ref>, the embedding of ERFNet-IntRA-KD is more structured compared with that of ERFNet and ERFNet-BiFPN. In particular, the features of ERFNet-IntRA-KD are more distinctly clustered according to their classes in the embedding, with similar distribution to the embedding of the ResNet-101 teacher. The results suggest the importance of structural information in knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>In this section, we investigate the effect of each component, i.e., different loss terms and the associated coefficients, on the final performance. Effect of different loss terms. From <ref type="figure" target="#fig_0">Fig. 10</ref>, we have . Regions of the model prediction, which are covered by the red dashed rectangle, are highlighted in the third row. The color bar of the deep embeddings is the same as that of the model prediction except the background, whose color is changed from black to pink for better visualization. Note that we crop the upper part of the label and model prediction for better visualization and we use t-SNE to visualize the feature maps (first row).</p><p>the following observations: (1) Considering all moments from both middle-and high-level features, i.e., the blue bar with L µ1 + L µ2 + L µ3 , brings the most performance gains.</p><p>(2) Attention map distillation, L a also brings considerable gains compared with the baseline without distillation. (3) Distillation of high-level features brings more performance gains than that of middle-level features. This may be caused by the fact that high-level features contain more semanticrelated information, which is beneficial to the segmentation task. (4) Inter-region affinity distillation and attention map distillation are complementary, leading to best performance (i.e., 43.2 mIoU as shown by the red vertical dash line).</p><p>Effect of loss coefficients. The coefficients of the attention map loss and affinity distillation loss are all set as 0. Hence, IntRA-KD is robust to the loss coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a simple yet effective distillation approach, i.e., IntRA-KD , to effectively transfer scene structural knowledge from a teacher model to a student model. The structural knowledge is represented as an inter-region affinity graph to capture similarity of feature distribution of different scene regions. We applied IntRA-KD to various lightweight models and observed consistent performance gains to these models over other contemporary distillation  <ref type="figure" target="#fig_0">Figure 10</ref>. Performance (mIoU) of ERFNet using different loss terms of IntRA-KD on ApolloScape-test. Here, "High" denotes the mimicking of high-level features (block 5) and "Middle" denotes the mimicking of middle-level features (block 3) of the teacher model. Lµ i denotes the variant where only the i-th order moment is deployed for inter-region affinity distillation. Here, "40.4" is the performance of ERFNet without distillation and "43.2" is the performance of ERFNet-IntRA-KD that considers Lµ 1 + Lµ 2 + Lµ 3 and La. The number besides each bar is performance gain brought by each loss term compared with ERFNet. methods. Promising results on three large-scale road marking segmentation benchmarks strongly suggest the effectiveness of IntRA-KD. Results on Cityscapes are provided in the supplementary material.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the affinity distillation process. FS and FT are the intermediate activations of the student and teacher models, respectively. G is the affinity graph that comprises nodes (feature vectors) and edges (cosine similarities). Note that each circle in the figure is a vector and different colors represent different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " A k r n Q + E B k w 2 W z 2 o s f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s q 2 R K 8 5 Z N X S b t e 8 8 5 r 9 b u L S q O a 1 1 G E E z i F K n h w C Q 2 4 g S a 0 g I G C Z 3 i F N 8 c 4 L 8 6 7 8 7 E Y L T j 5 z j H 8 g f P 5 A 7 V e k N s = &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 e / t E r u h 6 T Y 5 S 0 h d 5 C l X c / 7 M f M = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v Q V Z m p g i 4 L b l x W s A 9 s h 5 J J M 2 1 o J j M k d 4 Q y 9 C / c u F D E r X / j z r 8 x b W e h r Q c C h 3 P u J e e e I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M b u d + 5 4 l r I 2 L 1 g N O E + x E d K R E K R t F K j / 2 I 4 j g I s + 5 s U K 6 4 N X c B s k 6 8 n F Q g R 3 N Q / u o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P S v 3 U 8 I S y C R 3 x n q W K R t z 4 2 S L x j F x Y Z U j C W N u n k C z U 3 x s Z j Y y Z R o G d n C c 0 q 9 5 c / M / r p R j e + J l Q S Y p c s e V H Y S o J x m R + P h k K z R n K q S W U a W G z E j a m m j K 0 J Z V s C d 7 q y e u k X a 9 5 l 7 X 6 / V W l U c 3 r K M I Z n E M V P L i G B t x B E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W I 4 W n H z n F P 7 A + f w B x h W Q 5 g = = &lt; / l a t e x i t &gt; Input image, X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 e / t E r u h 6 T Y 5 S 0 h d 5 C l X c / 7 M f M = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v Q V Z m p g i 4 L b l x W s A 9 s h 5 J J M 2 1 o J j M k d 4 Q y 9 C / c u F D E r X / j z r 8 x b W e h r Q c C h 3 P u J e e e I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M b u d + 5 4 l r I 2 L 1 g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>5 9 3 5 W I 4 W n H z n F P 7 A + f w B x h W Q 5 g = = &lt; / l a t e x i t &gt; O &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 s J C U / L m I Q U r D w e a I x f G y x j Y J p M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>s 1 7 7 x W v 7 u o N K p 5 H U U 4 g V O o g g e X 0 I A b a E I L G C h 4 h l d 4 c 4 z z 4 r w 7 H 4 v R g p P v H M M f O J 8 / u G i Q 3 Q = = &lt; / l a t e x i t &gt; Segmentation map, O &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 s J C U / L m I Q U r D w e a I x f G y x j Y J p M = " &gt; A A A B 8 X i c b V B N S w M x F H x b v 2 r 9 q n r 0 E i x C T 2 W 3 C n o s e P F m B d u K b S n Z N N u G Z r N L 8 l Y o S / + F F w + K e P X f e P P f m G 3 3 o K 0 D g W H m P T J v / F g K g 6 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R 2 0 S J Z r z F I h n p B 5 8 a L o X i L R Q o + U O s O Q 1 9 y T v + 5 D r z O 0 9 c G x G p e 5 z G v B / S k R K B Y B S t 9 N g L K Y 7 9 I L 2 d D c o V t + b O Q V a J l 5 M K 5 G g O y l + 9 Y c S S k C t k k h r T 9 d w Y + y n V K J j k s 1 I v M T y m b E J H v G u p o i E 3 / X S e e E b O r D I k Q a T t U 0 j m 6 u + N l I b G T E P f T m Y J z b K X i f 9 5 3 Q S D q 3 4 q V J w g V 2 z x U Z B I g h H J z i d D o T l D O b W E M i 1 s V s L G V F O G t q S S L c F b P n m V t O s 1 7 7 x W v 7 u o N K p 5 H U U 4 g V O o g g e X 0 I A b a E I L G C h 4 h l d 4 c 4 z z 4 r w 7 H 4 v R g p P v H M M f O J 8 / u G i Q 3 Q = = &lt; / l a t e x i t &gt; L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p B C M N j X N 2 1 6 W c I Z 8 u T S 7 L j 0 W y yg = " &gt; A A A B 8 X i c b V A 9 S w N B F H w X v 2 L 8 i l r a L A Y h V b i L g p Y B G w u L C C Y R k x D 2 N n v J k r 2 9 Y / e d E I 7 8 C x s L R W z 9 N 3 b + G / e S K z R x Y G G Y e Y + d N 3 4 s h U H X / X Y K a + s b m 1 v F 7 d L O 7 t 7 + Q f n w q G 2 i R D P e Y p G M 9 I N P D Z d C 8 R Y K l P w h 1 p y G v u Q d f 3 K d + Z 0 n r o 2 I 1 D 1 O Y 9 4 P 6 U i J Q D C K Vn r s h R T H f p D e z g b l i l t z 5 y C r x M t J B X I 0 B + W v 3 j B i S c g V M k m N 6 X p u j P 2 U a h R M 8 l m p l x g e U z a h I 9 6 1 V N G Q m 3 4 6 T z w j Z 1 Y Z k i D S 9 i k k c / X 3 R k p D Y 6 a h b y e z h G b Z y 8 T / v G 6 C w V U / F S p O k C u 2 + C h I J M G I Z O e T o d C c o Z x a Q p k W N i t h Y 6 o p Q 1 t S y Z b g L Z + 8 S t r 1 m n d e q 9 9 d V B r V v I 4 i n M A p V M G D S 2 j A D T S h B Q w U P M M r v D n G e X H e n Y / F a M H J d 4 7 h D 5 z P H 7 P Z k N o = &lt; / l a t e x i t &gt; F &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h 9 Q t i m W 9 3 m j u 4 G L K V 1 n V z V z M U m E = " &gt; A A A B 8 X i c b V B N S w M x F H x b v 2 r 9 q n r 0 E i x C T 2 W 3 C n o s C O K x g m 3 F t p R s m m 1 D s 9 k l e S u U p f / C i w d F v P p v v P l v z L Z 7 0 N a B w D D z H p k 3 f i y F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b a J E M 9 5 i k Y z 0 g 0 8 N l 0 L x F g q U / C H W n I a + 5 B 1 / c p 3 5 n S e u j Y j U P U 5 j 3 g / p S I l A M I p W e u y F F M d + k N 7 M B u W K W 3 P n I K v E y 0 k F c j Q H 5 a / e M G J J y B U y S Y 3 p e m 6 M / Z R q F E z y W a m X G B 5 T N q E j 3 r V U 0 Z C b f j p P P C N n V h m S I N L 2 K S R z 9 f d G S k N j p q F v J 7 O E Z t n L x P + 8 b o L B V T 8 V K k 6 Q K 7 b 4 K E g k w Y h k 5 5 O h 0 J y h n F p C m R Y 2 K 2 F j q i l D W 1 L J l u A t n 7 x K 2 v W a d 1 6 r 3 1 1 U G t W 8 j i K c w C l U w Y N L a M A t N K E F D B Q 8 w y u 8 O c Z 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f q r u Q 1 A = = &lt; / l a t e x i t &gt; F &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h 9 Q t i m W 9 3 m j u 4 G L K V 1 n V z V z M U m E = " &gt; A A A B 8 X i c b V B N S w M x F H x b v 2 r 9 q n r 0 E i x C T 2 W 3 C n o s C O K x g m 3 F t p R s m m 1 D s 9 k l e S u U p f / C i w d F v P p v v P l v z L Z 7 0 N a B w D D z H p k 3 f i y F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b a J E M 9 5 i k Y z 0 g 0 8 N l 0 L x F g q U / C H W n I a + 5 B 1 / c p 3 5 n S e u j Y j U P U 5 j 3 g / p S I l A M I p W e u y F F M d + k N 7 M B u W K W 3 P n I K v E y 0 k F c j Q H 5 a / e M G J J y B U y S Y 3 p e m 6 M / Z R q F E z y W a m X G B 5 T N q E j 3 r V U 0 Z C b f j p P P C N n V h m S I N L 2 K S R z 9 f d G S k N j p q F v J 7 O E Z t n L x P + 8 b o L B V T 8 V K k 6 Q K 7 b 4 K E g k w Y h k 5 5 O h 0 J y h n F p C m R Y 2 K 2 F j q i l D W 1 L J l u A t n 7 x K 2 v W a d 1 6 r 3 1 1 U G t W 8 j i K c w C l U w Y N L a M A t N K E F D B Q 8 w y u 8 O c Z 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f q r u Q 1 A = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Generation of AOI. Take one class label map L as example. We perform smoothing on L and obtain one AOI map M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>[µ 1</head><label>1</label><figDesc>(k, 1), µ 1 (k, 2), . . . , µ 1 (k, c)] = µ 1 (k) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m x q y 6 F Y y s u Q 0 J m 1 c n b B 1 + W C t 8 4 = " &gt; A A A C K H i c b V D L S s N A F J 3 4 r P U V d e l m s A g t l J J E Q T d i w Y 3 L C v Y B a Q i T y a Q d O n k w M x F K 6 O e 4 8 V f c i C j S r V / i p I 2 i r Q e G O Z x z 7 8 y 9 x 0 s Y F d I w p t r K 6 t r 6 x m Z p q 7 y 9 s 7 u 3 r x 8 c d k S c c k z a O G Y x 7 3 l I E E Y j 0 p Z U M t J L O E G h x 0 j X G 9 3 k f v e B c E H j 6 F 6 O E + K E a B D R g G I k l e T q 1 3 Y / T F 2 z O q q b t f o 3 t R T 1 Y y l + B F x z 4 F X f i 5 k v x q G 6 M m V M c q f m 6 h W j Y c w A l 4 l Z k A o o 0 H L 1 V / U 0 T k M S S c y Q E L Z p J N L J E J c U M z I p 9 1 N B E o R H a E B s R S M U E u F k s 0 U n 8 F Q p P g x i r k 4 k 4 U z 9 3 Z G h U O Q T q s o Q y a F Y 9 H L x P 8 9 O Z X D p Z D R K U k k i P P 8 o S B m U M c x T g z 7 l B E s 2 V g R h T t W s E A 8 R R 1 i q b M s q B H N x 5 W X S s R r m W c O 6 O 6 8 0 r S K O E j g G J 6 A K T H A B m u A W t E A b Y P A I n s E b e N e e t B f t Q 5 v O S 1 e 0 o u c I / I H 2 + Q V l Y K P B &lt; / l a t e x i t &gt; h f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A P d 7 Z 1 I H s F V k c R J + R + o X 2 G K H 3 d Y = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M b u d + 5 4 l r I 2 L 1 i N O E + x E d K R E K R t F K D + N B O C h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w x s / E y p J k S u 2 X B S m k m B M 5 n + T o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b R r V e + q W r u v V x q 1 P I 4 i n M E 5 X I I H 1 9 C A O 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g B A I o 2 5 &lt; / l a t e x i t &gt; w f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p z Y w j y d s 5 I 8 X d G C F U 9 U Y 7 J O q f K 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 o J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 Z u a 3 H 1 F p H s s H M 0 n Q j + h Q 8 p A z a q x 0 / 9 Q P + + W K W 3 X n I K v E y 0 k F c j T 6 5 a / e I G Z p h N I w Q b X u e m 5 i / I w q w 5 n A a a m X a k w o G 9 M h d i 2 V N E L t Z / N T p + T M K g M S x s q W N G S u / p 7 I a K T 1 J A p s Z 0 T N S C 9 7 M / E / r 5 u a 8 N r P u E x S g 5 I t F o W p I C Y m s 7 / J g C t k R k w s o U x x e y t h I 6 o o M z a d k g 3 B W 3 5 5 l b R q V e + i W r u 7 r N R r e R x F O I F T O A c P r q A O t 9 C A J j A Y w j O 8 w p s j n B f n 3 f l Y t B a c f O Y Y / s D 5 / A F W / I 3 I &lt; / l a t e x i t &gt; ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Performing moment pooling on deep feature F ∈ R h f ×w f ×c . We use the pooling of first moment as an example. where |M(:, :, k)| computes the number of non-zero elements in M(:, :, k) and µ r (k) ∈ R c , r ∈ {1, 2, 3}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Comparison between IntRA-KD and BiFPN on ENet, ERFNet and ResNet-18 on ApolloScape-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Performance of different methods on (a) ApolloScape, (b) CULane and (c) LLAMAS testing sets. The number below each image denotes the accuracy for (a) and (c), F1-measure for (b). Ground-truth labels are drawn on the input image. Second rows of (a) and (c) are enlarged areas covered by the red dashed rectangle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>5 Figure 9 .</head><label>59</label><figDesc>ERFNet-IntRA-KD, 41.1 (a) ERFNet, 39.0 (b) ERFNet-BiFPN, 39.6 (c) ResNet-101, 42.Deep feature embeddings (first row) and predictions (second row) of (a) ERFNet (b) ERFNet-BiFPN (c) ResNet-101 (teacher) (d) ERFNet-IntRA-KD. The number next to the model's name denotes accuracy (%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Basic information of three road marking segmentation datasets.</figDesc><table><row><cell cols="3">Name ApolloScape [11] 114, 538 103, 653 # Frame Train</cell><cell>Validation 10, 000</cell><cell>Test 885</cell><cell>Resolution 3384 × 2710</cell><cell cols="2"># Class Temporally continuous ? 36 √</cell></row><row><cell>CULane [14] LLAMAS [1]</cell><cell>133, 235 79, 113</cell><cell>88, 880 58, 269</cell><cell>9, 675 10, 029</cell><cell>34, 680 10, 815</cell><cell>1640 × 590 1276 × 717</cell><cell>5 5</cell><cell>× √</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of different methods on ApolloScape-test.</figDesc><table><row><cell>Type</cell><cell>Algorithm</cell><cell>mIoU</cell></row><row><cell></cell><cell>Wide ResNet-38 [25]</cell><cell>42.2</cell></row><row><cell>Baseline</cell><cell>ENet</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance of different methods on CULane-test. To save space, baseline, teacher, student, self distillation and teacherstudent distillation in the first column are abbreviated as B, T, S, SD and TSD, respectively.</figDesc><table><row><cell>Type</cell><cell>Algorithm</cell><cell>F1</cell><cell>Runtime (ms)</cell><cell># Param (M)</cell></row><row><cell></cell><cell>SCNN [14]</cell><cell>71.6</cell><cell>133.5</cell><cell>20.72</cell></row><row><cell>B</cell><cell>ResNet-18-SAD [8]</cell><cell>70.5</cell><cell>25.3</cell><cell>12.41</cell></row><row><cell></cell><cell>ResNet-34-SAD [8]</cell><cell>70.7</cell><cell>50.5</cell><cell>22.72</cell></row><row><cell>T</cell><cell>ResNet-101 [5]</cell><cell>72.8</cell><cell>171.2</cell><cell>52.53</cell></row><row><cell>S</cell><cell>ERFNet [20]</cell><cell>70.2</cell><cell></cell><cell></cell></row><row><cell>SD</cell><cell>ERFNet-DKS [22] ERFNet-SAD [8]</cell><cell>70.6 71.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ERFNet-KD [6] ERFNet-SKD</cell><cell>70.5</cell><cell>10.2</cell><cell>2.49</cell></row><row><cell>TSD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance of different methods on LLAMAS-test.</figDesc><table><row><cell>Type</cell><cell>Algorithm</cell><cell>mAP</cell></row><row><cell></cell><cell>SCNN</cell><cell></cell></row><row><cell>Baseline</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1 to normalize the loss values. Here, we test different selections of the loss coefficients, i.e., selecting coefficient value from {0.05, 0.10, 0.15}. ERFNet-IntRA-KD achieves similar performance in all benchmarks, i.e., {43.18, 43.20, 43.17} mIoU in ApolloScape, {72.36, 72.39, 72.38} F 1 -measure in CULane and {0.597, 0.598, 0.598} mAP in LLAMAS.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised labeled lane markers using maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soussan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An embarrassingly simple approach for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengya</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dagmapper: Learning to map by discovering lane topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2911" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection CNNs by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to steer by mimicking features from heterogeneous auxiliary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8433" to="8440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lane marking detection based on adaptive threshold segmentation and road classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Biomimetics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Apolloscape dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qichuan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge distillation via instance relationship graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiang</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2604" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial CNN for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A general framework for road marking detection and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang Jie</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tirthankar</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Mh Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Frazzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International IEEE Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="619" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply-supervised knowledge synergy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Apolloscape open dataset for autonomous driving and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qichuan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the ResNet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Central moment discrepancy for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08811</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometric constrained joint lane segmentation and lane boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bidirectional feature pyramid network with recurrent attention residual modules for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
