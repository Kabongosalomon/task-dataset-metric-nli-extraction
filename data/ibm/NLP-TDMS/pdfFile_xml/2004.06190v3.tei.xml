<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1 A Divide-and-Conquer Approach to the Summarization of Long Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexios</forename><surname>Gidiotis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
						</author>
						<title level="a" type="main">IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1 A Divide-and-Conquer Approach to the Summarization of Long Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Summarization of long documents</term>
					<term>neural summarization</term>
					<term>text summarization</term>
					<term>natural language processing</term>
					<term>deep learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel divide-and-conquer method for the neural summarization of long documents. Our method exploits the discourse structure of the document and uses sentence similarity to split the problem into an ensemble of smaller summarization problems. In particular, we break a long document and its summary into multiple source-target pairs, which are used for training a model that learns to summarize each part of the document separately. These partial summaries are then combined in order to produce a final complete summary. With this approach we can decompose the problem of long document summarization into smaller and simpler problems, reducing computational complexity and creating more training examples, which at the same time contain less noise in the target summaries compared to the standard approach. We demonstrate that this approach paired with different summarization models, including sequence-to-sequence RNNs and Transformers, can lead to improved summarization performance. Our best models achieve results that are on par with the state-of-the-art in two two publicly available datasets of academic articles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S UMMARIZATION is closely related to data compression and information understanding, both of which are key to information science and retrieval. Being able to produce informative and well-written document summaries has the potential to greatly improve the success of both information discovery systems and human readers that are trying to quickly skim large numbers of documents for important information. Indeed, automatic summarization has been recently recognized as one of the most important natural language processing (NLP) tasks, yet one of the least solved ones <ref type="bibr" target="#b0">[1]</ref>.</p><p>This work is concerned with the neural summarization of long documents, such as academic articles and financial reports. In previous years, neural summarization approaches have mainly focused on short pieces of text that typically come from news articles <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This is also reflected in the amount of datasets that exist for this particular problem <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Summarizing long documents is a very different problem to newswire summarization. In academic articles for example, the input text can range from 2,000 to 7,000 words, while in the case of newswire articles it rarely exceeds 700 words <ref type="bibr" target="#b13">[14]</ref>. Similarly, the expected summary of a news article is less than 100 words long, while the abstract of an academic article can easily exceed 200 words.</p><p>The increased input and output length lead neural summarization methods to a much higher computational complexity, making it extremely hard to train models that have enough capacity to perform this task. This is more • A. Gidiotis is with the School of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece and also with Atypon Hellas, Vasilissis Olgas 212, Thessaloniki, Greece (e-mail: gidiotis@csd.auth.gr)</p><p>• G. Tsoumakas is with the School of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece (e-mail: greg@csd.auth.gr).</p><p>prominent with abstractive summarization models where the complexity of text generation becomes prohibitive for very long sequences. Most importantly, long documents introduce a lot of noise to the summarization process. Indeed, one of the major difficulties in summarizing a long document is that large parts of the document are not really key to its narrative and thus should be ignored. Finally, long summaries typically contain a number of diverse key information points from a document, which are more difficult to produce, compared to the more focused information contained in short summaries. Certain methods have tried to address these problems by limiting the size of the input document, either by selecting specific sections that are more informative <ref type="bibr" target="#b14">[15]</ref>, or by first employing a more efficient extractive model that learns to identify and select the most important parts of the input <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>. While this reduces the noise and the computational cost in processing a long document, there remain the computational cost and information diversity issues in producing a long summary. In the context of Transformer models with self-attention, sparse attention mechanisms such as Big Bird <ref type="bibr" target="#b18">[18]</ref> manage to increase the input length by a large amount but they still cannot scale to very long summaries.</p><p>In contrast to the above methods that aim to produce a complete summary at once, we propose a novel divideand-conquer approach that first breaks both the document and its target summary into multiple smaller source-target pairs, then trains a neural model that learns to summarize these smaller document parts, and finally during inference aggregates the partial summaries in order to produce a final complete summary. By decomposing the problem of long document summarization into smaller summarization problems, our approach reduces the computational complexity of the summarization task. At the same time, our approach increases the number and, more importantly, the arXiv:2004.06190v3 [cs.CL] <ref type="bibr" target="#b23">23</ref> Sep 2020 quality of the training examples by having source and target summary pairs that are focused on a specific aspect of the text, which results in better alignment between them and less noise. This leads to a decomposition of the summarization problem into simpler summarization problems that are easier to learn. Empirical results on two publicly available datasets of academic articles, show that our approach can enhance the ability of summarization models and lead to overall improved results. We show that using a 3 years-old sequence-to-sequence model <ref type="bibr" target="#b3">[4]</ref>, our approach manages to achieve surprisingly good results, surpassing recent more advanced models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In addition, when paired with a very strong Transformer model such as PEGASUS <ref type="bibr" target="#b19">[19]</ref> our method produces results that are on par with the state-ofthe-art on both datasets. This paper is based on past work <ref type="bibr" target="#b20">[20]</ref> that assumed the existence of structured summaries, such as those available for some of the biomedical articles indexed in PubMed.</p><p>Here we lift this assumption by using sentence level Rouge similarities in order to match sentences of the summary with parts of the document and automatically create sourcetarget pairs for training. This is a key advancement, since the vast majority of academic documents are not accompanied by structured abstracts. Also, such an approach makes this work applicable to any type of document, from academic articles to blog posts and financial documents. Ultimately, our proposed method allows advanced summarization methods to be used in a number of different applications that previously might have been infeasible.</p><p>The rest of this work is structured as follows. Section 2 gives a brief overview of the related work. Section 3 describes in detail the proposed method. Section 4 presents the experimental setup and Section 5 discusses the results of our experiments. Finally, Section 6 concludes this works and points to future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A variety of solutions have been proposed to the problem of automatic summarization. These include simple unsupervised methods <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, graph-based methods that involve arranging the input text in a graph and then using ranking or graph traversal algorithms in order to construct the summary <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> and neural methods, which are discussed in more detail in the following subsection. Subsequently we review related work on long document summarization and summarization of academic articles, which are the most common type of long documents in the summarization literature. Finally, we provide and overview of summarization datasets with emphasis on academic article summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural text summarization</head><p>Closely following the advances in neural machine translation <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref> and language modeling <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref> and, fueled by the increased availability of computational resources as well as large annotated datasets <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, neural summarization is nowadays achieving state-of-theart results.</p><p>Extractive methods aim to select the salient sentences from the input and combine them, typically by concatenation, in order to generate a summary. This is usually approached as a binary classification problem, where for each sentence the model decides whether it should be included in the summary or not <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>. On the other hand, abstractive methods try to encode the input into a hidden representation and then use a decoder conditioned on that representation to generate the summary <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b34">[34]</ref>. In addition to these two main categories, there also exist hybrid approaches that combine both extractive and abstractive methods either by using pointer-generators <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b35">[35]</ref> or by fusing extractive and abstractive models <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>.</p><p>In order to encode the input text, different methods are using different variations of encoders based on RNNs <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b35">[35]</ref> or convolutional networks <ref type="bibr" target="#b15">[16]</ref>. One notable addition here is <ref type="bibr" target="#b36">[36]</ref> which introduces the Rotational Unit of Memory (RUM). RUM is a different type of RNN unit that can be superior to conventional LSTMs in some summarization scenarios. Finally, given the increased popularity and success of large pre-trained Transformers <ref type="bibr" target="#b29">[29]</ref> in various NLP tasks, many recent methods employ Transformer models <ref type="bibr" target="#b5">[6]</ref>. Towards that direction a variety of pre-training objectives have been suggested that are better suited for the task of abstractive summarization <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[19]</ref>.</p><p>In an effort to enhance performance and address some common shortcomings of neural summarization models, policy learning <ref type="bibr" target="#b37">[37]</ref> has been proposed <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref> to further improve summarization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Long document summarization</head><p>Most of the aforementioned approaches are mainly focused on summarizing short documents (e.g. news articles), in order to produce short summaries (e.g. headlines).</p><p>In cases where the input and target sequences are longer, for example academic articles, the complexity of models that process the complete article at once increases dramatically making such methods infeasible. Different approaches attempt to solve this problem by exploiting the structure of a document. For example, <ref type="bibr" target="#b35">[35]</ref> makes use of multiple "encoder agents" where each one processes a different paragraph of the input. A decoder is based on the hidden states of all agents in order to generate the final summary. The model is trained end-to-end using a combination of Maximum Likelihood Estimation (MLE) and Reinforcement Learning (RL) objectives. This approach exploits the structure of an article to shorten the input sequences of each encoding agent. On the other hand, the dependency between encoder agents makes it hard to parallelize, while the single decoder still experiences the same difficulties with long output sequences.</p><p>Also, <ref type="bibr" target="#b15">[16]</ref> uses a hybrid model with an "extractor agent" that selects salient sentences and an "abstractor agent" that re-writes each of the extracted sentences separately. Each submodel is trained separately with MLE and then the full end-to-end model is trained with RL. This model achieves significant improvements in performance as well as speed during training and decoding. However, while this method, effectively reduces the complexity of the abstractive model that works on single sentences, the extractive model still has to process the whole document. Although extractive models are much more efficient when processing long sequences, there is a limit to the amount of information they can process at once.</p><p>Lastly, Big Bird <ref type="bibr" target="#b18">[18]</ref> tries to deal with the problem of long document summarization by replacing the full self-attention of Transformer models with a sparse attention mechanism that can scale to inputs that are many times longer. This helps the model use a lot more context when summarizing a document and scale to a lot longer sequences without losing the advantages of full attention. Nevertheless, this method might struggle to scale to documents of arbitrary length and does little to exploit the underlying structure of documents.</p><p>In contrast, we treat each section of the text as a separate summarization instance and as a result our method is easily parallelizable. Furthermore, each summarization instance has to deal with significantly shorter input and output sequences than each of these methods. By exploiting the structure of a document it is possible to scale to documents of arbitrary length such as review papers or financial reports. On the downside, the lack of communication during the summarization of different sections may lead to sectionlevel repetitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Summarizing academic articles</head><p>Existing approaches for summarizing academic articles include extractive models that perform sentence selection <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref> and hybrid models that first select and then re-write sentences from the full text <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In addition, the Pre-training with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-sequence (PEGASUS) <ref type="bibr" target="#b19">[19]</ref> model is a Transformer encoder-decoder pre-trained on massive corpora of documents (Web and news articles) that has demonstrated great potential on various summarization benchmarks, including academic articles. The optimization objective of PEGASUS is called Gap Sentence Generation (GSG), where whole sentences of the input are masked and the model attempts to generate these gap-sentences from the rest of the input. This objective was proposed by the authors of the PEGASUS paper, because it is better aligned with the summarization task and allows for better adaptation of the model during fine-tuning and overall improved performance. The performance and scaling capabilities of PEGASUS can be further improved with the addition of the sparse attention mechanism of Big Bird.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Summarization datasets</head><p>A number of publicly available datasets of short articles, such as the New York Times <ref type="bibr" target="#b10">[11]</ref>, Gigaword <ref type="bibr" target="#b11">[12]</ref>, CNN/Daily Mail <ref type="bibr" target="#b9">[10]</ref> and Newsroom <ref type="bibr" target="#b12">[13]</ref> are commonly used as a benchmark for many of the earlier summarization methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b43">[43]</ref>. When focusing on the task of academic article summarization, several large scale datasets have been introduced. The arXiv and PubMed datasets <ref type="bibr" target="#b13">[14]</ref> were created using open access articles from the corresponding popular repositories. PMC-SA [20] is a dataset of open access articles from PubMed Central, where the abstract of each article is structured into sections similar to the full text. Finally, the Science Daily dataset <ref type="bibr" target="#b36">[36]</ref> was created by crawling stories from the Science Daily web site <ref type="bibr" target="#b0">1</ref> . Each story is about a recent scientific paper and is also accompanied by a short summary that is used as target for training and evaluation.</p><p>In addition to the datasets mentioned above, there is also the TAC2014 biomedical summarization dataset 2 . TAC2014 contains 20 topics, each consisting of one reference article and several articles citing it. Additionally, each reference article is accompanied by four scientific summaries that are written by domain experts. This dataset has been used in the earlier literature <ref type="bibr" target="#b41">[41]</ref>, but since it is rather small, it is not suitable for the training of neural summarization models. Another more recent dataset that is focused on scientific articles from the Computational Linguistics domain is the dataset of the CL-SciSumm 2016 shared task <ref type="bibr" target="#b44">[44]</ref>. It is composed of 30 annotated sets of open access citing and reference papers accompanied by hand-written summaries. This is also a rather small dataset that is not suitable for neural summarization approaches. Finally, ScisummNet <ref type="bibr" target="#b45">[45]</ref> is a medium scale dataset of 1,000 articles from the Computational Linguistics domain that are manually-annotated for summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>We propose a divide-and-conquer approach for the summarization of long documents. In this section, we present a training algorithm for the partial summarization systems as well as the methodology we are following at prediction time. Finally, we discuss different model variants that can be combined with this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Divide-and-conquer summarization</head><p>We argue that a very efficient way of dealing with long documents is to train a summarization model that learns to summarize separately the different sections of the document. Our approach assumes that long documents are structured into discrete sections and exploits this discourse structure by working on each section separately. Each section of the document is treated as a different example during the training of the model by pairing it with a distinct summarization target.</p><p>A first idea for achieving this pairing would be to use the whole summary of the document as target for each different section. However, this approach would be problematic for a couple of reasons. First of all, having very long target sequences is very demanding in terms of computational resources. This problem would be even more apparent if instead of an RNN model we decided to use a Transformerbased model, since the computational complexity and memory requirements of full attention Transformers explode for very long sequences. Secondly, the summary will most likely include information that is irrelevant to some sections of the document. For example, information about the conclusions of an academic article in its abstract will most likely be irrelevant to the section describing the methods. As a result, it would be impossible for the model to generate these parts of the target sequence and this may result in poor performance.</p><p>We introduce Divide-ANd-ConquER (DANCER) summarization, a method that automatically splits the summary of a document into sections and pairs each of these sections to the appropriate section of the document, in order to create distinct target summaries. Splitting a summary into sections is not straightforward, apart from the limited case of structured abstracts of academic articles <ref type="bibr" target="#b20">[20]</ref>. In DANCER we employ ROUGE metrics <ref type="bibr" target="#b46">[46]</ref> in order to match each part of the summary with a section of the document. Similar to <ref type="bibr" target="#b14">[15]</ref>, a summary is represented as a list of M sentences A = (a 1 , . . . , a M ). In addition, each document is represented as a list of K sections (s 1 , . . . , s K ) and each section s k of the document as a list of N sentences s k = (s k 1 , . . . , s k N ). We compute the ROUGE-L precision, between each sentence of the summary a m and each sentence of the document s k n . Given two word sequences x = (x 1 , . . . , x I ) and y = (y 1 , . . . , y L ) with lengths I and L respectively, the longest common sub-sequence (LCS) is the common subsequence with the maximum length. If LCS(x, y) is the length of the longest common sub-sequence of x and y, then ROUGE-L precision between x and y, P LCS (x, y) is computed as follows:</p><formula xml:id="formula_0">P LCS (x, y) = LCS(x, y) L<label>(1)</label></formula><p>In more detail, once we have computed the ROUGE-L precision between the summary sentence a m and all the sentences of the document, we find the full text sentence s kmax nmax with the highest ROUGE-L precision score and we assign a m to be part of the summary of section k max . We repeat this process until all sentences of the summary have been assigned to one document section. Then we group all summary sentences by section and concatenate the sentences corresponding to the same section in order to create the target summary for that section.</p><p>This approach is mainly inspired by the input-target sentence alignment method that is commonly used to create sentence level targets for extractive summarization <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b43">[43]</ref>. In the extractive summarization context, ROUGE metrics are used in order to match each target sentence with the most similar input sentence. We extend this idea and use the most similar input sentence as an indicator to find the most relevant section of the input for each target sentence and then group target sentences based on their corresponding sections. Other sentence similarity metrics such as BLEU could also be explored in this setup but we leave this for future work.</p><p>During training, each section of the document is used as input text and the corresponding part of the summary is the target summary. The training itself is performed with simple teacher forcing <ref type="bibr" target="#b47">[47]</ref>, where we are minimizing the negative log likelihood of the target summary sequence y = (y 1 , . . . , y N ) given the input sequence x.</p><formula xml:id="formula_1">loss = − N t=1 logP (y t |y 1 , . . . y t−1 , x)<label>(2)</label></formula><p>We have found that this training strategy has several advantages over other methods proposed in the literature.</p><p>Firstly, by breaking down the problem into multiple smaller problems we greatly reduce the complexity and make it much easier to solve. We believe that this is a very efficient way to approach the summarization of long documents, since it greatly reduces the length of both the input and more importantly, the output sequences. Also, since the target summaries for each section are selected based on the ROUGE-L scores of each sentence, we create a better and more focused matching between the source and the target sequences and avoid having parts of the target summary that are irrelevant to the input sequence. This property prevents us from penalizing the model for not predicting information that was absent in the input text.</p><p>Secondly, by splitting each training document into multiple input-target pairs we create a lot more training examples. This is especially beneficial for neural summarization models because by splitting each document into multiple examples we can effectively make use of more training content. This becomes clearer if we think of a neural summarization decoder as a conditional language model that cannot process an unlimited amount of text from each training example. The way that we approach the training allows us to effectively distribute the source and target texts into more training examples and thus enable us to train our model on a larger amount of textual content which leads to improved output quality.</p><p>Finally, the method itself is simple and model agnostic and can employ different summarization models, from encoder-decoder RNNs to Transformers. It can also be combined with other more sophisticated methods that perform sentence extraction before the main summarization process, since it has been observed that pointer neural networks sometimes struggle at selecting relevant parts of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Section selection</head><p>When working with long structured documents it is usually the case that not all sections of the document are key to the document. If we take as an example an academic article, sections like literature review or background are not essential when trying to summarize the main points of the article. On the other hand, sections like the introduction and conclusion usually include quite a lot of the important information that we want to include in the summary. Another similar example would be financial reports that are also structured in sections. Some of those sections, usually referred to as "front-end" sections, include key information and reviews that are core to the narrative, while others consist mostly of financial statements and are less useful for producing a summary <ref type="bibr" target="#b48">[48]</ref>.</p><p>What's more, by trying to include sections that are not really important to the overall summary we can possibly end up adding a lot of noise and overall reducing the quality of the generated summary. With that in mind we decided that by selecting specific section types and only including those into the summary we can improve the overall quality of the summarization results.</p><p>We are following the same approach described in <ref type="bibr" target="#b20">[20]</ref> in order to select the sections we want to use for summarization. First we classify each section into different section types like introduction, methods and conclusion based on a  heuristic keyword matching of some common keywords in the section header. The specific keywords used for the classification are presented in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Based on experiments on the arXiv and PubMed datasets, we have found that for each document the abstract has on average ∼ 6.5 and ∼ 6.3 sentences respectively. After classifying the article sections and pairing them with the target summaries created by DANCER we end up having the distribution of target sentences per section type shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>From this distribution we observe that the majority of summary sentences, especially for the arXiv dataset, are assigned to the introduction section followed by the methods and conclusion sections. The results section is paired with significantly fewer sentences while the literature section is almost never matched with any summary sentences. Based on that observation, when generating the summary we select and use only the sections of the full text that are classified introduction, methods, results and conclusion ignoring the literature section.</p><p>This simple method very effectively allows us to filter out parts of the article that are less important for the summary, like the literature review, and leads to summaries that are more focused.</p><p>One of the obvious weaknesses of this method is that in some articles the section headers cannot be matched by the heuristic rules and as a result they will be discarded by the heuristic method. Exploring more sophisticated methods that use machine learning to identify the type of each section should be explored in future work. Although these section categories are meaningful when working on academic articles, if the proposed method is extended to different domains (e.g. financial documents), then a new categorization of sections would be required. Towards that direction, a sound idea would be to use machine learning in order to do the section selection. In that scenario a machine learning model can be used in order to make the decision if a given section should be included in the summary. This direction that closely resembles hybrid extractive-abstractive summarization models (although it works on a section level instead of a sentence level) also requires further exploration in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model variants</head><p>Here we will describe the different summarization models that we combined with DANCER for our experiments. The first model is an RNN based Pointer-Generator model similar to <ref type="bibr" target="#b3">[4]</ref> in two different variants. The second is the PEGASUS model <ref type="bibr" target="#b19">[19]</ref> which is based on Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Pointer-Generator</head><p>The Pointer-Generator model is based on the sequence-tosequence RNN paradigm that has been widely adopted in the pre-Transformer literature. The sequence-to-sequence architecture includes an encoder of bidirectional LSTM units that encodes the input in it's hidden state and a unidirectional LSTM decoder that autoregressively generates the output one word at a time. Given an input sequence x = (x 1 , . . . , x T ) the encoder produces a sequence of hidden states h. On each time step t the decoder takes as input the encoder state h, the previous word and has a hidden state s t .</p><p>This model is also equipped with an attention mechanism similar to <ref type="bibr" target="#b28">[28]</ref> that generates an attention distribution at each decoder step as in equations 3 and 4 where v, W h , W s and b attn are learned during training.</p><formula xml:id="formula_2">e t = v T tanh(W h h + W s s t + b attn )<label>(3)</label></formula><formula xml:id="formula_3">α t = sof tmax(e t )<label>(4)</label></formula><p>From the attention distribution we produce a context vector h * t as shown in equation 5. The context vector is a sum of the encoder hidden states weighted by the attention distribution α t .</p><formula xml:id="formula_4">h * t = T i=1 α t i h i<label>(5)</label></formula><p>The context vector is concatenated with the decoder state s t and fed through two linear layers to produce the vocabulary distribution P vocab as shown in equation 6. This is essentially the probability distribution over all words in the vocabulary given the input sequence x and the sequence y generated so far. Again here V , V , b and b are learnable parameters.</p><formula xml:id="formula_5">P vocab = sof tmax(V (V [s t , h * t ] + b) + b )<label>(6)</label></formula><p>Finally, the model also uses a copying mechanism <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b49">[49]</ref> that has the ability to copy a specific token directly from the input based on a switch mechanism. The token generated at each time step is determined by the vocabulary distribution P vocab and the pointer generator probability p gen as shown in equations 7 and 8. Vectors w T h * , w s , w t x and scalar b ptr are learnable parameters, σ is the sigmoid function and P f inal (w) is the probability of generating word w.</p><formula xml:id="formula_6">p gen = σ(w T h * h * t + w s s t + w t x x t + b ptr ) (7) P f inal (w) = p gen P vocab (w) + (1 − p gen ) i:wi=w α t i<label>(8)</label></formula><p>This specific model architecture was proposed by <ref type="bibr" target="#b3">[4]</ref> and its variants have been adopted in various other works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b35">[35]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> better illustrates the full Pointer-Generator model.</p><p>One of the advantages of this model is the ability to both extract tokens from the input and generate new tokens with a language model. The language model has the ability to rewrite parts of the text and improve the fluency of the generated text. The copying mechanism is especially important in the case of scientific articles because they include a lot of out-of-vocabulary technical terms as well as symbols. Those cannot possibly be covered by a fixed vocabulary since this will lead to a huge vocabulary and thus make the computational cost of the embedding and softmax layers prohibitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Rotational Unit of Memory</head><p>Incorporating rotational units of memory (RUM) into a sequence-to-sequence model can lead to improved summarization results <ref type="bibr" target="#b36">[36]</ref>. In particular, including RUM units in the model results in larger gradients during training thus leading to a more stable training and better convergence. In contrast, the gates of LSTM units typically have tanh activation functions and as a result the gradients very quickly become small despite using gradient clipping. We created a variant of our model, where we replaced the LSTM units of the decoder with RUM units. We decided to keep the LSTM units for the encoder, since it has been shown that a mixture of both unit types is usually advantageous <ref type="bibr" target="#b36">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">PEGASUS</head><p>The PEGASUS model is a Transformer based sequence-tosequence model that is pre-trained on massive corpora of unsupervised data (Web and news articles). The model itself is a standard Transformer encoder-decoder similar to <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>. The pre-trained model can be further fine-tuned for summarization tasks and is selected here because it has demonstrated great potential on various summarization benchmarks, including CNN/Daily Mail <ref type="bibr" target="#b9">[10]</ref>, Gigaword <ref type="bibr" target="#b34">[34]</ref>, NEWSROOM <ref type="bibr" target="#b12">[13]</ref>, arXiv and PubMed <ref type="bibr" target="#b13">[14]</ref>.</p><p>What makes the PEGASUS model a promising approach for the summarization task is its pre-training strategy. Gap Sentence Generation (GSG) is a self-supervised objective engineered specifically for abstractive summarization. By masking whole sentences from a document and generating these gap-sentences from the rest of the document encourages the model to understand the whole-document and generate sentences in a summary-like fashion. In addition, they propose a strategy that aims to choose important sentences for masking rather than randomly selected ones.</p><p>One key difference of the PEGASUS model with the Pointer-Generator model is that it operates at the level of subword tokens instead of word tokens. This is a common practice for many Transformer models <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref> and enables the model to learn and use a wide variety of words with only a limited vocabulary. In particular, the pre-trained version of PEGASUS uses a vocabulary built with the Sen-tencePiece Unigram algorithm <ref type="bibr" target="#b50">[50]</ref> although the authors of the paper also experimented with Byte Pair Encoding (BPE) <ref type="bibr" target="#b51">[51]</ref>. The use of subword vocabularies is in fact so effective that there is no need to employ copying mechanisms in the context of this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Compiling the article summary</head><p>When we are generating the summary of an article, the following steps are taken. We split the article in sections and select the appropriate sections to use. Then we autoregressively generate a summary for each section of the input text using simple beam search decoding <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b53">[53]</ref>. Finally, we compose the complete summary by concatenating the individual summaries.</p><p>Since the summarization of each section is independent of the other sections, our approach is highly parallelizable. At test time, we can very easily process all sections of the document in parallel and thus make the summary generation a lot faster. This can be ideal for systems that are trying to offer summarization as an online service, where the efficiency of the model is an important factor.</p><p>One common problem with this type of generative models is that parts of the input might be attended multiple times resulting in repetitions and, in certain situations, the whole decoded sequence may end up in a degenerate repetitive text. This behavior is especially prominent in RNN models. In order to deal with this issue, multiple different approaches have been proposed in the literature. We avoided using the coverage mechanism proposed in <ref type="bibr" target="#b3">[4]</ref>, since this approach modifies the training strategy and adds more complexity to the model. Instead, for our Pointer-Generator model, we opted for a simpler yet effective approach that tries to deal with repetition at the decoding phase and was proposed by <ref type="bibr" target="#b4">[5]</ref>. During beam search decoding we prevent the decoder from outputting the same trigram multiple times. In order to do this we set the output probability p(y t ) = 0, when outputting y t would create a trigram already existing in the generated hypothesis of the current beam. For the PEGASUS model, based on our experimental results there was a minimal number of repetitions within the section summaries generated by the model. This means that we did not need to use a repetition avoidance mechanism.</p><p>Although the aforementioned methods can effectively deal with word and sentence level repetitions, they cannot deal with section level repetitions. Since each individual summary does not have access to the summaries of other sections it is possible that certain information might be repeated in multiple section summaries. The exploration of different strategies that can address this issue is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>Here we describe the experiments we conducted with DANCER and the different summarization models on two different datasets in order to demonstrate the effectiveness of the method. We first introduce the two datasets and present the details of the models we are using as well as the training and evaluation setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We employed two large-scale publicly available summarization datasets that focus on scientific papers, namely arXiv and PubMed <ref type="bibr" target="#b13">[14]</ref>. The arXiv dataset was created directly from L A T E X files that were taken from the arXiv repository of electronic preprints. The files were processed and converted to plain text using Pandoc 3 to preserve section information. All citation markers and math formulas were replaced by special tokens. The resulting dataset includes approximately 215k documents with abstracts. The average full text length is 6,913 words and the average abstract length is 292 words. <ref type="bibr" target="#b2">3</ref>. https://pandoc.org</p><p>The PubMed dataset was created from the XML files that are part of the Open Access collection of the PubMed Central (PMC) repository. In contrast to the arXiv dataset, the citation markers were completely removed, while the math equations were converted to plain text. This dataset consists of approximately 133k documents with abstracts. The average full text length is 3,224 words and the average abstract length is 214 words.</p><p>Although there is an obvious inconsistency between the pre-processing steps applied to the two datasets we decided to not perform any additional pre-processing in order to be comparable with previously published work. For the same reason we use the predefined training, validation and test set splits. Both datasets are already processed in such a way that only the first level section headings are used as section information and all subsections headings were included as plain text. Also, all figures and tables have already been removed along with text styling options for both datasets. As discussed in Section 3, our method splits each document into multiple training examples based on the discourse structure of the document. As a result, we end up with a lot more training examples than documents. Detailed statistics for both datasets are presented in <ref type="table" target="#tab_1">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pointer-Generator</head><p>Our LSTM Pointer-Generator model is implemented in Tensorflow and is based on the original implementation 4 of <ref type="bibr" target="#b3">[4]</ref>. The hyperparameter selection is similar to the setup suggested in <ref type="bibr" target="#b3">[4]</ref>. Our model has a bidirectional LSTM layer of 256 units for the encoder and a unidirectional LSTM layer of 256 units for the decoder. We restrict the vocabulary to 50,000 word tokens for both the input and output and use word embeddings of size 128. We do not use pre-trained word embeddings, but rather learn them from scratch during training, as suggested in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Our models were trained on a single Nvidia 1080 GPU with a batch size of 16. We train all of our models using Adagrad <ref type="bibr" target="#b54">[54]</ref> with 0.15 learning rate and initialize the accumulator to 0.1. We clip the gradients to have a maximum norm of 2, but avoid using any regularization. During training we are regularly (every 3,000 steps) measuring the loss and the ROUGE-1 F-score on the validation set of the dataset in order to monitor the learning of our model. We end the training when the validation loss stops improving.</p><p>For the training, input sequences are truncated to 500 word tokens while padding the shorter ones with zeros to the same length. In our experiments we found that the target sequences created with DANCER rarely exceed 100 words and the average target length is 69 words as shown in <ref type="table" target="#tab_1">Table  2</ref>. Considering that fact we restrict the length of each target summary to the first 100 words for computational efficiency. We have found that it is preferable to train with the full length sequences from the beginning of the training rather than starting off with highly truncated sequences and then increasing the sequence length after convergence. This is in contrast to the common practice suggested in <ref type="bibr" target="#b3">[4]</ref>. We believe one possible reason might be that training with very short and generic sequences first could lead the model to converge into a local optimum and have a hard time getting out of there once the sequence length is increased.</p><p>For the prediction phase, we use beam search decoding with 4 beams and generate a maximum of 120 tokens per section. We are also using the mechanism described previously to avoid repeating the same trigrams. Once we have generated a summary for each section, we concatenate the generated summaries in order to get the final summary.</p><p>For the RUM variant of the Pointer-Generator model we keep the encoder part the same but we replace the 4. https://github.com/abisee/pointer-generator LSTM units of the decoder with RUM units. The RUM unit implementation is taken from the original code 5 of <ref type="bibr" target="#b36">[36]</ref>. All other parameters are similar to the ones used for the LSTM based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">PEGASUS</head><p>We are using the pre-trained PEGASUS model and the Tensorflow code <ref type="bibr" target="#b5">6</ref> that was open-sourced by the authors of the paper. The model itself is the PEGASUS LARGE model described in the paper. It has 16 Transformer blocks for the encoder and decoder with hidden size of 1,024 units, 16 selfattention heads and feed-forward layer size of 4,096 units. It is pre-trained on a combination of the C4 and HugeNews datasets with the GSG objective. We are using the checkpoints open sourced by the authors of the PEGASUS paper to initialize our model and further fine-tune them using DANCER on the arXiv and PubMed datasets.</p><p>Our models are fine-tuned on a cloud compute instance with a single Nvidia Tesla T4 GPU. We fine-tune using Adafactor <ref type="bibr" target="#b55">[55]</ref> with a learning rate of 0.0001 and a batch size of 6 due to GPU memory limitations. During our fine-tuning we are using input sequences of 512 subwords and target sequences of 128 subwords. This is different than the original PEGASUS setup that uses 1,024 and 256 subwords respectively again due to limited resources. The rest of the hyper-parameters are identical to the ones used in the original paper. The subword vocabulary used is the Unigram vocabulary that was built and open sourced by the PEGASUS paper and has 96,000 subwords. The arXiv model is fine-tuned with DANCER for 60k steps, while the PubMed model is fine-tuned for 40k steps. Our models were not extensively fine-tuned since this was outside the scope of our paper. Therefore, additional hyper-parameter tuning and more fine-tuning steps could potentially lead to even better performance.</p><p>For the prediction phase, we use beam search decoding with 5 beams and generate a maximum of 128 tokens per section and then combine them to get the final summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and state-of-the-art methods</head><p>We compare DANCER with several well known extractive and abstractive baselines as well as state-of-the-art methods. The baseline methods we are comparing against are a simple Lead-10 extractor, which extracts the first 10 sentences of the input, LexRank <ref type="bibr" target="#b23">[23]</ref>, SumBasic <ref type="bibr" target="#b22">[22]</ref>, LSA <ref type="bibr" target="#b21">[21]</ref>, Attention Seq2Seq <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b34">[34]</ref>, Pointer-Generator Seq2Seq <ref type="bibr" target="#b3">[4]</ref>, Discourse-Aware Summarizer <ref type="bibr" target="#b13">[14]</ref> Sent-CLF, Sent-PTR and TLM-I+E <ref type="bibr" target="#b14">[15]</ref>. Attention Seq2Seq is an abstractive sequenceto-sequence model with attention. Pointer-Generator is similar to our LSTM Pointer-Generator model without DANCER. Discourse-Aware Summarizer is a hierarchical extension of the Pointer-Generator model. Sent-CLF and Sent-PTR are extractive models also based on hierarchical LSTMs whith Sent-CLF treating the sentence selection as a sequence classification problem while Sent-PTR uses a sentence pointer to select which sentences to extract. TLM-I+E is a hybrid model that first uses either Sent-PTR or Sent-CLF to extract sentences and then a Transformer language model 5. https://github.com/rdangovs/rotational-unit-of-memory 6. https://github.com/google-research/pegasus </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head><p>The results of our experiments on the arXiv and PubMed datasets are shown in <ref type="table" target="#tab_2">Tables 3 and 4</ref> respectively. We are reporting the full-length F-score of the ROUGE-1, ROUGE-2 and ROUGE-L metrics <ref type="bibr" target="#b46">[46]</ref> computed using the official pyrouge package 7 . All our reported ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE script. The results of SumBasic, LexRank, LSA, Attention Seq2Seq, Pointer-Generator Seq2Seq and Discourse-Aware Summarizer are taken directly from <ref type="bibr" target="#b13">[14]</ref>, while the results of Sent-CLF, Sent-PTR and TLM-I+E come from <ref type="bibr" target="#b14">[15]</ref>. Finally, results of PEGASUS and BigBird-PEGASUS are taken from <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Transformer vs LSTM vs RUM</head><p>Looking at the comparisons between the different DANCER variants we can see that the PEGASUS model is the clear winner. This is expected since it is a much more powerful and advanced model which makes use of extensive unsupervised pre-training. The price for the better performance is the increased requirements in terms of memory and processing power of PEGASUS compared to the simpler RNN models. On the other hand both RNN models exhibit similar performance with the RUM model outperforming the LSTM model on the arXiv dataset, while performing slightly worse on PubMed. Given the observation that LSTM based models tend to copy more phrases from the source than RUM based models <ref type="bibr" target="#b36">[36]</ref>, we hypothesize that the target abstracts in PubMed include a higher amount of text that is copied directly from the full text, compared to arXiv. <ref type="bibr" target="#b6">7</ref>. https://pypi.org/project/pyrouge/0.1.3  <ref type="figure">Fig. 3</ref>. The percentage of N-grams that are copied directly from the source to the target summary for both datasets. The percentages are high for both datasets but for the PubMed dataset we observe a higher percentage of copied 2-grams, 3-grams, 4-grams. This implies that the abstracts of the articles are in fact very much extractive and as a result this dataset favors extractive approaches more.</p><p>In order to validate this hypothesis we computed the percentage of n-grams in the target summaries that are copied from the source. In <ref type="figure">Figure 3</ref> we show these percentages for both datasets. It is clear that the target abstracts in the PubMed dataset have a greater percentage of copied 2grams, 3-grams and 4-grams compared to the arXiv dataset.</p><p>In addition, we found that when using a decoder with RUM units, the training is more stable than when using a decoder with LSTM units and converges steadily at a lower loss value. This is in line with the observation that RUM based models exhibit larger gradients and as a result have more robust training compared to LSTM based models <ref type="bibr" target="#b36">[36]</ref>. On the other hand, we also found that models with a RUM based decoder need more steps to converge to the final loss, compared to models with an LSTM based decoder.</p><p>Overall, based on our experiments and analysis of the three different DANCER models, we conclude that the DANCER PEGASUS model is clearly superior to the RNN models. Nevertheless, when combined with DANCER both RNN models achieve a surprisingly good performance, although not as good as the PEGASUS model. This leads us to believe that there might still be some merit in using RNN sequence-to-sequence models especially in some low resource scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DANCER vs baselines and the state of the art</head><p>Based on the numbers of tables 3 and 4 we can see that DANCER works well with both the Pointer-Generator and the PEGASUS model. DANCER improves the performance of the Pointer-Generator model by almost 10 ROUGE-1 points which is a very significant improvement considering that the underlying model is the same. This model is also on par with models such as TLM-I+E as well as Sent-PTR despite using a significantly simpler architecture.</p><p>When combined with stronger models such as PEGASUS we can see that DANCER can still lead to improved results with minimal additional effort and resources. Moreover, we empirically show that DANCER can take advantage of strong pre-trained models, such as PEGASUS, and increase the effectiveness of task specific fine-tuning.</p><p>Our experiments show that DANCER PEGASUS is on par with the BigBird-PEGASUS model, which is the current state-of-the-art, without modifying the underlying model architecture of PEGASUS. With that in mind, it is possible that a combination of BigBird-PEGASUS with DANCER could further improve results although it was not in the scope of this work to explore that. Furthermore, more extensive optimization of the DANCER PEGASUS model, as well as additional training could lead to even better results but this was not the focus of this work. In general, the experimental results suggest that DANCER is a very easy to implement way to boost the performance of different summarization models with minimal additional effort and resources.</p><p>Going back to <ref type="figure">Figure 3</ref>, we notice that both datasets have a high percentage of text copied directly from the source, which explains the high performance of all extractive approaches, even simple ones, like LexRank and Lead-10. Usually it is way easier for extractive models to achieve higher ROUGE scores due to the way that ROUGE metrics are calculated. Since the metric is purely based on the overlap of the the generated text with the target text and in many cases the target summary includes a parts that are copied from the source input, ROUGE scores clearly favor extractive summarization approaches. Nevertheless, we can see that advanced abstractive models such as PEGASUS, BigBird-PEGASUS and DANCER PEGASUS manage to outperform most extractive models by a significant margin. This is important since abstractive summarization is a more challenging task, but also more closely resembles the way humans do summarization.</p><p>In the Appendix of this paper we present sample summaries for a couple of papers generated by our models trained on the arXiv dataset. These samples demonstrate the quality of the summaries we can produce using our proposed methods as well as directly compare the outputs produced by the different summarization models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented DANCER, a novel summarization method for long documents. We focused on the summarization of aca-demic articles, but the same method can easily be applied to different types of long documents, such as financial reports. We have demonstrated quantitatively through experiments on the arXiv and PubMed datasets that this method combined with a basic sequence-to-sequence RNN model can still achieve good performance. We also show that using a stronger model such as PEGASUS we can achieve results that are on par with the state-of-the-art on both datasets.</p><p>We have also evaluated the advantages of using a combination of LSTM and RUM units inside the sequence-tosequence model in terms of ROUGE F1 as well as training stability and convergence. We have found that including RUM units in the decoder of the model can lead to a more stable training and better convergence as well as improved ROUGE scores, when the target sequence includes less text directly copied from the source sequence.</p><p>Overall, we have focused on the effectiveness of our proposed method regardless of the complexity of the core model. We emphasize that DANCER is a simple yet effective extension that can boost the performance of different summarization models with minimal additional effort and resources. In future work we would like to combine DANCER with more complex summarization models that could potentially further improve summarization quality as well as apply DANCER summarization on domains other than academic articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX EXAMPLES OF GENERATED SUMMARIES</head><p>In order to demonstrate the high quality of the abstracts produced by our models, we generated summaries from a couple of notable papers in our field. We are presenting examples generated from both the DANCER LSTM Pointer-Genarator and DANCER PEGASUS models as well as the ROUGE1-F1 scores for comparison. The models used to generate those summaries were trained on the arXiv dataset. We also provide the original abstract of each paper for reference and comparison purposes.</p><p>In the case of the first paper shown in <ref type="table" target="#tab_4">Table 5</ref> we see that although the PEGASUS model performs better in terms of ROUGE1 score, neither of the two models achieve very good results and both generated summaries are significantly different from the original abstract. On the other hand, for the second paper shown in <ref type="table">Table 6</ref> we see that both models achieve high scores with the Pointer-Generator model outperforming the PEGASUS model. , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement) Pointer-Generator summary Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Radford et al., 2018). The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The BERT model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. These results enable the same pre-trained model to successfully tackle a broad set of NLP tasks. In particular, these findings enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures. ROUGE1-F1 35.0 PEGASUS summary Language model pre-training has been shown to be effective for improving many natural language processing tasks such as sentence-level paraphrasing and entity recognition tasks. However, current approaches to pre-trained language models are restricted to unidirectional language models. In this paper, we propose a new approach to pre-trained language models based on bidirectional encoder transformers (BERT). BERT is inspired by the pre-training objective of cloze task <ref type="bibr">(Taylor et al., 1953)</ref>, where the goal is to predict some masked language representations from the input. We introduce BERT and its detailed implementation in this paper. The BERT model is first initialized with the pre-trained parameters, and all of the parameters are finetuned using labeled data from the downstream tasks. Rich unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks. ROUGE1-F1 <ref type="bibr">36.52</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The distribution of summary sentences per section type after the section classification and alignment using DANCER. For the PubMed dataset the sentences are more evenly distributed among the introduction, methods, results and conclusion sections while for the arXiv dataset the majority of sentences is assigned to the introduction and methods section. In both dataset it can be clearly seen that the literature section is almost never matched with any summary sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the core Pointer-Generator model. For each decoder timestep the model has a probability to either generate words from a fixed vocabulary or copy words from the source text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Here we present the different section types and the common keywords that are used in order to classify them. If the header of a section includes any of the keywords associated with a specific section type it is classified in that section type. Sections that can't be matched with any section type are ignored.</figDesc><table><row><cell>section keywords</cell></row><row><cell>introduction introduction, case</cell></row><row><cell>literature background, literature, related</cell></row><row><cell>methods method(s), techniques, methodology</cell></row><row><cell>results result(s), experimental, experiment(s)</cell></row><row><cell>conclusion conclusion(s), concluding, discussion, limitations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Statistics about the two datasets that are used in our experiments. Since we are creating multiple examples from each document the example and target lengths are much smaller than the document and summary lengths respectively.</figDesc><table><row><cell></cell><cell cols="2">Arxiv PubMed</cell></row><row><cell># documents</cell><cell>215k</cell><cell>133k</cell></row><row><cell cols="2"># examples 584,396</cell><cell>385,229</cell></row><row><cell>avg. document length (words)</cell><cell>6,913</cell><cell>3,224</cell></row><row><cell>avg. summary length (words)</cell><cell>292</cell><cell>214</cell></row><row><cell>avg. example length (words)</cell><cell>1,018</cell><cell>639</cell></row><row><cell>avg. target length (words)</cell><cell>69</cell><cell>69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 ROUGE</head><label>3</label><figDesc>F1 results on arXiv test set. Underlined are the top performing models in each category while bold is the overall top performing model.</figDesc><table><row><cell cols="4">Model Type ROUGE-1 ROUGE-2 ROUGE-L</cell></row><row><cell>SumBasic Ext</cell><cell>29.47</cell><cell>6.95</cell><cell>26.3</cell></row><row><cell>LexRank Ext</cell><cell>33.85</cell><cell>10.73</cell><cell>28.99</cell></row><row><cell>LSA Ext</cell><cell>29.91</cell><cell>7.42</cell><cell>25.67</cell></row><row><cell>Lead-10 Ext</cell><cell>35.52</cell><cell>10.33</cell><cell>31.44</cell></row><row><cell>Sent-CLF Ext</cell><cell>34.01</cell><cell>8.71</cell><cell>30.41</cell></row><row><cell>Sent-PTR Ext</cell><cell>42.32</cell><cell>15.63</cell><cell>38.06</cell></row><row><cell>Attention Seq2Seq Abs</cell><cell>29.3</cell><cell>6.00</cell><cell>25.56</cell></row><row><cell>PEGASUS Abs</cell><cell>44.21</cell><cell>16.95</cell><cell>38.83</cell></row><row><cell>BigBird-PEGASUS Abs</cell><cell>46.63</cell><cell>19.02</cell><cell>41.77</cell></row><row><cell>Pointer-Generator Mix</cell><cell>32.06</cell><cell>9.04</cell><cell>25.16</cell></row><row><cell>Discourse-Aware Mix</cell><cell>35.8</cell><cell>11.05</cell><cell>31.8</cell></row><row><cell>TLM-I+E Mix</cell><cell>42.43</cell><cell>15.24</cell><cell>24.08</cell></row><row><cell cols="2">Our Models</cell><cell></cell><cell></cell></row><row><cell>DANCER LSTM Mix</cell><cell>41.87</cell><cell>15.92</cell><cell>37.61</cell></row><row><cell>DANCER RUM Mix</cell><cell>42.7</cell><cell>16.54</cell><cell>38.44</cell></row><row><cell>DANCER PEGASUS Abs</cell><cell>45.01</cell><cell>17.60</cell><cell>40.56</cell></row><row><cell cols="4">similar to [31] conditioned on the extracted sentences to</cell></row><row><cell cols="4">generate the summary text. State-of-the-art models include</cell></row><row><cell cols="4">the original PEGASUS model fine-tuned without DANCER</cell></row><row><cell cols="4">on the two datasets and the BigBird-PEGASUS variant that</cell></row><row><cell cols="4">is based on the pre-trained PEGASUS model extended with</cell></row><row><cell>sparse attention.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 ROUGE</head><label>4</label><figDesc>F1 results on PubMed test set. Underlined are the top performing models in each category while bold is the overall top performing model.</figDesc><table><row><cell cols="4">Model Type ROUGE-1 ROUGE-2 ROUGE-L</cell></row><row><cell>SumBasic Ext</cell><cell>37.15</cell><cell>11.36</cell><cell>33.43</cell></row><row><cell>LexRank Ext</cell><cell>39.19</cell><cell>13.89</cell><cell>34.59</cell></row><row><cell>LSA Ext</cell><cell>33.89</cell><cell>9.93</cell><cell>29,70</cell></row><row><cell>Lead-10 Ext</cell><cell>37.45</cell><cell>14.19</cell><cell>34.07</cell></row><row><cell>Sent-CLF Ext</cell><cell>45.01</cell><cell>19.91</cell><cell>41.16</cell></row><row><cell>Sent-PTR Ext</cell><cell>43.3</cell><cell>17.92</cell><cell>39.47</cell></row><row><cell>Attention Seq2Seq Abs</cell><cell>31.55</cell><cell>8.52</cell><cell>27.38</cell></row><row><cell>PEGASUS Abs</cell><cell>45.97</cell><cell>20.15</cell><cell>41.34</cell></row><row><cell>BigBird-PEGASUS Abs</cell><cell>46.32</cell><cell>20.65</cell><cell>42.33</cell></row><row><cell>Pointer-Generator Mix</cell><cell>35.86</cell><cell>10.22</cell><cell>29.69</cell></row><row><cell>Discourse-Aware Mix</cell><cell>38.93</cell><cell>15.37</cell><cell>35.21</cell></row><row><cell>TLM-I+E Mix</cell><cell>41.43</cell><cell>15.89</cell><cell>24.32</cell></row><row><cell cols="2">Our Models</cell><cell></cell><cell></cell></row><row><cell>DANCER LSTM Mix</cell><cell>44.09</cell><cell>17.69</cell><cell>40.27</cell></row><row><cell>DANCER RUM Mix</cell><cell>43.98</cell><cell>17.65</cell><cell>40.25</cell></row><row><cell>DANCER PEGASUS Abs</cell><cell>46.34</cell><cell>19.97</cell><cell>42.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Title BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Original abstract We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018)</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 6</head><p>Title Neural Machine Translation by Jointly Learning to Align and Translate Original abstract Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoderdecoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixedlength vector is a bottleneck in improving the performance of this basic encoderdecoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrasebased system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.  <ref type="formula">2003)</ref> which consists of the encoder and the decoder for a language pair that are jointly trained to maximize the probability of a correct translation given a source sentence. We show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance. The performance of the RNNsearch is as high as that of the conventional phrase-based translation system (Moses), when only the sentences consisting of known words are considered. This is a significant achievement, considering that Moses uses a separate monolingual corpus (418m words) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec. One of the motivations behind the proposed approach was the basic encoder-decoder approach to underperform with long sentences. We show that the proposed approach provides an intuitive way to inspect the (soft-) alignment between the words in a generated translation and those in a source sentence. This is done by visualizing the annotation weights. In this paper, we propose a novel approach to neural machine translation, called an encoder-decoder approach, encodes a whole input sentence into a fixed-length vector from which a translation will be decoded. We conjectured that the proposed RNNsearch outperforms the conventional encoder-decoder model (RNNencdec) significantly, regardless of the sentence length and that it is much more robust to the length of a source sentence. ROUGE1-F1 54.17 PEGASUS summary Neural machine translation is a newly emerging approach to machine translation, recently proposed by Kalchbrenner and Blunsom, Sutskever et al. and Cho et al. The proposed RNNsearch outperforms the conventional RNNencdec when only the sentences consisting of known words are considered. More importantly, the performance of the RNNsearch is as high as that of the conventional phrase-based translation system (Moses) when only the sentences consisting of known words are considered. The proposed approach provides an intuitive way to inspect the (soft-)alignment between the words in a generated translation. This is done by visualizing the weights associated with the annotation of the source sentence and those associated with the annotation of the target word. The conventional approach to neural machine translation, called an encoderdecoder approach, encodes a whole input sentence into a fixed-length context vector from which a translation will be decoded. We conjectured that the use of a fixed-length context vector is problematic for translating long sentences, based on a recent empirical study reported by Cho et al. ROUGE1-F1 52.12</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pointer-Generator summary</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Boiling the Information Ocean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://tiny.cc/45ohlz" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 2016 SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 2017 Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Learning Representations</title>
		<meeting>the 2018 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text Summarization with Pretrained Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3721" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Machine Learning</title>
		<meeting>the 2019 International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="42" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ProphetNet: Predicting Future N-gram for Sequenceto-Sequence Pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04063</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The new york times annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="708" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03186</idno>
		<title level="m">On Extractive and Abstractive Neural Document Summarizationwith Transformer Language Models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bottom-Up Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4098" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<title level="m">Big bird: Transformers for longer sequences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08777</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured Summarization of Academic Publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gidiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications in Computer and Information Science</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1168</biblScope>
			<biblScope unit="page" from="636" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using latent semantic analysis in text summarization and summary evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jezek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 International Conference on Information System Implementation and Modeling</title>
		<meeting>the 2004 International Conference on Information System Implementation and Modeling</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond SumBasic: Task-focused summarization with sentence simplification and lexical expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1606" to="1618" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LexRank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Query-oriented text summarization based on hypergraph transversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Lierde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Title: Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1317" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sentence Centrality Revisited for Unsupervised Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 2019 Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6236" to="6247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oussalah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1356" to="1372" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
		<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language Models are Unsupervised Multitask Learners -Enhanced Reader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radford</forename><surname>Alec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Child</forename><surname>Rewon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename><surname>Dario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Ilya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Classify or select: Neural architectures for extractive document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Supervised Approach to Extractive Summarisation of Scientific Papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Computational Natural Language Learning</title>
		<meeting>the 2017 Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Communicating Agents for Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1662" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rotational Unit of Memory: A Novel Representation Unit for RNNs with Scalable Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dangovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatalović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soljačić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="121" to="138" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selfcritical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2017 Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep transfer reinforcement learning for text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Keneshloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 SIAM International Conference on Data Mining. Philadelphia</title>
		<meeting>the 2019 SIAM International Conference on Data Mining. Philadelphia</meeting>
		<imprint>
			<publisher>PA: Society for Industrial and Applied Mathematics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="675" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ranking Sentences for Extractive Summarization with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1747" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generating extractive summaries of scientific paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Whidby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="165" to="201" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scientific Article Summarization Using Citation-Context and Article Discourse Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="390" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scientific document summarization via citation contextualization and scientific discourse</title>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="287" to="303" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Overview of the CL-SciSumm 2016 Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rustagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 joint workshop on Bibliometric-enhanced Information Retrieval and Natural language processing for Digital Libraries</title>
		<meeting>the 2016 joint workshop on Bibliometric-enhanced Information Retrieval and Natural language processing for Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI Conference on Artificial Intelligence</title>
		<meeting>the 2019 AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7386" to="7393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Workshop on Text Summarization Branches Out, Post Conference Workshop of ACL</title>
		<meeting>the 2004 Workshop on Text Summarization Branches Out, Post Conference Workshop of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gradient-based learning algorithms for recurrent networks and their computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Backpropagation: Theory, architectures, and applications</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="433" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">MultiLing 2019: Financial Narrative Summarisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Haj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Workshop MultiLing 2019: Summarization Across Languages, Genres and Sources</title>
		<meeting>the 2019 Workshop MultiLing 2019: Summarization Across Languages, Genres and Sources</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">610</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 2018 Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 -Long Papers</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Audio chord recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Society for Music Information Retrieval Conference</title>
		<meeting>the 2013 International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning, ICML 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
