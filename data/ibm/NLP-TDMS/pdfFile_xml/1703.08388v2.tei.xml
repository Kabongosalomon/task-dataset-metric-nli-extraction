<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepVisage: Making face recognition simple yet with powerful generalization skills</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abul</forename><surname>Hasnat</surname></persName>
							<email>md-abul.hasnat@ec-lyon.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire LIRIS</orgName>
								<address>
									<postCode>69134</postCode>
									<settlement>École centrale de Lyon, Ecully</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Bohné</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Safran Identity &amp; Security</orgName>
								<address>
									<postCode>92130</postCode>
									<settlement>Issy-les-Moulineaux</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Milgram</surname></persName>
							<email>jonathan.milgram@safrangroup.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Safran Identity &amp; Security</orgName>
								<address>
									<postCode>92130</postCode>
									<settlement>Issy-les-Moulineaux</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Gentric</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Safran Identity &amp; Security</orgName>
								<address>
									<postCode>92130</postCode>
									<settlement>Issy-les-Moulineaux</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Chen</surname></persName>
							<email>liming.chen@ec-lyon.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire LIRIS</orgName>
								<address>
									<postCode>69134</postCode>
									<settlement>École centrale de Lyon, Ecully</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepVisage: Making face recognition simple yet with powerful generalization skills</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition (FR) methods report significant performance by adopting the convolutional neural network (CNN) based learning methods. Although CNNs are mostly trained by optimizing the softmax loss, the recent trend shows an improvement of accuracy with different strategies, such as task-specific CNN learning with different loss functions, fine-tuning on target dataset, metric learning and concatenating features from multiple CNNs. Incorporating these tasks obviously requires additional efforts. Moreover, it demotivates the discovery of efficient CNN models for FR which are trained only with identity labels. We focus on this fact and propose an easily trainable and single CNN based FR method. Our CNN model exploits the residual learning framework. Additionally, it uses normalized features to compute the loss. Our extensive experiments show excellent generalization on different datasets. We obtain very competitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and CACD datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition (FR) is one of the most demanding computer vision tasks, due to its practical use in numerous applications, such as biometric, surveillance and humanmachine interaction. The state-of-the-art FR methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20]</ref> surpassed human performance (97.53%) and achieved significant accuracy on the standard labeled faces in the wild (LFW) <ref type="bibr" target="#b13">[14]</ref> benchmark. These remarkable results are achieved by training the deep convolutional neural network (CNN) <ref type="bibr" target="#b9">[10]</ref> with large databases <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>The facial image databases mostly provide the identity labels. These labels allow the CNN models to be easily trained with the softmax loss. FR methods generally use the trained CNN model to extract facial features and then perform verification by computing distance or recognition with a classifier. However, from our extensive study (see Sect. 2), we observe that recent methods include different additional strategies to obtain better performance, such as: 1. train CNN with different loss functions <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>: requires carefully preparing the image pairs/triplets by maintaining certain constrains <ref type="bibr" target="#b28">[29]</ref>, because arbitrary pairs/triplets do not contribute to the training. Online triplet generation requires a larger batch size (e.g., <ref type="bibr" target="#b28">[29]</ref> used 1.8K images in a mini-batch with 40 images/identity), which is excessive for a limited resource machine. On the other hand, using offline triplets can be critical as many of them will be useless while training progresses. The joint optimization <ref type="bibr" target="#b30">[31]</ref> with Softmax and Contrastive losses not only requires specific training data (with identity and pair labels) but also complicates the training procedure.</p><p>2. fine-tune CNN: requires training on each target dataset, which restricts the ability to generalize.</p><p>3. metric learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9]</ref>: requires particular form of training data (e.g., triplets). Moreover, it does not always guarantee to enhance performance <ref type="bibr" target="#b36">[37]</ref>. <ref type="bibr" target="#b3">4</ref>. concatenating features from multiple CNNs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20]</ref>: requires additional training data of different forms and train CNNs for each form. Besides, it is necessary to explore the particular modalities that can contribute to enhance performance.</p><p>The use of the above strategies requires significant efforts in terms of data preparation or selection and computing resources. On the other hand, recent results on the Im-ageNet challenge <ref type="bibr" target="#b25">[26]</ref> indicate that deeper CNNs enhance performance of different computer vision tasks. These observations raise the following question -can we achieve state-of-the-art results with a single CNN model which is trained only once with the identity labels? Our research is motivated by this question and we aim to address it by developing a simple yet robust single-CNN based FR method. Moreover, we want that our once-trained single CNN based FR method generalizes well across different datasets.</p><p>In this research, our primary objective is to discover an efficient CNN architecture. We follow the recent findings, which suggest that deeper CNNs perform better on a number of computer vision tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10]</ref>. We construct a deep CNN model with 27 convolutional and 1 fully connected (FC) layers, which incorporates the residual learning framework <ref type="bibr" target="#b12">[13]</ref>. Moreover, we aim to find an efficient way to train our CNN only with the identity labels. Recently, <ref type="bibr" target="#b38">[39]</ref> achieves high FR performance with a CNN trained from the identity labels. However, they perform joint optimization using the softmax and center loss <ref type="bibr" target="#b38">[39]</ref> (CL). CL improves the features discrimination among different classes. It follows the principle that, features learned from a deep CNN should minimize the intra-class distances. Interestingly, we observe (see <ref type="figure" target="#fig_2">Fig 3)</ref> that an equivalent representation can be achieved by normalizing the CNN features before computing the loss. Therefore, we train our CNN using the softmax loss with the normalized features.</p><p>With our single CNN model, first we evaluate on the LFW <ref type="bibr" target="#b13">[14]</ref> benchmark and observe that it obtains 99.62% accuracy. In order to demonstrate its effectiveness, we evaluated it on different challenging face verification tasks, such as face templates matching on the IJB-A <ref type="bibr" target="#b15">[16]</ref> dataset, video faces matching on the YouTube Faces <ref type="bibr" target="#b39">[40]</ref> (YTF) dataset and cross age face matching on the CACD <ref type="bibr" target="#b2">[3]</ref> dataset. Our method achieves 82.4% TAR@FAR=0.001 on IJB-A <ref type="bibr" target="#b15">[16]</ref>, 96.24% accuracy on YTF <ref type="bibr" target="#b39">[40]</ref> and 99.13% accuracy on CACD <ref type="bibr" target="#b2">[3]</ref>. These results indicate that our method achieves very competitive and state-of-the-art results. Moreover, it generalizes very well across different datasets.</p><p>We summarize our contributions as follows: (a) conduct extensive study and provide (Sec 2) a review and methodological comparison of the state-of-the-art methods; (b) propose (Sect. 3) an efficient single CNN based FR method; (c) conduct (Sect. 4) extensive experiments on different datasets, which demonstrate that our method has excellent generalization ability; and (d) perform (Sect. 4.3) an indepth analysis to identify the influences of different aspects.</p><p>In the remaining part of this paper, first we study and analyze the state-of-the-art FR methods in Section 2, describe our proposed method in Section 3, present experimental results, perform analysis of our method and discuss them in Section 4 and finally draw conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work, state-of-the-art FR methods</head><p>Face recognition (FR) in unconstrained environment attracts significant interest from the community. Recent methods exploited deep CNN models and achieved remarkable results on the LFW <ref type="bibr" target="#b13">[14]</ref> benchmark. Besides, numerous methods have been evaluated on the IJB-A <ref type="bibr" target="#b15">[16]</ref> dataset. We study 1 and analyze these methods based on several key aspects: (a) details of the CNN model; (b) loss functions used; (c) incorporation of additional learning strategy; (d) number of CNNs and (e) the training database used.</p><p>Recent methods tend to learn CNN based features using a deep architecture (e.g., 10 or more layers). This is inspired from the extraordinary success on the ImageNet <ref type="bibr" target="#b25">[26]</ref> challenge by famous CNN architectures <ref type="bibr" target="#b9">[10]</ref>, such as AlexNet, VGGNet, GoogleNet, ResNet, etc. The FR methods commonly use these architectures as their baseline model (directly or slightly modified). For example, AlexNet is used by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>, VGGNet is used by <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref> and GoogleNet is used by <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29]</ref>. CASIA-Webface <ref type="bibr" target="#b43">[44]</ref> proposed a simpler CNN model, which is used by <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>. Several methods, such as <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31]</ref> use a model with lower depth but increase its complexity with locally connected convolutional layers. Besides, <ref type="bibr" target="#b45">[46]</ref> use 4 parallel 10 layers CNNs to learn features from different facial regions. We follow the ResNet <ref type="bibr" target="#b12">[13]</ref> based deep CNN model.</p><p>FR methods often train multiple CNNs and accumulate features from all of them to construct the final facial descriptors. It provides an additional boost to the performance. Different types of inputs are used to train these multiple CNNs: (a) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref> used image-crops focused on certain facial regions (eyes, nose, lips, etc.); (b) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref> used different modality of input images, such as 2D, 3D, frontalized and synthesized faces at different poses and (c) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20]</ref> used different training databases with varying number of images. We do not follow this approach and train only one CNN.</p><p>The CNN model parameters are learned by optimizing loss functions, which are defined based on the given task (e.g., classification, regression) and the available information (e.g., class labels, price). The softmax loss <ref type="bibr" target="#b9">[10]</ref> is a common choice for classification tasks. It is often used by the FR methods to create good face representation by training the CNN as an identity classifier. It requires only the identity labels. The contrastive loss <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7]</ref> is used by <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref> for face verification and requires face image pairs and similarity labels. The triplet loss <ref type="bibr" target="#b28">[29]</ref> is used by <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref> for face verification and requires the face triplets. Recently the center loss <ref type="bibr" target="#b38">[39]</ref> is proposed to enhance feature discrimination, which uses the identity labels.</p><p>We use the softmax loss.</p><p>Several methods use multiple loss functions and train CNN using joint optimization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25]</ref>. The other way is to use them sequentially <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref>, i.e., first train with the softmax and then train with the other loss. We observe that using multiple loss functions complicates the training data preparation task and the CNN training procedure. Therefore, we avoid this type of strategies.</p><p>Fine-tuning the CNN parameters is a particular form of transfer learning. It is commonly employed by several methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref> on the IJB-A <ref type="bibr" target="#b15">[16]</ref> dataset. It refines the CNN parameters from a previously learned model using a target specific training dataset. Several methods do not directly use the raw CNN features but employ an additional learning strategy. The metric/distance learning strategy based on the Joint Bayesian method <ref type="bibr" target="#b3">[4]</ref> is a popular one and used by <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9]</ref>. Recently, two different strategies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b27">28]</ref> have been proposed to learn feature embedding using face triplets. Another strategy, called template adaptation <ref type="bibr" target="#b7">[8]</ref>, exploits an additional SVM classifier. Apart from these, principal component analysis (PCA) is used by several methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref> to learn a dataset specific projection matrix. <ref type="bibr" target="#b41">[42]</ref> learns an aggregation module to compute scores among two videos. The above methods often need training data from the target datasets. Moreover, they <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> may need to carefully prepare the training data, e.g., triplets. We do not need any such learning strategies.</p><p>The use of a large facial training dataset is important to achieve high FR accuracy <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">46]</ref>. <ref type="bibr" target="#b45">[46]</ref> provided an in-depth analysis and demonstrated the effect of the dataset size and the number of identities for FR. Following the high demand of a large FR dataset, several publicly available datasets have been released recently. Among them, CASIA-WebFace <ref type="bibr" target="#b43">[44]</ref> is used by numerous methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref>. Several researches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref> enlarge it by synthesizing facial images with different shapes and poses based on the 3D face models. Recently, the MSCeleb <ref type="bibr" target="#b10">[11]</ref> dataset has been publicly released. It contains the largest collection of facial images and identities. We exploit it to develop our FR method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Our FR method, called DeepVisage, consists in preprocessing face image, learning CNN based facial features and computing similarity. Following the recent trend <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref>, we exploit the CNN as the core component. Our deep CNN model follows the residual learning framework <ref type="bibr" target="#b12">[13]</ref>. Moreover, it intelligently exploits feature normalization, which is a crucial step, see Sect. 4.3. Our pre-processing stage consists in the detection of the face and facial landmarks, which are used to create a normalized face image. We compute the cosine similarity among the features of a pair of faces as the verification score. Below, we describe these elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Building blocks and deep CNN architecture</head><p>Convolutional networks: We begin with the basic ideas of CNN <ref type="bibr" target="#b17">[18]</ref>: (a) local receptive fields with identical weights via the convolution operation and (b) spatial subsampling via the pooling operation. At a particular layer l, the convolution of the input f Op,l−1</p><p>x,y to obtain the k th output feature map f C,l</p><p>x,y,k , can be expressed as:</p><formula xml:id="formula_0">f C,l x,y,k = w l k T f Op,l−1 x,y + b l k<label>(1)</label></formula><p>where, w l k and b l k are the shared weights and bias. C denotes convolution and Op (for l &gt; 1) denotes various tasks, such as convolution, sub-sampling or activation. For l = 1, Op represents the input image. Sub-sampling or pooling performs a simple local operation, such as computing the average or maximum value in a local spatial neighborhood followed by reducing spatial resolution. We apply max pooling for our CNN, which has the following form:</p><formula xml:id="formula_1">f P,l x,y,k = max (m,n)∈Nx,y f Op,l−1 m,n,k<label>(2)</label></formula><p>where, N x,y denotes the local spatial neighborhood of (x, y) coordinate and P denotes the pooling operation.</p><p>In order to ensure non-linearity of the network, the feature maps are passed through a non-linear activation function, e.g., the Rectified Linear Unit (ReLU) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>: f l x,y,k = max(f l−1 x,y,k , 0). We apply the Parametric Rectified Linear Unit (PReLU) <ref type="bibr" target="#b11">[12]</ref> as the activation function, which has the following form:</p><formula xml:id="formula_2">f A,l x,y,k = max(f Op,l−1 x,y,k , 0) + λ k min(f Op,l−1 x,y,k , 0) (3)</formula><p>where, λ k is a trainable parameter to control the slope of the linear function for the negative input values and A denotes activation operation. At the basic level, a CNN is constructed by stacking series of convolution, activation and pooling layers, see LeNet-5 <ref type="bibr" target="#b17">[18]</ref> for an example. Often a layer with full connections is placed at the end of the stacked layers, called the fully connected (FC) layer. It takes all points (neurons) from the previous layer as input and connects it to all points (neurons) of the output layer.</p><p>Residual learning framework <ref type="bibr" target="#b12">[13]</ref>: A recent trend <ref type="bibr" target="#b9">[10]</ref> on the ImageNet <ref type="bibr" target="#b25">[26]</ref> challenge shows that deeper CNNs achieve better results. However, it increases the model complexity, which makes it harder to optimize the loss of the CNN model. Besides, they may generate higher training error than a shallower CNN <ref type="bibr" target="#b12">[13]</ref>. The residual learning framework <ref type="bibr" target="#b12">[13]</ref> provides a solution to these problems.</p><p>For a stack of a few layers, residual learning fits a mapping F(f ) := H(f ) − f instead of fitting the underlying mapping H(f ). Therefore, the original mapping is formulated as F(f ) + f , which means directly adding the input feature map f with the output of the stacked layers F(f ). This idea can be easily implemented with the notion of shortcut connection. Formally, the output of a residual block R can be expressed as:</p><formula xml:id="formula_3">f R,l x,y,k = f Op,l−q x,y + F(f Op,l−q x,y , {W k })<label>(4)</label></formula><p>where, f Op,l−q x,y represents the input feature map, F(.) is the residual mapping to be learned, W k is the parameters of the k th residual block and q is the total number of stacked layers within the residual block. The flexible form of the residual function F(.) allows to stack multiple layers with different types of operations, such as convolution, pooling, activation etc. All of the residual blocks in our CNN consist of two convolution layers with different numbers of neurons. Each convolution is followed by a PReLU activation.</p><p>Loss function: Deep CNNs are trained by optimizing loss function. We use the softmax loss, which is widely used for classification:</p><formula xml:id="formula_4">L Sof tmax = − N i=1 log e w T y i fi+by i K j=1 e w T j fi+bj<label>(5)</label></formula><p>where, f i and y i are the features and true class label of the i th image. w j and b j denote the weights and bias of the j th class. N and K denote the number of training samples and the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature normalization (FN):</head><p>It is often used as a necessary step in many learning algorithms. It ensures that all of the features have equal contribution to the cost function <ref type="bibr" target="#b35">[36]</ref>. With deep CNNs, we cannot guarantee this by only normalizing the input image pixels, because the scale of features (from the final FC layer) may change due to a series of operations at different layers. Therefore, to avoid the influence of un-normalized features during cost computation, we provide normalized features f N r i to the softmax loss as:</p><formula xml:id="formula_5">f N r = f Op −µ √ σ 2 ,</formula><p>where µ and σ 2 are the mean and variance. During training, we apply normalization by computing µ and σ from the samples of each mini-batch. Moreover, we maintain the moving average of µ and σ and use them to normalize the test samples. Note that, this is a specific case of the popular batch normalization (BN) technique <ref type="bibr" target="#b14">[15]</ref> with scale γ = 1 and shift β = 0.</p><p>Proposed CNN architecture: Our CNN model consists of 27 convolution (Conv), 4 pooling (Pool) and 1 fully connected (FC) layers. Each convolution uses a 3 × 3 kernel and is followed by a PReLU activation function. The CNN progresses from the lower to higher depth by decreasing the spatial resolution using a 2 × 2 max Pool layer while gradually increasing the number of feature maps from 32 to 512. We use a FC layer of 512 neurons after the last Conv layer. We normalize (see FN above) the output of this FC layer and consider it as the desired feature representation of the input image. Finally, we use the softmax layer to compute the loss and optimize it during training. Our CNN model incorporates the residual learning framework <ref type="bibr" target="#b12">[13]</ref>, see <ref type="figure" target="#fig_0">Fig.  1</ref> for the details. Overall, it comprises 40.5M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image pre-processing and face verification</head><p>Pre-processing: We maintain the same form of the 2D face image during training and testing. Our pre-processing steps are: (a) detect 2 face and landmarks using the MTCNN <ref type="bibr" target="#b44">[45]</ref> detector; (b) normalize the face image by applying a 2D similarity transformation. The transformation parameters are computed from the location of the detected landmarks on the image and pre-set coordinates in a 112×96 image frame; and (c) convert to grayscale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face verification:</head><p>We verify a pair of face images <ref type="bibr" target="#b13">[14]</ref>, templates <ref type="bibr" target="#b15">[16]</ref> (contains multiple images and video frames) and videos <ref type="bibr" target="#b39">[40]</ref> (given as frames) using the following steps:</p><p>1. pre-process: apply the pre-processing 3 stage described in the previous paragraph.</p><p>2. extract facial feature/representation: we use the trained CNN model to extract the facial feature descriptor. For an image i, we obtain its descriptor f i by taking element-wise maximum of the features from its original f i,o and horizontally flipped version f i,f . In order to perform verification based on template <ref type="bibr" target="#b15">[16]</ref> and video <ref type="bibr" target="#b39">[40]</ref>, we obtain the descriptor for an identity by taking element-wise average of the features from all of the images/frames.</p><p>3. compute verification score: for a given pair of facial features, we compute the cosine similarity as the veri-fication score. We compare this score to a threshold to decide whether two images belong to the same person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments, Results and Discussion</head><p>Our experiments consist of first training the CNN model and then use it to extract facial features and perform different types (single-image <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>, multi-image or video <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>) of face verification. In order to verify the effectiveness, we experiment on several datasets, namely LFW <ref type="bibr" target="#b13">[14]</ref>, IJB-A <ref type="bibr" target="#b15">[16]</ref>, YTF <ref type="bibr" target="#b39">[40]</ref> and CACD <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CNN Training</head><p>We collect the training images from the cleaned 4 version of the MS-Celeb-1M <ref type="bibr" target="#b10">[11]</ref> database, which consists of 4.47M images of 62.5K identities. We train our CNN model using only the identity label of each image. We use 95% images (4.2M images) for training and 5% images (232K images) for monitoring and evaluating the loss and accuracy. We train our CNN using the stochastic gradient descent method and momentum set to 0.9. Moreover, we apply L2 regularization with the weight decay set to 5e −4 . We begin the CNN training with a learning rate 0.1 for 2 epochs. Then we decrease it after each epoch by a factor 10. We stop the training after 5 epochs. We use 120 images in each mini-batch. During training, we apply data augmentation by horizontally flipping the images. Note that, during evaluation on a particular dataset, we do not apply any additional CNN training or fine-tuning and dimension reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Evaluation</head><p>Now we evaluate our proposed FR method, called Deep-Visage, on the most commonly used and challenging facial image datasets based on their specified protocols.</p><p>Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b13">[14]</ref>: LFW is one of the most popular and challenging databases for evaluating unconstrained FR methods. It consists of 13,233 images of 5,759 identities. It has different evaluation protocols. We follow the unrestricted-labeled-outside-data protocol based on the recent trend <ref type="bibr" target="#b16">[17]</ref>. The FR task requires verifying 6000 image pairs in 10 folds and report the accuracy. These pairs are equally divided into genuine and impostor pairs and comprises 7.7K images of 4,281 identities. <ref type="table" target="#tab_0">Table 1</ref> provides the results of our method along with the other state-of-the-art methods. We observe that, our method achieves significant accuracy (99.62%) and among the top performers, despite the fact that: (a) we use single CNN, whereas Baidu <ref type="bibr" target="#b19">[20]</ref> used 10 CNNs to obtain 99.77% and (b) we train CNN with comparatively much less amount of data and identities, whereas FaceNet <ref type="bibr" target="#b28">[29]</ref> used 200M images of 8M identities to obtain 99.63%. The results in the <ref type="table" target="#tab_0">Table 1</ref> indicates saturation, because all of the methods achieve close to or more than human performance (97.53%). Besides, it is argued that matching only 6K pairs is insufficient to justify a method w.r.t. the real world FR scenario <ref type="bibr" target="#b18">[19]</ref>. We address these issues by two ways: (a) employ more challenging evaluation metrics and (b) evaluate with the other challenging datasets. To this aim, first we follow the BLUFR LFW protocol <ref type="bibr" target="#b18">[19]</ref> and measure the true accept rate (TAR) at a low false accept rate (FAR). BLUFR <ref type="bibr" target="#b18">[19]</ref> protocol exploits all images of the LFW dataset and evaluates methods based on 10 trials experiments. Each trial computes 47M pair-matching scores (157K positives, 46.9M negatives), which is significantly higher than the 6K scores used in the standard protocol. Within this protocol, we compute the verification rate (VR) at FAR=0.1% and compare with the methods which reported results 5 in this protocol. We observe that: DeepVisage (proposed) (98.65) &gt; CenterLoss 6 <ref type="bibr" target="#b38">[39]</ref> (92.97%) &gt; F SS <ref type="bibr" target="#b36">[37]</ref> (89.8%) &gt; CASIA <ref type="bibr" target="#b43">[44]</ref> (80.26%) , i.e., our method obtains the best results published so far. Therefore, this result together with the <ref type="table" target="#tab_0">Table 1</ref> confirm the remarkable performance of Deep- <ref type="bibr" target="#b4">5</ref> We do not include results from Baidu <ref type="bibr" target="#b19">[20]</ref> (VR@FAR: 99.11% for single CNN and 99.41% for 10-CNNs ensembles). The reason is that, we are not sure if they compute results based on the BLUFR protocol <ref type="bibr" target="#b18">[19]</ref> or based on the 6K pairs. Note that, we obtain 99.7% on VR@FAR=0.1% using the 6K pair-matching scores of the standard protocol. <ref type="bibr" target="#b5">6</ref> Results computed from the features publicly provided by the authors.  <ref type="bibr" target="#b23">[24]</ref> N N 0.805 0.604 Face-Aug-Pose-Syn <ref type="bibr" target="#b22">[23]</ref> N N 0.886 0.725 Deep Multipose <ref type="bibr" target="#b0">[1]</ref> N N 0.787 -Pose aware FR <ref type="bibr" target="#b21">[22]</ref> N N 0.826 0.652 TPE <ref type="bibr" target="#b27">[28]</ref> N N 0.871 0.766 All-In-One <ref type="bibr" target="#b24">[25]</ref> N N 0.893 0.787</p><p>All-In-One <ref type="bibr">[</ref> IARPA Janus Benchmark A (IJB-A) <ref type="bibr" target="#b15">[16]</ref>: The recently proposed IJB-A database aims at raising the difficulty of FR by incorporating more variations in pose, illumination, expression, resolution and occlusion. It consists of 5,712 images and 2,085 videos of 500 identities. The FR task compares two templates. A template is a set of images and video-frames. The evaluation protocol requires computing the true accept rate (TAR) at a fixed false accept rate (FAR) with various values, e.g., 0.01 and 0.001. <ref type="table" target="#tab_1">Table 2</ref> presents our results along with the other state-ofthe-art methods. We separate the results (with a horizontal line) to distinguish two categories: (1) methods only using a pre-trained CNN; our method belongs to this category and (2) methods use additional learning, such as CNN finetuning and metric learning. From the comparison among the 1 st category of methods, we observe that, our method provides the best result for FAR at 0.001% and competitive (second best) at 0.01%. By comparing it to the 2 nd category we observe that, it is also very competitive and provide better results than numerous methods from this category. Besides, similar to <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>, it is possible to exploit our CNN features and further improve the final results with external learning, such as TA <ref type="bibr" target="#b7">[8]</ref>, NAN <ref type="bibr" target="#b41">[42]</ref> and TPE <ref type="bibr" target="#b27">[28]</ref>.</p><p>YouTube Faces <ref type="bibr" target="#b39">[40]</ref> (YTF): The YTF dataset is a widely used FR dataset of unconstrained videos. It consists of 3,425 videos of 1,595 identities. YTF evaluation requires matching 5000 video pairs in 10 folds and report average accuracy. Each fold consists of 500 video pairs and ensures subject-mutually exclusive property. We follow the restricted protocol of YTF, i.e., access to only the similarity information. We report our result in <ref type="table" target="#tab_3">Table 3</ref>, along with the state-of-the-art methods. Results show that our method provides the best accuracy (96.24%). <ref type="table" target="#tab_3">Table 3</ref> also provides the results (separated with a horizontal line) from unrestricted protocol, i.e., access to similarity and identity information of the test data. We observe that our method is very competitive to the best accuracy, although it follows the restricted protocol. The VGG Face <ref type="bibr" target="#b23">[24]</ref> provides results with both protocols and shows that accuracy increases significantly (from restricted-91.6% to unrestricted-97.3%) when they learn their CNN feature embedding using the YTF training data. Based on this observation, we can predict that our result (96.24%) can be further enhanced by training or fine tuning with the YTF data. Cross-Age Celebrity Dataset (CACD) <ref type="bibr" target="#b2">[3]</ref>: CACD is a recently released dataset, which aims to ensure large variations of the ages in the wild. It consists of 163,446 images of 2000 identities with the age range from 16 to 62. CACD evaluation requires verifying 4000 image pairs in ten folds and report average accuracy. <ref type="table">Table 4</ref> reports the results of DeepVisage along with the state-of-the-art methods. It shows that our method provides the best accuracy. Moreover, it is better than LF-CNN <ref type="bibr" target="#b37">[38]</ref>, which is a recent method specialized on age invariant face recognition. The evaluations of DeepVisage (proposed method) across different challenging datasets prove that it not only achieves significant performance but also generalizes very well. It overcomes several of the difficulties which make unconstrained FR a challenging task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis and Discussion</head><p>We perform further analysis to highlight the influences of several aspects, such as: (a) training datasets; (b) CNN <ref type="table">Table 4</ref>: Comparison of the state-of-the-art methods evaluated on the CACD <ref type="bibr" target="#b2">[3]</ref> dataset. VGG <ref type="bibr" target="#b23">[24]</ref> result is obtained from <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FR method</head><p>Accuracy (%) DeepVisage (proposed) 99.13 LF-CNNs <ref type="bibr" target="#b37">[38]</ref> 98.50 MFM-CNN <ref type="bibr" target="#b40">[41]</ref> 97.95 VGG Face <ref type="bibr" target="#b23">[24]</ref> 96.00 CARC <ref type="bibr" target="#b2">[3]</ref> 87.60 Human, Avg.</p><p>85.70 Human, Voting <ref type="bibr" target="#b2">[3]</ref> 94.20 models and depth; (c) normalization and (d) activation functions. Therefore, we modify and train our CNN model and observe the accuracy and TAR@FAR=0.01 on LFW. <ref type="table" target="#tab_4">Table  5</ref> presents the results. First, we study the influence of training the proposed CNN with different datasets. It helps us to understand the capacity of the CNN to learn facial representation and identify the requirements to achieve better performance. The top part of <ref type="table" target="#tab_4">Table 5</ref> presents the analysis w.r.t. different datasets, from which we observe that: (a) CNN performance increased by training with larger number of images as well as identities, the best results are obtained with the largest dataset, i.e., MSCeleb <ref type="bibr" target="#b10">[11]</ref>; (b) synthesized images help to enhance performance, we see this from the pose augmented CASIA <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23]</ref> dataset; (c) a dataset with more variations per identity helps even with a relatively lower number of images and identities, we see this by comparing the CA-SIA <ref type="bibr" target="#b43">[44]</ref> and UMD <ref type="bibr" target="#b1">[2]</ref> datasets; and (d) large number of images with smaller number of identities may not help, we see this from the VGG Face <ref type="bibr" target="#b23">[24]</ref> dataset. Besides, we analyze the dataset uniformity or balance issue, i.e., number of images-per-identity, see bottom part of of <ref type="table" target="#tab_4">Table 5</ref>. We use the MSCeleb <ref type="bibr" target="#b10">[11]</ref> dataset for this experiment. We see that, while maintaining certain balance is necessary, it is equality important to train CNN with a larger dataset. We obtain the best performance by keeping only the identities with 30 images or more.</p><p>Next, we analyze the results based on different CNN components and models. <ref type="table" target="#tab_5">Table 6</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref> present the results with different forms, where we train all settings with the CASIA <ref type="bibr" target="#b43">[44]</ref> dataset. Our observations are: (a) the proposed CNN model obtains better performance by including feature normalization (FN) before loss computation, we see this by comparing with the center loss <ref type="bibr" target="#b38">[39]</ref> and without FN based results and (b) it obtains better accuracy than the other commonly used CNNs (for FR), such as the VGG-Net <ref type="bibr" target="#b23">[24]</ref> and CASIA-Net <ref type="bibr" target="#b43">[44]</ref>. Note that, we do not directly compare with other loss functions (within our CNN model) as the center loss <ref type="bibr" target="#b38">[39]</ref> has been shown to be more efficient than those. Additionally, we trained our CNN with ReLU instead of PReLU and observe that it decreases accuracy by approximately 0.5%. In terms of complexity (measured with the  number of parameters in <ref type="table" target="#tab_5">Table 6</ref>), our model is more complex than the simpler models (Cas-Net and CN-mod). However, it is much simpler than the VGG-Net <ref type="bibr" target="#b23">[24]</ref>. Results indicate that, while a simpler model may limit 7 the FR performance, a complex model is prone to overfitting. Perhaps this is the reason why the VGG-Net <ref type="bibr" target="#b23">[24]</ref> requires additional fine-tuning on the target datasets. The above analyses justify the efficiency of our proposed CNN model. We observe that, feature normalization (FN) before the loss computation plays a significant role in the performance. In order to gain further insights, we conduct experiments and visualize the features of the MNIST digits in the 2D space. This is similar to the visualization recently shown in <ref type="bibr" target="#b38">[39]</ref> and hence we also provide a comparison with the center loss (CL). The CNN is composed of 6 convolution, 2 pool and 1 FC (with 2 neurons for 2D visualization) lay- ers. We optimize it using the softmax loss. <ref type="figure" target="#fig_2">Fig. 3</ref> provides the illustration, from which we observe that: (a) FN provides a better feature discrimination in the normalized 2D space, see  <ref type="figure" target="#fig_2">Fig. 3-d</ref>. These observations reveal that, by exploiting the FN appropriately we can ensure feature discrimination and hence no additional loss function, e.g., CL, is necessary. Finally, we investigate the incorrect results by observing the face image pairs in which DeepVisage failed. Appendix A provides the illustrations of the false accept/reject cases from the different datasets. We observe that, on LFW it failed (11/20 error cases) when the eyes are occluded by glasses or a cap. Incorrect CACD results and higher false rejection rate indicate that our method (although provides best accuracy) encounters difficulties to recognize the same person from the images of different ages. Incorrect results from YTF often suffers from high pose and perhaps low image resolution. IJB-A results reveal that our method needs to take care of the face images with extreme pose variations. Indeed, during the IJB-A experiments, we are forced to keep a large number of images as un-normalized due to the failure of landmarks detection for them. Based on empirical evidences, we believe that these un-normalized faces cause the degradation of our performance. Besides, the results from YTF and IJB-A indicate that we may need to use a better distance computation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we present a single-CNN based FR method which achieves state-of-the-art performance and exhibits excellent ability of generalize across different FR datasets. Our method, called DeepVisage, performs face verification based on a given pair of single images, templates and videos. It consists in a deep CNN model which is simple and straightforward to train. Overall, DeepVisage is very easy to implement, thanks to the residual learning framework, feature normalization, softmax loss and the simplest distance. It successfully demonstrates that, in order to achieve state-of-the-art results it is not necessary to develop a complicated FR method by using complex training data preparation and CNN learning procedure. We foresee several future perspectives of this work, such as: (a) train CNN with a larger and more balanced dataset, which can be constructed by combining multiple publicly available datasets or by adopting the face synthesizing strategy <ref type="bibr" target="#b22">[23]</ref> with the existing one; (b) enhance FR performance by incorporating failure detection based technique <ref type="bibr" target="#b29">[30]</ref>, particularly for face and landmarks detection and (c) incorporate better distance computation method for the template and video comparison, e.g., use softmax based distance <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analysis of the incorrect results</head><p>In this section, we provide examples of the incorrect results observed from the face verification experiments on different datasets.</p><p>LFW <ref type="bibr" target="#b13">[14]</ref>: <ref type="figure" target="#fig_4">Figure 4</ref> provides the examples of the failure cases on the LFW <ref type="bibr" target="#b13">[14]</ref> benchmark. The ratio of false accept vs reject is 1:1.56. Note that our method achieves 99.62% accuracy on LFW. In <ref type="figure" target="#fig_4">Figure 4</ref>(b) three pairs are marked with red colored rectangles. These pairs are erroneously labeled in the dataset, which means our method makes correct judgment on them and hence the accuracy further increases to 99.67% by considering them as correct match. <ref type="bibr" target="#b2">[3]</ref>: <ref type="figure">Figure 5</ref> provides the examples of the failure cases on the CACD <ref type="bibr" target="#b13">[14]</ref> dataset. The ratio of false accept vs reject is 1:6. YTF <ref type="bibr" target="#b39">[40]</ref>: <ref type="figure">Figure 6</ref> provides few examples of the failure cases on the YTF <ref type="bibr" target="#b39">[40]</ref> dataset. The ratio of false accept vs reject is 1:2.2. In <ref type="figure">Figure 6</ref>, we only show top three mistakes (sorted based on their similarity score) in terms of false accept and reject. <ref type="figure">Figure 7</ref> provides few examples of the IJB-A <ref type="bibr" target="#b15">[16]</ref> failure cases. The ratio of false accept vs reject is 1:5.15. In <ref type="figure">Figure 7</ref>, we only show top three mistakes (sorted based on their similarity score). From the falsely rejected template pairs, we observe that: (a) one pair has only one image in the template; (b) the pre-processor fails to detect face as well as landmarks and (c) the images in the template have very high pose and large occlusion which causes important face attributes to be absent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CACD-VS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IJB-A [16]:</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the proposed CNN architecture. CoPr indicates convolution followed by the PReLU activation function. ResBl is a residual block which computes output = input + CoP r(CoP r(input)). # Replication indicates how many times the same block is sequentially replicated in the CNN model. # Filts denotes the number of feature maps. FN denotes feature normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the ROC plots for different CNN models evaluates on the LFW<ref type="bibr" target="#b13">[14]</ref> dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 -</head><label>3</label><figDesc>b; (b) CL enforces the features towards its representative center and hence shows discrimination, see Fig. 3-c and (c) CL+FN does not provide much additional discrimination, see Fig. 3-b and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>2D visualization of the MNIST [18] digits features, which are obtained by using same baseline CNN model and training settings. CL [39] parameters are set to λ = 0.003 and α = 0.5. a. CNN without FN and CL; b. CNN with FN; c. CNN with CL; and d. CNN with FN and CL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the false accepted/rejected image pairs from the LFW [14] benchmark. (a) false accepted pairs and (b) false rejected pairs. The red colored rectangles indicate the examples which were erroneously labeled in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Illustration of the false accepted/rejected image pairs from the CACD-VS [3] dataset. (a) false accepted pairs and (b) false rejected pairs. Illustration of the false accepted/rejected video pairs from the YTF [40] dataset. (a) false accepted pairs and (b) false rejected pairs. Illustration of the false accepted/rejected template pairs from the IJB-A [16] dataset. (a) false accepted pairs and (b) false rejected pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the state-of-the-art methods evaluated on the LFW benchmark<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell>FR method</cell><cell># of CNNs</cell><cell>Dataset Info</cell><cell>Acc %</cell></row><row><cell>DeepVisage (proposed)</cell><cell>1</cell><cell>4.48M, 62K</cell><cell>99.62</cell></row><row><cell>Baidu [20]</cell><cell>10</cell><cell>1.2M, 1.8K</cell><cell>99.77</cell></row><row><cell>Baidu [20]</cell><cell>1</cell><cell>1.2M, 1.8K</cell><cell>99.13</cell></row><row><cell>FaceNet [29]</cell><cell>1</cell><cell>200M, 8M</cell><cell>99.63</cell></row><row><cell>Sparse ConvNet [33]</cell><cell>25</cell><cell>0.29M, 12K</cell><cell>99.55</cell></row><row><cell>DeepID3 [31]</cell><cell>25</cell><cell>0.29M, 12K</cell><cell>99.53</cell></row><row><cell>Megvii [46]</cell><cell>4</cell><cell>5M, 0.2M</cell><cell>99.50</cell></row><row><cell>LF-CNNs [38]</cell><cell>25</cell><cell>0.7M, 17.2K</cell><cell>99.50</cell></row><row><cell>DeepID2+ [32]</cell><cell>25</cell><cell>0.29M, 12K</cell><cell>99.47</cell></row><row><cell>Center Loss [39]</cell><cell>1</cell><cell>0.7M, 17.2K</cell><cell>99.28</cell></row><row><cell>MM-DFR [9]</cell><cell>8</cell><cell>0.49M, 10.57K</cell><cell>99.02</cell></row><row><cell>VGG Face [24]</cell><cell>1</cell><cell>2.6M, 2.6K</cell><cell>98.95</cell></row><row><cell>MFM-CNN [41]</cell><cell>1</cell><cell>5.1M, 79K</cell><cell>98.80</cell></row><row><cell>VIPLFaceNet [21]</cell><cell>1</cell><cell>0.49M, 10.57K</cell><cell>98.60</cell></row><row><cell>Webscale [35]</cell><cell>4</cell><cell>4.5M, 55K</cell><cell>98.37</cell></row><row><cell>AAL [43]</cell><cell>1</cell><cell>0.49M, 10.57K</cell><cell>98.30</cell></row><row><cell>FSS [37]</cell><cell>9</cell><cell>0.49M, 10.57K</cell><cell>98.20</cell></row><row><cell>Face-Aug-Pose-Syn [23]</cell><cell>1</cell><cell>2.4M, 10.57K</cell><cell>98.06</cell></row><row><cell>CASIA-Webface [44]</cell><cell>1</cell><cell>0.49M, 10.57K</cell><cell>97.73</cell></row><row><cell>Unconstrained FV [5]</cell><cell>1</cell><cell>0.49M, 10.5K</cell><cell>97.45</cell></row><row><cell>Deepface [34]</cell><cell>3</cell><cell>4.4M, 4K</cell><cell>97.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the state-of-the-art methods evaluated on the IJB-A benchmark<ref type="bibr" target="#b15">[16]</ref>. '-' indicates the information for the entry is unavailable. Methods which incorporates external training (ExTr) or CNN fine-tuning (FT) with IJB-A training data are separated with a horizontal line. VGG-Face result was provided by<ref type="bibr" target="#b26">[27]</ref>. T@F denotes the True Accept Rate at a fixed False Accept Rate (TAR@FAR).</figDesc><table><row><cell>FR method</cell><cell cols="2">ExTr FT</cell><cell>T@F 0.01</cell><cell>T@F 0.001</cell></row><row><cell>DeepVisage (proposed)</cell><cell>N</cell><cell>N</cell><cell>0.887</cell><cell>0.824</cell></row><row><cell>VGG Face</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the state-of-the-art methods evaluated on the Youtube Face<ref type="bibr" target="#b39">[40]</ref>. Ad.Tr. denotes additional training is used.</figDesc><table><row><cell>FR method</cell><cell>Ad.Tr.</cell><cell>Accuracy (%)</cell></row><row><cell>DeepVisage (proposed)</cell><cell>N</cell><cell>96.24</cell></row><row><cell>VGG Face [24]</cell><cell>N</cell><cell>91.60</cell></row><row><cell>Sparse ConvNet [33]</cell><cell>N</cell><cell>93.50</cell></row><row><cell>FaceNet [29]</cell><cell>N</cell><cell>95.18</cell></row><row><cell>DeepID2+ [32]</cell><cell>N</cell><cell>93.20</cell></row><row><cell>Center Loss [39]</cell><cell>N</cell><cell>94.90</cell></row><row><cell>MFM-CNN [41]</cell><cell>N</cell><cell>93.40</cell></row><row><cell>CASIA-Webface [44]</cell><cell>Y</cell><cell>92.24</cell></row><row><cell>Deepface [34]</cell><cell>Y</cell><cell>91.40</cell></row><row><cell>VGG Face [24]</cell><cell>Y</cell><cell>97.30</cell></row><row><cell>NAN [42]</cell><cell>Y</cell><cell>95.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Analysis of the influences from training databases, size and number of classes. T@F denotes the True Accept Rate at a fixed False Accept Rate (TAR@FAR).</figDesc><table><row><cell></cell><cell></cell><cell>Aspect</cell><cell></cell><cell>Add. info</cell><cell>Acc %</cell><cell>T@F 0.01</cell></row><row><cell></cell><cell></cell><cell>DB</cell><cell></cell><cell>Size, Class</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CASIA [44]</cell><cell></cell><cell cols="2">0.43M, 10.6K 99.00</cell><cell>0.988</cell></row><row><cell></cell><cell cols="2">Pose-CASIA [23]</cell><cell></cell><cell cols="2">1.26M, 10.6K 99.15</cell><cell>0.992</cell></row><row><cell></cell><cell cols="2">UMDFaces [2]</cell><cell></cell><cell>0.34M, 8.5K</cell><cell>99.15</cell><cell>0.992</cell></row><row><cell></cell><cell cols="2">VGG Face [24]</cell><cell></cell><cell>1.6M, 2.6K</cell><cell>98.40</cell><cell>0.975</cell></row><row><cell></cell><cell cols="2">MSCeleb [11]</cell><cell></cell><cell>4.2M, 62.5K</cell><cell>99.62</cell><cell>0.997</cell></row><row><cell></cell><cell cols="2">Min samp/id</cell><cell></cell><cell>Size, Class</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>10</cell><cell></cell><cell cols="2">4.48M, 62.7K 99.56</cell><cell>0.996</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell></cell><cell cols="2">4.47M, 62.5K 99.62</cell><cell>0.997</cell></row><row><cell></cell><cell></cell><cell>50</cell><cell></cell><cell cols="2">3.91M, 47.3K 99.60</cell><cell>0.997</cell></row><row><cell></cell><cell></cell><cell>70</cell><cell></cell><cell>3.11M, 33K</cell><cell>99.55</cell><cell>0.996</cell></row><row><cell></cell><cell></cell><cell>100</cell><cell></cell><cell>1.5M, 12.7K</cell><cell>99.23</cell><cell>0.991</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True positive rate</cell><cell>0.94 0.92</cell><cell></cell><cell></cell><cell cols="2">BaseCNN Casia-Net BaseCNN+CL-FN</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>BaseCNN-FN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VGGNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.02</cell><cell>0.04</cell><cell>0.06</cell><cell>0.08</cell><cell>0.1</cell><cell>0.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>False positive rate</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Study the influences from CNN related issues. All CNN models are trained with the CASIA<ref type="bibr" target="#b43">[44]</ref> dataset. CL-center loss<ref type="bibr" target="#b38">[39]</ref>, FN-feature normalization. CN-mod modifies the Cas-Net<ref type="bibr" target="#b43">[44]</ref> by replacing Pool layer with a FC layer of 512 neurons.</figDesc><table><row><cell>Settings</cell><cell># params</cell><cell>Acc %</cell><cell>T@F 0.01</cell></row><row><cell>Base-CNN (proposed)</cell><cell>40.5M</cell><cell>99.00</cell><cell>0.988</cell></row><row><cell>Base-CNN -FN</cell><cell>40.5M</cell><cell>97.40</cell><cell>0.954</cell></row><row><cell>Base-CNN + CL -FN</cell><cell>44.8M</cell><cell>98.85</cell><cell>0.986</cell></row><row><cell>VGG-Net [24]</cell><cell>182M</cell><cell>95.15</cell><cell>0.883</cell></row><row><cell>Cas-Net [44]</cell><cell>6M</cell><cell>97.10</cell><cell>0.938</cell></row><row><cell>CN-mod</cell><cell>8M</cell><cell>97.50</cell><cell>0.956</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We consider only the CNN based methods. For the others, we refer readers to the recently published survey<ref type="bibr" target="#b16">[17]</ref> for LFW and<ref type="bibr" target="#b15">[16]</ref> for IJB-A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In case of multiple faces, we take the face closer to the image center.<ref type="bibr" target="#b2">3</ref> If the landmarks detector fails we keep the face image by cropping it based on the given/detected bounding box.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We take the list of 5.05M faces provided by<ref type="bibr" target="#b40">[41]</ref> and keep nonoverlapping (with test set) identities which has at least 30 images after successful landmarks detection.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We train the CN-mod (seeTable 6) with the MSCeleb dataset and observed that, compared to our proposed CNN model CN-mod provides lower results and generalizes poorly.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition using deep multi-pose representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lekust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01484</idno>
		<title level="m">Umdfaces: An annotated face dataset for training deep networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition and retrieval using cross-age reference coding with cross-age celebrity dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="804" to="815" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="566" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unconstrained face verification using deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE WACV</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An end-to-end system for unconstrained face verification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE ICCV-W</title>
		<meeting>of IEEE ICCV-W</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03958</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust face recognition via multimodal deep face representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07108</idno>
		<title level="m">Recent advances in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/1607.08221</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A benchmark study of large-scale unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE IJCB</title>
		<meeting>of IEEE IJCB</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">VIPLFaceNet: An open source deep face recognition sdk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03892</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pose-aware face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4838" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do We Really Need to Collect Millions of Faces for Effective Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.05417</idno>
		<title level="m">Triplet probabilistic embedding for face verification and clustering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Triplet similarity embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03418</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Failure detection for facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06451</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<title level="m">DeepID3: Face recognition with very deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2892" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparsifying neural network connections for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koutroumbas</surname></persName>
		</author>
		<title level="m">Pattern Recognition, Fourth Edition</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>4th edition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face search at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Latent factor guided convolutional neural networks for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4893" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02683</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face recognition via active annotation and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia</title>
		<meeting>of ACM Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1058" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04690</idno>
		<title level="m">Naive-deep face recognition: Touching the limit of lfw benchmark or not</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
