<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Encoding robust representation for graph generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmian</forename><surname>Zou</surname></persName>
							<email>dzou@umn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Mathematics and its Applications</orgName>
								<orgName type="institution" key="instit1">University of Minnesota</orgName>
								<orgName type="institution" key="instit2">Twin Cities Minneapolis</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
							<email>lerman@umn.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<addrLine>Twin Cities Minneapolis</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Encoding robust representation for graph generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative networks have made it possible to generate meaningful signals such as images and texts from simple noise. Recently, generative methods based on GAN and VAE were developed for graphs and graph signals. However, the mathematical properties of these methods are unclear, and training good generative models is difficult. This work proposes a graph generation model that uses a recent adaptation of Mallat's scattering transform to graphs. The proposed model is naturally composed of an encoder and a decoder. The encoder is a Gaussianized graph scattering transform, which is robust to signal and graph manipulation. The decoder is a simple fully connected network that is adapted to specific tasks, such as link prediction, signal generation on graphs and full graph and signal generation. The training of our proposed system is efficient since it is only applied to the decoder and the hardware requirements are moderate. Numerical results demonstrate stateof-the-art performance of the proposed system for both link prediction and graph and signal generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Generative neural networks have been successfully applied to various tasks such as the generation of images and texts. Their development is based on fruitful methods of deep learning, such as convolutional and recurrent neural networks. These and other methods of deep learning, which were initially developed for problems in the Euclidean domain <ref type="bibr" target="#b0">1</ref> , have been successfully generalized to address supervised learning tasks in the graph domain. In particular, a variety of graph convolutional networks have been developed, including networks with prescribed parameters <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and trained networks <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b6">[6]</ref>. It is natural to use these tools to build generative models in the graph domain.</p><p>Most generative graph networks directly use standard graph networks by following either of the following two generative frameworks: the generative adversarial network (GAN) <ref type="bibr" target="#b7">[7]</ref> and the variational auto-encoder (VAE) <ref type="bibr" target="#b8">[8]</ref>. In a GAN, a generator and an auxiliary adversarial discriminator are trained together. On the other hand, in VAE, an encoder and a decoder (or generator) are both trained according to Bayesian models. Both frameworks contain two components (generator and discriminator or encoder and decoder), where each of them requires training. For GAN, training two components corresponds to a difficult min-max problem. On the other hand, training the two components in VAE can be described as a relatively easier This research has been supported by NSF award DMS-18-30418. <ref type="bibr" target="#b0">1</ref> We remark that the Euclidean and graph domains include scenarios whose underlying datasets have Euclidean and graph structures, respectively. non-convex minimization problem. However, it is a crude approximation to the motivating variational inference formulation. Given an encoding process with guaranteed mathematical properties, one can focus on training only the decoder, which in this case is the generator. In the Euclidean domain, <ref type="bibr" target="#b9">[9]</ref> uses the scattering transform as an encoder, which is robust to deformations of input signals, and learn a generative model by minimizing the l 1 -loss for reconstructing the training images. We adopt a similar method, using a graph scattering transform as an encoder for graph signals, which is robust to signal and graph manipulations, and train neural networks corresponding to respective tasks. Note that the graph scattering transform can be either carried out in the spectral domain <ref type="bibr" target="#b0">[1]</ref> or in the graph domain <ref type="bibr" target="#b1">[2]</ref>. More specifically, spectral-domain wavelets <ref type="bibr" target="#b10">[10]</ref> are one-dimensional wavelets applied to the eigenvalues of the graph Laplacian, whereas graph-domain wavelets, or diffusion wavelets <ref type="bibr" target="#b11">[11]</ref>, are multi-scale functions on the vertices of the graph that use dyadic powers of the diffusion operator. The advantage of the former graph scattering transform is its guaranteed robustness to signal manipulation, which is a consequence of its energy preservation <ref type="bibr" target="#b0">[1]</ref>. We therefore emphasize this transform here, but we also experiment with the other transform. We also remark that the spectral-domain wavelets of the former transform allow flexible choices of wavelet functions. We note that a conventional generative scattering network <ref type="bibr" target="#b9">[9]</ref> is not as competitive as state-of-the-art results based on GAN and VAE for image generation tasks. This is probably due to the complexity of some Euclideantype datasets and their nontrivial high frequency components. Nevertheless, graph-type datasets have a discrete nature, and they often do not exhibit high frequency components. Therefore, the graph scattering transform might be competitive and efficient for specific generative tasks in the graph domain.</p><p>We consider three types of graph generation tasks:</p><p>• Link prediction: In this task, one is interested in predicting whether two vertices from the same graph are connected. The common input includes a graph with missing edges and features of vertices. The goal is to decide whether an edge exists between any pair of vertices. This can be viewed as generating a graph from a latent representation of the partially available graph. A well-known application of this task is the prediction of citations. Common citation datasets were collected by <ref type="bibr" target="#b12">[12]</ref> and further pre-processed by <ref type="bibr" target="#b13">[13]</ref>. These datasets of publications and citations contain features for each publication as well as citation linkage, which are modeled by an undirected graph. In the pre-processed data, one only partially knows the citation linkage and the task is to recover the citations for all pairs of publications. • Signal generation on graphs: In this task, the graph is fixed and the set of vertices and edges is known. The goal is to generate signals on the given graph. Although we are unaware of convincing data of this type, we believe that this is a potentially useful task. Among the three tasks we review here, it is most similar to generation tasks in Euclidean domains. We can thus enforce some graph structure in special Euclidean-type or grid-type datasets. Here we pursue this idea with the Fashion-MNIST dataset of images of clothing items <ref type="bibr" target="#b14">[14]</ref>. We do not consider the domain of a 28 × 28 pixel image as Euclidean, but associate to it a graph, where each pixel is connected by an edge with its nearby neighbors. Each image is then a signal on this graph. One then needs graph-based methods for generating these signals. • Graph and signal generation: In this task, one needs to generate both the graph structure and the signals on the graph. An interesting application is the design of chemical molecules. A network learns from a given dataset, such as QM9 <ref type="bibr" target="#b15">[15]</ref>, to generate both the atoms (signals on vertices) and the bonds (edges) from a latent sample. This can be used as a purely machine learning-based approach for the design of new drugs <ref type="bibr" target="#b16">[16]</ref>.</p><p>Our proposed method is easy to implement. Furthermore, the adjustment of the structure of the decoder to the three types of tasks does not require a lot of effort. Unlike GAN or VAE, the model in this paper only requires training the generator. Meanwhile, there is flexibility in designing the graph wavelets and in choosing the decoder structure. We believe it is a highly adaptable method for various graph tasks and indeed our numerical experiments demonstrate competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>We overview previous relevant works as follows: §II-A reviews generative scattering networks, §II-B reviews graph convolutional networks, and §II-C reviews some recent graph generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scattering networks for generative models</head><p>The generative scattering network <ref type="bibr" target="#b9">[9]</ref> can be considered as an encoder-decoder system in which one only needs to train the decoder. The feature extraction part of the encoder is a scattering transform <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref> with fixed parameters. It provides multi-scale signal representation, which is Lipschitz continuous with respect to small deformations. The next part of the encoder aims to map the transformed signals into samples of a Gaussian latent variable. We refer to this step as Gaussianization. We later describe in §III two Gaussianization methods. The decoder D can be taken to be a multi-layer perceptron (MLP) and is trained by minimizing the reconstruction loss. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the structure of a generative scattering model. In order to generate samples, initial samples are generated from the latent Gaussian variable and then the decoder is applied to them resulting in the final samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph convolutional networks</head><p>Convolution is a key contributor for the recent success of deep learning. In the Euclidean domain, convolutional networks are helpful in learning multi-scale representations. The same idea was introduced to graphs by exploiting the spectral graph representation, that is, the spectral decomposition of the graph weight matrix or graph Laplacian <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b19">[19]</ref>. A common proposal for a graph convolution uses pointwise multiplication of the graph Fourier-transformed signals, where the graph Fourier transform uses the basis of the spectral decomposition of the Graph Laplacian in place of the discrete Fourier basis. One can apply nonlinear functions such as the ReLU for each graph vertex. A variety of good approximations to the spectral approach are able to speed up the spectral decomposition process and maintain accuracy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>.</p><p>A special type of graph convolutional network (GCN) is the graph scattering network <ref type="bibr" target="#b0">[1]</ref>, which does not require training and was proved to be approximately invariant to permutations and stable to sufficiently small signal or graph manipulations. A graph scattering network uses graph wavelets <ref type="bibr" target="#b10">[10]</ref> defined on the eigenspace of the graph Laplacian to construct multilayer models. Alternatively, <ref type="bibr" target="#b1">[2]</ref> construct graph scattering transforms using an earlier graph wavelet transform <ref type="bibr" target="#b11">[11]</ref>. In general, the graph wavelets of <ref type="bibr" target="#b10">[10]</ref> are more flexible as one can choose different kinds of wavelets on the spectral domain according to different tasks. While the graph wavelets of <ref type="bibr" target="#b11">[11]</ref> are not flexible, they use the normalized graph Laplacian and the corresponding diffusion map and metric, which might be natural for particular applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph generative networks</head><p>Several recent papers address graph generation networks with either a GAN or a VAE structure, where specific designs are often needed for different applications.</p><p>Recent graph generative models with a GAN structure include NetGAN <ref type="bibr" target="#b20">[20]</ref>, GraphGAN <ref type="bibr" target="#b21">[21]</ref> and MolGAN <ref type="bibr" target="#b22">[22]</ref>. NetGAN aims to generate graphs with properties, such as max degree and triangle count, that are similar to training samples. GraphGAN generates distribution of edges in order to solve node classification and recommendation problems whose tasks are very similar to that of the link prediction problem.</p><p>MolGAN is designed for molecule generation and generates signals with respect to both vertices and edges.</p><p>Recent graph generative models with a VAE structure include VGAE <ref type="bibr" target="#b13">[13]</ref>, GraphVAE <ref type="bibr" target="#b23">[23]</ref> and JT-VAE <ref type="bibr" target="#b24">[24]</ref>. VGAE aims to solve the link prediction problem by completing the adjacency matrix. GraphVAE is designed for molecule generation. It generates the adjacency matrix as well as the vertex and edge features. JT-VAE is specifically designed for generating complex molecules while enforcing validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPH GENERATIVE SCATTERING NETWORK</head><p>Existing graph generation networks require training either GAN or VAE, which is a difficult task. Furthermore, the design of a graph generation network is often complex <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b24">[24]</ref> and its hyperparameter selection might be difficult. In view of these obstacles, we propose here the graph generative scattering network. It is composed of two components: an encoder and a decoder. The encoder is a graph scattering network <ref type="bibr" target="#b0">[1]</ref>, which is followed by a Gaussianization step. It produces a latent Gaussianized representation for the graph signal. It is also used to form a latent "Gaussian distribution." The parameters of the graph scattering network are predetermined, unlike the parameters of a generative auto-encoder which are learned. The decoder is trained by using the Gaussianized latent representation of the data and minimizing a loss function chosen according to the corresponding task. In all of the graph-specific tasks we mentioned in §I, the decoder can be taken to be a network with fully-connected layers, whose structure is determined by the specific task. Generation is obtained by applying the trained decoder to the latent Gaussian distribution. More details on forming the encoder and decoder are provided in §III-A and §III-B, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of the Encoder</head><p>In order to fully understand the formation of the encoder, we review the graph scattering network of <ref type="bibr" target="#b0">[1]</ref> and explain how to form a Gaussian distribution from its output. We consider a graph G = (V, E) with |V | = N vertices. A signal in L 2 (V ; R K ) can be regarded as a matrix X ∈ R N ×K . The scattering transform can be regarded as a function that acts on the columns of X. Let L ∈ R N ×N be the unnormalized graph Laplacian L = D −W , where D is the diagonal matrix of degrees and W is the weight (adjacency) matrix whose (n, m)-th entry is the weight of the edge connecting vertices v n and v m . Its spectral decomposition can be written as L =</p><formula xml:id="formula_0">N −1 l=0 λ l u l u * l , with 0 = λ 0 ≤ · · · ≤ λ N −1 .</formula><p>We assume a limiting scale, J ∈ Z, and dyadic wavelets, φ and ψ, satisfying</p><formula xml:id="formula_1">|φ −J | 2 + j&gt;−J |ψ j | 2 = 1, wherê ψ j (ω) =ψ(2 −j ω) for j &gt; −J andφ −J (ω) =φ(2 −J ω).</formula><p>For f ∈ R N , the graph wavelet transform <ref type="bibr" target="#b10">[10]</ref> is</p><formula xml:id="formula_2">f * ψ j = N −1 l=0 u l u * l fψ(2 −j λ l ), for j &gt; −J ; f * φ −J = N −1 l=0 u l u * l fφ(2 J λ l ) .</formula><p>For any m no larger than the number of layers, a path p = (j 1 , · · · , j m ) is a vector of m scales of the graph wavelets, which satisfy 0 ≤ j 1 , · · · , j m ≤ J −1. The scattering propagator with respect to a path p is obtained by consecutive application of convolutions with wavelets of these scales and absolute values, which serve as nonlinearities, in the following way</p><formula xml:id="formula_3">U [p]f = f * ψ j1 * ψ j2 * · · · * ψ jm .</formula><p>The scattering transform with respect to the path p is obtained by</p><formula xml:id="formula_4">S[p]f = U [p]f * φ −J .</formula><p>Let P denote the collection of all paths of length no larger than the number of layers. The scattering transform of f with respect to P is</p><formula xml:id="formula_5">S[P]f = (S[p]f ) p∈P .</formula><p>A simple illustration of the scattering transform is provided in <ref type="figure" target="#fig_0">Fig. 1</ref> of <ref type="bibr" target="#b0">[1]</ref>. Note that the scattering transform depends on the underlying graph. For simplicity, we exclude this dependence from our notation.</p><p>For</p><formula xml:id="formula_6">the K-dimensional signal X = [X 1 | · · · |X K ] ∈ R N ×K , the scattering transform is S[P]X = (S[P]X k ) K k=1 .</formula><p>We remark that if the set P has L elements, then defining M = LK, S[P](X) can be represented as a matrix in R N ×M or a vector in R N ·M . We denote the latter vector byX. Zou and Lerman <ref type="bibr" target="#b0">[1]</ref> establish various theoretical properties of the scattering transform S[P]. In particular, they show that it preserves the energy-that is, the l 2 norm-of each signal. The usefulness of this property for graph generation will be discussed later. Another important property they establish is the robustness of the scattering transform to small perturbations of the signal and the graph <ref type="bibr">[1, §5]</ref>. This robustness implies that similar signals and graphs are encoded as similar latent codes.</p><p>The last step of the encoder maps the transformed data points so that one may possibly generate them with a Gaussian distribution, as required by the generator. We earlier referred to this mapping as Gaussianization. We describe two possible mappings that were previously suggested for related tasks. Following <ref type="bibr" target="#b9">[9]</ref>, one such mapping is whitening. Specifically, let {X (t) } T t=1 be the scattering transform vectors of the input samples and letX be the representing matrix of</p><formula xml:id="formula_7">{X (t) } T t=1 . That is,X = (X (t) ) T t=1 ∈ R T ×N M .</formula><p>As advocated in <ref type="bibr" target="#b9">[9]</ref>, a dimension reduction by PCA can be further applied to {X (t) } T t=1 . Next, using the following mean and sample covariance of the scattering transform vectors</p><formula xml:id="formula_8">µ = 1 T T t=1X (t) and Σ = 1 T T t=1 (X (t) − µ)(X (t) − µ) * ,</formula><p>the whitening map A is</p><formula xml:id="formula_9">AX = Σ −1/2 (X − µ) .<label>(1)</label></formula><p>The whitened samples are uncorrelated and the hope in <ref type="bibr" target="#b9">[9]</ref> is that their distribution will be close to that of a normal distribution with identity covariance. The output of the encoder with a whitening transformation is guaranteed to have similar robustness results to signal and graph manipulations as the ones established in <ref type="bibr" target="#b0">[1]</ref> for the graph scattering transform with additional factor of Σ −1/2 , where · denotes the spectral norm of the corresponding matrix. We remark that due to the initial dimension reduction that removed small eigenvalues of the sample covariance of the transformed data points, Σ −1/2 , is not expected to be large.</p><p>One problem with the whitening process is that the distribution of its output may not be close to Gaussian. While traditional Gaussianization methods <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> can improve the distribution of the output, their encoding may not be robust to signal and graph manipulation.</p><p>An alternative to whitening is a spherization procedure inspired by <ref type="bibr" target="#b27">[27]</ref>. That is, the data points {X (t) } T t=1 are normalized to lie on the unit sphere in the Euclidean norm. The hope is that these mapped data points can be generated from a Gaussian with their sample mean and covariance. In general, this may not be the case, though <ref type="bibr" target="#b27">[27]</ref> have a heuristic and incomplete argument for this property in a different setting. This argument can be made precise by using the Gaussian Annulus Theorem (see e.g., <ref type="bibr" target="#b28">[28,</ref><ref type="bibr">Theorem 2.9]</ref>).</p><p>Due to the energy preservation property of the full graph scattering transform of <ref type="bibr" target="#b0">[1]</ref>, one can instead spherize the input data points and the result of the encoder will be the same. Therefore, in theory, the encoder is robust to signal and graph manipulation with respect to the original data after spherization. Due to this property and similarly to <ref type="bibr" target="#b27">[27]</ref>, we do not initialize the spherization by centering with the sample mean. In practice, since scattering is only applied to a finite number of layers, the energy is contracted. Therefore, it is better to normalize after the scattering and this is what we do in our experiments. The final stage of the encoder with spherization calculates the sample mean µ s and covariance σ s for the spherized scattering output and fits a Gaussian N (µ s , σ s ), which is used as the latent distribution for sampling.</p><p>We denote the original samples by {X (t) } T t=1 and the corresponding data matrix in R T ×N M by X . We further denote the output of the encoder (with either whitening or spherization) by Φ[P](X ), or in short Φ(X ). We note that the mapping Φ can also be applied to any signal X ∈ R N ×K . We denote the feature vector corresponding to the K-dimensional signal X by z = Φ[P](X) ∈ R N ×M . We will refer to z as a latent code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of the decoder</head><p>Recall that the decoder is a network with fully-connected layers. We describe its architecture according to the following three different tasks.</p><p>1) Link prediction: For link prediction, we encode the features of the partially available graph into a latent vector, and use the same vector to generate the full graph via the learned decoder. Note that in this task only one fixed graph  is given, and thus no Gaussianization procedure is applied in the encoder. That is, the linear transformation A in <ref type="formula" target="#formula_9">(1)</ref> is the identity. The input includes a weight matrix W train , which contains weights for the partially available edges, and a feature matrix X ∈ R N ×K of K-dimensional signals on the N nodes. The encoder is a scattering network Φ that maps X and W train ∈ R N ×N into a latent representation z ∈ R N ×M . As in <ref type="bibr" target="#b13">[13]</ref>, the decoder is a simple network D such that D(z) = σ(D(z)D(z) T ), where σ is the softmax function andD is an MLP. The network D, whose parameters are those ofD, is trained to minimize the cross-entropy loss function</p><formula xml:id="formula_10">L(D) = i,j:W (i,j) =0 [− log D(Φ(X, W ))(i, j)] . (2)</formula><p>The structure of the entire network is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>2) Signal generation on graphs: For signal generation on graphs, which we also refer to as graph signal generation, one is given a fixed graph domain and different signals on the nodes of this graph and the goal is to generate similar signals. An input random variable X ∈ R N ×K is first mapped by the scattering transform to S[P](X) and then to a Gaussian z = Φ[P](X) ∈ R N ×M . The decoder D is taken to be an MLP that maps z to a matrix D(z) in R N ×K . The parameters of D are obtained by minimizing the reconstruction loss function</p><formula xml:id="formula_11">L(D) = T −1 T t=1 X (t) − D(Φ(X (t) )) ,<label>(3)</label></formula><p>where {X (t) } T t=1 are the training data points. The structure of the generative network is the same as in <ref type="figure" target="#fig_0">Fig. 1</ref>, where in the current case S is the graph scattering transform. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the generation procedure.</p><p>3) Graph and signal generation: The scattering transform can be used as an encoder for generating both the graph and the signal on it. We train two MLP's D 1 and D 2 , where both take the Gaussian random variable z = Φ(X) as input. The network D 1 is used to generate the graph signal X and the network D 2 (z) = σ(D 2 (z)D 2 (z) T ) is used to generate the weight matrix W . They are trained at the same time, with the reconstruction loss function</p><formula xml:id="formula_12">L(D 1 , D 2 ) =T −1 T t=1 W (t) − D 1 (Φ(X (t) )) + X (t) − D 2 (Φ(X (t) )) ,<label>(4)</label></formula><p>where {(X (t) , W (t) )} T t=1 are the training data points. The norms can be replaced with cross-entropy losses if one wants the outputs to be categorical, as in the molecule generation task. <ref type="figure" target="#fig_3">Fig. 4</ref> illustrates this generation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We test our proposed method, whose code is available at https://github.com/dmzou/SCAT, and compare it with other available algorithms using datasets addressing the three different tasks reviewed in §I.</p><p>Even though we advocate using the graph scattering network of <ref type="bibr" target="#b0">[1]</ref> (due to its robustness to signal manipulation), we also tested our generation algorithm with the graph scattering network of <ref type="bibr" target="#b1">[2]</ref>. When addressing link prediction (so a Gaussianization procedure is not applied), we denote by SCAT-S and SCAT-D our proposed generative procedure with the spectral and diffusion networks of <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref> respectively. For the other two applications, we distinguish between the two Gaussianization procedures: whitening and spherization. We use "W" to denote whitening and "N" to denote normalization, i.e., spherization. We use, as above, "S" to denote the spectral graph scattering transform in <ref type="bibr" target="#b0">[1]</ref> and "D" to denote the diffusion graph scattering transform in <ref type="bibr" target="#b1">[2]</ref>. We denote the four resulting networks by SCAT-SW, SCAT-DW, SCAT-SN and SCAT-DN.</p><p>Following <ref type="bibr" target="#b0">[1]</ref> we use in all experiments the simple Shannon wavelet and the limiting scale J = 3 for SCAT-S, SCAT-SW and SCAT-SN. We similarly use J = 3 and choose t = 3 (t is the power of multiplying diffusion operator) for SCAT-D, SCAT-DW and SCAT-DN. For link prediction, we take 2-layer scattering as it performs better on the validation set; for the other two tasks, we take 3-layer scattering.</p><p>Our method requires moderate memory and GPU and we easily tested it on a PC with 8GB RAM and GTX1060 GPU.  <ref type="table" target="#tab_0">Cora  2708  5429  7  1433  Citeseer  3327  4732  6  3703  Pubmed  19717 44338  3  500</ref> Comparison with VGAE on the Pubmed dataset demanded more advanced GPU. We thus report results of all experiments on a Linux machine with 64GB RAM and GTX1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Link prediction for citation data</head><p>We predict links for the three citation datasets of <ref type="bibr" target="#b12">[12]</ref>: Cora, Citeseer and Pubmed. Each dataset contains information about publications in certain fields and the corresponding citation linkage between these publications. This information can be embedded in a graph in which the publications and citations are represented by vertices and edges, respectively. Although the citation link is directed, we follow the common convention of assuming an undirected and unweighted graph. That is, if any of two papers cite the other, an edge is drawn between them. <ref type="table" target="#tab_0">Table I</ref> lists some characteristics of the three datasets.</p><p>We use the preprocessing step suggested in <ref type="bibr" target="#b13">[13]</ref> and documented in https://github.com/tkipf/gae for all tests. It divides the original edges into 85% training, 5% validating and 10% testing sets.</p><p>For SCAT-S and SCAT-D, the dimension of the output of the scattering transform is reduced to 128. The decoderD in both models is taken to be a single dense layer of size 512, activated by the ReLU function. In order to minimize (2), we use the Adam optimizer <ref type="bibr" target="#b29">[29]</ref> with a learning rate of 0.001, where we train 1,000 epochs for each run. We record the following two common scores: area under curve (AUC) and average precision (AP). Similarly to <ref type="bibr" target="#b13">[13]</ref>, we take 10 runs for each setting and record the average and standard deviation. <ref type="table" target="#tab_0">Table II</ref> reports our results for SCAT-S and SCAT-D, together with results for GAE and VGAE <ref type="bibr" target="#b13">[13]</ref>. We note that SCAT-S improves over the previous results for all the three datasets, while SCAT-D achieves comparable results with GAE and VGAE. The averaged time for the scattering transform of SCAT-S and SCAT-D, applied to Cora/Citeseer/Pubmed, is 2.82s/6.43s/492.90s and 1.77s/4.31s/346.43s respectively. SCAT-S requires more time due to the spectral decomposition, whose average time for the 3 datasets is 1.24s/2.16s/317.43s. <ref type="table" target="#tab_0">Table III</ref> reports the training time for each epoch. This table implies that SCAT-S and SCAT-D are much more efficient. This is because only parameters in the decoder need to be updated. Combining the times of the scattering transforms reported above, which are executed once, and the training times for each epoch in <ref type="table" target="#tab_0">Table III</ref>, multiplied by the number of epochs, we conclude that the total times of SCAT-S and SCAT-D are more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Signal generation on graphs</head><p>We use the Fashion-MNIST dataset <ref type="bibr" target="#b14">[14]</ref> for a sanity check of SCAT for the problem of graph signal generation. Any element of this dataset is a 28 × 28 grayscale pixel image II: Results for link prediction using the citation datasets. We report the mean and standard deviation over 10 runs for each setting. All models for the same dataset are trained based on the same training and validating links.  and can be considered as a graph in the following way: the pixels are the graph vertices, and nearby pixels are connected by graph edges. The edges and weights are formed as in <ref type="bibr" target="#b0">[1]</ref>. That is, each pixel represents a vertex and it is connected with its four nearest neighbors with weight e −1 and its four nearest diagonal neighbors with weight e −2 . The encoder of this network is a graph scattering transform. Its output dimension is reduced to 256 from 28 × 28 × 13 = 10,192. The decoder is an MLP of two hidden layers of size 512. In order to minimize (3), we use the Adam optimizer with a learning rate of 0.001, where we train 2,000 epochs for each run. To avoid mode collapse, we have restricted the dataset to the "boots" category, which contains 5,454 training examples. Sample images from this category are demonstrated in <ref type="figure" target="#fig_5">Fig. 6a</ref>.</p><p>To generate an "image" (or graph signal), we take a sample z ∈ R N from N (0, I) for SCAT-SW and SCAT-DW, and from N (µ s , Σ s ) for SCAT-SN and SCAT-DN, and report the output of the decoder. Figs. 6b-6e illustrate samples generated by these networks, while using the graph associated with a 28×28 pixel image. We report the 1 reconstruction loss defined in (3) in <ref type="table" target="#tab_0">Table IV</ref>. Note that the averaged 1 -norm of the training images is 174.56. We also report the times for scattering and training a single epoch.</p><p>Our experiments indicate that training models with spherization converges faster than with whitening. To see this, we plot the reconstruction loss as a function of epoch for the first 500 epochs for the four methods we tested in <ref type="figure" target="#fig_4">Fig. 5</ref>. It is clear that SCAT-SN and SCAT-DN converge much faster in the first 300 epochs.</p><p>The images generated in <ref type="figure" target="#fig_6">Figs. 6b-6e</ref> are of similar quality for all the scattering methods. Note that some high frequency  components, that is, details, of these images are missing. This phenomena is is common with the Euclidean generative scattering network of <ref type="bibr" target="#b9">[9]</ref>. Nevertheless, common graph-type data often do not have high-frequency components. For instance, in the molecular data reviewed in §IV-C, the signals on each vertex just take five different values. For comparison with other methods, Figs. 6f and 6g illustrate samples generated from graph generative models that combine VAE/GAN with the graph convolutional layers proposed in <ref type="bibr" target="#b5">[5]</ref>. Specifically, for what we call VAE-GCN, we construct a graph-based VAE in which the encoder consists of two graph convolutional layers while the decoder is an MLP of two hidden layers. The latent mean and variance both have dimension 256 and the hidden layers of the MLP have dimension 512. For what we call GAN-GCN, we replace the discriminator of a vanilla GAN with two graph convolutional layers and use an MLP with two hidden layers of dimension 512 for the generator.</p><p>All three methods are trained using Adam with a learning rate of 0.001. The latent noise for both GAN-GCN and GAN-FCN is of dimension 256. The training time for each epoch for VAE-GCN and GAN-GCN is 3.947s and 1.761s respectively, which is much slower than training SCAT models since parameters from both the generator and the discriminator need to be updated for GAN-GCN, and parameters from both the encoder and the decoder need to be updated for VAE-GCN. Observing <ref type="figure" target="#fig_5">Figs. 6f and 6g</ref>, we see that samples from VAE-GCN look blurrier than those of SCAT models and also miss high-frequency information. Samples from GAN-GCN are very noisy and still miss high-frequency information and suggest that the model suffers from a severe mode collapse.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph generation for molecular data</head><p>We test graph and signal generation using the molecular dataset QM9 <ref type="bibr" target="#b15">[15]</ref>. This dataset contains 134k molecules made of the following atoms: C, H, O, N, and F. There are two common ways of embedding these kinds of datasets into an interpretable feature space. Kusner et al. <ref type="bibr" target="#b30">[30]</ref> treat molecules as "words" by looking at their simplified molecular-input lineentry (SMILE) strings. Graphs are also commonly used to represent molecules, where the graph vertices represent the atoms composing the molecule and the graph edges are the bonds. The vertex signals assign the four different atom types to the vertices. While there are five type of atoms, H is automatically determined by the other atoms and the given chemical bonds. Therefore, only labels of the four heavy atoms (C, O, N or F) need to be assigned. Since the dataset has molecules with at most 9 heavy atoms, we assume graphs with 9 vertices and assign a dummy value for vertices without assignment of a heavy atom. Each heavy atom and the dummy one are assigned a one-hot vector, i.e., a unit coordinate vector in R 5 , and each atom is represented by one of these five onehot vectors.</p><p>For graph generation, there is no unique benchmark for checking the quality of the generated graphs. Bojchevski et al. <ref type="bibr" target="#b20">[20]</ref> proposed to use graph properties such as the max degree and the number of triangles for graph generation. However, it is often hard to compare these graph properties and there is no motivation for using them for molecule generation. Samanta et al. <ref type="bibr" target="#b31">[31]</ref> proposed to check validity (whether a sample is a valid chemical molecule), uniqueness (whether a sample is unique among all generated samples) and novelty (whether a sample is different from any sample in the training data). We use these measures since they are more quantitative and experiments on QM9 by <ref type="bibr" target="#b22">[22]</ref> and <ref type="bibr" target="#b23">[23]</ref> also report them. They are checked after converting the graphs into SMILE strings using the RDKit package (https://www.rdkit.org/). The full QM9 dataset is used for training. This choice is the same as that in <ref type="bibr">[22, §5.3]</ref>. It is different than <ref type="bibr" target="#b23">[23]</ref>, in which a small set of molecules is used for training and only molecules with 9 heavy atoms are considered.</p><p>As explained in §III-B3, for this application of graph generation, the SCAT decoders for both vertices and edges are MLP's. In our experiments, both of them have three hidden layers of dimension 128, 256, 512, respectively. We take the encoder to be a graph scattering transform, with output dimension reduced to 135 (15 for each vertex). In order to minimize (4), we use the Adam optimizer with a learning rate of 0.001, where we train 300 epochs. <ref type="table" target="#tab_5">Table V</ref> reports the computational times of scattering and training on this dataset.</p><p>Using SCAT, we generate 10k molecules and record the validity, uniqueness, and novelty in <ref type="table" target="#tab_0">Table VI</ref>. For comparison, we also record results reported for GraphVAE in <ref type="bibr" target="#b23">[23]</ref> and our test of MolGAN <ref type="bibr" target="#b22">[22]</ref> based on the codes available at All the SCAT models achieve high scores in uniqueness and novelty. Note that SCAT-DW and SCAT-DN achieve slightly higher uniqueness and novelty than SCAT-SW and SCAT-SN, but they have notably lower score in validity. We remark that it is possible to achieve high validity with the scattering generative models. We were able to achieve 93.9/17.6/98.6 for valid/unique/novel scores if we train SCAT-SW using an MLP with a single hidden layer with dimension 5 for the vertices and 32 for the edges. However, a simpler model leads to severe mode collapse, as implied by the low uniqueness score. We observed that this simple model assigns "carbon" to a lot of vertices, and it has two effects: first, it is easier to construct valid molecules 2 . Second, it causes mode collapse as there is not much variety of molecular decompositions.</p><p>We show samples of molecules generated by SCAT-SW with a decoder with three hidden layers in <ref type="figure" target="#fig_5">Fig. 7a</ref> and samples generated by SCAT-SW with a decoder with one hidden layer in <ref type="figure">Fig. 7b</ref>. It can be seen that many molecules in <ref type="figure">Fig. 7b</ref> are composed only of carbon (and hydrogen); in contrast, there are more oxygen and nitrogen in the samples in <ref type="figure" target="#fig_5">Fig. 7a</ref>. We believe that in order to explore new design of drugs, it is more important to generate molecules with more variety. Therefore, the results from three-layer decoders are more meaningful.</p><p>As a comparison, the three measures reported for GraphVAE 2 A carbon vertex can have degree at most four while keeping the molecule valid. The largest possible degree is only three for a nitrogen vertex and two for an oxygen vertex. are moderate. MolGAN has good validity and novelty scores, but low uniqueness score, which indicates a mode collapse <ref type="bibr" target="#b22">[22]</ref>. This mode collapse is even more severe when either validity or novelty is used for RL, although the model achieves perfect score for validity or novelty, respectively. We remark that MolGAN uses three hidden layers of the same dimensions and we thus believe that the mode collapse is not due to a lowcomplex structure of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed the graph generative scattering network as a generative model for graphs and graph signals. The network applies a prescribed encoder which does not require training and is robust to signal perturbation and graph deformations. Numerical experiments show competitive results for the tasks of link prediction in citation data and molecule generation. Although scattering usually takes time, it is still more efficient to train scattering-based models due to the smaller number of parameters to update in training. We believe the graph generative scattering network has the potential to be used in a wider range of applications on graphs.</p><p>We experimented with two possible choices for the scattering transform and two choices for Gaussianization. Overall, the scattering described in <ref type="bibr" target="#b0">[1]</ref> achieves better performance than the scattering described in <ref type="bibr" target="#b1">[2]</ref> for link prediction and molecule generation. We remark that for molecule generation the scattering of <ref type="bibr" target="#b1">[2]</ref> achieves slightly better scores of uniqueness and novelty, however, its validity score is significantly worse. There is no significant difference in the results for the two methods of Gaussianization. However, the spherization-based methods (SCAT-SN and SCAT-DN) converge much faster for graph signal generation.</p><p>We used the Fashion-MNIST dataset as a sanity check for the graph signal generation, since we are unaware of a more convincing application for this specific task. In this application, we do not expect graph-based methods to compete with general methods, because graphs only retain partial spatial relationships. Indeed, the resolution of the generated images is not as good as that of the original images. Nevertheless, since the results in the similar discrete tasks of link prediction and molecule generation are competitive, we believe that SCAT also bears promise for graph signal generation when the signals are of low resolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The structure of a generative scattering network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Sketch of a graph scattering network for link prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Sketch of a graph scattering network for signal generation on graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Sketch of the graph scattering network for graph and signal generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Reconstruction loss with respect to epoch for training SCAT models for the Fashion-MNIST data. No difference is noted after 500 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )</head><label>a</label><figDesc>Original data. (b) SCAT-SW. (c) SCAT-SN. (d) SCAT-DW. (e) SCAT-DN. (f) VAE-GCN. (g) GAN-GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Original and generated images for boots of Fashion-MNIST data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Characteristics of three citation datasets</figDesc><table><row><cell>Dataset</cell><cell>Vertices</cell><cell>Edges Classes Features</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>S 94.48 ± 0.15 94.63 ± 0.17 97.27 ± 0.12 97.57 ± 0.12 97.52 ± 0.03 97.19 ± 0.04 SCAT-D 92.08 ± 0.09 93.05 ± 0.11 92.54 ± 0.14 94.16 ± 0.12 92.73 ± 0.17 93.56 ± 0.09 GAE 91.34 ± 0.52 92.62 ± 0.38 92.37 ± 0.67 93.72 ± 0.58 96.35 ± 0.18 96.53 ± 0.16 VGAE 91.14 ± 0.40 92.16 ± 0.29 92.70 ± 0.76 93.93 ± 0.57 95.68 ± 0.35 95.92 ± 0.32</figDesc><table><row><cell>Dataset</cell><cell>Cora</cell><cell>Citeseer</cell><cell></cell><cell>Pubmed</cell><cell></cell></row><row><cell>AUC (%)</cell><cell>AP (%)</cell><cell>AUC (%)</cell><cell>AP (%)</cell><cell>AUC (%)</cell><cell>AP (%)</cell></row><row><cell>SCAT-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Time for training an epoch for the citation data</figDesc><table><row><cell>Dataset</cell><cell cols="2">SCAT-S SCAT-D</cell><cell>GAE</cell><cell>VGAE</cell></row><row><cell>Cora</cell><cell>8.1ms</cell><cell>8.1ms</cell><cell>209.1ms</cell><cell>206.4ms</cell></row><row><cell>Citeseer</cell><cell>8.1ms</cell><cell>8.1ms</cell><cell>298.6ms</cell><cell>302.3ms</cell></row><row><cell>Pubmed</cell><cell>64.9ms</cell><cell cols="3">64.9ms 7832.6ms 7889.2ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Reconstruction loss and time for scattering transform (for the complete training data) and training an epoch for the Fashion-MNIST dataset</figDesc><table><row><cell></cell><cell cols="2">SCAT-SW SCAT-DW</cell><cell>SCAT-SN</cell><cell>SCAT-DN</cell></row><row><cell>Reconstr. loss</cell><cell>0.0482</cell><cell>0.0532</cell><cell>0.0513</cell><cell>0.0548</cell></row><row><cell>Time (scattering)</cell><cell>1,128.6ms</cell><cell cols="3">1,484.3ms 1,148.3ms 1,566.0ms</cell></row><row><cell>Time (epoch)</cell><cell>39.4ms</cell><cell>39.1ms</cell><cell>30.7ms</cell><cell>30.6ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Time for the scattering transform (for the complete training data) and training an epoch for the QM9 dataset</figDesc><table><row><cell></cell><cell cols="4">SCAT-SW SCAT-DW SCAT-SN SCAT-DN</cell></row><row><cell>Time (scattering)</cell><cell>137.12s</cell><cell>94.75s</cell><cell>132.01s</cell><cell>95.95s</cell></row><row><cell>Time (epoch)</cell><cell>3.68s</cell><cell>3.66s</cell><cell>3.71s</cell><cell>3.69s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison of graph generation by GraphVAE, MolGAN and SCAT using the QM9 dataset. Values are reported in percentages according to 10k generated samples. Since the GraphVAE code is unavailable, results are copied from the paper and marked with parenthesis. Without RL, it takes approximately 11 seconds to train an epoch; with RL for validity or uniqueness, it takes 15-18s to train an epoch; with RL for novelty, it takes 93-122s to train an epoch.</figDesc><table><row><cell>Algorithm</cell><cell>Valid</cell><cell>Unique</cell><cell>Novel</cell></row><row><cell>GraphVAE</cell><cell>(55.7)</cell><cell>(76.0)</cell><cell>(61.6)</cell></row><row><cell>GraphVAE (imp)</cell><cell>(56.2)</cell><cell>(42.0)</cell><cell>(75.8)</cell></row><row><cell>GraphVAE (no GM)</cell><cell>(81.0)</cell><cell>(24.1)</cell><cell>(61.0)</cell></row><row><cell>MolGAN (no RL)</cell><cell>90.4</cell><cell>31.1</cell><cell>97.8</cell></row><row><cell>MolGAN (RL Valid)</cell><cell>100.0</cell><cell>0.3</cell><cell>13.6</cell></row><row><cell>MolGAN (RL Unique)</cell><cell>99.2</cell><cell>37.1</cell><cell>64.5</cell></row><row><cell>MolGAN (RL Novel)</cell><cell>98.5</cell><cell>0.6</cell><cell>100.0</cell></row><row><cell>SCAT-SW</cell><cell>65.4</cell><cell>92.7</cell><cell>86.9</cell></row><row><cell>SCAT-DW</cell><cell>38.0</cell><cell>98.1</cell><cell>94.2</cell></row><row><cell>SCAT-SN</cell><cell>64.9</cell><cell>92.0</cell><cell>85.7</cell></row><row><cell>SCAT-DN</cell><cell>47.4</cell><cell>98.3</cell><cell>92.0</cell></row><row><cell cols="4">https://github.com/nicola-decao/MolGAN. For MolGAN, we</cell></row><row><cell cols="4">perfrom four different tests. The first one (no RL) applies</cell></row><row><cell cols="4">MolGAN without reinforcement learning (RL). The next three</cell></row><row><cell cols="4">(RL Valid, RL Unique and RL Novel) apply RL for validity,</cell></row><row><cell cols="4">uniqueness and novelty, respectively. An RL step is done after</cell></row><row><cell cols="4">each five epochs. For each setting we only report the best result</cell></row><row><cell cols="4">among six choices of the parameter λ (0.01, 0.05, 0.1, 0.25,</cell></row><row><cell cols="4">0.5, 0.75) and three choices of the dropout rate (0, 0.1, 0.25)</cell></row><row><cell>used in [22].</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank Radu Balan, Aurobrata Ghosh, and Maneesh Singh for discussions on the link prediction problem and Alex Gutierrez for helpful comments on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Graph convolutional neural networks via scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00099</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Diffusion scattering transforms on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08829</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Samples of generated molecules via SCAT-SW. The number below each molecule is the Quantitative Estimate of Drug-likeness (QED) score [32], which is automatically generated for the figures by RDKit</title>
		<imprint/>
	</monogr>
	<note>We are not trying to optimize it</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Supervised community detection with hierarchical graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative networks as inverse problems with scattering transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Angles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diffusion wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="94" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning (NIPS-16 BDL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Von Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Molecular denovo design through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Group invariant scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1331" to="1398" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NetGAN: Generating graphs via random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="610" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GraphGAN: Graph representation learning with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MolGAN: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">GraphVAE: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03480</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2323" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gaussianization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iterative gaussianization: from ica to random rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="549" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Pas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="600" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Foundations of Data Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grammar variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Designing random graph models using variational autoencoders with applications to chemical design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05283</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quantifying the chemical beauty of drugs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bickerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature chemistry</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
