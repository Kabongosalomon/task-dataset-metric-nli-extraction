<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Generation from Scene Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Google Cloud AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Google Cloud AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Generation from Scene Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What I cannot create, I do not understand -Richard Feynman</p><p>The act of creation requires a deep understanding of the thing being created: chefs, novelists, and filmmakers must understand food, writing, and film at a much deeper level than diners, readers, or moviegoers. If our computer vision systems are to truly understand the visual world, they must be able not only recognize images but also to generate them.</p><p>Aside from imparting deep visual understanding, methods for generating realistic images can also be practically useful. In the near term, automatic image generation can aid the work of artists or graphic designers. One day, we might replace image and video search engines with algorithms that generate customized images and videos in response to the individual tastes of each user.</p><p>As a step toward these goals, there has been exciting re- * Work done during an internship at Google Cloud AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Scene Graph</head><p>Ours StackGAN <ref type="bibr" target="#b59">[59]</ref> [ <ref type="bibr" target="#b47">47]</ref> sheep grass sky ocean  <ref type="figure">Figure 1</ref>. State-of-the-art methods for generating images from sentences, such as StackGAN <ref type="bibr" target="#b59">[59]</ref>, struggle to faithfully depict complex sentences with many objects. We overcome this limitation by generating images from scene graphs, allowing our method to reason explicitly about objects and their relationships.</p><p>cent progress on text to image synthesis <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b59">59]</ref> by combining recurrent neural networks and Generative Adversarial Networks <ref type="bibr" target="#b11">[12]</ref> to generate images from natural language descriptions. These methods can give stunning results on limited domains, such as fine-grained descriptions of birds or flowers. However as shown in <ref type="figure">Figure 1</ref>, leading methods for generating images from sentences struggle with complex sentences containing many objects.</p><p>A sentence is a linear structure, with one word following another; however as shown in <ref type="figure">Figure 1</ref>, the information conveyed by a complex sentence can often be more explicitly represented as a scene graph of objects and their relationships. Scene graphs are a powerful structured representation for both images and language; they have been used for semantic image retrieval <ref type="bibr" target="#b22">[22]</ref> and for evaluating <ref type="bibr" target="#b0">[1]</ref> and improving <ref type="bibr" target="#b31">[31]</ref> image captioning; methods have also been developed for converting sentences to scene graphs <ref type="bibr" target="#b47">[47]</ref> and for predicting scene graphs from images <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b58">58]</ref>.</p><p>In this paper we aim to generate complex images with many objects and relationships by conditioning our generation on scene graphs, allowing our model to reason explicitly about objects and their relationships.</p><p>With this new task comes new challenges. We must develop a method for processing scene graph inputs; for this we use a graph convolution network which passes information along graph edges. After processing the graph, we must bridge the gap between the symbolic graph-structured input and the two-dimensional image output; to this end we construct a scene layout by predicting bounding boxes and segmentation masks for all objects in the graph. Having predicted a layout, we must generate an image which respects it; for this we use a cascaded refinement network (CRN) <ref type="bibr" target="#b5">[6]</ref> which processes the layout at increasing spatial scales. Finally, we must ensure that our generated images are realistic and contain recognizable objects; we therefore train adversarially against a pair of discriminator networks operating on image patches and generated objects. All components of the model are learned jointly in an end-to-end manner.</p><p>We experiment on two datasets: Visual Genome <ref type="bibr" target="#b26">[26]</ref>, which provides human annotated scene graphs, and COCO-Stuff <ref type="bibr" target="#b2">[3]</ref> where we construct synthetic scene graphs from ground-truth object positions. On both datasets we show qualitative results demonstrating our method's ability to generate complex images which respect the objects and relationships of the input scene graph, and perform comprehensive ablations to validate each component of our model. Automated evaluation of generative images models is a challenging problem unto itself <ref type="bibr" target="#b52">[52]</ref>, so we also evaluate our results with two user studies on Amazon Mechanical Turk. Compared to StackGAN <ref type="bibr" target="#b59">[59]</ref>, a leading system for text to image synthesis, users find that our results better match COCO captions in 68% of trials, and contain 59% more recognizable objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Image Models fall into three recent categories: Generative Adversarial Networks (GANs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">40]</ref> jointly learn a generator for synthesizing images and a discriminator classifying images as real or fake; Variational Autoencoders <ref type="bibr" target="#b24">[24]</ref> use variational inference to jointly learn an encoder and decoder mapping between images and latent codes; autoregressive approaches <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b53">53]</ref> model likelihoods by conditioning each pixel on all previous pixels.</p><p>Conditional Image Synthesis conditions generation on additional input. GANs can be conditioned on category labels by providing labels as an additional input to both generator and discriminator <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">35]</ref> or by forcing the discriminator to predict the label <ref type="bibr" target="#b37">[37]</ref>; we take the latter approach.</p><p>Reed et al. <ref type="bibr" target="#b42">[42]</ref> generate images from text using a GAN; Zhang et al. <ref type="bibr" target="#b59">[59]</ref> extend this approach to higher resolutions using multistage generation. Related to our approach, Reed et al. generate images conditioned on sentences and keypoints using both GANs <ref type="bibr" target="#b41">[41]</ref> and multiscale autoregressive models <ref type="bibr" target="#b43">[43]</ref>; in addition to generating images they also predict locations of unobserved keypoints using a separate generator and discriminator operating on keypoint locations.</p><p>Chen and Koltun <ref type="bibr" target="#b5">[6]</ref> generate high-resolution images of street scenes from ground-truth semantic segmentation using a cascaded refinement network (CRN) trained with a perceptual feature reconstruction loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">21]</ref>; we use their CRN architecture to generate images from scene layouts.</p><p>Related to our layout prediction, Chang et al. have investigated text to 3D scene generation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>; other approaches to image synthesis include stochastic grammars <ref type="bibr" target="#b19">[20]</ref>, probabalistic programming <ref type="bibr" target="#b27">[27]</ref>, inverse graphics <ref type="bibr" target="#b28">[28]</ref>, neural de-rendering <ref type="bibr" target="#b55">[55]</ref>, and generative ConvNets <ref type="bibr" target="#b56">[56]</ref>.</p><p>Scene Graphs represent scenes as directed graphs, where nodes are objects and edges give relationships between objects. Scene graphs have been used for image retrieval <ref type="bibr" target="#b22">[22]</ref> and to evaluate image captioning <ref type="bibr" target="#b0">[1]</ref>; some work converts sentences to scene graphs <ref type="bibr" target="#b47">[47]</ref> or predicts grounded scene graphs for images <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b58">58]</ref>. Most work on scene graphs uses the Visual Genome dataset <ref type="bibr" target="#b26">[26]</ref>, which provides human-annotated scene graphs.</p><p>Deep Learning on Graphs. Some methods learn embeddings for graph nodes given a single large graph <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b13">14]</ref> similar to word2vec <ref type="bibr" target="#b34">[34]</ref> which learns embeddings for words given a text corpus. These differ from our approach, since we must process a new graph on each forward pass.</p><p>More closely related to our work are Graph Neural Networks (GNNs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">46]</ref> which generalize recursive neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b48">48]</ref> to operate on arbitrary graphs. GNNs and related models have been applied to molecular property prediction <ref type="bibr" target="#b6">[7]</ref>, program verification <ref type="bibr" target="#b29">[29]</ref>, modeling human motion <ref type="bibr" target="#b18">[19]</ref>, and premise selection for theorem proving <ref type="bibr" target="#b54">[54]</ref>. Some methods operate on graphs in the spectral domain <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">25]</ref> though we do not take this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to develop a model which takes as input a scene graph describing objects and their relationships, and which generates a realistic image corresponding to the graph. The primary challenges are threefold: first, we must develop a method for processing the graph-structured input; second, we must ensure that the generated images respect the objects and relationships specified by the graph; third, we must ensure that the synthesized images are realistic.</p><p>We convert scene graphs to images with an image generation network f , shown in <ref type="figure">Figure 2</ref>, which inputs a scene graph G and noise z and outputs an imageÎ = f (G, z).</p><p>The scene graph G is processed by a graph convolution network which gives embedding vectors for each object; as shown in <ref type="figure">Figures 2 and 3</ref>, each layer of graph convolution mixes information along edges of the graph.</p><p>We respect the objects and relationships from G by using the object embedding vectors from the graph convolution network to predict bounding boxes and segmentation masks for each object; these are combined to form a scene layout, shown in the center of <ref type="figure">Figure 2</ref>, which acts as an intermediate between the graph and the image domains.</p><p>The output imageÎ is generated from the layout using a cascaded refinement network (CRN) <ref type="bibr" target="#b5">[6]</ref>, shown in the right  <ref type="figure">Figure 2</ref>. Overview of our image generation network f for generating images from scene graphs. The input to the model is a scene graph specifying objects and relationships; it is processed with a graph convolution network ( <ref type="figure">Figure 3</ref>) which passes information along edges to compute embedding vectors for all objects. These vectors are used to predict bounding boxes and segmentation masks for objects, which are combined to form a scene layout ( <ref type="figure">Figure 4</ref>). The layout is converted to an image using a cascaded refinement network (CRN) <ref type="bibr" target="#b5">[6]</ref>. The model is trained adversarially against a pair of discriminator networks. During training the model observes ground-truth object bounding boxes and (optionally) segmentation masks, but these are predicted by the model at test-time. half of <ref type="figure">Figure 2</ref>; each of its modules processes the layout at increasing spatial scales, eventually generating the imageÎ. We generate realistic images by training f adversarially against a pair of discriminator networks D img and D obj which encourage the imageÎ to both appear realistic and to contain realistic, recognizable objects.</p><p>Each of these components is described in more detail below; the supplementary material describes the exact architecures used in our experiments.</p><p>Scene Graphs. The input to our model is a scene graph <ref type="bibr" target="#b22">[22]</ref> describing objects and relationships between objects. Given a set of object categories C and a set of relationship categories R, a scene graph is a tuple (O, E) where O = {o 1 , . . . , o n } is a set of objects with each o i ∈ C, and</p><formula xml:id="formula_0">E ⊆ O × R × O is a set of directed edges of the form (o i , r, o j ) where o i , o j ∈ O and r ∈ R.</formula><p>As a first stage of processing, we use a learned embedding layer to convert each node and edge of the graph from a categorical label to a dense vector, analogous to the embedding layer typically used in neural language models.</p><p>Graph Convolution Network. In order to process scene graphs in an end-to-end manner, we need a neural network module which can operate natively on graphs. To this end we use a graph convolution network composed of several graph convolution layers.</p><p>A traditional 2D convolution layer takes as input a spatial grid of feature vectors and produces as output a new spatial grid of vectors, where each output vector is a function of a local neighborhood of its corresponding input vector; in this way a convolution aggregates information across local neighborhoods of the input. A single convolution layer can operate on inputs of arbitrary shape through the use of weight sharing across all neighborhoods in the input.</p><p>Our graph convolution layer performs a similar function: given an input graph with vectors of dimension D in at each node and edge, it computes new vectors of dimension D out for each node and edge. Output vectors are a function of a neighborhood of their corresponding inputs, so that each graph convolution layer propagates information along edges of the graph. A graph convolution layer applies the same function to all edges of the graph, allowing a single layer to operate on graphs of arbitrary shape.</p><p>Concretely, given input vectors v i , v r ∈ R Din for all objects o i ∈ O and edges (o i , r, o j ) ∈ E, we compute output vectors for v i , v r ∈ R Dout for all nodes and edges using three functions g s , g p , and g o , which take as input the triple of vectors (v i , v r , v j ) for an edge and output new vectors for the subject o i , predicate r, and object o j respectively.</p><p>To compute the output vectors v r for edges we simply set v r = g p (v i , v r , v j ). Updating object vectors is more complex, since an object may participate in many relationships; as such the output vector v i for an object o i should depend on all vectors v j for objects to which o i is connected via graph edges, as well as the vectors v r for those edges. To this end, for each edge starting at o i we use g s to compute a candidate vector, collecting all such candidates in the set V s i ; we similarly use g o to compute a set of candidate vectors V o i for all edges terminating at o i . Concretely,</p><formula xml:id="formula_1">V s i = {g s (v i , v r , v j ) : (o i , r, o j ) ∈ E} (1) V o i = {g o (v j , v r , v i ) : (o j , r, o i ) ∈ E}.<label>(2)</label></formula><p>The output vector for v i for object o i is then computed as</p><formula xml:id="formula_2">v i = h(V s i ∪ V o i )</formula><p>where h is a symmetric function which pools an input set of vectors to a single output vector. An example computational graph for a single graph convolution layer is shown in <ref type="figure">Figure 3</ref>.</p><p>In our implementation, the functions g s , g p , and g o are implemented using a single network which concatenates its three input vectors, feeds them to a multilayer perceptron (MLP), and computes three output vectors using fullyconnected output heads. The pooling function h averages its input vectors and feeds the result to a MLP. <ref type="figure">Figure 3</ref>. Computational graph illustrating a single graph convolution layer. The graph consists of three objects o1, o2, and o3 and two edges (o1, r1, o2) and (o3, r2, o2). Along each edge, the three input vectors are passed to functions gs, gp, and go; gp directly computes the output vector for the edge, while gs and go compute candidate vectors which are fed to a symmetric pooling function h to compute output vectors for objects.</p><formula xml:id="formula_3">v 1 v r1 v 2 v r2 v 3 v' 1 v' r1 v' 2 v' r1 v' 3 g s g p g o g s g p g o h h h</formula><p>Scene Layout. Processing the input scene graph with a series of graph convolution layers gives an embedding vector for each object which aggregates information across all objects and relationships in the graph.</p><p>In order to generate an image, we must move from the graph domain to the image domain. To this end, we use the object embedding vectors to compute a scene layout which gives the coarse 2D structure of the image to generate; we compute the scene layout by predicting a segmentation mask and bounding box for each object using an object layout network, shown in <ref type="figure">Figure 4</ref>.</p><p>The object layout network receives an embedding vector v i of shape D for object o i and passes it to a mask regression network to predict a soft binary maskm i of shape M × M and a box regression network to predict a bounding box b i = (x 0 , y 0 , x 1 , y 1 ). The mask regression network consists of several transpose convolutions terminating in a sigmoid nonlinearity so that elements of the mask lies in the range (0, 1); the box regression network is a MLP.</p><p>We multiply the embedding vector v i elementwise with the maskm i to give a masked embedding of shape D×M × M which is then warped to the position of the bounding box using bilinear interpolation <ref type="bibr" target="#b17">[18]</ref> to give an object layout. The scene layout is then the sum of all object layouts.</p><p>During training we use ground-truth bounding boxes b i to compute the scene layout; at test-time we instead use predicted bounding boxesb i .</p><p>Cascaded Refinement Network. Given the scene layout, we must synthesize an image that respects the object positions given in the layout. For this task we use a Cascaded Refinement Network <ref type="bibr" target="#b5">[6]</ref> (CRN). A CRN consists of a series of convolutional refinement modules, with spatial resolution doubling between modules; this allows generation to proceed in a coarse-to-fine manner.</p><p>Each module receives as input both the scene layout (downsampled to the input resolution of the module) and the output from the previous module. These inputs are concatenated channelwise and passed to a pair of 3 × 3 convolution  <ref type="figure">Figure 4</ref>. We move from the graph domain to the image domain by computing a scene layout. The embedding vector for each object is passed to an object layout network which predicts a layout for the object; summing all object layouts gives the scene layout.</p><p>Internally the object layout network predicts a soft binary segmentation mask and a bounding box for the object; these are combined with the embedding vector using bilinear interpolation to produce the object layout.</p><p>layers; the output is then upsampled using nearest-neighbor interpolation before being passed to the next module. The first module takes Gaussian noise z ∼ p z as input, and the output from the last module is passed to two final convolution layers to produce the output image.</p><p>Discriminators. We generate realistic output images by training the image generation network f adversarially against a pair of discriminator networks D img and D obj .</p><p>A discriminator D attempts to classify its input x as real or fake by maximizing the objective <ref type="bibr" target="#b11">[12]</ref> </p><formula xml:id="formula_4">L GAN = E x∼preal log D(x) + E x∼pfake log(1 − D(x)) (3)</formula><p>where x ∼ p fake are outputs from the generation network f . At the same time, f attempts to generate outputs which will fool the discriminator by minimizing L GAN . <ref type="bibr" target="#b0">1</ref> The patch-based image discriminator D img ensures that the overall appearance of generated images is realistic; it classifies a regularly spaced, overlapping set of image patches as real or fake, and is implemented as a fully convolutional network, similar to the discriminator used in <ref type="bibr" target="#b16">[17]</ref>.</p><p>The object discriminator D obj ensures that each object in the image appears realistic; its input are the pixels of an object, cropped and rescaled to a fixed size using bilinear interpolation <ref type="bibr" target="#b17">[18]</ref>. In addition to classifying each object as real or fake, D obj also ensures that each object is recognizable using an auxiliary classifier <ref type="bibr" target="#b37">[37]</ref> which predicts the object's category; both D obj and f attempt to maximize the probability that D obj correctly classifies objects.</p><p>Training. We jointly train the generation network f and the discriminators D obj and D img . The generation network is trained to minimize the weighted sum of six losses: Text Two sheep, one eating grass with a tree in front of a mountain; the sky has a cloud.</p><p>A person riding a wave and a board by the water with sky above.</p><p>A boy standing on grass looking at a kite and the sky with the field under a mountain Two busses, one behind the other and a tree behind the second; both busses have winshields.</p><p>A person above a playingfield and left of another person left of grass, with a car left of a car above the grass.</p><p>One broccoli left of another, which is inside vegetables and has a carrot below it.</p><p>Three people with the first two inside a fence and the first left of the third.</p><p>A person above the trees inside the sky, with a skateboard surrounded by sky. Text Two cars, one parked on a street with a tree along it, and a window in front of a house and a house with a roof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layout Image</head><formula xml:id="formula_5">(a) (b) (c) (d) (e) (f) (g) (h)</formula><p>Sky above a man riding a horse; the man has a leg and the horse has a leg and a tail.</p><p>A boat on top of water; there is also sky, rock, and a bird.</p><p>A glass by a plate with food on it, and another glass by a plate.</p><p>A tie above clothes and inside a person, with a wall panel surrounding the person.</p><p>A tree right of a person left of a horse above grass, with clouds above the grass.</p><p>An elephant above grass and inside trees surrounding another elephant.</p><p>Clouds above a boat and a building above a river, with trees left of the river.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layout</head><p>Image GT Layout <ref type="figure">Figure 5</ref>. Examples of 64 × 64 generated images using graphs from the test sets of Visual Genome (left four columns) and COCO (right four columns). For each example we show the input scene graph and a manual translation of the scene graph into text; our model processes the scene graph and predicts a layout consisting of bounding boxes and segmentation masks for all objects; this layout is then used to generate the image. We also show some results for our model using ground-truth rather than predicted scene layouts. Some scene graphs have duplicate relationships, shown as double arrows. For clarity, we omit masks for some stuff categories such as sky, street, and water.</p><formula xml:id="formula_6">(i) (j) (k) (l) (m) (n) (o) (p)</formula><p>• Box loss L box = n i=1 b i −b i 1 penalizing the L 1 difference between ground-truth and predicted boxes • Mask loss L mask penalizing differences between groundtruth and predicted masks with pixelwise cross-entropy; not used for models trained on Visual Genome • Pixel loss L pix = I −Î 1 penalizing the L 1 difference between ground-truth generated images • Image adversarial loss L img GAN from D img encouraging generated image patches to appear realistic • Object adversarial loss L obj GAN from the D obj encouraging each generated object to look realistic • Auxiliarly classifier loss L obj AC from D obj , ensuring that each generated object can be classified by D obj Implementation Details. We augment all scene graphs with a special image object, and add special in image relationships connecting each true object with the image object; this ensures that all scene graphs are connected. car on street line on street sky above street bus on street line on street sky above street car on street bus on street line on street sky above street car on street bus on street line on street sky above street kite in sky car on street bus on street line on street sky above street kite in sky car below kite car on street bus on street line on street sky above street building behind street car on street bus on street line on street sky above street building behind street window on building sky above grass zebra standing on grass sky above grass sheep standing on grass  <ref type="figure">Figure 6</ref>. Images generated by our method trained on Visual Genome. In each row we start from a simple scene graph on the left and progressively add more objects and relationships moving to the right. Images respect relationships like car below kite and boat on grass.</p><p>We train all models using Adam <ref type="bibr" target="#b23">[23]</ref> with learning rate 10 −4 and batch size 32 for 1 million iterations; training takes about 3 days on a single Tesla P100. For each minibatch we first update f , then update D img and D obj .</p><p>We use ReLU for graph convolution; the CRN and discriminators use discriminators use LeakyReLU <ref type="bibr" target="#b33">[33]</ref> and batch normalization <ref type="bibr" target="#b15">[16]</ref>. Full details about our architecture can be found in the supplementary material, and code will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We train our model to generate 64 × 64 images on the Visual Genome <ref type="bibr" target="#b26">[26]</ref> and COCO-Stuff <ref type="bibr" target="#b2">[3]</ref> datasets. In our experiments we aim to show that our method generates images of complex scenes which respect the objects and relationships of the input scene graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>COCO. We perform experiments on the 2017 COCO-Stuff dataset <ref type="bibr" target="#b2">[3]</ref>, which augments a subset of the COCO dataset <ref type="bibr" target="#b30">[30]</ref> with additional stuff categories. The dataset annotates 40K train and 5K val images with bounding boxes and segmentation masks for 80 thing categories (people, cars, etc.) and 91 stuff categories (sky, grass, etc.).</p><p>We use these annotations to construct synthetic scene graphs based on the 2D image coordinates of the objects, using six mutually exclusive geometric relationships: left of, right of, above, below, inside, and surrounding.</p><p>We ignore objects covering less than 2% of the image, and use images with 3 to 8 objects; we divide the COCO-Stuff 2017 val set into our own val and test sets, leaving us with 24,972 train, 1024 val, and 2048 test images.</p><p>Visual Genome. We experiment on Visual Genome <ref type="bibr" target="#b26">[26]</ref> version 1.4 (VG) which comprises 108,077 images annotated with scene graphs. We divide the data into 80% train, 10% val, and 10% test; we use object and relationship categories occurring at least 2000 and 500 times respectively in the train set, leaving 178 object and 45 relationship types.</p><p>We ignore small objects, and use images with between 3 and 30 objects and at least one relationship; this leaves us with 62,565 train, 5,506 val, and 5,088 test images with an average of ten objects and five relationships per image.</p><p>Visual Genome does not provide segmentation masks, so we omit the mask prediction loss for models trained on VG. <ref type="figure">Figure 5</ref> shows example scene graphs from the Visual Genome and COCO test sets and generated images using our method, as well as predicted object bounding boxes and segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results</head><p>From these examples it is clear that our method can generate scenes with multiple objects, and even multiple instances of the same object type: for example <ref type="figure">Figure 5</ref>  These examples also show that our method generates images which respect the relationships of the input graph; for example in (i) we see one broccoli left of a second broccoli, with a carrot below the second broccoli; in (j) the man is riding the horse, and both the man and the horse have legs which have been properly positioned. <ref type="figure">Figure 5</ref> also shows examples of images generated by our method using ground-truth rather than predicted object layouts. In some cases we see that our predicted layouts can vary significantly from the ground-truth objects layout. For example in (k) the graph does not specify the position of the bird and our method renders it standing on the ground, but in the ground-truth layout the bird is flying in the sky. Our model is sometimes bottlenecked by layout prediction, such as (n) where using the ground-truth rather than predicted layout significantly improves the image quality.</p><p>In <ref type="figure">Figure 6</ref> we demonstrate our model's ability to generate complex images by starting with simple graphs on the left and progressively building up to more complex graphs. From this example we can see that object positions are influenced by the relationships in the graph: in the top sequence adding the relationship car below kite causes the car to shift to the right and the kite to shift to the left so that the relationship is respected. In the bottom sequence, adding the relationship boat on grass causes the boat's position to shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We demonstrate the necessity of all components of our model by comparing the image quality of several ablated versions of our model, shown in <ref type="table" target="#tab_3">Table 1</ref>; see supplementary material for example images from ablated models.</p><p>We measure image quality using Inception score 2 <ref type="bibr" target="#b45">[45]</ref> which uses an ImageNet classification model <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b50">50]</ref> to encourage recognizable objects within images and diversity across images. We test several ablations of our model:</p><p>No gconv omits graph convolution, so boxes and masks are predicted from initial object embedding vectors. It cannot reason jointly about the presence of different objects, and can only predict one box and mask per category.</p><p>No relationships uses graph convolution layers but ignores all relationships from the input scene graph except <ref type="bibr" target="#b1">2</ref> Defined as exp(EÎ KL(p(y|Î) p(y))) where the expectation is taken over generated imagesÎ and p(y|Î) is the predicted label distribution. for trivial in image relationships; graph convolution allows this model to jointly about objects. Its poor performance demonstrates the utility of the scene graph relationships.</p><p>No discriminators omits both D img and D obj , relying on the pixel regression loss L pix to guide the generation network. It tends to produce overly smoothed images.</p><p>No D obj and No D img omit one of the discriminators. On both datasets, using any discriminator leads to significant improvements over models trained with L pix alone. On COCO the two discriminators are complimentary, and combining them in our full model leads to large improvements. On VG, omitting D img does not degrade performance.</p><p>In addition to ablations, we also compare with two GT Layout versions of our model which omit the L box and L mask losses, and use ground-truth bounding boxes during both training and testing; on COCO they also use groundtruth segmentation masks, similar to Chen and Koltun <ref type="bibr" target="#b5">[6]</ref>. These methods give an upper bound to our model's performance in the case of perfect layout prediction.</p><p>Omitting graph convolution degrades performance even when using ground-truth layouts, suggesting that scene graph relationships and graph convolution have benefits beyond simply predicting object positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Object Localization</head><p>In addition to looking at images, we can also inspect the bounding boxes predicted by our model. One measure of box quality is high agreement between predicted and ground-truth boxes; in <ref type="table" target="#tab_4">Table 2</ref> we show the object recall of our model at two intersection-over-union thresholds.</p><p>Another measure for boxes is variety: predicted boxes for objects should vary in response to the other objects and relationships in the graph. <ref type="table" target="#tab_4">Table 2</ref> shows the mean percategory standard deviations of box position and area.</p><p>Without graph convolution, our model can only learn to predict a single bounding box per object category. This model achieves nontrivial object recall, but has no variety in its predicted boxes, as σ x = σ area = 0.</p><p>Using graph convolution without relationships, our model can jointly reason about objects when predicting bounding boxes; this leads to improved variety in its predictions. Without relationships, this model's predicted boxes have less agreement with ground-truth box positions.  <ref type="figure">Figure 7</ref>. We performed a user study to compare the semantic interpretability of our method against StackGAN <ref type="bibr" target="#b59">[59]</ref>. Top: We use StackGAN to generate an image from a COCO caption, and use our method to generate an image from a scene graph constructed from the COCO objects corresponding to the caption. We show users the caption and both images, and ask which better matches the caption. Bottom: Across 1024 val image pairs, users prefer the results from our method by a large margin.</p><p>Our full model with graph convolution and relationships achieves both variety and high agreement with ground-truth boxes, indicating that it can use the relationships of the graph to help localize objects with greater fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">User Studies</head><p>Automatic metrics such as Inception scores and box statistics give a coarse measure of image quality; the true measure of success is human judgement of the generated images. For this reason we performed two user studies on Mechanical Turk to evaluate our results.</p><p>We are unaware of any previous end-to-end methods for generating images from scene graphs, so we compare our method with StackGAN <ref type="bibr" target="#b59">[59]</ref>, a state-of-the art method for generating images from sentence descriptions.</p><p>Despite the different input modalities between our method and StackGAN, we can compare the two on COCO, which in addition to object annotations also provides captions for each image. We use our method to generate images from synthetic scene graphs built from COCO object annotations, and StackGAN 3 to generate images from COCO captions for the same images. Though the methods receive different inputs, they should generate similar images due to the correspondence between COCO captions and objects.</p><p>For user studies we downsample StackGAN images to 64 × 64 to compensate for differing resolutions; we repeat all trials with three workers and randomize order in all trials.</p><p>Caption Matching. We measure semantic interpretability by showing users a COCO caption, an image generated by StackGAN from that caption, and an image generated by our method from a scene graph built from the COCO objects corresponding to the caption. We ask users to select the image that better matches the caption. An example image pair and results are shown in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caption StackGAN [59] Ours Scene Graph</head><p>A man flying through the air while riding a bike.  <ref type="figure">Figure 8</ref>. We performed a user study to measure the number of recognizable objects in images from our method and from Stack-GAN <ref type="bibr" target="#b59">[59]</ref>. Top: We use StackGAN to generate an image from a COCO caption, and use our method to generate an image from a scene graph built from the COCO objects corresponding to the caption. For each image, we ask users which COCO objects they can see in the image. Bottom: Across 1024 val image pairs, we measure the fraction of things and stuff that users can recognize in images from each method. Our method produces more objects.</p><p>This experiment is biased toward StackGAN, since the caption may contain information not captured by the scene graph. Even so, a majority of workers preferred the result from our method in 67.6% of image pairs, demonstrating that compared to StackGAN our method more frequently generates complex, semantically meaningful images.</p><p>Object Recall. This experiment measures the number of recognizable objects in each method's images. In each trial we show an image from one method and a list of COCO objects and ask users to identify which objects appear in the image. An example and results are snown in <ref type="figure">Figure 8</ref>.</p><p>We compute the fraction of objects that a majority of users believed were present, dividing the results into things and stuff. Both methods achieve higher recall for stuff than things, and our method achieves significantly higher object recall, with 65% and 61% relative improvements for thing and stuff recall respectively. This experiment is biased toward our method since the scene graph may contain objects not mentioned in the caption, but it demonstrates that compared to StackGAN, our method produces images with more recognizable objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have developed an end-to-end method for generating images from scene graphs. Compared to leading methods which generate images from text descriptions, generating images from structured scene graphs rather than unstructured text allows our method to reason explicitly about objects and relationships, and generate complex images with many recognizable objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Inputs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operation</head><p>Output shape  <ref type="table">Table 3</ref>. Network architecture for the first network g used in graph convolution; this single network implements the three functions gs, gp, and go from the main text.</p><formula xml:id="formula_7">(1) - Subject vector v s D in (2) - Relationship vector v r D in (3) - Object vector v o D in (4) (1), (2), (3) Concatenate 3D in (5) (4) Linear(3D in → H) H (6) (5) ReLU H (7) (6) Linear(H → 2H + D out ) 2H + D out (8) (7) ReLU 2H + D out (9)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A. Network Architecture</p><p>Here we describe the exact network architectures for all components of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Graph Convolution Layer</head><p>As described in Section 3 of the main paper, we process the input scene graph with a graph convolution network composed of several graph convolution layers.</p><p>A graph convolution layer accepts as input a vector of dimension D in for each node and edge in the graph, and computes new vectors of dimension D out for each node and edge. A single graph convolution layer can be applied to graphs of any size of shape due to weight sharing. A single graph convolution layer proceeds in two stages.</p><p>First, along relationship of the scene graph we apply three functions g s , g p , and g o ; these functions take as input the vectors v s , v r , and v o for the starting node, edge, and ending node of the relationship and produce new vectors for the two nodes and the edge. The new vector for the edge v r = g p (v s , v r , v o ) has dimension D out , and is used as the output vector from the graph convolution layer for the edge. The new vectors for the starting and ending nodes v</p><formula xml:id="formula_8">s = g s (v s , v r , v o ) andṽ o = g o (v s , v r , v o )</formula><p>are candidate vectors of dimension H. In practice the three functions g s , g p , and g o and implemented with a single multilayer perceptron (MLP) whose architecture is shown in <ref type="table">Table 3</ref>.</p><p>As a second stage of processing, for each object in the scene graph we collect all of its candidate vectors and process them with a symmeitric pooling function h which converts the set of candidate vectors into a a single vector of dimension D out . Concretely, for object o i in the scene graph   where o i appears as the object of the relationship. The pooling function h takes as input the two sets of vectors V s i and V o i , averages them, and feeds the result to an MLP to compute the output vector v i for object o i from the graph convolution layer. The exact architecture of the network we use for h is shown in <ref type="table" target="#tab_9">Table 4</ref>.</p><formula xml:id="formula_9">G, let V s i = {g s (v i , v r , v j ) : (o i , r, o j ) ∈ G}</formula><p>Overall a graph convolution layer has three hyperparameters defining its size: the input dimension D in , the hidden dimension H, and the output dimension D out . We can therefore specify a graph convolution layer with the notation gconv(D in → H → D out ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Graph Convolution Network</head><p>The input scene graph is processed by a graph convolution network, the exact architecture of which is shown in <ref type="table" target="#tab_10">Table 5</ref>. Our network first embeds the objects and relationships of the graph with embedding layers to produce vectors of dimension D in = 128; we then use five layers of graph convolution with D in = D out = 128 and H = 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Inputs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operation</head><p>Output Shape (1) -Object embedding vector 128 <ref type="bibr" target="#b1">(2)</ref> (1) Linear(128 → 512) 512 (3)</p><p>(2) ReLU 512 <ref type="bibr" target="#b3">(4)</ref> (3) Linear(512 → 4) 4 <ref type="table">Table 6</ref>. Architecture of the box regression network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Box Regression Network</head><p>We predict bounding boxes for images using a box regression network. The input to the box regression network are the final embedding vectors for objects produced by the graph convolution network. The output from the box regression network is a predicted bounding box for the object, parameterized as (x 0 , y 0 , x 1 , y 1 ) where x 0 , x 1 are the left and right coordinates of the box and y 0 , y 1 are the top and bottom coordinates of the box; all box coordinates are normalized to be in the range [0, 1]. The architecture of the box regression network is shown in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Mask Regression Network</head><p>We predict segmentation masks for images using a mask regression network. The input to the mask regression network are the final embedding vectors for objects from the graph convolution network, and the output from the mask regresion network is a M × M segmentation mask with all elements in the range (0, 1). The mask regression network is composed of a sequence of upsampling and convolution layers, terminating in a sigmoid nonlinearity; its exact architecture is shown in <ref type="table" target="#tab_12">Table 7</ref>.</p><p>The main text of the paper states that the mask regression network uses transpose convolution, but in fact it uses upsampling and stride-1 convolutions as shown in <ref type="table" target="#tab_12">Table 7</ref>. This error will be corrected in the camera-ready version of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Scene Layout</head><p>The final embedding vectors for objects from the graph convolution network are combined with the predicted bounding boxes and segmentation masks for objects to give a scene layout. The conversion from vectors, masks, and boxes to scene layouts does not have any learnable parameters.</p><p>The scene layout has shape D × H × W where D = 128 is the dimension of embededing vectors for objects from the graph convolution network and H × W = 64 × 64 is the output resolution at which images will be generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Cascaded Refinement Network</head><p>The scene layout is converted to an image using a Cascaded Refinement Network (CRN) consisting of a number of Cascaded Refinement Modules (CRMs).  Each CRM recieves as input the scene layout of shape D × H × W = 128 × 64 × 64 and the previous feature map, and outputs a new feature map twice the spatial size of the input feature map. Internally each CRM upsamples the input feature map by a factor of 2, and downsamples the layout using average pooling the match the size of the upsampled feature map; the two are concatenated and processed with two convolution layers. A CRM taking input of shape C in × H in × W out and producing an output of shape C out ×H out ×W out (with H out = 2H in and W out = 2W in is denoted as CRM(H in × W in , C in → C out ). The exact architecture of our CRMs is shown in <ref type="table" target="#tab_13">Table 8</ref>.</p><p>Our Cascaded Refinement Network consists of five Cascaded Refinement Modules. The input to the first module is Gaussian noise of shape 32 × 2 × 2 and the output from the final module is processed with two final convolution layers to produce the output image. The architecture of the CRN is shown in <ref type="table" target="#tab_14">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Batch Normalization in the Generator</head><p>Most implementations of batch normalization operate in two modes. In train mode, minibatches are normalized using the empirical mean and variance of features; in eval mode a running mean of feature means and variances are used to normalize minibatches instead. We found that training models in train mode and running them in eval mode at Index Inputs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operation</head><p>Output Shape  test-time led to significant image artifacts. To overcome this limitation while still benefitting from the optimization benefits that batch normalization provides, we train our models for 100K iterations using batch normalization in train mode, then continue training for an additional 900K iterations with batch normalization in eval mode.</p><formula xml:id="formula_10">(1) - Scene Layout D × H × W (2) - Input features C in × H in × W in (3) (1) Average Pooling D × H out × W out (4) (2) Upsample C in × H out × W out (5) (3), (4) Concatenation (D + C in ) × H out × W out (6) (5) Conv(3 × 3, D + C in → C out ) C out × H out × W out (7) (6) Batch Normalization C out × H out × W out (8) (7) LeakyReLU C out × H out × W out (9) (8) Conv(3 × 3, C out → C out ) C out × H out × W out (10) (9) Batch Normalization C out × H out × W out (11) (10) LeakyReLU C out × H out × W out</formula><p>Since discriminators are not used at test-time, batch normalization in the discriminators is always used in train mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8. Object Discriminator</head><p>Our object discriminator D obj inputs image pixels corresponding to objects in real or generated images; objects are cropped using their bounding boxes to a spatial size of 32 × 32 using differentiable bilinear interpolation. The object discriminator serves two roles: it classifies objects as real or fake, and also uses an auxiliary classifier which attempts to classify each object. The exact architecture of our object discriminator is shown in <ref type="table" target="#tab_3">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Inputs</head><p>Operation  <ref type="table" target="#tab_3">Table 10</ref>. Architecture of our object discriminator D obj . The input to the object discriminator is a 32 × 32 crop of an object in either a generated or real image. The object discriminator outputs both a score for real / fake (11) and a classification score over the object categories C <ref type="bibr" target="#b11">(12)</ref>. In this model all convolution layers have stride 2 and no zero padding. LeakyReLU uses a negative slope of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9. Image Discriminator</head><p>Our image discriminator D img inputs a real or fake image, and classifies an overlapping grid of 8 × 8 image patches from its input image as real or fake. The exact architecture of our image discriminator is shown in <ref type="table" target="#tab_3">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10. Higher Image Resolutions</head><p>We performed preliminary experiments with a version of our model that produces 128 × 128 images rather than 64 × 64 images. For these models we compute the scene layout at 128×128 rather than at 64×64; we also add an extra Cascaded Refinement Module to our Cascaded Refinement Network; we add one additional convolutional layer to both D obj and D img , and for these models D obj receives a 64 × 64 crop of objects rather than a 32 × 32 crop. During trainging we reduce the batch size from 32 to 24.</p><p>The images in <ref type="figure">Figure 6</ref> from the main paper were generated from a version of our model trained to produce 128 × 128 images from Visual Genome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image Loss Functions</head><p>In <ref type="figure">Figure 9</ref> we show additional qualitative results from our model trained on COCO, comparing the results from different ablated versions of our model.</p><p>Omitting the discriminators from the model (L1 only) tends to produce images that are overly smoothed. Without the object discriminator (No D obj ) objects tend to be less recognizable, and without the image discriminator (No D img ) the generated images tend to appear less realistic overall, with low-level artifacts. Our model trained to use ground-truth layouts rather than predicting its own layouts (GT Layout) tends to produce higher-quality images, but  <ref type="figure">Figure 9</ref>. Example images generated from our model and ablations on COCO. We show the original image, the synthetic scene graph generated from the COCO annotations for the image, and results from several versions of our model. Our model with no discriminators (L1 only) tends to be overly smooth; omitting the object discriminator (No D obj ) causes objects to be less recognizable; omitting the image discriminator (No Dimg) leads to low-level image artifacts. Using the ground-truth rather than predicted layout (GT Layout) tends to result in higher quality images. The bottom row shows a typical failure case, where all versions of our model struggle with complex scene graphs for indoor scenes. Graphs best viewed with magnification. <ref type="figure">Figure 10</ref>. Screenshots of the user interfaces for our user studies on Amazon Mechanical Turk. Left: User interface for the user study from <ref type="figure">Figure 7</ref> of the main paper. We show users an image generated by StackGAN from a COCO caption, and an image generated with our method from a scene graph built from the COCO object annotations corresponding to the caption. We ask users to select the image that best matches the caption. Right: User interface for the user study from <ref type="figure">Figure 8</ref> of the main paper. We show again show users images generated using StackGAN and our method, and we ask users which COCO objects are present in each image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) shows two sheep, (d) shows two busses, (g) contains three people, and (i) shows two cars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Ablation study using Inception scores. On each dataset we randomly split our test-set samples into 5 groups and report mean and standard deviation across splits. On COCO we generate five samples for each test-set image by constructing different synthetic scene graphs. For StackGAN we generate one image for each of the COCO test-set captions, and downsample their 256 × 256 output to 64 × 64 for fair comparison with our method.</figDesc><table><row><cell></cell><cell cols="2">Inception</cell></row><row><cell>Method</cell><cell>COCO</cell><cell>VG</cell></row><row><cell>Real Images (64 × 64)</cell><cell cols="2">16.3 ± 0.4 13.9 ± 0.5</cell></row><row><cell>Ours (No gconv)</cell><cell>4.6 ± 0.1</cell><cell>4.2 ± 0.1</cell></row><row><cell>Ours (No relationships)</cell><cell>3.7 ± 0.1</cell><cell>4.9 ± 0.1</cell></row><row><cell>Ours (No discriminators)</cell><cell>4.8 ± 0.1</cell><cell>3.6 ± 0.1</cell></row><row><cell>Ours (No D obj )</cell><cell>5.6 ± 0.1</cell><cell>5.0 ± 0.2</cell></row><row><cell>Ours (No D img )</cell><cell cols="2">5.6 ± 0.1 5.7 ± 0.3</cell></row><row><cell>Ours (Full model)</cell><cell cols="2">6.7 ± 0.1 5.5 ± 0.1</cell></row><row><cell cols="2">Ours (GT Layout, no gconv) 7.0 ± 0.2</cell><cell>6.0 ± 0.2</cell></row><row><cell>Ours (GT Layout)</cell><cell cols="2">7.3 ± 0.1 6.3 ± 0.2</cell></row><row><cell>StackGAN [59] (64 × 64)</cell><cell>8.4 ± 0.2</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Statistics of predicted bounding boxes. R@t is object recall with an IoU threshold of t, and measures agreement with ground-truth boxes. σx and σarea measure box variety by computing the standard deviation of box x-positions and areas within each object category and then averaging across categories.</figDesc><table><row><cell></cell><cell>R@0.3</cell><cell>R@0.5</cell><cell>σ x</cell><cell></cell><cell>σ area</cell></row><row><cell></cell><cell cols="5">COCO VG COCO VG COCO VG COCO VG</cell></row><row><cell cols="3">Ours (No gconv) 46.9 20.2 20.8 6.4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Ours (No rel.)</cell><cell cols="5">21.8 16.5 7.6 6.9 0.1 0.1 0.2 0.1</cell></row><row><cell>Ours (Full)</cell><cell cols="5">52.4 21.9 32.2 10.6 0.1 0.1 0.2 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>be the set of candidate vectors for o i from relationships where o i appears</figDesc><table><row><cell cols="2">Index Inputs</cell><cell>Operation</cell><cell>Output Shape</cell></row><row><cell cols="2">(1) (2) (3) (1), (2) --</cell><cell>Subject candidate set V s i Object candidate set V o i Set union</cell><cell>|V s i | × H |V o i | × H i | + |V o (|V s i |) × H</cell></row><row><cell>(4)</cell><cell>(3)</cell><cell>Mean over axis 0</cell><cell>H</cell></row><row><cell>(5)</cell><cell>(4)</cell><cell>Linear(H → H)</cell><cell>H</cell></row><row><cell>(6)</cell><cell>(5)</cell><cell>ReLU</cell><cell>H</cell></row><row><cell>(7)</cell><cell>(6)</cell><cell>Linear(H → D out )</cell><cell>D out</cell></row><row><cell>(8)</cell><cell>(7)</cell><cell>ReLU</cell><cell>D out</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Network architecture for the second network h used in graph convolution; this network implements a symmetric pooling function to convert the set of all candidate vectors for an object into a single output vector.</figDesc><table><row><cell cols="2">Index Inputs</cell><cell>Operation</cell><cell>Output Shape</cell></row><row><cell>(1)</cell><cell>-</cell><cell>Graph objects</cell><cell>O</cell></row><row><cell>(2)</cell><cell>-</cell><cell>Graph relationships</cell><cell>R</cell></row><row><cell>(3)</cell><cell>(1)</cell><cell>Object Embedding</cell><cell>O × 128</cell></row><row><cell>(4)</cell><cell>(2)</cell><cell>Relationship embedding</cell><cell>R × 128</cell></row><row><cell cols="4">(5) (1), (2) gconv(128 → 512 → 128) O × 128, R × 128</cell></row><row><cell>(6)</cell><cell cols="3">(5) gconv(128 → 512 → 128) O × 128, R × 128</cell></row><row><cell>(7)</cell><cell cols="3">(6) gconv(128 → 512 → 128) O × 128, R × 128</cell></row><row><cell>(8)</cell><cell cols="3">(7) gconv(128 → 512 → 128) O × 128, R × 128</cell></row><row><cell>(9)</cell><cell cols="3">(8) gconv(128 → 512 → 128) O × 128, R × 128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Architecture of the graph convolution network used to process input scene graphs. The input scene graph has O objects and R relationships. Due to weight sharing in graph convolutions, the same network can process graphs of any size or topology. The notation gconv(Din → H → Dout) is graph convolution with input dimension Din, hidden dimension H, and output dimension Dout. as the subject, and let V o i = {g o (v j , v r , v i ) : (o j , r, o i ) ∈ G} be the set of candidate vectors for o i from relationships</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>Architecture of the mask regression network. For 3D tensors we use C × H × W layout, where C is the number of channels in the feature map and H and W are the height and width of the feature map. The notation Conv(K × K, Cin → Cout) is a convolution with K × K kernels, Cin input channels and Cout output channels; all convolutions are stride 1 with zero padding so that their input and output have the same spatial size. Upsample is a 2 × 2 nearest-neighbor upsampling.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 .</head><label>8</label><figDesc>Architecture of a Cascaded Refinement Module CRM(Hin × Win, Cin → Cout). The module accepts as input the scene layout, and an input feature map of shape Cin × Hin × Win and produces as output a feature map of shape Cout × Hout × Wout where Hout = 2Hin and Wout = 2Win. For LeakyReLU nonlinearites we use negative slope 0.2.</figDesc><table><row><cell cols="2">Index Inputs</cell><cell>Operation</cell><cell>Output Shape</cell></row><row><cell>(1)</cell><cell>-</cell><cell>Scene Layout</cell><cell>128 × 64 × 64</cell></row><row><cell>(2)</cell><cell>-</cell><cell>Gaussian Noise</cell><cell>32 × 2 × 2</cell></row><row><cell cols="4">(3) (1), (2) CRN(2 × 2, 32 → 1024) 1024 × 4 × 4</cell></row><row><cell cols="4">(4) (1), (3) CRN(4 × 4, 1024 → 512) 512 × 8 × 8</cell></row><row><cell cols="4">(5) (1), (4) CRN(8 × 4, 512 → 256) 256 × 16 × 16</cell></row><row><cell cols="4">(6) (1), (5) CRN(16 × 16, 256 → 128) 128 × 32 × 32</cell></row><row><cell cols="4">(7) (1), (6) CRN(32 × 32, 128 → 64) 64 × 64 × 64</cell></row><row><cell>(8)</cell><cell>(7)</cell><cell>Conv(3 × 3, 64 → 64)</cell><cell>64 × 64 × 64</cell></row><row><cell>(9)</cell><cell>(8)</cell><cell>LeakyReLU</cell><cell>64 × 64 × 64</cell></row><row><cell>(10)</cell><cell>(9)</cell><cell>Conv(1 × 1, 64 → 3)</cell><cell>3 × 64 × 64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 .</head><label>9</label><figDesc>Architecture of our Cascaded Refinement Network. CRM is a Cascaded Refinement Module, shown inTable 8. LeakyReLU uses a negative slope of 0.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>× 4, 64 → 128, s2) 128 × 8 × 8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Output Shape</cell></row><row><cell>(1)</cell><cell>-</cell><cell>Object crop</cell><cell>3 × 32 × 32</cell></row><row><cell>(2)</cell><cell>(1)</cell><cell>Conv(4 × 4, 3 → 64, s2)</cell><cell>64 × 16 × 16</cell></row><row><cell>(3)</cell><cell>(2)</cell><cell>Batch Normalization</cell><cell>64 × 16 × 16</cell></row><row><cell>(4)</cell><cell>(3)</cell><cell>LeakyReLU</cell><cell>64 × 16 × 16</cell></row><row><cell cols="3">(5) Conv(4 (6) (4) (5) Batch Normalization</cell><cell>128 × 32 × 32</cell></row><row><cell>(7)</cell><cell>(6)</cell><cell>LeakyReLU</cell><cell>128 × 32 × 32</cell></row><row><cell>(8)</cell><cell cols="3">(7) Conv(4 × 4, 128 → 256, s2) 256 × 4 × 4</cell></row><row><cell>(9)</cell><cell>(8)</cell><cell>Global Average Pooling</cell><cell>256</cell></row><row><cell>(10)</cell><cell>(9)</cell><cell>Linear(256 → 1024)</cell><cell>1024</cell></row><row><cell>(11)</cell><cell>(10)</cell><cell>Linear(1024 → 1)</cell><cell>1</cell></row><row><cell>(12)</cell><cell>(10)</cell><cell>Linear(1024 → |C|)</cell><cell>|C|</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In practice, to avoid vanishing gradients f typically maximizes the surrogate objective Ex∼p fake log D(x) instead of minimizing L GAN<ref type="bibr" target="#b11">[12]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use the pretrained COCO model provided by the authors at https://github.com/hanzhanggit/StackGAN-Pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We thank Shyamal Buch, Christopher Choy, De-An Huang, and Ranjay Krishna for helpful comments and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Inputs</head><p>Operation  <ref type="table">Table 11</ref>. Architecture of our image discriminator Dimg. The input to the image discriminator is either a real or fake image, and it classifies an overlapping 8 × 8 grid of patches in the input image as either real or fake. All but the final convolution have a stride of 2, and all convolutions use no padding. LeakyReLU uses a negative slope of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>requires both bounding-box and segmentation mask annotations at test-time.</head><p>The bottom row of <ref type="figure">Figure 9</ref> also shows a typical failure case, where all models struggle to synthesize a realistic image from a complex scene graph for an indoor scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. User Study</head><p>As discussed in Section 4.5 of the main paper, we perform two user studies on Amazon Mechanical Turk to compare the perceptual quality of images generated from our method with those generated using StackGAN.</p><p>In the first user study, we show users an image generated from a COCO caption using StackGAN, and an image generated using our method from a scene graph built from the COCO object annotations corresponding to the caption. We ask users to select the image that better matches the caption. In each trial of this user study the order of our image and the image from StackGAN are randomized.</p><p>In the second user study, we again show users images generated using both methods, and we ask users to select the COCO objects that are visible in the image. In this experiment, if a single image contains multiple instances of the same object category then we only ask about its presence once. In each Mechanical Turk HIT users see an equal number of results from StackGAN and our method, and the order in which they are presented is randomized.</p><p>For both studies we use 1024 images from each method generated from COCO val annotations. All images are seen by three workers, and we report all results using majority opinions.</p><p>StackGAN produces 256 × 256 images, but our method produces 64 × 64 images. To prevent the differing image resolution from affecting worker opinion, we downsample StackGAN results to 64 × 64 using bicubic interpolation before presenting them to users.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03716</idno>
		<title level="m">Coco-stuff: Thing and stuff classes in context</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Text to 3d scene generation with rich lexical grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning spatial knowledge for text to 3d scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convolutional Neural Networks for Visual Recognition</title>
		<imprint>
			<publisher>Winter semester</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">231</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Configurable, photorealistic image rendering and ground truth synthesis by sampling stochastic grammars representing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00112</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Picture: A probabilistic programming language for scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mansinghka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved image captioning via policy gradient optimization of spider</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Vision and Language Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Premise selection for theorem proving by deep graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural scene derendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">On support relations and semantic scene graphs. ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
