<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking BiSeNet For Real-time Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Fan</surname></persName>
							<email>fanmingyuan@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
							<email>laishenqi@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
							<email>huangjunshi@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
							<email>weixiaoming@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Chai</surname></persName>
							<email>chaizhenhua@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Xiaolin</roleName><forename type="first">Junfeng</forename><surname>Luo</surname></persName>
							<email>luojunfeng@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Meituan</surname></persName>
						</author>
						<title level="a" type="main">Rethinking BiSeNet For Real-time Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BiSeNet <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> has been proved to be a popular twostream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of taskspecific design. To handle these problems, we propose a novel and efficient structure named Short-Term Dense Concatenate network (STDC network) by removing structure redundancy. Specifically, we gradually reduce the dimension of feature maps and use the aggregation of them for image representation, which forms the basic module of STDC network. In the decoder, we propose a Detail Aggregation module by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to predict the final segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effectiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images. Code is available at https://github.com/ MichaelFan01/STDC-Seg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2104.13188v1 [cs.CV] 27 Apr 2021</head><p>Recently, there are fast-growing practical applications for real-time semantic segmentation. In this circumstance,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a classic and fundamental topic in computer vision, which aims to assign pixellevel labels in images. The prosperity of deep learning greatly promotes the performance of semantic segmentation by making various breakthroughs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref>, coming with fast-growing demands in many applications, e.g., autonomous driving, video surveillance, robot sensing, and so * Equal contribution. † Co-corresponding author.  <ref type="figure">Figure 1</ref>. Speed-Accuracy performance comparison on the Cityscapes test set. Our methods are presented in red dots while other methods are presented in blue dots. Our approaches achieve state-of-the-art speed-accuracy trade-off. on. These applications motivate researchers to explore effective and efficient segmentation networks, particularly for mobile field.</p><p>To fulfill those demands, many researchers propose to design low-latency, high-efficiency CNN models with satisfactory segmentation accuracy. These real-time semantic segmentation methods have achieved promising performance on various benchmarks. For real-time inference, some works, e.g., DFANet <ref type="bibr" target="#b17">[18]</ref> and BiSeNetV1 <ref type="bibr" target="#b27">[28]</ref> choose the lightweight backbones and investigate ways of feature fusion or aggregation modules to compensate for the drop of accuracy. However, these lightweight backbones borrowed from image classification task may not be perfect for image segmentation problem due to the deficiency of taskspecific design. Besides the choice of lightweight backbones, restricting the input image size is another commonly used method to promote the inference speed. Smaller input resolution seems to be effective, but it can easily neglect the detailed appearance around boundaries and small objects. To tackle this problem, as shown in <ref type="figure">Figure 2</ref>(a), BiSeNet <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> adopt multi-path framework to combine the low-level details and high-level semantics. However, <ref type="bibr">Figure 2</ref>. Illustration of architectures of BiSeNet <ref type="bibr" target="#b27">[28]</ref> and our proposed approach. (a) presents Bilateral Segmentation Network (BiSeNet <ref type="bibr" target="#b27">[28]</ref>), which use an extra Spatial Path to encode spatial information. (b) demonstrates our proposed method, which use a Detail Guidance module to encode spatial information in the lowlevel features without an extra time-cosuming path.</p><p>adding an additional path to get low-level features is timeconsuming, and the auxiliary path is always lack of lowlevel information guidance.</p><p>To this end, we propose a novel hand-craft network for the purpose of faster inference speed, explainable structure, and competitive performance to that of existing methods. First, we design a novel structure, called Short-Term Dense Concatenate module (STDC module), to get variant scalable receptive fields with a few parameters. Then, the STDC modules are seamlessly integrated into U-net architecture to form the STDC network, which greatly promote network performance in semantic segmentation task.</p><p>In details, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we concatenate response maps from multiple continuous layers, each of which encodes input image/feature in different scales and respective fields, leading to multi-scale feature representation. To speed up, the filter size of layers is gradually reduced with negligible loss in segmentation performance. The details structure of STDC networks can be found in <ref type="table">Table 2</ref>.</p><p>In the phase of decoding, as shown in <ref type="figure">Figure 2</ref>(b), instead of utilizing an extra time-consuming path, Detail Guidance are adopted to guide the low-level layers for the learning of spatial details. We first utilize Detail Aggregation module to generate detail ground-truth. Then, the binary cross-entropy loss and dice loss are employed to optimize the learning task of detail information, which is considered as one type of side-information learning. It should be noted that this side-information is not required in the inference time. Finally, the spatial details from low-level layers and semantic information from deep layers are fused to predict the semantic segmentation results. The whole architecture of our method is shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>Our main contributions can be summarized as follows: </p><formula xml:id="formula_0">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Efficient Network Designs</head><p>Model design plays an important role in computer vision tasks. SqueezeNet <ref type="bibr" target="#b15">[16]</ref> used the fire module and certain strategies to reduce the model parameters. Mo-bileNet V1 <ref type="bibr" target="#b12">[13]</ref> utilized depth-wise separable convolution to reduce the FLOPs in inference phase. ResNet <ref type="bibr" target="#b8">[9]</ref> [10] adopted residual building layers to achieve outstanding performance. MobileNet V2 <ref type="bibr" target="#b24">[25]</ref> and ShuffleNet <ref type="bibr" target="#b28">[29]</ref> used group convolution to reduce computation cost while maintaining comparable accuracy. These works are particularly designed for the image classification tasks, and their extensions to semantic segmentation application should be carefully tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generic Semantic Segmentation</head><p>Traditional segmentation algorithms, e.g., threshold selection, super-pixel, utilized the hand-crafted features to assign pixel-level labels in images. With the development of convolution neural network, methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14]</ref> based on FCN <ref type="bibr" target="#b22">[23]</ref> achieved impressive performance on various benchmarks. The Deeplabv3 <ref type="bibr" target="#b2">[3]</ref> adopted an atrous spatial pyramid pooling module to capture multi-scale context. The SegNet <ref type="bibr" target="#b0">[1]</ref> utilized the encoder-decoder structure to recover the high-resolution feature maps. The PSPNet <ref type="bibr" target="#b31">[32]</ref> devised a pyramid pooling to capture both local and global context information on the dilation backbone. Both dilation backbone and encoder-decoder structure can simultaneously learn the low-level details and high-level semantics. However, most approaches require large computation cost due to the high-resolution feature and the complicate network connections. In this paper, we propose an efficient and effective architecture which achieves good trade-off between speed and accuracy. there are two mainstreams to devise efficient segmentation methods. (i) lightweight backbone. DFANet [18] adopted a lightweight backbone to reduce computation cost and devised a cross-level feature aggregation module to enhance performance. DFNet <ref type="bibr" target="#b20">[21]</ref> utilized "Partial Order Pruning" algorithm to obtain a lightweight backbone and efficient decoder. (ii) multi-branch architecture. ICNet <ref type="bibr" target="#b30">[31]</ref> devised the multi-scale image cascade to achieve good speedaccuracy trade-off. BiSeNetV1 <ref type="bibr" target="#b27">[28]</ref> and BiSeNetV2 <ref type="bibr" target="#b26">[27]</ref> proposed two-stream paths for low-level details and highlevel context information, separately. In this paper, we propose an efficient lightweight backbone to provide scalable receptive field. Furthermore, we set a single path decoder which uses detail information guidance to learn the lowlevel details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Real-time Semantic Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>BiSeNetV1 <ref type="bibr" target="#b27">[28]</ref> utilizes lightweight backbones, e.g., ResNet18 and spatial path as encoding networks to form two-steam segmentation architecture. However, the classification backbones and two-stream architecture may be inefficient due to the structure redundancy. In this section, we first introduce the details of our proposed STDC network. Then we present the whole arhitecture of our single-stream method with detail guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Design of Encoding Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Short-Term Dense Concatenate Module</head><p>The key component of our proposed network is the Short-Term Dense Concatenate module (STDC module).  <ref type="table">Table 1</ref>. Receptive Field of blocks in our STDC module. RF denotes Receptive Field, S means stride, Note that if stride=2, the 1 × 1 RF of Block1 is turned into 3 × 3 RF by Average Pool operation. and we use ConvX i to denote the operations of i-th block. Therefore, the output of i-th block is calculated as follows:</p><formula xml:id="formula_1">RF(S = 1) 1 × 1 3 × 3 5 × 5 7 × 7 1 × 1, 3 × 3 5 × 5, 7 × 7 RF(S = 2) 1 × 1 3 × 3 7 × 7 11 × 11 3 × 3 7 × 7, 11 × 11</formula><formula xml:id="formula_2">x i = ConvX i (x i−1 , k i )<label>(1)</label></formula><p>where x i−1 and x i are the input and output of i-th block, separately. ConvX includes one convolutional layer, one batch normalization layer and ReLU activation layer, and k i is the kernel size of convolutional layer. In STDC module, the kernel size of first block is 1, and the rest of them are simply set as 3. Given the channel number of STDC module's output N , the filter number of convolutional layer in i-th block is N/2 i , except the filters of last convolutional layer, whose number is the same to that of previous convolutional layer. In image classification tasks, its a common practice to using more channels in higher layers. But in semantic segmentation tasks, we focus on scalable receptive field and multi-scale informations. Low-level layers need enough channels to encode more fine-grained informations with small receptive field, while high-level layers with large receptive field focus more on high-level information induction, setting the same channel with low-level layers may cause information redundancy. Down-sample is only happened in Block2. To enrich the feature information, we concatenate x 1 to x n feature maps as the output of STDC module by skip-path. Before concatenation, the response maps of different blocks in STDC module is down-sampled to the same spatial size by average pooling operation with 3 × 3 pooling size, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c). In our setting, the final output of STDC module is:</p><formula xml:id="formula_3">x output = F (x 1 , x 2 , ..., x n )<label>(2)</label></formula><p>where x output denotes the STDC module output, F is the fusion operation in our method, while x 1 , x 2 , ..., x n are feature maps from all n blocks. In the consideration of efficiency, we adopt concatenation as our fusion operation. In our method, we use the STDC module in 4 blocks. <ref type="table">Table 1</ref> presents the receptive field of blocks in STDC module, and x output thus gathers multi-scale information from all blocks, We claim that our STDC module has two advantages: (1) we elaborately tune the filter size of blocks by gradually decreasing in geometric progression manner, leading to significant reduction in computation complexity. all blocks, which preserves scalable respective fields and multi-scale information.</p><p>Given the input channel dimension M and output channel dimension N , the parameter number of STDC module is:</p><formula xml:id="formula_5">S param = M × 1 × 1 × N 2 1 + n−1 i=2 N 2 i−1 × 3 × 3 × N 2 i + N 2 n−1 × 3 × 3 × N 2 n−1 = N M 2 + 9N 2 2 3 × n−3 i=0 1 2 2i + 9N 2 2 2n−2 = N M 2 + 3N 2 2 × (1 + 1 2 2n−3 )<label>(3)</label></formula><p>As shown in <ref type="bibr">Equation 3</ref>, the parameter number of STDC module is dominated by the predefined input and output channel dimension, while the number of blocks has slight impact on the parameter size. Particularly, if n reaches the maximum limit, the parameter number of STDC module almost keeps constant, which is only defined by M and N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Network Architecture</head><p>We demonstrate our network architecture in <ref type="figure" target="#fig_1">Figure 3</ref>(a). It consists of 6 stages except input layer and prediction layer. Generally, Stage 1∼5 down-sample the spatial resolution of the input with a stride of 2, respectively, and the Stage 6 outputs the prediction logits by one ConvX, one global average pooling layer and two fully connected layer. The Stage 1&amp;2 are usually regarded as low-level layers for appearance feature extraction. In pursuit of efficiency, we only use one convolutional block in each of Stage 1&amp;2, which is proved to be sufficient according to our experiences. The number of STDC module in Stage 3, 4, 5 is carefully tuned in our network. Within those stages, the first STDC module in each stage down-samples the spatial resolution with a stride of 2. The following STDC modules in each stage keep the spatial resolution unchanged.</p><p>We denote the output channel number of stage as N l , where l is the index of stage. In practice, we empirically set N 6 as 1024, and carefully tune the channel number of rest stages, until reaching a good trade-off between accuracy and efficiency. Since our network mainly consists of Short-Term Dense Concatenate modules, we call our network STDC network. <ref type="table">Table 2</ref> shows the detailed structure of our STDC networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Design of Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Segmentation Architecture</head><p>We use the pretrained STDC networks as the backbone of our encoder and adopt the context path of BiSeNet <ref type="bibr" target="#b27">[28]</ref> to encode the context information. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>(a), we use the Stage 3, 4, 5 to produce the feature maps with down-sample ratio 1/8, 1/16, 1/32, respectively. Then we use global average pooling to provide global context information with large receptive field. The U-shape structure are deployed to up-sample the features stem from global feature, and combine each of them with the counterparts from last two stages (Stage 4&amp;5) in our encoding phase. Following BiSeNet <ref type="bibr" target="#b27">[28]</ref>, we use Attention Refine module to refine the combination features of every two stages. For the final semantic segmentation prediction, we adopt Feature Fusion module in BiSeNet <ref type="bibr" target="#b27">[28]</ref> to fuse the 1/8 down-sampled feature from Stage 3 in the encoder and the counterpart from the decoder. We claim that the features of these two stages are in different levels of feature representation. The feature from the encoding backbone preserves rich detail information, while the feature from the decoder contains context information due to the input from global pooling layer. Specifically, the Seg Head includes a 3 × 3 Conv-BN-ReLU operator followed with a 1 × 1 convolution to get the output dimension N , which is set as the number of classes. We adopt cross-entry loss with Online Hard Example Mining to optimize the semantic segmentation learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Detail Guidance of Low-level Features</head><p>We visualize the features of BiSeNet's spatial path in <ref type="figure" target="#fig_4">Figure 5(b)</ref>. Compared with the backbone's low-level layers(Stage 3) of same downsample ratio, spatial path can encode more spatial detail, e.g., boundary, corners. Based on this observation, we propose a Detail Guidance module to guide the low-level layers to learn the spatial information in single-stream manner. We model the detail prediction  as a binary segmentation task. We first generate the detail map ground-truth from the segmentation ground-truth by Laplacian operator as shown in <ref type="figure" target="#fig_3">Figure 4</ref> (c). As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>(a), we insert the Detail Head in Stage 3 to generate the detail feature map. Then we use the detail ground-truth as the guidance of detail feature map to guide the low-level layers to learn the feature of spatial details. As shown in <ref type="figure" target="#fig_4">Figure 5(d)</ref>, the feature map with detail guidance can encode more spatial details than aforementioned result presented in <ref type="figure" target="#fig_4">Figure 5</ref>(c). Finally, the learned detail features are fused with the context features from the deep block of the decoder for segmentation prediction. Detail Ground-truth Generation: We generate the binary detail ground-truth from the semantic segmentation groundtruth by our Detail Aggregation module, as shown in dashed blue box of <ref type="figure" target="#fig_3">Figure 4</ref>(c). This operation can be carried out by 2-D convolution kernel named Laplacian kernel and a trainable 1 × 1 convolution. We use the Laplacian operator shown in <ref type="figure" target="#fig_3">Figure 4</ref>(e) to produce soft thin detail feature maps with different strides to obtain mult-scale detail informations. Then we upsample the detail feature maps to the original size and fuse it with a trainable 1 × 1 convolution for dynamic re-wegihting. Finally, we adopt a threshold 0.1 to convert the predicted details to the final binary detail ground-truth with boundary and corner informations.</p><p>Detail Loss: Since the number of detail pixels is much less than the non-detail pixels, detail prediction is a classimbalance problem. Because weighted cross-entropy always leads to coarse results, following <ref type="bibr" target="#b6">[7]</ref>, we adopt binary cross-entropy and dice loss to jointly optimize the detail learning. Dice loss measures the overlap between predict maps and ground-truth. Also, it is insensitive to the number of foreground/background pixels, which means it can alleviating the class-imbalance problem. So for the predicted detail map with the height H and the width W , the detail loss L detail is formulated as follows:</p><formula xml:id="formula_6">L detail (p d , g d ) = L dice (p d , g d ) + L bce (p d , g d ) (4)</formula><p>where p d ∈ R H×W denotes the predicted detail and g d ∈ R H×W denotes the corresponding detail ground-truth. L bce denotes the binary cross-entropy loss while L dice denotes the dice loss, which is given as follows:</p><formula xml:id="formula_7">L dice (p d , g d ) = 1 − 2 H×W i p i d g i d + H×W i (p i d ) 2 + H×W i (g i d ) 2 + (5)</formula><p>where i denotes the i-th pixel and is a Laplace smoothing item to avoid zero division. In this paper we set = 1.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4(b)</ref>, we use a Detail Head to produce the detail map, which guide the shallow layer to encode spatial information. Detail Head includes a 3 × 3</p><p>Conv-BN-ReLU operator followed with a 1 × 1 convolution to get the output detail map. In the experiment, the Detail Head is proved to be effective to enhance the feature representation. Note that this branch is discarded in the inference phase. Therefore, this side-information can easily boost the accuracy of segmentation task without any cost in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We implement our method on three datasets: ImageNet <ref type="bibr" target="#b5">[6]</ref>, Cityscapes <ref type="bibr" target="#b4">[5]</ref> and CamVid <ref type="bibr" target="#b1">[2]</ref> to evaluate the effectiveness of our proposed backbone and segmentation network, respectively. We first introduce the datasets and implementation details. Then, we report our accuracy and speed results on different benchmarks compared with other algorithms. Finally, we discuss the impact of components in our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmarks and Evaluation Metrics</head><p>ImageNet. The ILSVRC <ref type="bibr" target="#b5">[6]</ref> 2012 is the most popular image classification dataset. It contains 1.2 million images for training, and 50,000 for validation with 1,000 categories. It is also widely used for training a pretrained model for downstream tasks, like object detection or semantic segmentation. Cityscapes. Cityscapes <ref type="bibr" target="#b4">[5]</ref> is a semantic scene parsing dataset, which is taken from a car perspective. It contains 5,000 fine annotated images and split into training, validation and test sets, with 2,975, 500 and 1,525 images respectively. The annotation includes 30 classes, 19 of which are used for semantic segmentation task. The images have a high resolution of 2, 048 × 1, 024, thus it is challenging for the real-time semantic segmentation. For fair comparison, we only use the fine annotated images in our experiments. CamVid. Cambridge-driving Labeled Video Database (Camvid) <ref type="bibr" target="#b1">[2]</ref> is a road scene dataset, which is taken from a driving automobile perspective. This dataset contains 701 annotated images extracted from the video sequence, in which 367 for training, 101 for validation and 233 for testing. The images have a resolution of 960 × 720 and 32 semantic categories, in which the subset of 11 classes are used for segmentation experiments. Evaluation Metrics. For classification evaluation, we use evaluate top-1 accuracy as the evaluation metrics following <ref type="bibr" target="#b8">[9]</ref>. For segmentation evaluation, we adopt mean of class-wise intersection over union (mIoU) and Frames Per Second (FPS) as the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Image Classification. We use mini-batch stochastic gradient descent (SGD) with batch size 64, momentum 0.9 and weight decay 1e −4 to train the model. Three training methods from <ref type="bibr" target="#b10">[11]</ref> are adopted, including learning rate warmup,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Resolution mIoU(%) FPS GhostNet <ref type="bibr" target="#b7">[8]</ref> 512 cosine learning rate policy and label smoothing. The total epochs is 300 with warmup strategy at the first 5 epochs, within which the learning rate starts from 0.001 to 0.1. The dropout before classification block is set to 0.2. We do not use other special data augmentations, and all of them are the same as <ref type="bibr" target="#b8">[9]</ref>. Semantic Segmentation. We use mini-batch stochastic gradient descent (SGD) with momentum 0.9, weight decay 5e −4 . The batch size is set as 48, 24 for the Cityscapes, CamVid dataset respectively. As common configuration, we utilize "poly" learning rate policy in which the initial rate is multiplied by (1 − iter max iter ) power . The power is set to 0.9 and the initial learning rate is set as 0.01. Besides, we train the model for 60, 000, 10, 000 iterations for the Cityscapes, CamVid dataset respectively, in which we adopt warmup strategy at the first 1000, 200 iterations.</p><p>Data augmentation contains color jittering, random horizontal flip, random crop and random resize. The scale ranges in [0.125, 1.5] and cropped resolution is 1024 × 512 for training Cityscapes. For training CamVid, the scale ranges in [0.5, 2.5] and cropped resolution is 960 × 720.</p><p>In all experiments, we conduct our experiments base on pytorch-1.1 on a docker. We perform all experiments under CUDA 10.0, CUDNN 7.6.4 and TensorRT 5.0.1.5 on NVIDIA GTX 1080Ti GPU with batch size 1 for benchmarking the computing power of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>This section introduces the ablation experiments to validate the effectiveness of each component in our method. Effectiveness of STDC Module. We adjust the block number of STDC module in STDC2 and present the result in <ref type="figure" target="#fig_6">Figure 7</ref>. According to our Equation 3, as the group number increases, the FLOPs decrease obviously. And the best performance is in 4 blocks. The benefits of more blocks become very small and a deeper network is bad for the parallel calculation and FPS. Hence, in this paper, we set the block number in STDC1 and STDC2 to 4. Effectiveness of Our backbone. To verify the effectiveness of our backbone designed for real-time segmentation, we adopt the latest lightweight backbones which has com-    <ref type="table" target="#tab_3">Table 4</ref>. To verify the effectiveness of our Detail Guidance, we show the comparison of different detail guidance strategies of STDC2-Seg on Cityscapes val dataset. To further demonstrate the capability of Detail Guidance, we first use the Spatial Path in BiSeNetV1 <ref type="bibr" target="#b27">[28]</ref> to encode the spatial information, then use the features generated from the Spatial Path to replace the features from Stage 3 D . The setting of experiment with Spatial Path are exactly the same with other experiments. As shown in <ref type="table" target="#tab_3">Table 4</ref>, Detail Guidance in STDC2-Seg can improve the mIoU without harming the inference speed. Adding Spatial Path to encode spatial information can also improve the performance on accuracy, but it increases the computation cost at the same time. Also we find our Detail Aggregation module encode the abundant detail information and yield the highest mIoU with aggregation of 1x, 2x, 4x detail features.</p><formula xml:id="formula_8">(a)Input (b)Stage3 (c)Stage3 D (d)Prediction (e)Prediction D (f)Groundtruth</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Compare with State-of-the-arts</head><p>In this part, we compare our methods with other existing state-of-the-art methods on three benchmarks, ImageNet, Cityscapes and CamVid. Results on ImageNet. As shown in e.g. EfficientNet-B0, the FPS of STDC2 network is 83.7% higher than that of baseline with competitive classification result.</p><p>Results on Cityscapes. As shown in <ref type="table">Table 6</ref>, we present the segmentation accuracy and inference speed of our proposed method on Cityscapes validation and test set. Following the previous methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22]</ref>, we use the training set and validation set to train our models before submitting to Cityscapes online server. At test phase, we first resize the image into the fixed size 512 × 1024 or 768 × 1536 to inference, then we up-sample the results to 1024 × 2048. Overall, our methods get the best speed-accuracy trade-off <ref type="bibr">Model</ref> Resolution Backbone mIoU(%) FPS ENet <ref type="bibr" target="#b23">[24]</ref> 720 × 960 no 51.3 61.2 ICNet <ref type="bibr" target="#b30">[31]</ref> 720 × 960 PSPNet50 67.1 34.5 BiSeNetV1 <ref type="bibr" target="#b27">[28]</ref> 720 × 960 Xception39 65.6 175 BiSeNetV1 <ref type="bibr" target="#b27">[28]</ref> 720 × 960 ResNet18 68.7 116.3 CAS <ref type="bibr" target="#b29">[30]</ref> 720 × 960 no 71.2 169 GAS <ref type="bibr" target="#b21">[22]</ref> 720 × 960 no 72. among all methods. We use 50 and 75 after the method name to represent the input size 512 × 1024 and 768 × 1536 respectively. For example, with the STDC1 backbone and 512 × 1024 input size, we name the method STDC1-Seg50. As shown in <ref type="table">Table 6</ref>, our STDC1-Seg50 achieves a significantly faster speed than baselines, i.e., 250.4 FPS, and still has 71.9% mIoU on test set, which is over 45.2% faster than the runner-up. Our STDC2-Seg50 using 512 × 1024 input size achieves 73.4% mIOU with 188.6 FPS, which is the state-of-the-art trade-off between performance and speed. For 768 × 1536 input size, our STDC2-Seg75 achieves the best mIOU 77.0% in validation set and 76.8% on test set at 97.0 FPS. Results on CamVid. We also evaluate our method on CamVid dataset. <ref type="table" target="#tab_5">Table 7</ref> shows the comparison results with other methods. With the input size 720 × 960, STDC1-Seg achieves 73.0% mIoU with 197.6 FPS which is the stateof-the-art trade-off between performance and speed. This further demonstrates the superior capability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we revisit the classical segmentation architecture BiSeNet <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> for structure optimization. Generally, the classification backbone and extra spatial path of BiSeNet greatly hinder the inference efficiency. Therefore, we propose a novel Short-Term Dense Concatenate Module to extract deep features with scalable receptive field and multi-scale information. Based on this module, STDC networks are designed and achieve competitive accuracy with high FPS in image classification.Using STDC networks as backbone, our detail-guided STDC-Seg achieves state-ofthe-art speed-accuracy trade-off in real-time semantic segmentation. Extensive experiments and visualization results indicates the effectiveness of our proposed STDC-Seg networks. In future, we extend our method by following directions: (i) the backbone will be validated in more tasks, e.g., object detection. (ii) we will explore deeper on the utilization of spatial boundary in semantic segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(a) General STDC network architecture. ConvX operation refers to the Conv-BN-ReLU. (b) Short-Term Dense Concatenate module (STDC module) used in our network. M denotes the dimension of input channels, N denotes the dimension of output channels. Each block is a ConvX operation with different kernel size. (c) STDC module with stride=2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3(b) and (c) illustrate the layout of STDC module. Specifically, each module is separated into several blocks, STDC module Block1 Block2 Block3 Block4 Fusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Overview of the STDC Segmentation network. ARM denotes Attention Refine module, and FFM denotes Feature Fusion Module in<ref type="bibr" target="#b27">[28]</ref>. The operation in the dashed red box is our STDC network. The operation in the dashed blue box is Detail Aggregation Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visual explanations for features in the spatial path and Stage 3 without and with Detail Guidance. The column with subscript D denotes results with Detail Guidance. The visualization shows that spatial path can encode more spatial detail,e.g., boundary, corners, than backbone's low-level layers, while our Detail Guidance module can do the same thing without extra computation cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visual comparison of our Detail Guidance on Cityscapes val set. The column with subscript D denotes results with Detail Guidance. The first row (a) shows the input images. (b) and (c) illustrate the heatmap of Stage 3 without and with Detail Guidance. (d) and (e) demonstrate the predictions without and with Detail Guidance. (f) is the ground-truth of input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Comparisons with different block number of STDC2 on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We propose the Detail Aggregation module to learn the decoder, leading to more precise preservation of spatial details in low-level layers without extra computation cost in the inference time.</figDesc><table><row><cell>• We conduct extensive experiments to present the ef-</cell></row><row><cell>fectiveness of our methods. The experiment results</cell></row><row><cell>present that STDC networks achieve new state-of-</cell></row><row><cell>the-art results on ImageNet, Cityscapes and CamVid.</cell></row><row><cell>Specifically, our STDC1-Seg50 achieves 71.9% mIoU</cell></row><row><cell>on the Cityscapes test set at a speed of 250.4 FPS on</cell></row><row><cell>one NVIDIA GTX 1080Ti card. Under the same ex-</cell></row><row><cell>periment setting, our STDC2-Seg75 achieves 76.8%</cell></row><row><cell>mIoU at a speed of 97.0 FPS.</cell></row></table><note>We design a Short-Term Dense Concatenate module (STDC module) to extract deep features with scalable receptive field and multi-scale information. This mod- ule promotes the performance of our STDC network with affordable computational cost.•</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the final output of STDC module is concatenated from</figDesc><table><row><cell>Stages</cell><cell cols="2">Output size KSize S</cell><cell cols="4">STDC1 STDC2 R C R C</cell></row><row><cell>Image</cell><cell>224×224</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>3</cell></row><row><cell>ConvX1</cell><cell cols="6">112×112 3×3 2 1 32 1 32</cell></row><row><cell>ConvX2</cell><cell>56×56</cell><cell cols="5">3×3 2 1 64 1 64</cell></row><row><cell>Stage3</cell><cell>28×28 28×28</cell><cell>2 1</cell><cell>1 1</cell><cell>256</cell><cell>1 3</cell><cell>256</cell></row><row><cell>Stage4</cell><cell>14×14 14×14</cell><cell>2 1</cell><cell>1 1</cell><cell>512</cell><cell>1 4</cell><cell>512</cell></row><row><cell>Stage5</cell><cell>7×7 7×7</cell><cell>2 1</cell><cell>1 1</cell><cell>1024</cell><cell>1 2</cell><cell>1024</cell></row><row><cell>ConvX6</cell><cell>7×7</cell><cell cols="5">1×1 1 1 1024 1 1024</cell></row><row><cell>GlobalPool</cell><cell>1×1</cell><cell>7×7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FC1</cell><cell></cell><cell></cell><cell></cell><cell>1024</cell><cell></cell><cell>1024</cell></row><row><cell>FC2</cell><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell></cell><cell>1000</cell></row><row><cell>FLOPs</cell><cell></cell><cell></cell><cell cols="2">813M</cell><cell cols="2">1446M</cell></row><row><cell>Params</cell><cell></cell><cell></cell><cell cols="2">8.44M</cell><cell cols="2">12.47M</cell></row></table><note>Table 2. Detailed architecture of STDC networks. Note that ConvX shown in the table refers to the Conv-BN-ReLU. The basic module of Stage 3, 4 and 5 is STDC module. KSize mean kernel size. S, R, C denote stride, repeat times and output channels respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 3. Lightweight backbone comparison on Cityscapes val set. All experients utilize the same decoder and same experiment settings.</figDesc><table><row><cell></cell><cell>× 1024</cell><cell>67.8</cell><cell>135.0</cell></row><row><cell>MobileNetV3 [12]</cell><cell>512 × 1024</cell><cell>70.1</cell><cell>148.3</cell></row><row><cell cols="2">EfficientNet-B0 [26] 512 × 1024</cell><cell>72.2</cell><cell>99.9</cell></row><row><cell>STDC2</cell><cell>512 × 1024</cell><cell>74.2</cell><cell>188.6</cell></row><row><cell>GhostNet [8]</cell><cell>768 × 1536</cell><cell>71.3</cell><cell>60.9</cell></row><row><cell>MobileNetV3 [12]</cell><cell>768 × 1536</cell><cell>73.0</cell><cell>70.4</cell></row><row><cell cols="2">EfficientNet-B0 [26] 768 × 1536</cell><cell>73.9</cell><cell>45.9</cell></row><row><cell>STDC2</cell><cell>768 × 1536</cell><cell>77.0</cell><cell>97.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Detail information comparison on Cityscapes val set. SP means method with Spatial Path and DG indicates Detail Guidance, inwhich 1x, 2x, 4x denotes detail features with different down-sample strides in Detail Aggregation module.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>more spatial information comparing to that of Stage 3 with-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>out detail guidance. Hence the final prediction of small</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>objects and boundaries are more precise. We show some</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>quantitative results in</cell></row><row><cell>Method</cell><cell>SP</cell><cell>DG 4x 2x 1x</cell><cell cols="2">mIoU(%) FPS</cell></row><row><cell>BiSeNetV1 [28]</cell><cell></cell><cell></cell><cell>69.0</cell><cell>105.8</cell></row><row><cell>STDC2-50</cell><cell></cell><cell></cell><cell>73.7</cell><cell>171.6</cell></row><row><cell>STDC2-50</cell><cell></cell><cell></cell><cell>73.0</cell><cell>188.6</cell></row><row><cell>STDC2-50</cell><cell></cell><cell></cell><cell>73.4</cell><cell>188.6</cell></row><row><cell>STDC2-50</cell><cell></cell><cell></cell><cell>73.6</cell><cell>188.6</cell></row><row><cell>STDC2-50</cell><cell></cell><cell></cell><cell>73.8</cell><cell>188.6</cell></row><row><cell>STDC2-50</cell><cell></cell><cell></cell><cell>73.9</cell><cell>188.6</cell></row><row><cell>STDC2-50</cell><cell></cell><cell></cell><cell>74.2</cell><cell>188.6</cell></row><row><cell cols="5">parable classification performance compared with STDC2</cell></row><row><cell cols="5">to formulate a semantic segmentation network with our de-</cell></row><row><cell cols="5">coder. As show in Table. 3, our STDC2 yield the best speed-</cell></row><row><cell cols="5">accuracy trade-off comapred with other lightweight back-</cell></row><row><cell>bones.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Effectiveness of Detail Guidance. We first visualize the</cell></row><row><cell cols="5">heatmap of the feature map of Stage 3 as shown in Fig-</cell></row><row><cell cols="5">ure 6. The features of Stage 3 with detail guidance encode</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 Table 6 .</head><label>56</label><figDesc>Comparisons with other state-of-the-art methods on Cityscapes. no indicates the method do not have a backbone.</figDesc><table><row><cell>, our STDC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Comparisons with other state-of-the-art methods on CamVid. no indicates the method do not have a backbone.</figDesc><table><row><cell>8</cell><cell>153.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to predict crisp boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huibing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinru</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="562" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8818" to="8827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongkyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9522" to="9531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">a. when humans meet machines: Towards efficient segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peike</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10120</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Partial order pruning: for best speed/accuracy trade-off in neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9145" to="9153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph-guided architecture search for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Customizable architecture search for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11641" to="11650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
