<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Facial Expression Recognition Using Features of Salient Facial Patches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Happy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurobinda</forename><surname>Routray</surname></persName>
						</author>
						<title level="a" type="main">Automatic Facial Expression Recognition Using Features of Salient Facial Patches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Facial expression analysis</term>
					<term>facial landmark detection</term>
					<term>feature selection</term>
					<term>salient facial patches</term>
					<term>low resolution image</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>acial expression, being a fundamental mode of communicating human emotions, finds its applications in human-computer interaction (HCI), health-care, surveillance, driver safety, deceit detection etc. Tremendous success being achieved in the fields of face detection and face recognition, affective computing has received substantial attention among the researchers in the domain of computer vision. Signals, which can be used for affect recognition, include facial expression, paralinguistic features of speech, body language, physiological signals (e.g. Electromyogram (EMG), Electrocardiogram (ECG), Electrooculogram (EOG), Electroencephalography (EEG), Functional Magnetic Resonance Imaging (fMRI) etc.). A review of signals and methods for affective computing is reported in <ref type="bibr" target="#b0">[1]</ref>, according to which, most of the research on facial expression analysis are based on detection of basic emotions <ref type="bibr" target="#b1">[2]</ref>: anger, fear, disgust, happiness, sadness, and surprise. A number of novel methodologies for facial expression recognition have been proposed over the last decade.</p><p>Effective expression analysis hugely depends upon the accurate representation of facial features. Facial Action Coding System (FACS) <ref type="bibr" target="#b2">[3]</ref> represents face by measuring all visually observable facial movements in terms of Action Units (AUs) and associates them with the facial expressions. Accurate detection of AUs depends upon proper identification and tracking of different facial muscles irre-spective of pose, face shape, illumination, and image resolution. According to Whitehill et al. <ref type="bibr" target="#b3">[4]</ref>, the detection of all facial fiducial points is even more challenging than expression recognition itself. Therefore, most of the existing algorithms are based on geometric and appearance based features. The models based on geometric features track the shape and size of the face and facial components such as eyes, lip corners, eyebrows etc., and categorize the expressions based on relative position of these facial components. Some researchers (e.g., <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>) used shape models based on a set of characteristic points on the face to classify the expressions. However, these methods usually require very accurate and reliable detection as well as tracking of the facial landmarks which are difficult to achieve in many practical situations. Moreover, the distance between facial landmarks vary from person to person, thereby making the person independent expression recognition system less reliable. Facial expressions involve change in local texture. In appearance-based methods <ref type="bibr" target="#b8">[9]</ref>, a bank of filters such as Gabor wavelets, Local Binary Pattern (LBP) etc. are applied to either the whole-face or specific face regions to encode the texture. The superior performance of appearance based methods to the geometry based features is reported in <ref type="bibr" target="#b3">[4]</ref>. The appearance-based methods generates high dimensional vector which are further represented in lower dimensional subspace by applying dimensionality reduction techniques, such as principal component analysis (PCA), linear discriminant analysis (LDA) etc. Finally, the classification is performed in learned subspace. Although the time and space costs are higher in appearance based methods, the preservation of discriminative information makes them very popular.</p><p>Extraction of facial features by dividing the face region into several blocks achieves better accuracy as reported by many researchers ( <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>). However, this approach fails with improper face alignment and occlusions. Some earlier works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> on extraction of features from specific face regions mainly determine the facial regions which contributes more toward discrimination of expressions based on the training data. However, in these approaches, the positions and sizes of the facial patches vary according to the training data. Therefore, it is difficult to conceive a generic system using these approaches. In this paper, we propose a novel facial landmark detection technique as well as a salient patch based facial expression recognition framework with significant performance at different image resolutions. The proposed method localizes face as well as the facial landmark points in an image, thereby extracting some salient patches that are estimated during training stage. The appearance features from these patches are fed to a multi-class classifier to classify the images into six basic expression classes. It is found that the proposed facial landmark detection system performs similar to the state-of-the-art methods in near frontal images with lower computational complexity. The appearance features with lower number of histogram bins are used to reduce the computation. Empirically the salient facial patches are selected with predefined positions and sizes, which contribute significantly towards classification of one expression from others. Once the salient patches are selected, the expression recognition becomes easy irrespective of the data. Affective computing aims at effective emotion recognition in low resolution images. The experimental results shows that the proposed system performed better in low resolution images. The paper is organized as follows. Section 2 presents a review of earlier works. The proposed framework is presented in Section 3. Section 4 and 5 discusses the facial landmark detection and feature extraction technique respectively. Experimental results and discussion are provided in Section 6. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>For better performance in facial expression recognition, the importance of detection of facial landmarks is undeniable. Face alignment is an essential step and is usually carried out by detection and horizontal positioning of eyes. Facial landmark detection is followed by feature extraction. Selection of features also affects the classification accuracy. In <ref type="bibr" target="#b17">[18]</ref>, an active Infra-Red illumination along with Kalman filtering is used for accurate tracking facial components. Performance is improved by the use of both geometric and appearance features. Here the initial positions of facial landmarks are figured out using face geometry, given the position of eyes, which is not convenient. Tian et al. <ref type="bibr" target="#b18">[19]</ref> also used relative distance (lip corner, eye, brow etc.) and transient features (wrinkles, furrows etc.) for recognizing AUs present in lower face. However, the use of Canny edge detector for extracting appearance features is not flexible in different illumination and determining the presence of furrows using threshold is uncertain. Uddin et al. <ref type="bibr" target="#b19">[20]</ref> reported good performance by using image difference method for observing changes in expressions. The major issue is the landmark selection which is carried out manually by matching the eye and mouth regions. In <ref type="bibr" target="#b20">[21]</ref>, a relative geometrical distance based approach is described which uses computationally expensive Gabor filters for landmark detection and tracking. They used combined SVM and HMM models as classifiers.</p><p>Deformable models, to fit into new data instances, have become popular for facial landmark detection. Active shape models (ASM) determine shape, scale and pose by fitting an appropriate point distribution model (PDM) to the object of interest. Active appearance models (AAM) <ref type="bibr" target="#b21">[22]</ref> combines both shape and texture models to represent the object, hence providing superior result to ASM. AAM is widely used ( <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>) for detection and tracking of non-rigid facial landmarks. However, its performance is poor in person independent scenarios. Manual placement of the landmark points in training data for construction of the shape model is a tedious task and time consuming process in these models. Constrained Local Model (CLM) framework proposed by Cristinacce et al. <ref type="bibr" target="#b27">[28]</ref> has been proved as a better tool for person independent facial landmark detection. All the above said deformable models use PCA to learn the variability of shapes and textures offline. CLM algorithm is further modified by Saragih et al. <ref type="bibr" target="#b28">[29]</ref> who proposed Regularized Landmark Mean Shift (RLMS) algorithm with improved landmark localization accuracy. Asthana et al. <ref type="bibr" target="#b29">[30]</ref> proposed Discriminative Response Map Fitting (DRMF) method for the CLM framework for the generic face fitting scenario in both controlled and natural imaging conditions. Though satisfactory results has been achieved using these deformable models, high computational cost is an obstacle in using them in real-time applications. Chew et al. <ref type="bibr" target="#b30">[31]</ref> established the fact that, appearance based models work robustly even with small alignment errors, and perform the same as that of a close to perfect alignment. Therefore, slight error in landmark detection will not hamper the purpose. In our experiments, we used a computationally inexpensive learningfree method for landmark detection that serves the purpose as efficiently as recent DRMF based CLM method <ref type="bibr" target="#b29">[30]</ref>.</p><p>An effective feature ideally discriminates between the expressions while minimizing the intra-class variance, and should be easily extracted from raw images of different resolutions. Among the appearance features, Gabor-wavelet representations have been widely adopted in face image analysis <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> due to their superior performance. However, the computation of Gabor-features is both time and memory intensive; besides, they are sensitive to scaling. Recently the Local Binary Patterns (LBP) proved themselves as an effective appearance features for facial image analysis <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Jabid et al. <ref type="bibr" target="#b10">[11]</ref> developed local facial descriptor based on Local Description Patterns (LDP) codes and obtained better performance than LBP features. Recently Dhall et al. <ref type="bibr" target="#b35">[36]</ref> reported higher performance of Local Phase Quantization (LPQ) in facial expression recognition. In <ref type="bibr" target="#b11">[12]</ref>, Local Directional Pattern Variance (LDPv) is proposed which encodes contrast information using local variance of directional responses. However, Shan et al. <ref type="bibr" target="#b36">[37]</ref> found LBP features to be robust for analysis of low resolution images. Therefore, we used the LBP histograms as appearance features.</p><p>PCA <ref type="bibr">( [38]</ref>, <ref type="bibr" target="#b38">[39]</ref>) and LDA ( <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>) are used as a tool for dimensionality reduction as well as classification in expression recognition. In <ref type="bibr" target="#b42">[43]</ref>, authors reported the higher performance of PCA-LDA fusion method. An encrypted domain based facial expression recognition system is proposed in <ref type="bibr" target="#b43">[44]</ref> which uses local fisher discriminant analysis to achieve accuracy as good as in normal images. Expression subspace is introduced in <ref type="bibr" target="#b44">[45]</ref> which explains that the same expressions lie on the same subspace and new expressions can be generated from one image by projecting it into different emotion subspaces.</p><p>Most of the proposed methods use full face image, while a few use features extracted from specific facial patches. In <ref type="bibr" target="#b12">[13]</ref>, face image is divided into several sub regions (7x6) and local features (7x6x59 dimensional features) are extracted. Then, the discriminative LBP histogram bins are selected by using Adaboost technique for optimum classification. Similar approaches are reported in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and <ref type="bibr" target="#b45">[46]</ref>. In such cases, small misalignment would cause displacement of the sub region locations, thereby increasing error in classification. Moreover, for different persons the size and shape of facial organs are not the same, so, it cannot be assured that the same facial position always present in one particular block in all images. Hence, local patch selection based approach is adopted in our experiments. In <ref type="bibr" target="#b46">[47]</ref>, authors divided the face into 64 sub regions and explored the common facial patches which are active for most expressions and special facial patches which are active for specific expressions. Using multi task sparse learning method, they used features of a few number of facial patches to classify facial expressions. Song et al. <ref type="bibr" target="#b15">[16]</ref> used eight facial patches based on specific landmark positions to observe the skin deformations caused by expressions. The authors have used binary classifiers to generate a Boolean variable for presence or absence of skin wrinkles. However, these patches do not include the texture of lip corners, which is important for expression recognition. Moreover, the occlusion of forehead by hair may result in false recognition. In <ref type="bibr" target="#b16">[17]</ref>, authors extracted Gabor features of different scales from the face image and trained using Adaboost to select the salient patches for each expression. However, the salient patch size and position is different when trained with different databases. Therefore, a unique criteria cannot be established for recognition of expressions in unknown images.</p><p>Some issues related to real-time detection of facial landmarks and expression recognition remain unaddressed so far. Most of the researches in this field are carried out on different datasets with suitable performance criteria befitting to the database. For example, selection of prominent facial areas improves the performance. However, in most of the literature, the size and position of these facial patches are reported to be different for different databases. Therefore, our experiments attempt to identify the salient facial areas having generalized discriminative features for expression classification. Selection of salient patches retaining discriminating features between each pair of facial expressions improved the accuracy. The size and location of patches are kept same for different databases for the purpose of generalization. In addition, the proposed framework has the potential to recognize expressions in low-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHODOLOGY</head><p>Changes in facial expressions involve contraction and expansion of facial muscles which alters the position of facial landmarks. Along with the facial muscles, the texture of the area also changes. This paper attempts to understand the contribution of different facial areas toward automatic expression recognition. In other words, the paper explores the facial patches which generates discriminative features to separate two expressions effectively.</p><p>The overview of the proposed method is shown in <ref type="figure">Fig.  1</ref>. Observations from <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b15">[16]</ref> suggest that accurate facial landmark detection and extraction of appearance features from active face regions improve the performance of expression recognition. Therefore, the first step is to localize <ref type="figure">Fig. 1</ref>. Overview of the proposed system the face followed by detection of the landmarks. A learning-free approach is proposed in which the eyes and nose are detected in the face image and a coarse region of interest (ROI) is marked around each. The lip and eyebrow corners are detected from respective ROIs. Locations of active patches are defined with respect to the location of landmarks. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the steps involved in automated facial landmark detection and active patch extraction. In training stage, all the active facial patches are evaluated and the ones having features of maximum variation between pairs of expressions are selected. These selected features are further projected into lower dimensional subspace and classified into different expressions using a multi-class classifier. The training phase includes pre-processing, selection of facial patches, extraction of appearance features and learning of the multi-class classifiers. In an unseen image, the process first detects the facial landmarks, then extracts the features from the selected salient patches, and finally classifies the expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FACIAL LANDMARK DETECTION</head><p>The facial patches which are active during different facial expressions are studied in <ref type="bibr" target="#b46">[47]</ref>. It is reported that some facial patches are common during elicitation of all basic expressions and some are confined to a single expression. The results indicate that these active patches are positioned below the eyes, in between the eyebrows, around the nose and mouth corners. To extract these patches from face image, we need to locate the facial components first followed by the extraction of the patches around these organs. Un-zueta et al. <ref type="bibr" target="#b47">[48]</ref> proposed a robust, learning-free, lightweight generic face model fitting method for localization of the facial organs. Using local gradient analysis, this method finds the facial features and adjusts the deformable 3D face model so that its projection on image will match the facial feature points. In this paper, such a learning-free approach was adopted for localization of facial landmarks. We have extracted the active facial patches with respect to the position of eyes, eyebrows, nose, and lip corners using the geometrical statistics of the face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1</head><p>Pre-processing A low pass filtering was performed using a 3x3 Gaussian mask to remove noise from the facial images followed by face detection for face localization. We used Viola-Jones technique <ref type="bibr" target="#b48">[49]</ref> of Haar-like features with Adaboost learning for face detection. It has lower computational complexity and was sufficiently accurate for detection of nearfrontal and near-upright face images. Using integral image calculation, it can detect face regardless of scale and location in real time. The localized face was extracted and scaled to bring it to a common resolution. This made the algorithm shift invariant, i.e. insensitive to the location of the face on image. Histogram equalization was carried out for lighting corrections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Eye and Nose Localization</head><p>To reduce the computational complexity as well as the false detection rate, the coarse region of interests (ROI) for eyes and nose were selected using geometrical positions of face. Both the eyes were detected separately using Haar classifiers trained for each eye. The Haar classifier returns the vertices of the rectangular area of detected eyes. The eye centers are computed as the mean of these coordinates. Similarly, nose position was also detected using Haar cascades. In our experiment, for more than 98% cases these parts were detected properly. In case the eyes or nose was not detected using Haar classifiers, the system relies on the landmark coordinates detected by anthropometric statistics of face. The position of eyes were used for up-right face alignment as the positions of eyes do not change with facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Lip Corner Detection</head><p>Inspired by the work of Nguyen et al. <ref type="bibr" target="#b49">[50]</ref>, we used facial topographies for detection of lip and eyebrow corners. The ROIs for lips and eyebrows were selected as a function of face width positioned with respect to the facial organs. The ROI for mouth was extracted using the position of  nose as reference <ref type="figure" target="#fig_2">(Fig. 3a)</ref>. The upper lip always produces a distinct edge which can be detected using a horizontal edge detector. Sobel edge detector <ref type="bibr" target="#b50">[51]</ref> was used for this purpose. In images with different expressions, a lot of edges were obtained which was further threshold by using Otsu method <ref type="bibr" target="#b51">[52]</ref>. In this process, a binary image was obtained containing many connected regions. Using connected component analysis, the spurious components having an area less than a threshold were removed. Further, morphological dilation operation was carried out on the resulting binary image. Finally, the connected component with largest area which was just below the nose region was selected as upper lip region. <ref type="figure" target="#fig_2">Fig. 3</ref> shows different stages of the process. The algorithm steps are given below. Algorithm 1. Lip corner detection Given: aligned face ROI and nose position 1: select coarse lips ROI using face width and nose position 2: apply Gaussian blur to the lips ROI 3: apply horizontal sobel operator for edge detection 4: apply Otsu-thresholding 5: apply morphological dilation operation 6: find the connected components 7: remove the spurious connected components using threshold technique to the number of pixels 8: scan the image from the top and select the first connected component as upper lip position 9: locate the left and right most positions of connected component as lip corners</p><p>Sometimes, due to shadow below the nose, the upper lip could not be segmented properly. A case is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. In such cases, the upper lip was not segmented as a whole and the connected component obtained at the end resembled half of the upper lip. Hence, the extreme ends of this connected component did not satisfy the bilateral symmetry property, i.e. the lip corners should have been at more or less equal distances from vertical central line of face. These situations were detected by putting a threshold to the ratio of distance between the lip corners to the maximum of distances of the lip corners from the vertical central line. In such cases, the second connected component below the nose was considered as the other part of upper lip. Thus the lip corners were detected with the help of two connected components. By using the above said methods, false detection of lip corner points were minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Eyebrow Corner Detection</head><p>With the knowledge of positions of eyes, the coarse ROIs of eyebrows were selected. The eyebrows were de-tected following the same steps as that of upper lip detection. However, we observed that performing an adaptive threshold operation before applying horizontal sobel operator improved the accuracy of eyebrow corner localization. The use of horizontal edge detector reduced the false detection of eyebrow positions due to partial occlusion by hair. The inner eyebrow corner was detected accurately in most of the images. <ref type="figure" target="#fig_4">Fig. 5</ref> shows intermediate steps in eyebrow corner detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Extraction of Active Facial Patches</head><p>During an expression, the local patches were extracted from the face image depending upon the position of active facial muscles. We have considered the appearance of facial regions exhibiting considerable variations during one expression. For example, wrinkle in upper nose region is prominent in disgust expression and absent in other expressions. Similarly, regions around lip corners undergo significant changes and its appearance features are dissimilar for different expressions. From our observations, supported by the research of Zhong et al. <ref type="bibr" target="#b46">[47]</ref>, we used the active facial patches as shown in <ref type="figure" target="#fig_5">Fig. 6</ref> for our experiment.   The patches does not have very fixed position on the face image. Rather, their location depends upon the positions of facial landmarks. The size of all facial patches were kept equal and was approximately one-ninth of the width of the face. Here onwards, we will refer the patches by the numbers assigned to it. As shown in <ref type="figure" target="#fig_3">Fig. 6, 1 , 4</ref> , 18 , and <ref type="bibr" target="#b18">19</ref> were directly extracted from the positions of lip corners and inner eyebrows respectively. <ref type="bibr" target="#b15">16</ref> was at the center of both the eyes; and 17 was the patch above <ref type="bibr" target="#b15">16</ref> . 3 and 6 were located in the midway of eye and nose. 14 and 15 were located just below eyes. 2 , 7 , and 8 were clubbed together and located at one side of nose position. 9 was located just below 1 . In a similar fashion 5 , 11 , 12 , and 13 were located. 10 was located at the center of position of 9 and 11 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FEATURE EXTRACTION AND CLASSIFICATION</head><p>LBP was widely used as a robust illumination invariant feature descriptor. This operator generates a binary number by comparing the neighbouring pixel values with the center pixel value <ref type="bibr" target="#b52">[53]</ref>. The pattern with 8 neighborhoods is given by</p><formula xml:id="formula_0">( , ) = ∑ 7 =0 ( − )2</formula><p>where is the pixel value at coordinate ( , ) and are the pixel values at coordinates in the neighborhood of ( , ), and where is the number of labels produced by LBP operator. Using different binwidths, the histograms can be grouped to discover different features. For instance, LBP with 8 neighboring points produces 256 labels. If we collect its histograms in 32 bins, then we are basically grouping the patterns [0,7], <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, …, and [248,255] together. This is same as ignoring the least significant bits of the unsigned integer, i.e. using patterns of one side of local neighborhood. <ref type="figure">Fig. 7</ref> shows the pattern generated due to 32 histogram bins where the upper row neighbors do not contribute towards the pattern label. We used 16, 32 and 256 bin histograms in the experiments. In addition, uniform LBP and rotation invariant uniform LBP values <ref type="bibr" target="#b52">[53]</ref> are also used in our experiment and their performances are compared. Uniformity measure ( ) corresponds to the number of bitwise transitions from 0 to 1 or vice-versa in a pattern when the bit pattern is traversed circularly. For instance, the pattern (00000001)2 and (00100110)2 have values 2 and 4 respectively. The pattern is called uniform (LBPu2) when ≤ 2. This reduces the length of the 8-neighborhood patterns to 59-bin histograms. The effect of rotation can be removed by assigning a unique identifier to each rotation invariant pattern, given by</p><formula xml:id="formula_1">2 = { ∑ 7 =0 ( − ), " " 9, ℎ</formula><p>Thus, the rotational invariant uniform LBP with 8 neighborhood produces 10 histogram bins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning Salient Facial Patches Across Expressions</head><p>In most of the literatures, all the facial features are concatenated to recognize the expression. However, this generates a feature vector of high dimension. We observed that the features from a fewer facial patches can replace the high dimensional features without significant diminution of the recognition accuracy. From human perception, not all facial patches are responsible for recognition of one expression. The facial patches responsible for recognition of each expression can be used separately to recognize that particular expression. Based on this hypothesis, we evaluated the performance of each facial patch for recognition of different expressions.</p><p>Further, some expressions share similar movements of facial muscles; features of such patches are redundant while classifying the expressions. Therefore, after extracting the active facial patches, we selected the salient facial patches responsible for discrimination between each pair of basic expressions. A facial patch is considered to be discriminative between two expressions, if the features extracted from this patch can classify the two expressions accurately. Note that not all active patches are salient for recognition of all expressions. For all possible pair of expressions ( 6 C 2 ), all the 19 active patches were evaluated by conducting a ten-fold cross validation test. The patches that result maximum discrimination were selected for representing the expressions.</p><p>The LBP histogram features in lower resolution images are sparse in nature because of the smaller patch area. LDA was applied for projecting these features to the discriminating dimensions and to choose the salient patches according to their discriminative performance. LDA finds the hyper-plane that minimizes the intra-class scatter ( ), while maximizing the inter-class scatter ( ). It is also used as a tool for interpretation of importance of the features. Hence it can be considered as a transformation into a lower dimensional space for optimal discrimination between classes. The intra-class scatter ( ) and inter-class scatter ( ) are given by <ref type="figure">Fig. 7</ref>. Patterns generated by one side of local neighborhood which produces 32 histogram bins where , is the th feature vector in th class. Here number of classes, having number of images in th class, have ̅ , the mean vector of all the training data, and ̅ , the mean of th class. LDA aims at maximization of while minimizing , i.e. maximization of ratio of determinant of to the determinant of .</p><formula xml:id="formula_2">= ∑ =1 ( ̅ − ̅ )( ̅ − ̅ ) = 1 − 1 ∑ ∑( , − ̅ )( , − ̅ ) =1 =1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Φ = arg Φ</head><p>|Φ Φ| |Φ Φ| This ratio is called as Fishers criterion. This can be computed by solving the generalized eigenvalue problem given as:</p><formula xml:id="formula_3">Φ − Φ Λ = 0 ⇒ −1 Φ = Φ Λ</formula><p>where Λ is the diagonal eigenvalue matrix and Φ is the set of discriminant vectors of and corresponding to the − 1 largest generalized eigenvalues. Thus, Fisher criterion is maximized when the projection matrix Φ is composed of eigenvectors of −1 , subject to being non-singular. As suggested by Belhumeur et al. <ref type="bibr" target="#b53">[54]</ref>, PCA was applied to the signal prior to LDA. By doing so, the signal was projected to lower dimensional space assuring the non-singularity of within class scatter matrix. Moreover, this PCA-LDA fusion <ref type="bibr" target="#b42">[43]</ref> improves the performance. Therefore, we applied PCA on the training set for dimensionality reduction followed by LDA.</p><p>We calculated the saliency of all facial patches for all pair of expressions and it was expressed in terms of saliency scores. The saliency of a patch represents the ability of the features from the patch to accurately classify a pair of expressions. The saliency score of a patch between a pair of expressions is the classification accuracy of the features from that patch in classifying the two expressions. Here PCA-LDA was used for classification purpose to determine the saliency score. In a similar fashion, saliency score of all patches for each pair of expressions were calculated. We used one-against-one strategy for expression classification purpose. While classifying between a pair of expressions, the features were extracted from those facial patches which have high saliency score. The feature vectors from the salient patches were concatenated to construct a higher dimensional feature vector. Thus, the dimension of the feature vector depends upon the number of patches selected for classification purpose.</p><p>We applied PCA to reduce the dimensionality of the feature vector. Thus, by projecting the feature vectors from salient patches to the optimal sub-space obtained by above method, we can find the lower dimensional vector with maximum discrimination for different classes. The weight vectors, corresponding to the salient patches of each pair of expression classes, generated during the training stage were used during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2</head><p>Multi-class Classification SVM was used for classification of extracted features into different expression categories. SVM <ref type="bibr" target="#b54">[55]</ref> is a popular machine learning algorithm which maps the feature vector to a different plane, usually to a higher dimensional plane, by a non-linear mapping, and finds a linear decision hyper plane for classification of two classes. Since SVM is a binary classifier, we implemented one-against-one (OAO) technique for multi-class classification <ref type="bibr" target="#b55">[56]</ref>. In OAO approach, a classifier is trained between each pair of classes; hence C 2 number of classifiers were constructed in total, where is the number of classes. Using voting strategy, a vector can be classified to the class having the highest number of votes. After several experiments with linear, polynomial, and radial basis function (RBF) kernels, we selected RBF kernels for its superior classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS AND DISCUSSION</head><p>The proposed method was evaluated by using two widely used facial expression databases, i.e., Japanese Female Facial Expressions (JAFFE) <ref type="bibr" target="#b56">[57]</ref> and Cohn-Kanade (CK+) <ref type="bibr" target="#b57">[58]</ref>. We have used ten-fold cross validation to evaluate the performance of the proposed method. As discussed earlier, face detection was carried out on all images followed by scaling to bring the face to a common resolution. Facial landmarks were detected and salient facial patches were extracted from each face image. During training stage, a SVM classifier was trained between each pair of expressions. Here the training data were the concatenated LBP histogram features extracted from the salient patches containing discriminative characteristics between the given pair of expression classes. Similarly, 6 C 2 numbers of SVM classifiers were constructed and used for evaluating the performance on the test-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiments on the Cohn-Kanade Database</head><p>The Cohn-Kanade database contains both male and female facial expression image sequences for the six basic emotions. In our experiments, the last image from each sequence was selected where the expression is at its peak intensity. The number of instances for each expression varies according to its availability. In our experiments on CK+ database, we used 329 images in total: anger (41), disgust (45), fear <ref type="bibr" target="#b52">(53)</ref>, happiness (69), sadness <ref type="bibr" target="#b55">(56)</ref>, and surprise (65). <ref type="figure" target="#fig_8">Fig. 8</ref>. The recognition rate in images with different face resolutions in CK+ database</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Analysis of histogram binwidth and face resolution</head><p>We determined the optimal resolution and binwidth of the histogram empirically. The expression recognition performance of different feature vectors were studied for face resolutions starting from 48x48 to 192x192. The low resolution images were obtained by down sampling the images. The classifiers were trained and evaluated at different resolutions of face images. In our experiments, we observed minimum accuracy of 82% at 48x48 face resolution as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. In <ref type="bibr" target="#b58">[59]</ref>, it is reported that with a face image of 48x64 resolution, some facial landmarks, such as, lip and eye corners are difficult to detect. Therefore, it is uncertain if expressions can be recognized at this resolution. However, from our experiment, we obtained pretty good accuracy at all resolutions. This establishes the robustness of the appearance features extracted from the salient patches at different resolutions. Since we have implemented a voting method, the result is based on votes of 6 C 2 classifiers, threby, reducing the classification error due to a single classifier.</p><p>We also observed that the use of LBPu2 features produced better accuracy compared to other features at all resolutions. The performance of the other feature vectors are alike. The uniform patterns removes the noisy estimates in the image by accumulating them into one histogram bin, thereby increasing the recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Performance improvement: use of blockhistograms</head><p>By implementing block-based feature extraction technique, more local features were added to the feature vector. This process makes the feature vector to be a combination of local as well as global features. Empirically we observed that the performance was improved when each selected patch was further divided into four equal blocks. In our experiments, the feature vector was obtained by concatenating the features obtained from each block of the salient patches and the results are shown in <ref type="figure">Fig. 9</ref>. It was observed that the 16-bin histograms, 32-bin histograms, 256bin histograms, and uniform LBP features performed alike at all resolutions. We performed the experiments on a standard face resolution of 96x96. At this resolution, all the features except LBPriu2 had similar performances. Feature vector with low dimension reduces the computational complexity. Therefore, we selected the histogram features with 16-bins as the optimal trade-off between speed and accuracy. <ref type="table" target="#tab_0">Table 1</ref> shows the confusion matrix of six emotions based on the proposed method. The quality of the overall classification is evaluated by calculating the macroaverage <ref type="bibr" target="#b59">[60]</ref> of precision, recall and F-score. The proposed system attained a balanced F-score of 94.39% with 94.1% recall and 94.69% precision.</p><p>As observed from <ref type="table" target="#tab_0">Table 1</ref>, surprise expression achieved best recognition rate which is usually characterized by open mouth and upward eyebrow movement. The system performed worst for anger expression and classification error was maximum between anger and sadness since they involve similar and subtle changes. Here onwards, all the experiments are based on face resolution of 96x96.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Optimum number of salient patches</head><p>Number of patches used for classification also affects the performance in terms of speed and accuracy. <ref type="figure">Fig. 10</ref> shows average of accuracies of all expressions with respect to the number of salient patches used for classification with a face resolution of 96x96. It is apparent from <ref type="figure">Fig. 10</ref> that the use of features from all the 19 patches can classify all expressions with an accuracy of 93.87%. It is clear that even the use of appearance features of a single salient patch can discriminate between each pair expressions efficiently with recognition rate of 91.19%. This implies that the use of rest of the features from other patches contribute minimum towards the discriminative features. More the number of patches used, more is the size of the feature vector.  <ref type="figure">Fig. 9</ref>. Improvement in the recognition rate at different resolutions by using block histograms <ref type="figure">Fig. 10</ref>. The recognition rate using different number of salient patches This increases the computational burden. Therefore, instead of using all the facial patches, we can rely on some salient facial patches for expression recognition. This will improve the computational complexity as well as robustness of the features especially when a face is partially occluded. In our experiments, we used top four salient patches in our experiments which results an accuracy close to 95%. Note that the combination of patches varies across different expression pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Performance comparison</head><p>The proposed method was compared with the results obtained by other approaches reported in the literature. Lack of the knowledge of the data and evaluation protocol used by different literatures makes the comparison task difficult. However, we compared the performance of the system with literatures that adopted similar protocols in CK+ dataset. <ref type="table">Table 2</ref> compares the performance of the proposed method with the state-of-the-art methods. From Table 2, Uddin et al. <ref type="bibr" target="#b19">[20]</ref> reported highest recognition performance for disgust, fear, and happiness probably due to the use of temporal features through Hidden Markov Model. However, the performance of the proposed system is comparable with the other systems as it achieved an average recognition rate of 94.09%. Nevertheless, the high recognition rate is obtained using features from specific facial patches and without using features of temporal domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments on JAFFE Database</head><p>While testing on JAFFE database, we used the same pa-rameters obtained for Cohn-Kanade database. In our experiments on JAFFE database, we used 183 images in total: anger <ref type="bibr" target="#b29">(30)</ref>, disgust <ref type="bibr" target="#b31">(32)</ref>, fear <ref type="bibr" target="#b28">(29)</ref>, happiness <ref type="bibr" target="#b30">(31)</ref>, sadness <ref type="bibr" target="#b30">(31)</ref>, and surprise <ref type="bibr" target="#b29">(30)</ref>. The confusion matrix, as in <ref type="table">Table 3</ref>, shows the consistent performance of the proposed method. An overall accuracy of 91.8% was obtained. From the experiments on JAFFE database, it was observed that the proposed system recognises all expressions with 91.8% recall and 92.63% precision achieving an average F-score of 92.22%. The system performed worst for sadness expression as it misclassified sadness as anger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiments on Fused Database</head><p>For generalization, we have fused the samples of two databases together to train the classifier <ref type="bibr" target="#b61">[62]</ref>. Sample Level fusion was performed by putting the images of both databases together. The training set was constructed by randomly electing 90% of the data from each expression of each database. The rest data were used as testing set. The models were trained, and their performances were evaluated on samples of individual databases in testing set. This experiment was repeated for ten times. By learning the features from different databases, the classifier performs better in various situations. All samples were treated with  equal probability of selection for training or testing. Therefore, it is expected that the database with more samples should dominate in performance. However, the proposed method performed well on both databases with a significant accuracy of 89.64% and 85.06% on CK+ and JAFFE databases respectively. The top four salient patches for classification of each pair of expressions is provided in <ref type="table">Table  5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Performance of the landmark detector</head><p>We have used the images of BIOID <ref type="bibr" target="#b62">[63]</ref> dataset without spectacles for evaluating the performance of the proposed landmark detection algorithm. It contains manually labelled facial landmarks which serves the purpose of ground truth during training and testing. The average Euclidian distance error from point to point for each landmark location is used as the distance measure, which is given as:</p><formula xml:id="formula_4">= 1 ∑ =1</formula><p>Here s are the Euclidean distance errors for the landmarks, is the number of landmarks, and is the distance between the eyes pupils used as the scaling factor. <ref type="figure" target="#fig_7">Fig. 11</ref> shows the cumulative distribution of the detection accuracy by using proposed method. Its performance was compared with the performance of the recent DRMF method based CLM model <ref type="bibr" target="#b29">[30]</ref>. The performance of expression recognition by the two landmark detection methods was also compared. After detection of facial landmarks, the active patches were extracted and the procedures as discussed in section 5 were followed for expression classification. As shown in <ref type="table">Table 4</ref>, both the methods produce almost similar accuracy in CK+ database while the proposed landmark detection method takes very small time in comparison to the other one. Although the DRMF based CLM method finds the facial organ locations accurately, it sometimes fails to fit to their shapes in different expressions, especially on lips. When the mouth is open or tightly closed, or when the lip corners are pulled down, it at times generates average lip-shape instead of completely fitting to lips. Thus, lip corner patches were extracted at incorrect locations resulting poor classification. However, the proposed landmark detection method accurately finds the facial landmarks in most of the images thereby extracting features from appropriate patch locations. The slight misalignment error is automatically taken care by the appearance feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper has presented a computationally efficient facial expression recognition system for accurate classification of the six universal expressions. It investigates the relevance of different facial patches in the recognition of different facial expressions. All major active regions on face are extracted which are responsible for the face deformation during an expression. The position and size of these active regions are predefined. The system analyses the active patches and determines the salient areas on face where the features are discriminative for different expressions. Using the appearance features from the salient patches, the system performs the one-against-one classification task and determines the expression based on majority vote.</p><p>In addition, a facial landmark detection method is described which detects some facial points accurately with less computational cost. Expression recognition is carried out using the proposed landmark detection method as well as the recently proposed CLM model based on DRMF method. In both cases, recognition accuracy is almost similar, whereas computational cost of the proposed learningfree method is significantly less. Promising results has been obtained by using block based LBP histogram features of the salient patches. Extensive experiments has been carried out on two facial expression databases and the combined dataset. Experiments are conducted using various binwidths of LBP histograms, uniform LBP and rotation invariant LBP features. Low dimensional features are preferred, with sufficient recognition accuracy, to decrease computational complexity. Therefore, the 16-bin LBP histogram features are used from four salient patches at face resolution of 96x96 for obtaining best performance with a suitable trade-off between speed and accuracy. Our system found the classification between anger and sadness troublesome in all databases. The system appears to perform well in CK+ dataset with an F-score of 94.39%. Using the TABLE 5 THE SALIENT PATCHES DERIVED FROM FUSION OF CK+ AND JAFFE DATABASES salient patches obtained by training on CK+ dataset, the system achieves an F-score of 92.22% in JAFFE dataset. This proves the generic performance of the system. The performance of the proposed system is comparable with the earlier works with similar approach, nevertheless our system is fully automated.</p><p>Interestingly, the local features at the salient patches provide consistent performance at different resolutions. Thus, the proposed method can be employed in real-world applications with low resolution imaging in real-time. Security cameras, for instance, provide low resolution images which can be analysed effectively using the proposed framework.</p><p>Instead of the whole face, the proposed method classifies the emotion by assessing a few facial patches. Thus, there is a chance of improvement in performance with partially occluded images which has not been addressed in this study. This analysis is confined to databases without facial hairs. There is a possibility of improvement by using different appearance features. Dynamics of expression in temporal domain is also not considered in this study. It would be interesting to explore the system incorporated with motion features from different facial patches. The execution time reported for the proposed algorithm is based on an un-optimized MATLAB code. However, the optimal implementation of the proposed framework will significantly improve the computational cost and real-time expression recognition can be achieved with substantial accuracy. Further analysis and efforts are required to improve the performance by addressing some of the above mentioned issues. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F</head><label></label><figDesc>---------------- S L Happy and A. Routray are with the Department of Electrical Engineering, Indian Institute of Technology, Kharagpur, India. E-mail: {happy,aroutray}@iitkgp.ac.in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Framework for automated facial landmark detection and active patch extraction, (a) face detection, (b) coarse ROI selection for eyes and nose, (c) eyes and nose detection followed by coarse ROI selection for eyebrows and lips, (d) detection of corners of lip and eyebrows, (e) finding the facial landmark locations, (f) extraction of active facial patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Lip corner localization, (a) lips ROI, (b) applying horizontal Sobel edge detector, (c) applying Otsu threshold, (d) removing spurious, (e) applying morphological operations to render final connected component for lip corner localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Lip corner localization in a case where upper lip is not entirely connected, (a-c) same asFig. 3, (d-e) selection of two connected components by scanning from top, (f) localized lip corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Eyebrow corner localization, (a) rectangles showing search ROI and plus marks showing the detection result, (b &amp; f) eye ROIs, (c &amp; g) applying adaptive threshold on ROIs, (d &amp; h) applying horizontal sobel edge detector followed by Otsu threshold and morphological operations, (e &amp; i) final connected components for corner localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Position of facial patches</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The histograms of LBP image can be utilized as feature descriptors, given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Comparison between performance of the proposed landmark detection method and the DRMF based CLM model in BIOID database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>8 SL</head><label>8</label><figDesc>Happy has received the B.Tech. (Hons.) degree from Institute of Technical Education and Research (ITER), India in 2011. Now he is pursuing the M. S. degree from Indian Institute of Technology Kharagpur, India. His research interests include pattern recognition, computer vision and facial expression analysis. Aurobinda Routray has received his Masters degrees in 1991 from IIT Kanpur, India and his PhD in 1999 from Sambalpur University, India. He has also worked as a postdoctoral researcher at Purdue University, USA, during 2003-2004. He is currently working as a professor in the Department of Electrical Engineering, Indian Institute of Technology, Kharagpur. His research interests include non-linear and statistical signal processing, signal based fault detection and diagnosis, real time and embedded signal processing, numerical linear algebra, and data driven diagnostics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 THE</head><label>1</label><figDesc>CONFUSION MATRIX USING PROPOSED METHOD ON CK+ DATABASE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 PERFORMANCETABLE 4</head><label>24</label><figDesc>COMPARISON OF DIFFERENT STATE-OF-THE-ART APPROACHES ON CK+ DATABASE COMPARISON BETWEEN PERFORMANCE AND TIME COMPLEXITY OF THE PROPOSED LANDMARK DETECTION METHOD AND THE DRMF BASED CLM MODEL ON CK+ DATABASE (Recognition accuracy is obtained by using the proposed salient patch extraction based method. The execution time analysis is based on unoptimized MATLAB code in a Dual-Core Intel Pentium i5 CPU with 3.2 GHz.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pro-</cell></row><row><cell></cell><cell>[20]</cell><cell>[61]</cell><cell>[47]</cell><cell>[16]</cell><cell>[17]</cell><cell>posed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>system</cell></row><row><cell>An</cell><cell>82.5</cell><cell>87.03</cell><cell>71.39</cell><cell>90.56</cell><cell>87.1</cell><cell>87.8</cell></row><row><cell>Di</cell><cell>97.5</cell><cell>91.58</cell><cell>95.33</cell><cell>86.04</cell><cell>90.2</cell><cell>93.33</cell></row><row><cell>Fe</cell><cell>95</cell><cell>90.98</cell><cell>81.11</cell><cell>84.61</cell><cell>92</cell><cell>94.33</cell></row><row><cell>Ha</cell><cell>100</cell><cell>96.92</cell><cell>95.42</cell><cell cols="2">93.61 98.07</cell><cell>94.2</cell></row><row><cell>Sa</cell><cell>92.5</cell><cell>84.58</cell><cell>88.01</cell><cell cols="3">90.24 91.47 96.42</cell></row><row><cell>Su</cell><cell>92.5</cell><cell>91.23</cell><cell>98.27</cell><cell>92.3</cell><cell>100</cell><cell>98.46</cell></row><row><cell cols="6">Avg 93.33 90.38 88.255 89.56 93.14</cell><cell>94.09</cell></row><row><cell cols="7">An = anger, Di = disgust, Fe = fear, Ha = happiness, Sa =</cell></row><row><cell cols="7">sadness, Su = surprise, and Avg = Average recognition</cell></row><row><cell>rate.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">THE CONFUSION MATRIX USING PROPOSED METHOD ON</cell></row><row><cell></cell><cell></cell><cell cols="3">JAFFE DATABASE</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Prof. Jeffery Cohn for the use of the Cohn-Kanade database, and Dr. Michael J. Lyons for the use of the JAFFE database. The authors gratefully acknowledge the contribution of the reviewers' comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Affect detection: An interdisciplinary review of models, methods, and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="37" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Innate and Universal Facial Expressions: Evidence from Developmental and Cross-Cultural Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Izard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bull</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="288" to="299" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Salt Lake City, UT: A Human Face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-05" />
		</imprint>
	</monogr>
	<note>FACS Manual</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Emotions in Nature and Artifact</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manifold of facial expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Analysis and Modeling of Faces and Gestures</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamics of facial expression: Recognition of facial ac-tions and their temporal segments from face profile image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial action recognition for facial expression analysis from static face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1449" to="1461" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facial expression recognition from video sequences: Temporal and static modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="160" to="187" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition: feature extraction and selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lajevardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image and Video Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="159" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust facial expression recognition based on local directional pattern</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jabid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETRI Journal</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="784" to="794" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Local Directional Pattern Variance (LDPv) based Face Descriptor for Human Facial Expression Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jabid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th IEEE Int. Conf. on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="526" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing facial expressions automatically from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Braspenning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of ambient intelligence and smart environments</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="479" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust facial expression recognition using local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Discriminative LBP-Histogram Bins for Facial Expression Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gritti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image ratio features for facial expression recognition application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="779" to="788" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Part B: Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facial expression recognition using facial movement features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tjondronegoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="229" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active and dynamic information fusion for facial expression understanding from image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="699" to="714" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognizing lower face action units for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="484" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An enhanced independent component-based human facial expression recognition from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2216" to="2224" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Combined support vector machines and hidden markov models for modeling facial action temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Human-Computer Interaction</publisher>
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AAM derived face representations for robust facial action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The painful face-pain expression recognition using active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1788" to="1796" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial expression recognition and synthesis based on an appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="723" to="740" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating aam fitting methods for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Affective Computing and Intelligent Interaction and Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A real-time facial expression recognition system based on active appearance models using gray images and edge images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature Detection and Tracking with Constrained Local Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">In the pursuit of effective affective computing: The relationship between features and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1006" to="1016" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Part B: Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing facial expression: machine learning and application to spontaneous behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lainscsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic classification of single facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1357" to="1362" />
			<date type="published" when="1999-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A discriminative feature space for detecting and recognizing faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A real time facial expression classification system using Local Binary Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Happy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Routray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Int. Conf. on Intelligent Human Computer Interaction</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Emotion recognition using PHOG and LPQ features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition and Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face recognition using kernel principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="40" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A principal component analysis of facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1179" to="1208" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A new facial expression recognition method based on local gabor filter bank and pca plus lda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="86" to="96" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discriminative filter based regression learning for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1192" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A comprehensive empirical study on linear subspace methods for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="153" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Design of face recognition algorithm using PCA-LDA combined for hybrid data preprocessing and polynomial-based RBF neural networks: Design and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1451" to="1466" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition in the Encrypted Domain Based on Local Fisher Discriminant Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rahulamathavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-W</forename><forename type="middle">P J A</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Parish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="92" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Projection into expression subspaces for face recognition from single sample per person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohammadzade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hatzinakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="82" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Local binary patterns for multi-view facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="558" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient generic face model fitting to images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Unzueta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pimenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goenetxea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="321" to="334" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Realtime face detection and lip feature extraction using fieldprogrammable gate arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Halupka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheikholeslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="902" to="912" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Part B: Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Pearson Education</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multiresolution grayscale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A comparison of methods for multiclass support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Japanese Female Facial Expressions (JAFFE)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gyoba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database of digital images</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The Extended Cohn-Kande Dataset (CK+): A complete facial expression dataset for action unit and emotionspecified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd IEEE Workshop on CVPR for Human Communicative Behavior Analysis</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of face recognition</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="487" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A systematic analysis of performance measures for classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gauss-Laguerre wavelet textural feature fusion with geometrical information for facial expression identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Facial expression analysis across databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia Technology</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="317" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust face detection using the hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jesorsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frischholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<meeting><address><addrLine>Halmstad, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
