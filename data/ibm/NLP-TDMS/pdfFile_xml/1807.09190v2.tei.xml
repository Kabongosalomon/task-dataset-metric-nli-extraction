<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PReMVOS: Proposal-generation, Refinement and Merging for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
							<email>luiten@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
							<email>voigtlaender@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PReMVOS: Proposal-generation, Refinement and Merging for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address semi-supervised video object segmentation, the task of automatically generating accurate and consistent pixel masks for objects in a video sequence, given the first-frame ground truth annotations. Towards this goal, we present the PReMVOS algorithm (Proposalgeneration, Refinement and Merging for Video Object Segmentation). Our method separates this problem into two steps, first generating a set of accurate object segmentation mask proposals for each video frame and then selecting and merging these proposals into accurate and temporally consistent pixel-wise object tracks over a video sequence in a way which is designed to specifically tackle the difficult challenges involved with segmenting multiple objects across a video sequence. Our approach surpasses all previous state-of-the-art results on the DAVIS 2017 video object segmentation benchmark with a J &amp;F mean score of 71.6 on the test-dev dataset, and achieves first place in both the DAVIS 2018 Video Object Segmentation Challenge and the YouTube-VOS 1st Large-scale Video Object Segmentation Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video Object Segmentation (VOS) is the task of automatically estimating the object pixel masks in a video sequence and assigning consistent object IDs to these object masks over the video sequence. This can be seen as extension of instance segmentation from single frames to videos, and also as an extension of multi object tracking from tracking bounding boxes to tracking pixel masks. This framework motivates our work in separating the VOS problem into two subproblems. The first being the instance segmentation task of generating accurate object segmentation mask proposals for all of the objects in each frame of the video, and the second being the multi object tracking task of selecting and merging these mask proposals to generate accurate and temporally consistent pixelwise object tracks throughout a video sequence. Semi-supervised Video Object Segmentation focuses on the VOS task for certain objects for which the ground truth mask for the first video frame is given. The DAVIS datasets <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b25">[26]</ref> present a state-of-the-art testing ground for this task. In this paper we present the PReMVOS (Proposal-generation, Refinement and Merging for Video Object Segmentation) algorithm for tackling the semi-supervised VOS task. This method involves generating coarse object proposals using a Mask R-CNN like object detector, followed by a refinement network that produces accurate pixel masks for each proposal. We then select and link these proposals over time using a merging algorithm that takes into account an objectness score, the optical flow warping, a Re-ID feature embedding vector, and spatial constraints for each object proposal. We adapt our networks to the target video domain by fine-tuning on a large set of augmented images generated from the first-frame ground truth. An overview of our method, PReMVOS, can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. Our method surpasses all current state-of-the-art results on all of the DAVIS benchmarks and achieves the best results in the 2018 DAVIS Video Object Segmentation Challenge <ref type="bibr" target="#b19">[20]</ref> and the YouTube-VOS 1st Large-scale Video Object Segmentation Challenge <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Current state-of-the-art methods for VOS fall into one of two paradigms. The first is objectness estimation with domain adaptation from first-frame finetuning. This approach, first proposed in <ref type="bibr" target="#b1">[2]</ref>, uses fully convolutional networks to estimate the objectness of each pixel by fine-tuning on the first-frame ground truth. This approach was expanded upon by <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> by using semantic segmentation guidance and iterative fine-tuning, respectively. The second paradigm, used in several state-of-the-art methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, involves propagating the mask from the previous frame using optical flow and then refining these estimates using a fully convolutional network. The methods proposed in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b16">[17]</ref> expand this idea by using a network to calculate a re-identification (ReID) embedding vector for proposed masks and using this to improve the object reidentification after an object has been occluded. <ref type="bibr" target="#b14">[15]</ref> improves upon the mask propagation paradigm by training on a huge set of augmented images generated from the first-frame ground truth. Our method tackles the VOS task in an inherently different way than any of the previous papers in the literature. However, we adopt ideas presented in all of the above papers such as the use of ReID embedding vectors, optical flow proposal warping and fine-tuning on a large set of images augmented from the first frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We propose PReMVOS as a novel approach for addressing the VOS task. This approach is designed to produce more accurate and temporally consistent pixel masks across a video, especially in the challenging multi-object VOS task. Instead of predicting object masks directly on the video pixels, as done in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b29">[30]</ref>, a key idea of our approach is to instead detect regions of interest as coarse object proposals using an object detection network, and to then predict accurate masks only on the cropped and resized bounding boxes. We also present a new proposal merging algorithm in order to predict more temporally consistent pixel masks. The methods presented in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> create temporally consistent proposals by generating their proposals directly from the previous frame's proposals warped using optical flow into the current frame. Instead, our method generates proposals independently for each frame and then selects and links these proposals using a number of cues such as optical flow based proposal warping, ReID embeddings and objectness scores, as well as taking into account the presence of other objects in the multi-object VOS scenario. This novel paradigm for solving the VOS task allows us to predict both more accurate and more temporally consistent pixel masks than all previous methods and achieves state-of-the-art results across all datasets. <ref type="figure">Figure 2</ref> shows an overview of each of the components of the PReMVOS algorithm and how these work together to solve the VOS task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Augmentation</head><p>For each video we generate a set of 2500 augmented images using the first-frame ground truth. We use the Lucid Data Dreaming method proposed in <ref type="bibr" target="#b14">[15]</ref> but only generate single images (not image pairs). This method removes the objects, automatically fills in the background, and then randomly transforms each object and the background before randomly reassembling the objects in the scene. Finetuning on this set of augmented images allows us to adapt our networks directly to the target video domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposal Generation</head><p>We generate coarse object proposals using a Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> network implementation by <ref type="bibr" target="#b30">[31]</ref> with a ResNet101 <ref type="bibr" target="#b9">[10]</ref> backbone. We adjust this network to be category agnostic by replacing the N classes with just one class by mapping all classes to a single foreground class for detecting generic objects. We train this network starting from pre-trained ImageNet <ref type="bibr" target="#b5">[6]</ref> weights on both the COCO <ref type="bibr" target="#b18">[19]</ref> and Mapillary <ref type="bibr" target="#b21">[22]</ref> datasets jointly. We then fine-tune a separate version of this network for each video for three epochs of the 2500 augmented images. This network generates coarse mask proposals, bounding boxes, and objectness scores for each image in the video sequence. We extract proposals with a score greater than 0.05 and also perform non-maximum suppression removing proposals which have an IoU of 66% or greater with a proposal with a higher score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposal Refinement</head><p>The Proposal-Refinement Network is a fully convolutional network inspired by <ref type="bibr" target="#b33">[34]</ref> and based on the DeepLabv3+ <ref type="bibr" target="#b3">[4]</ref> architecture. This network takes as input a 385 × 385 image patch that has been cropped and resized from an approximate bounding box around an object of interest. A 50 pixel (in the original image) margin is first added to the bounding box in all directions. We add a fourth channel to the input image which encodes the original bounding box as a pixel mask to the input image. Starting from weights pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref>, COCO <ref type="bibr" target="#b18">[19]</ref>, and PASCAL <ref type="bibr" target="#b6">[7]</ref>, we train this network on the Mapillary <ref type="bibr" target="#b21">[22]</ref> dataset using random flipping, random gamma augmentations and random bounding box jitter <ref type="bibr" target="#b33">[34]</ref> up to 5% in each dimension, to produce an accurate object segmentation, given an object's bounding box. We then fine-tune a separate version of this network for five epochs for each video on the 2500 augmented images. We then use this network to generate accurate pixel mask proposals for each of the previously generated coarse proposals, by only taking the bounding box of these proposals as input into the Refinement network and discarding the coarse mask itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mask Propagation using Optical Flow</head><p>As part of our proposal merging algorithm we use the optical flow between successive image pairs to warp a proposed mask into the next frame, to calculate the temporal consistency between two mask proposals. We calculate the Optical Flow using FlowNet 2.0 <ref type="bibr" target="#b11">[12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">ReID Embedding Vectors</head><p>We further use a triplet-loss based ReID embedding network to calculate a ReID embedding vector for each mask proposal. We use the feature embedding network proposed in <ref type="bibr" target="#b23">[24]</ref>. This is based on a wide ResNet variant <ref type="bibr" target="#b31">[32]</ref> pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref> and then trained on the COCO dataset <ref type="bibr" target="#b18">[19]</ref> using cropped bounding boxes resized to 128 × 128 pixels. This uses a triplet loss to learn an embedding space in which crops of different classes are separated and crops of the same class are grouped together. It is trained using the batch-hard loss with a softplus margin proposed in <ref type="bibr" target="#b10">[11]</ref>. We then fine-tune this network using the crops of each object from the generated 2500 augmented images for each of the 90 video sequences (242 objects) in the DAVIS 2017 val, test-dev and test-challenge datasets combined in order to have both enough positive and negative examples to train a network with a triplet-based loss. This network generates a ReID vector which differentiates all of the objects in these datasets from one other, which is used to compare the visual similarity of our generated object proposals and the first-frame ground truth object masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Proposal Merging</head><p>Our proposal merging algorithm works in a greedy manner. Starting from the ground truth object masks in the first-frame, it builds tracks for each frame by scoring each of the proposals on their likeliness to belong to a particular object track. We have exactly one track for each ground truth object and we make hard decisions for which proposals to add to each track each timestep in a greedy manner. The proposal with the highest track score is added to each track. This track score is calculated as an affine combination of five separate sub-scores, each with values between 0 and 1. In the following, taking the complement of a score means subtracting it from 1.</p><p>The first sub-score is the Objectness score. The objectness score s obj,t,i,j for the j-th track of the i-th proposal c t,i at time t is given by</p><formula xml:id="formula_0">s obj,t,i,j (c t,i ) = M askObj(c t,i ),<label>(1)</label></formula><p>where M askObj(·) denotes the confidence value provided by the Proposal Generation network.</p><p>The second score is a ReID score, calculated using the Euclidean distance between the first-frame ground truth ReID embedding vector r(f j ) and the ReID embedding vector r(c t,i ) of the current mask proposal, where r(·) denotes applying the ReID network, · denotes the L2 norm, and f j is the bounding box of the j-th ground truth object in the first frame. This distance is then normalized by dividing it by the maximum distance for all proposals in a video from the ground truth embedding vector of interest. The complement is then taken to convert from a distance into a similarity score.</p><formula xml:id="formula_1">s reid,t,i,j (c t,i , f j ) = 1 − r(c t,i ) − r(f j ) maxt ,ĩ r(ct ,ĩ ) − r(f j )<label>(2)</label></formula><p>The third score is a Mask Propagation score. This is calculated for each possible object track as the IoU between the current mask proposal and the warped proposal that was decided for in the previous time-step for this object track, warped into the current time-step using the optical flow:</p><formula xml:id="formula_2">s maskprop,t,i,j (c t,i , p t−1,j ) = IoU (c t,i , warp(p t−1,j )),<label>(3)</label></formula><p>where p t−1,j is is the previously selected proposal for timestep t − 1 for object track j and warp(·) applies optical flow mask warping from frame t − 1 to t. The fourth score is an Inverse ReID score. This is calculated as the complement of the maximum ReID score for the current mask proposal and all other object tracks k except the object track of interest j:</p><formula xml:id="formula_3">s inv reid,t,i,j = 1 − max k =j (s reid,t,i,k ).<label>(4)</label></formula><p>The fifth score is an Inverse Mask Propagation score. This is calculated as the complement of the maximum Mask Propagation IoU score for the current mask proposal and all other object tracks k except the object track of interest j:</p><formula xml:id="formula_4">s inv maskprop,t,i,j = 1 − max k =j (s maskprop,t,i,k ).<label>(5)</label></formula><p>All five scores are combined together by</p><formula xml:id="formula_5">s comb,t,i,j = q∈{objectness,reid,maskprop,inv reid,inv maskprop} α q s q,t,i,j ,<label>(6)</label></formula><p>where q α q = 1 and all α q ≥ 0. The greedy decisions are then made by p t,j = c t,kj , where</p><formula xml:id="formula_6">k j = argmax i s comb,t,i,j .<label>(7)</label></formula><p>In cases where the selected proposals for the different objects within one time-step overlap, we assign the overlapping pixels to the proposal with the highest combined track score. We present results with both an equal weighting for each of the five sub-score components and where the weights are tuned using random-search hyper-parameter optimisation evaluated against the DAVIS 2017 val set. The values of these optimised weights are shown in <ref type="table" target="#tab_4">Table 4</ref>. We ran this optimisation for 25000 random parameter values. For the results on the 2018 DAVIS Challenge we also present results using an ensemble of the results using the top 11 sets of parameter values, using a simple pixel-wise majority vote to ensemble the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our algorithm on the set of DAVIS <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b25">[26]</ref> datasets and benchmarks. <ref type="table" target="#tab_1">Table 1</ref> shows our results on the three DAVIS benchmarks. The DAVIS 2017 test-dev and val datasets contain multiple objects per video sequence, whereas the DAVIS 2016 val dataset contains a single object per sequence. The metrics of interest are the J score, calculated as the average IoU between the proposed masks and the ground truth mask, and the F score, calculated as an average boundary similarity measure between the boundary of the proposed masks and the ground truth masks. For more details on these metrics see <ref type="bibr" target="#b25">[26]</ref>.</p><p>On all of the datasets our method gives results better than all other state-ofthe-art methods for both the F metric and the mean of the J and F score. We also produce either the best, or comparable to the best, results on the J metric for each dataset. These results show that the novel proposed VOS paradigm performs better than the current VOS paradigms in predicting both accurate and temporally consistent mask proposals. <ref type="table">Table 2</ref> shows our results both with and without ensembling on the DAVIS 2017/2018 test-challenge dataset evaluated during the 2018 DAVIS Challenge compared to the top six other competing methods. Our method gives the best results and gets first place in the 2018 DAVIS Challenge. <ref type="figure" target="#fig_1">Figure 3</ref> shows qualitative results of our method on four video sequences from the 2017 val dataset. These results show that our method produces both accurate and temporally consistent results across the video sequences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Proposal Refinement</head><p>We perform an ablation study to investigate the effect of the Refinement network on the accuracy of the mask proposals. We compare the coarse proposals generated from a state-of-the-art instance segmentation method, Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>, to the refined proposals generated by feeding the bounding boxes of these coarse proposals into our Refinement Network. In order to evaluate these results we calculate the best proposal for each object in each frame which maximizes the IoU score with the ground-truth mask. We then evaluate using the standard DAVIS evaluation metrics using these merged proposals. This oracle merging algorithm allows us to evaluate the accuracy of our proposal generation separately from our merging algorithm.  <ref type="table">Table 3</ref> shows the quantitative IoU (J ) and boundary measure (F) improvement for our refined proposals over the Mask R-CNN proposals. Our method results in a 5.9% improvement in IoU and a 7.9% improvement in boundary measure over just using the Mask R-CNN proposals. <ref type="figure">Figure 4</ref> also visualizes the qualitative results of the improved accuracy of the refined masks over the generated Mask R-CNN masks. In all examples the refined proposals match the ground truth masks more closely than the coarse proposals and capture the boundary contours at a much higher fidelity. This is due to the refinement network extracting deep features only over the area of interest for each object, and not over the whole image as is done in the case for Mask R-CNN. The Refinement Network is also able to recover parts of objects that were completely lost in the coarse proposals, for example in the two examples of bicycles shown. This is because the Refinement Network takes as input only the bounding box of the coarse mask proposal, not the coarse mask itself, and relies only on it's trained knowledge of what is inherently an object in order to generate the refined segmentation masks. Also when the bounding box does not accurately cover the whole object, as is the case with the bicycle in the second column of <ref type="figure">Figure 4</ref>, the refinement network is still able to recover the accurate mask as it takes into account a 50 pixel margin around the coarse mask proposal bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Proposal Merging</head><p>We perform a further ablation study to investigate the effect of each of the merging algorithm sub-score components on the accuracy of the merging algorithm. <ref type="table" target="#tab_4">Table 4</ref> shows the results of this ablation study on the DAVIS 2017 val dataset.</p><p>We first present an upper bound baseline result for our merging algorithm. This is calculated by choosing the best proposal for each object in each frame which maximizes the IoU score with the ground-truth mask. This oracle merging method gives a 81.2 J &amp;F mean score.</p><p>We then present the results of our merging algorithm where the weights for each of the five sub-score components were optimised using random-search hyper-parameter optimisation evaluated against the DAVIS 2017 val set. This optimisation was done by evaluating 25000 random affine combinations of the 5 component weights and selecting the set of weights that resulted in the best IoU score. This result gives another upper bound of 78.2 J &amp;F mean score, as this was evaluated on the same dataset as the weight values were optimised on. Our greedy selection algorithm based on a combination of the 5 sub-component scores is able to reach a J &amp;F mean score that is only 3% lower than the hypothetical maximum, showing that these carefully selected 5 sub-score components are sufficient to generate accurate and consistent object tracks, even in difficult cases such as multiple similar objects and large occlusions. These opimised weights used on the 2017 test-dev dataset presented in <ref type="table" target="#tab_1">Table 1</ref>, all other dataset results in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_4">Table 4</ref> use equal weights for the five sub-components.</p><p>The naive combination with all 5 sub-score components has a J &amp;F mean score of 77.8 which is only 0.4 below that with optimised weights. This indicates that the merging algorithm is relatively robust to the exact weights and that what is more important is the presence of all five components.</p><p>The Objectness sub-score separates well-defined mask proposals with accurate boundary contours from proposals with boundaries that are less likely to  model a consistent object. It is also able to distinguish objects of interest given in the first frame from other objects in the scene, as this score was trained in the Mask R-CNN fine-tuning process to identify these objects and ignore the others. However, this sub-score component is unable to distinguish between different objects of interest if more than one is present in a video sequence. The ReID sub-score is used to distinguish between objects that look visually different from each other. This works well to separate objects such as bikes and people from each other in the same video sequence, but does not work as well on sequences with multiple similar looking objects.</p><formula xml:id="formula_7">75.0 - - 74.2 - - 73.5 - - 69.6 - - 71.1 - - 75.8 - - 69.3 - - 75.9 - - 74.3 2 - - - 72.7 - - - 64.7 - - - 69.1 - - - 57.9 - - - 68.7 - - - 74.3 - - - 68.8 - - - 74.0 - - - 47.3 - - - 73.6 1 - - - - 29.5 - - - - 67.4 - - - - 44.3 - - - - 72.8 - - - - 34.4</formula><p>The MaskProp sub-score is used for temporal consistency. This can distinguish well between very similar objects if they are separated in the spatial domain of the image. However, this sub-score cannot deal with cases where objects heavily occlude each other, or completely disappear before later returning.</p><p>The InvReID and InvMaskProp sub-scores are used to force the selected mask proposals in each frame to be as distinguishable from each other as possible. Just using the other 3 components often results in failure cases where the same or very similar mask proposals are chosen for different objects. This occurs when similar looking objects overlap or when one object disappears. These two subcomponents work by distinguishing proposals that are visually and temporally inconsistent with other objects in the video sequence, resulting in a signal of consistency with the object of interest. These components can separate well between the different objects of interest in a video sequence, but they are unable to separate these objects from possible background objects.</p><p>The results in <ref type="table" target="#tab_4">Table 4</ref> show that all five sub-scores are important for accurate proposal merging, as removing one of these components results in a loss of accuracy between 0.9, when only removing the InvReID sub-score, to 2.3, when removing the MaskProp sub-score. Without both the InvReID and InvMaskProp, the J &amp;F mean score decreases by 1.9 points, showing that these sub-scores that were introduced to promote spatial separation of the chosen proposals are an integral part of the merging algorithm. Only using these two sub-components, however, results in a score decrease of 30.5 points. Removing the two main key components, the ReID and MaskProp sub-scores, results in a loss of 8.2 points, whereas only having these two components results in a loss of 3.5 points. When used by itself, the MaskProp sub-score is the strongest component, resulting in a loss of 5.0 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Runtime Evaluation</head><p>We perform a runtime evaluation of the different components of the PReMVOS algorithm. The complete PReMVOS algorithm with first-frame data augmentation and fine-tuning was designed in order to produce the most accurate results without regards for speed requirements. For further evaluation, we present in Table 5 three versions of PReMVOS. The original version, a fast-finetuned version, and a version without any fine-tuning. The fast-finetuned version is fine-tuned for one third of the iterations as the original method, and instead of the slow Lucid Data Dreaming <ref type="bibr" target="#b14">[15]</ref>   flipping and brightness augmentations. Only one set of weights are fine-tuned over the whole DAVIS val set rather than different weights for each video. A combination of the specific proposals from the fine-tuned proposal network and the general proposals from a proposal network that was not fine-tuned on the validation set first-frames are used. The not-finetuned version of PReMVOS just uses the general proposals without any fine-tuning. <ref type="figure" target="#fig_2">Figure 5</ref> compares the quality and runtime of PReMVOS against other methods in the literature. Across all of the presented runtime scales, our method compares to or exceeds all other state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Further Large-scale Evaluation</head><p>In our results on the test set obtained 1st place in the 1st Large-scale Video Object Segmentation Challenge. We don't run the original PReMVOS method but the PReMVOS Fast-finetuned version (see Section 4.3), which can be evaluated on this larger dataset in a more reasonable amount of time. Our results are much better than <ref type="bibr" target="#b32">[33]</ref>, the only other method that has published results on this dataset. It is also better than all other methods that submitted results to the 2017 1st Large-scale Video Object Segmentation Challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we have presented a new approach for solving the video object segmentation task. Our proposed approach works by dividing this task into first generating a set of accurate object segmentation mask proposals for each video frame and then selecting and merging these proposals into accurate and temporally consistent pixel-wise object tracks over a video sequence. We have developed a novel approach for each of these sub-problems and have combined these into the PReMVOS (Proposal-generation, Refinement and Merging for Video Object Segmentation) algorithm. We show that this method is particularly well suited for the difficult multi-object video object segmentation task and that it produces results better than all current state-of-the-art results for semi-supervised video object segmentation on the DAVIS benchmarks, as well as getting the best score in the 2018 DAVIS Challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>PReMVOS overview. Overlay colours represent different object proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative results of PReMVOS on the DAVIS 2017 val dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Quality versus timing on the DAVIS 2017 val set. For methods that only publish runtime results on the DAVIS 2016 dataset, we take these timings as per object timings and extrapolate to the number of objects in the DAVIS 2017 val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Ours</cell><cell cols="7">DyeNet MRF Lucid ReID OSVOS-S OnAVOS OSVOS [17] [1] [15] [18] [21] [29][30] [2]</cell></row><row><cell></cell><cell cols="2">J &amp;F Mean 71.6</cell><cell>68.2</cell><cell>67.5</cell><cell>66.6</cell><cell>66.1</cell><cell>57.5</cell><cell>56.5</cell><cell>50.9</cell></row><row><cell></cell><cell></cell><cell>Mean 67.5</cell><cell>65.8</cell><cell>64.5</cell><cell>63.4</cell><cell>64.4</cell><cell>52.9</cell><cell>52.4</cell><cell>47.0</cell></row><row><cell>17</cell><cell>J</cell><cell>Recall 76.8</cell><cell>-</cell><cell>-</cell><cell>73.9</cell><cell>-</cell><cell>60.2</cell><cell>-</cell><cell>52.1</cell></row><row><cell>T-D</cell><cell></cell><cell>Decay 21.7</cell><cell>-</cell><cell>-</cell><cell>19.5</cell><cell>-</cell><cell>24.1</cell><cell>-</cell><cell>19.2</cell></row><row><cell></cell><cell></cell><cell>Mean 75.7</cell><cell>70.5</cell><cell>70.5</cell><cell>69.9</cell><cell>67.8</cell><cell>62.1</cell><cell>59.6</cell><cell>54.8</cell></row><row><cell></cell><cell>F</cell><cell>Recall 84.3</cell><cell>-</cell><cell>-</cell><cell>80.1</cell><cell>-</cell><cell>70.5</cell><cell>-</cell><cell>59.7</cell></row><row><cell></cell><cell></cell><cell>Decay 20.6</cell><cell>-</cell><cell>-</cell><cell>19.4</cell><cell>-</cell><cell>21.9</cell><cell>-</cell><cell>19.8</cell></row><row><cell></cell><cell cols="2">J &amp;F Mean 77.8</cell><cell>74.1</cell><cell>70.7</cell><cell>-</cell><cell>-</cell><cell>68.0</cell><cell>67.9</cell><cell>60.3</cell></row><row><cell></cell><cell></cell><cell>Mean 73.9</cell><cell>-</cell><cell>67.2</cell><cell>-</cell><cell>-</cell><cell>64.7</cell><cell>64.5</cell><cell>56.6</cell></row><row><cell>17</cell><cell>J</cell><cell>Recall 83.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.2</cell><cell>-</cell><cell>63.8</cell></row><row><cell>Val</cell><cell></cell><cell>Decay 16.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.1</cell><cell>-</cell><cell>26.1</cell></row><row><cell></cell><cell></cell><cell>Mean 81.7</cell><cell>-</cell><cell>74.2</cell><cell>-</cell><cell>-</cell><cell>71.3</cell><cell>71.2</cell><cell>63.9</cell></row><row><cell></cell><cell>F</cell><cell>Recall 88.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.7</cell><cell>-</cell><cell>73.8</cell></row><row><cell></cell><cell></cell><cell>Decay 19.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>18.5</cell><cell>-</cell><cell>27.0</cell></row><row><cell></cell><cell cols="2">J &amp;F Mean 86.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.5</cell><cell>85.5</cell><cell>80.2</cell></row><row><cell></cell><cell></cell><cell>Mean 84.9</cell><cell>86.2</cell><cell>84.2</cell><cell>-</cell><cell>-</cell><cell>85.6</cell><cell>86.1</cell><cell>79.8</cell></row><row><cell>16 Val</cell><cell>J</cell><cell>Recall 96.1 Decay 8.8</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>96.8 5.5</cell><cell>96.1 5.2</cell><cell>93.6 14.9</cell></row><row><cell></cell><cell></cell><cell>Mean 88.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.5</cell><cell>84.9</cell><cell>80.6</cell></row><row><cell></cell><cell>F</cell><cell>Recall 94.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.9</cell><cell>89.7</cell><cell>92.6</cell></row><row><cell></cell><cell></cell><cell>Decay 9.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.2</cell><cell>5.8</cell><cell>15.0</cell></row><row><cell></cell><cell>T</cell><cell>Mean 36.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>21.7</cell><cell>19.0</cell><cell>37.8</cell></row></table><note>Our results and other state-of-the-art results on the three DAVIS datasets: the 2017 test-dev set (17 T-D), the 2017 val set (17 Val), and the 2016 val set (16 Val). On the 17 Val and 16 Val datasets we use the naive merging component weights, whereas on the 17 T-D dataset we use the weights optimised using the 17 Val set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Our results (with and without ensembling) on the DAVIS test-challenge dataset compared with the top five other competitors in the 2018 DAVIS Challenge. Quantitative results of an ablation study on the 2017 val dataset showing the effect of the Refinement Network on the accuracy of generated mask proposals. Presented results are calculated using oracle merging (see Section 4.1).Fig. 4. Qualitative results showing the effect of the Refinement Network on the mask proposal accuracy. Results are calculated using oracle merging (Section 4.1).</figDesc><table><row><cell></cell><cell cols="2">Ours Ours (Ens)</cell><cell cols="5">DyeNet ClassAgno. OnlineGen. Lucid ContextBased [16] VOS [35] VOS [8] [14] VOS [28]</cell></row><row><cell cols="3">J &amp;F Mean 74.7 71.8</cell><cell>73.8</cell><cell>69.7</cell><cell>69.5</cell><cell>67.8</cell><cell>66.3</cell></row><row><cell></cell><cell>Mean 71.0</cell><cell cols="2">67.9 71.9</cell><cell>66.9</cell><cell>67.5</cell><cell>65.1</cell><cell>64.1</cell></row><row><cell>J</cell><cell cols="2">Recall 79.5 75.9</cell><cell>79.4</cell><cell>74.1</cell><cell>77.0</cell><cell>72.5</cell><cell>75.0</cell></row><row><cell></cell><cell>Decay 19.0</cell><cell>23.2</cell><cell>19.8</cell><cell>23.1</cell><cell>15.0</cell><cell>27.7</cell><cell>11.7</cell></row><row><cell></cell><cell cols="2">Mean 78.4 75.6</cell><cell>75.8</cell><cell>72.5</cell><cell>71.5</cell><cell>70.6</cell><cell>68.6</cell></row><row><cell>F</cell><cell cols="2">Recall 86.7 82.9</cell><cell>83.0</cell><cell>80.3</cell><cell>82.2</cell><cell>79.8</cell><cell>80.7</cell></row><row><cell></cell><cell>Decay 20.8</cell><cell>24.7</cell><cell>20.3</cell><cell>25.9</cell><cell>18.5</cell><cell>30.2</cell><cell>13.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>J mean</cell><cell>F mean</cell><cell></cell><cell>J &amp;F mean</cell></row><row><cell cols="2">Without Refinement</cell><cell></cell><cell>71.2</cell><cell>77.3</cell><cell></cell><cell>74.2</cell></row><row><cell cols="2">With Refinement</cell><cell></cell><cell>77.1</cell><cell>85.2</cell><cell></cell><cell>81.2</cell></row><row><cell></cell><cell>Boost</cell><cell></cell><cell>5.9</cell><cell>7.9</cell><cell></cell><cell>7.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results of an ablation study on the DAVIS 2017 val dataset showing the effect of each of the merging algorithm sub-score components on the accuracy of the merging algorithm. The oracle merging result indicates an upper bound for the merging algorithm performance (Section 4.1). The components given as percentages indicate the optimal component weights calculated using hyper-parameter optimisation on the 2017 val set. The components given by a checkmark ( ) have equal weights for each checked components. For each group of results with the same number of components the best result is expressed in bold and the worst in italics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>image augmentations, it uses simple rotations, translations, Runtime analysis of the different components of the PReMVOS algorithm. Times are in seconds per frame, averaged over the DAVIS 2017 val set. Augmentation Generation is run on 48 CPU cores, and Fine-tuning is done on 8 GPUs. Otherwise, everything is run sequentially on one GPU / CPU core.</figDesc><table><row><cell></cell><cell></cell><cell>Augm. Gen.</cell><cell>Fine-tuning</cell><cell>Prop. Gen.</cell><cell>Prop. Refine.</cell><cell>ReID</cell><cell>Optic. Flow</cell><cell>Warping</cell><cell>Merging</cell><cell>Total</cell><cell>Av. # Prop.</cell><cell>Mean J &amp;F</cell></row><row><cell></cell><cell>Original</cell><cell cols="10">23.4 12.3 0.41 1.04 0.05 0.14 0.32 0.02 37.4 17.52 77.8</cell></row><row><cell cols="12">Fast-finetuned 0.02 3.9 0.26 0.45 0.03 0.14 0.20 0.02 5.02 9.28 73.7</cell></row><row><cell cols="12">Not-finetuned 0.00 0.00 0.14 0.33 0.02 0.14 0.16 0.02 0.81 6.87 65.7</cell></row><row><cell>Region and contour quality (J &amp; F)</cell><cell>.1 50 60 70 80</cell><cell></cell><cell cols="4">1 Time per frame (seconds) 10</cell><cell></cell><cell></cell><cell>100</cell><cell cols="2">Original (Ours) Fast-finetuned (Ours) Not-finetuned (Ours) OSVOS [2] OnAVOS [30] OSVOS-S [21] DyeNet [17] RGMP [23] OSMN [36] FAVOS [5]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 Table 6 .</head><label>66</label><figDesc>we present the results of PReMVOS on the new YouTube-VOS dataset<ref type="bibr" target="#b32">[33]</ref>, the largest VOS dataset to date (508 test video compared to 30 in DAVIS), Results on the YouTube-VOS dataset, using the PReMVOS Fast-finetuned version of PReMVOS. These results obtained 1st place in the the 1st Large-scale Video Object Segmentation Challenge. '2nd','3rd','4th' refers to the other competitors results in this challenge with that ranking. Bold results are the best results for that metric.</figDesc><table><row><cell></cell><cell>Overall</cell><cell>J seen</cell><cell>J unseen</cell><cell>F seen</cell><cell>F unseen</cell></row><row><cell>Ours</cell><cell>72.2</cell><cell>73.7</cell><cell>64.8</cell><cell>77.8</cell><cell>72.5</cell></row><row><cell>Seq2Seq [33]</cell><cell>70.0</cell><cell>66.9</cell><cell>66.8</cell><cell>74.1</cell><cell>72.3</cell></row><row><cell>2nd</cell><cell>72.0</cell><cell>72.5</cell><cell>66.3</cell><cell>75.2</cell><cell>74.1</cell></row><row><cell>3rd</cell><cell>69.9</cell><cell>73.6</cell><cell>62.1</cell><cell>75.5</cell><cell>68.4</cell></row><row><cell>4th</cell><cell>68.4</cell><cell>70.6</cell><cell>62.3</cell><cell>72.8</cell><cell>67.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This project was funded, in parts, by ERC Consolidator Grant DeeViSe (ERC-2017-COG-773161).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CNN in MRF: Video object segmentation via inference in a CNN-based higher-order spatio-temporal MRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09453</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive video object segmentation with online data generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2018 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposal-generation, Refinement and Merging for the YouTube-VOS Challenge on Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Large-scale Video Object Segmentation Challenge -ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2018 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2018 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video object segmentation with re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for the davis challenge on video object segmentation 2018</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2018 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">Video object segmentation without temporal information. PAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Large-scale object discovery and detector adaptation from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08832</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sorkine-Hornung: Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Context-based instance segmentation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ton-That</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2018 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for the 2017 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/tensorpack/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00461</idno>
		<title level="m">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep grabcut for object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Class-agnostic video object segmentation without semantic re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2018 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
