<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
							<email>jielei@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab Seattle</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab Seattle</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
							<email>tlberg@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discoursebased coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In video captioning, the task is to generate a natural language description capturing the content of a video. Recently, dense video captioning <ref type="bibr" target="#b15">(Krishna et al., 2017)</ref> has emerged as an important task in this field, where systems first generate a list of temporal event segments from a video, then decode a coherent paragraph (multi-sentence) description from the generated segments. <ref type="bibr" target="#b17">Park et al. (2019)</ref> simplifies this task as generating a coherent paragraph from a provided list of segments, removing the requirements for generating the event segments, and focusing on decoding better paragraph captions from the segments. As noted by ; <ref type="bibr" target="#b17">Park et al. (2019)</ref>, generating paragraph descriptions for videos can be very challenging due to the difficulties of having relevant, less redundant, as well as coherent generated sentences.</p><p>Towards this goal,  proposed a variant of the LSTM network <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997</ref>) that generates a new sentence conditioned on previously generated sentences by passing the LSTM hidden states throughout the entire decoding process. <ref type="bibr" target="#b17">Park et al. (2019)</ref> further augmented the above LSTM caption generator with a set of three discriminators that score generated sentences based on defined metrics, i.e., relevance, linguistic diversity, and inter-sentence coherence. Though different, both these methods use LSTMs as the language decoder.</p><p>Recently, transformers <ref type="bibr" target="#b26">(Vaswani et al., 2017</ref>) have proven to be more effective than RNNs (e.g., LSTM <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref>, GRU <ref type="bibr" target="#b4">(Chung et al., 2014)</ref>, etc.), demonstrating superior performance in many sequential modeling tasks <ref type="bibr" target="#b26">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b37">Zhou et al., 2018;</ref><ref type="bibr" target="#b7">Devlin et al., 2019;</ref>. <ref type="bibr" target="#b37">Zhou et al. (2018)</ref> first introduced the transformer model to the video paragraph captioning task, with a transformer captioning module decoding natural language sentences from encoded video segment representations. This transformer captioning model is essentially the same as the original transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> for machine translation, except that it takes a video representation rather than a source sentence representation as its encoder input. However, in such design, each video segment caption is decoded individually without knowing the context (i.e., previous video segments and the captions that have already been generated), thus often leading to inconsistent and redundant sentences w.r.t. previously generated sentences (see <ref type="figure" target="#fig_1">Figure 3</ref> for examples).  recognize this problem as context fragmentation in the task of language modeling, where the transformers are operating on separated fixed-length segments, without any information flow across segments. Therefore, to generate more coherent video paragraphs, it is imperative to build a model that can span over multiple video segments and capture longer range dependencies.</p><p>Hence, in this work, we propose the Memory-Augmented Recurrent Transformer (MART) model (see Section 3 for details), a transformer-based model that uses a shared encoder-decoder architecture augmented with an external memory module to enable the modeling of the previous history of video segments and sentences. Compared to the vanilla transformer video paragraph captioning model <ref type="bibr" target="#b37">(Zhou et al., 2018)</ref>, our first architecture change is the unified encoder-decoder design, i.e., the encoder and decoder in MART use shared transformer layers rather than separated as in <ref type="bibr" target="#b37">Zhou et al. (2018)</ref>; <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref>. This unified encoderdecoder design is inspired by recent transformer language models <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref><ref type="bibr" target="#b24">Sun et al., 2019)</ref> to prevent overfitting and reduce memory usage. Additionally, the memory module works as a memory updater that updates its memory state using both the current inputs and previous memory state. The memory state can be interpreted as a container of the highly summarized video segments and caption history information. At the encoding stage, the current video segment representation is enhanced with the memory state from the previous step using cross-attention <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>. Hence, when generating a new sentence, MART is aware of the previous contextual information and can generate paragraph captions with higher coherence and lower repetition.</p><p>Transformer-XL  is a recently proposed transformer language model that also uses recurrence, and is able to resolve context fragmentation for language modeling . Different from MART that uses a highly-summarized memory to remember history information, Transformer-XL directly uses hidden states from previous segments. We modify the Transformer-XL framework for video paragraph captioning and present it as an additional comparison. We benchmark MART on two standard datasets: ActivityNet Captions <ref type="bibr" target="#b15">(Krishna et al., 2017)</ref> and YouCookII <ref type="bibr" target="#b36">(Zhou et al., 2017)</ref>. Both automatic evaluation and human evaluation show that MART generates more satisfying results than previous LSTM-based approaches <ref type="bibr" target="#b35">Zhou et al., 2019;</ref><ref type="bibr" target="#b34">Zhang et al., 2018)</ref> and transformer-based approaches <ref type="bibr" target="#b37">(Zhou et al., 2018;</ref>. In particular, MART can generate more coherent (e.g., coreference and order), less redundant paragraphs without losing paragraph accuracy (visual relevance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence <ref type="bibr" target="#b28">(Wang et al., 2019;</ref><ref type="bibr" target="#b32">Xu et al., 2016;</ref><ref type="bibr" target="#b2">Chen and Dolan, 2011;</ref><ref type="bibr" target="#b19">Pasunuru and Bansal, 2017a)</ref> or multiple sentences <ref type="bibr" target="#b22">(Rohrbach et al., 2014;</ref><ref type="bibr" target="#b15">Krishna et al., 2017;</ref><ref type="bibr" target="#b37">Zhou et al., 2018;</ref><ref type="bibr" target="#b8">Gella et al., 2018;</ref><ref type="bibr" target="#b17">Park et al., 2019)</ref> are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments.</p><p>One line of work <ref type="bibr" target="#b37">(Zhou et al., 2018</ref><ref type="bibr" target="#b35">(Zhou et al., , 2019</ref> addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the generated single sentence descriptions. Though individual sentences may precisely describe the corresponding event segments, when put together the sentences often become inconsistent and redundant. Another line of works <ref type="bibr" target="#b8">Gella et al., 2018)</ref> use the LSTM decoder's last (word) hidden state from the previous sentence as the initial hidden state for the next sentence decoding, thus enabling information flow from previous sentences to subsequent sentences. While these methods have shown better performance than their single sentence counterpart, they are still undesirable as the sentence-level recurrence is achieved at word-level, and the context history information quickly decays due to vanishing gradients <ref type="bibr" target="#b18">(Pascanu et al., 2013)</ref> problem. Additionally, these designs also have difficulty modeling long-term dependencies <ref type="bibr" target="#b11">(Hochreiter et al., 2001)</ref>. In comparison, the recurrence in MART resides in the sentence or segment level and is thus more robust to the aforementioned problems. AdvInf <ref type="bibr" target="#b17">(Park et al., 2019)</ref> augments the above LSTM word-level recurrence methods with adversarial inference, using a set of separately trained discriminators to re-rank the generated sentences. The techniques in AdvInf can be viewed as an orthogonal way of generating captions with better quality.</p><p>Transformers Transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> is used as the basis of our approach. Different from RNNs (e.g., LSTM <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref>, GRU <ref type="bibr" target="#b4">(Chung et al., 2014)</ref>, etc) that use recurrent structure to model long-term dependencies, transformer relies on self-attention to learn the dependencies between input words. Transformers have proven to be more efficient and powerful than RNNs, with superior performance in many sequential modeling tasks, including machine translation <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>, language modeling/pre-training <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref> and multi-modal representation learning <ref type="bibr" target="#b25">(Tan and Bansal, 2019;</ref><ref type="bibr" target="#b24">Chen et al., 2019;</ref><ref type="bibr" target="#b24">Sun et al., 2019)</ref>. Additionally, <ref type="bibr" target="#b37">Zhou et al. (2018)</ref> have shown that a transformer model can generate better captions than the LSTM model.</p><p>However, transformer architectures are still unable to model history information well. This problem is identified in the task of language modeling as context fragmentation , i.e., each language segment is modeled individually without knowing its surrounding context, leading to inefficient optimization and inferior performance. To resolve this issue, Transformer-XL  introduces the idea of recurrence to the transformer language model. Specifically, the modeling of a new language segment in Transformer-XL is conditioned on hidden states from previous language segments. Experimental results show Transformer-XL has stronger language modeling capability than the non-recurrent transformer. Transformer-XL directly uses all the hidden states from the previous segment to enable recurrence. In comparison, our MART uses highly summarized memory states, making it more efficient in passing useful semantic or linguistic cues to future sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Though our method provides a general temporal multi-modal learning framework, we focus on the video paragraph captioning task in this paper. Given a video V , with several temporally ordered event segments [e 1 , e 2 , ..., e T ], the task is to generate a coherent paragraph consisting of multiple sen- The girl dances around the room. <ref type="figure">Figure 1</ref>: Vanilla transformer video captioning model <ref type="bibr" target="#b37">(Zhou et al., 2018)</ref>. PE denotes Positional Encoding, TE denotes token Type Embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PE</head><p>tences [s 1 , s 2 , ..., s T ] to describe the whole video, where sentence s t should describe the content in the segment e t . In the following, we first describe the baseline transformer that generates sentences without recurrent architecture, then introduce our approach -Memory-Augmented Recurrent Transformer (MART). Besides, we also compare MART with the recently proposed Transformer-XL  in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: Vanilla Transformer</head><p>We start by introducing the vanilla transformer video paragraph captioning model proposed by <ref type="bibr" target="#b37">Zhou et al. (2018)</ref>, which is an application of the original transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> model for video paragraph captioning. An overview of the model is shown in <ref type="figure">Figure 1</ref>. The core of the architecture is the scaled dot-product attention. Given query matrix Q ∈ R Tq×d k , key matrix K ∈ R Tv×d k and value matrix V ∈ R Tv×dv , the attentional output is computed as:</p><formula xml:id="formula_0">A(Q, K, V ) = softmax QK √ d k , dim=1 V,</formula><p>where softmax(·, dim=1) denotes performing softmax at the second dimension of the the input. Combining h paralleled scaled dot-product attention,  we obtain the multi-head attention <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>, we denote it as MultiHeadAtt(Q, K, V). The attention formulation discussed above is quite general. It can be used for various purposes, such as self-attention <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> where query, key, and value matrix are all the same, and crossattention <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> where the query matrix is different from the key and value matrix.</p><p>In this paper, we also use multi-head attention for memory aggregation and update, as discussed later. The vanilla transformer video paragraph captioning model has N encoder layers and N decoder layers. At the l-th encoder layer, the multi-head attention module takes the last layer's hidden states H l−1 as inputs and performs self-attention. The attentional outputs are then projected by a feedforward layer. At the l-th decoder layer, the model first encodes the last decoder layer's hidden states using masked multi-head attention. 2 It then uses multi-head attention, with the masked outputs as query matrix, and the hidden states H l from l-th encoder layer as key and value matrix to gather information from the encoder side. Similarly, a feed-forward layer is used to encode the sentences further. Residual connection <ref type="bibr" target="#b10">(He et al., 2016)</ref> and layer-normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> are applied for each layer, for both encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory-Augmented Recurrent Transformer</head><p>The vanilla transformer captioning model follows the classical encoder-decoder architecture, where the encoder and decoder network are separated. In comparison, the encoder and decoder are shared in MART, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (left). The video and text inputs are firstly separately encoded and normalized. We denote the encoded video and text embeddings as H 0 video ∈ R T video ×d and H 0 text ∈ R Ttext×d , where T video and T text are the lengths of video and text, respectively. d denotes the hidden size. We then concatenate these two embeddings as input to the transformer layers:</p><formula xml:id="formula_1">H 0 =[H 0 video ; H 0 text ] ∈ R Tc×d , where [; ]</formula><p>denotes concatenation, T c =T video + T text . This unified encoder-decoder design is inspired by recent works on multi-modal representation learning <ref type="bibr" target="#b24">Sun et al., 2019)</ref>. We also use two trainable token type embedding vectors to indicate whether an input token is from video or text, similar to <ref type="bibr" target="#b7">Devlin et al. (2019)</ref> where the token type embeddings are added to indicate different input sequences. We ignore the video token positions and only consider the text token positions when calculating loss and generating words.</p><p>While the aforementioned vanilla transformer is a powerful method, it is less suitable for video paragraph captioning due to its inability to utilize video segments and sentences history information. Thus, given the unified encoder-decoder transformer, we augment it with an external memory module, which helps it to utilize video segments and the corresponding caption history to generate the next sentence. An overview of the memory module is shown in <ref type="figure" target="#fig_0">Figure 2</ref> (left). At step t, i.e., decoding the t-th video segment, the l-th layer aggregates the information from both its intermediate hidden statesH l t ∈ R Tc×d and the memory states M l t−1 ∈ R Tm×d (T m denotes memory state length or equivalently #slots in the memory) from the last step, using a multi-head attention. The input query matrix of the multihead attention Q=H l t , key and value matrices are</p><formula xml:id="formula_2">K, V =[M l t−1 ;H l t ] ∈ R (Tm+Tc)×d .</formula><p>The memory augmented hidden states are further encoded using a feed forward layer and then merged with the intermediate hidden statesH l t using a residual connection and layer norm to form the hidden states output H l t ∈ R Tc×d . The memory state M l t−1 is updated as M l t , using the intermediate hidden statesH l t . This process is conducted in the Memory Updater module, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. We summarize the procedure below:</p><formula xml:id="formula_3">S l t = MultiHeadAtt(M l t−1 ,H l t ,H l t ), C l t = tanh(W l mc M l t−1 + W l sc S l t + b l c ), Z l t = sigmoid(W l mz M l t−1 + W l sz S l t + b l z ), M l t = (1 − Z l t ) C l t + Z l t M l t−1 ,</formula><p>where denotes Hadamard product, W l mc , W l sc , W l mz , and W l sz are trainable weights, b l c and b l z are trainable bias. C l t ∈ R Tm×d is the internal cell state. Z l t ∈ R Tm×d is the update gate that controls which information to retain from the previous memory state, and thus reducing redundancy and maintaining coherence in the generated paragraphs.</p><p>This update strategy is conceptually similar to LSTM <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref> and GRU <ref type="bibr" target="#b4">(Chung et al., 2014)</ref>. It differs in that multi-head attention is used to encode the memory state and thus multiple memory slots are supported instead of a single one in LSTM and GRU, which gives it a higher capacity of modeling complex relations. Recent works <ref type="bibr" target="#b23">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b9">Graves et al., 2014;</ref><ref type="bibr" target="#b29">Xiong et al., 2016a)</ref> introduce a memory component into neural networks, where the memory is mainly designed to memorize facts in the input context to support downstream tasks, e.g., copy <ref type="bibr" target="#b9">(Graves et al., 2014)</ref> or question answering <ref type="bibr" target="#b23">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b29">Xiong et al., 2016a)</ref>. In comparison, the memory in MART is designed to memorize the sequence generation history to support the coherent generation of the next sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with Transformer-XL</head><p>Transformer-XL  is a recently proposed transformer-based language model that uses a segment-level recurrence mechanism to capture the long-term dependency in context. In <ref type="figure" target="#fig_0">Figure 2</ref> (right) we show a modified version of Transformer-XL for video paragraph captioning. At step t, at its l-th layer, Transformer-XL takes as inputs the last layer's hidden states from both the current step and the last step, which we denote as H l−1 t and SG(H l−1 t−1 ), where SG(·) stands for stop-gradient, and is used to save GPU memory and computation . The input query matrix of the multi-head attention Q = H l−1 t , key and value matrices are K, V = [SG(H l−1 t−1 ); H l−1 t ]. Note the multi-head attention here is integrated with relative positional encoding .</p><p>Both designed to leverage the long-term dependency in context, the recurrence in Transformer-XL is between H l t and H l−1 t−1 , which shifts one layer downwards per step. This mismatch in representation granularity may potentially be harmful to the learning process and affect the model performance. In contrast, the recurrence in MART is betweenH l t and M l t−1 (updated usingH l t−1 ) of the same layer. Besides, Transformer-XL directly uses all the hidden states from the last step to enable recurrence, which might be less effective as less relevant and repetitive information is also passed along. In comparison, MART achieves recurrence by using memory states that are highly summarized from previous steps, which may help the model to reduce redundancy and only keep important information from previous steps.</p><p>We conducted experiments on two popular benchmark datasets, ActivityNet Captions <ref type="bibr" target="#b15">(Krishna et al., 2017)</ref> and YouCookII <ref type="bibr" target="#b36">(Zhou et al., 2017)</ref>. We evaluate our proposed MART and compare it with various baseline approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Evaluation Metrics</head><p>Datasets ActivityNet Captions <ref type="bibr" target="#b15">(Krishna et al., 2017)</ref> contains 10,009 videos in train set, 4,917 videos in val set. Each video in train has a single reference paragraph while each video in val has two reference paragraphs. <ref type="bibr" target="#b17">Park et al. (2019)</ref> uses the same set of videos (though different segments) in val for both validation and test. To allow better evaluation of the models, we use splits provided by <ref type="bibr" target="#b35">Zhou et al. (2019)</ref>, where the original val set is split into two subsets: ae-val with 2,460 videos for validation and ae-test with 2,457 videos for test. This setup makes sure the videos used for test will not be seen in validation. YouCookII <ref type="bibr" target="#b36">(Zhou et al., 2017)</ref> contains 1,333 training videos and 457 validation videos. Each video has a single reference paragraph. Both datasets come with temporal event segments annotated with human written natural language sentences. On average, there are 3.65 event segments for each video in ActivityNet Captions, 7.7 segments for each video in YouCookII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Preprocessing</head><p>We use aligned appearance and optical flow features extracted at 2FPS to represent videos, provided by <ref type="bibr" target="#b37">Zhou et al. (2018)</ref>. Specifically, for appearance, 2048D feature vectors from the 'Flatten-673' layer in <ref type="bibr">ResNet-200 (He et al., 2016)</ref> are used; for optical flow, 1024D feature vectors from the 'global pool' layer of BN-Inception <ref type="bibr" target="#b13">(Ioffe and Szegedy, 2015)</ref> are used. Both networks are pre-trained on ActivityNet (Caba Heilbron et al., 2015) for action recognition, provided by <ref type="bibr" target="#b31">(Xiong et al., 2016b)</ref>. We truncate sequences longer than 100 for video and 20 for text and set the maximum number of video segments to 6 for ActivityNet Captions and 12 for YouCookII. Finally, we build vocabularies based on words that occur at least 5 times for ActivityNet Captions and 3 times for YouCookII. The resulting vocabulary contains 3,544 words for ActivityNet Captions and 992 words for YouCookII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics (Automatic and Human)</head><p>We evaluate the captioning performance at paragraph-level, following <ref type="bibr" target="#b17">(Park et al., 2019;</ref>, reporting numbers on standard metrics, including BLEU@4 <ref type="bibr" target="#b16">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b6">(Denkowski and Lavie, 2014)</ref>, CIDEr-D <ref type="bibr" target="#b27">(Vedantam et al., 2015)</ref>. Since these metrics mainly focus on whether the generated paragraph matches the ground-truth paragraph, they fail to evaluate the redundancy of these multi-sentence paragraphs. Thus, we follow previous works <ref type="bibr" target="#b17">(Park et al., 2019;</ref> to evaluate repetition using R@4. It measures the degree of N-gram (N=4) repetition in the descriptions. Besides the automated metrics, we also conduct human evaluations to provide additional comparisons between the methods. We consider two aspects in human evaluation, relevance (i.e., how related is a generated paragraph caption to the content of the given video) and coherence (i.e., whether a generated paragraph caption reads fluently and is linguistically coherent over its multiple sentences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>MART is implemented in PyTorch <ref type="bibr" target="#b21">(Paszke et al., 2017)</ref>. We set the hidden size to 768, the number of transformer layers to 2, and the number of attention heads to 12. For positional encoding, we follow <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref> to use the fixed scheme. For memory module, we set the length of recurrent memory state to 1, i.e., T m =1. We optimize the model following the strategy used by <ref type="bibr" target="#b7">Devlin et al. (2019)</ref>. Specifically, we use Adam (Kingma and <ref type="bibr" target="#b14">Ba, 2014)</ref> with an initial learning rate of 1e-4, β 1 =0.9, β 2 =0.999, L2 weight decay of 0.01, and learning rate warmup over the first 5 epochs. We train the model for at most 50 epochs with early stopping using CIDEr-D and batch size 16. We use greedy decoding as we did not observe better performance using beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>Vanilla Transformer This model originates from the transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>, proposed by <ref type="bibr" target="#b37">Zhou et al. (2018)</ref> (more details in Section 3.1). It takes a single video segment as input and independently generates a single sentence describing the given segment. Note that <ref type="bibr" target="#b37">Zhou et al. (2018)</ref> also have a separate proposal generation module, but here we only focus on its captioning module. To obtain paragraph-level captions, the independently generated single sentence captions are concatenated as the output paragraph.   Transformer-XL Transformer-XL is proposed by  for modeling long-term dependency in natural language. Here we adapt it for video paragraph captioning (more details in Section 3.3). The original design of Transformer-XL stops gradients from passing between different recurrent steps to save GPU memory and computation. To enable a more fair comparison with our model, we implemented a version that allows gradient flow through different steps, calling this Transformer-XLRG (Transformer-XL with Recurrent Gradient).</p><p>AdvInf AdvInf <ref type="bibr" target="#b17">(Park et al., 2019)</ref> uses a set of three discriminators to do adversarial inference on a strong LSTM captioning model. The input features of the LSTM model are the concatenation of image recognition, action recognition, and object detection features. To encourage temporal coherence between consecutive sentences, the last hidden state from the previous sentence is used as input to the decoder <ref type="bibr" target="#b8">Gella et al., 2018)</ref>. The three discriminators are trained adversarially and are specifically designed to reduce repetition and encourage fluency and relevance in the generated paragraph.</p><p>GVD An LSTM based model for grounded video description <ref type="bibr" target="#b35">(Zhou et al., 2019)</ref>. It uses densely detected object regions as inputs, with a grounding module that grounds generated words to the regions. Additionally, we also consider a GVD variant (GVDsup) that uses grounding supervision from <ref type="bibr" target="#b35">Zhou et al. (2019)</ref>.</p><p>MFT MFT  uses an LSTM model with a similar sentence-level recurrence as in AdvInf <ref type="bibr" target="#b17">(Park et al., 2019)</ref>.</p><p>HSE HSE <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref> is a hierarchical model designed to learn both clip-sentence and paragraph-video correspondences. Given the learned contextualized video embedding, HSE uses a 2-layer LSTM to generate captions. For AdvInf, MFT, HSE, GVD, and GVDsup, we obtain generated sentences from the authors. We only report their performance on ActivityNet Captions ae-val split to enable a fair comparison, as (i) AdvInf, MFT and HSE have different settings as ours, where ae-test videos are included as part of their validation set; (ii) we do not have access to the ae-test predictions of GVD and GVDsup. For vanilla transformer, Transformer-XL and Transformer-XLRG, we borrow/modify the model implementations from the original authors and train them under the same settings as MART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Automatic Evaluation <ref type="table" target="#tab_3">Table 1</ref> shows the results of MART and several transformer baseline methods. We observe stronger or comparable performance for the language metrics (B@4, M, C) for  both ActivityNet Captions and YouCookII datasets. For R@4, MART produces significantly better results compared to the three transformer baselines, showing its effectiveness in reducing redundancy in the generated paragraphs. <ref type="table" target="#tab_4">Table 2</ref> shows the comparison of MART with state-of-the-art models on ActivityNet Captions. MART achieves the best scores for both CIDEr-D and R@4 and has a comparable performance for B@4 and METEOR. Note that the best B@4 model, GVDsup <ref type="bibr" target="#b35">(Zhou et al., 2019)</ref>, and the best METEOR model, AdvInf <ref type="bibr" target="#b17">(Park et al., 2019)</ref>, both use strong detection features, and GVDsup has also used grounding supervision. Regarding the repetition score R@4, MART has the highest score. It outperforms the strong adversarial model AvdInf <ref type="bibr" target="#b17">(Park et al., 2019)</ref> even in an unfair comparison where AdvInf uses extra detection features. Additionally, AdvInf has a time-consuming adversarial training and decoding process where a set of discriminator models are trained and used to re-rank candidate sentences, while MART can do much faster inference with only greedy decoding and no further post-processing. The comparisons in <ref type="table" target="#tab_3">Table 1</ref> and <ref type="table" target="#tab_4">Table 2</ref> show that MART is able to generate less redundant (thus more coherent) paragraphs while maintaining relevance to the videos.</p><p>Human Evaluation In addition to the automatic metrics, we also run human evaluation on Amazon Mechanical Turk (AMT) with 200 randomly sampled videos from ActivityNet Captions ae-test split, where each video was judged by three different AMT workers. We design a set of pairwise experiments <ref type="bibr" target="#b20">(Pasunuru and Bansal, 2017b;</ref><ref type="bibr" target="#b17">Park et al., 2019)</ref>, where we compare two models at a time. AMT workers are instructed to choose which caption is better or the two captions are not distinguishable based on relevance and coherence, respectively. The models are anonymized, and the predictions are shuffled. In total, we have 54 work-   <ref type="table" target="#tab_6">Table 3</ref> we show human evaluation results, where the scores are calculated as the percentage of workers that have voted a certain option. With its sentence-level recurrence mechanism, MART is substantially better than the vanilla transformer model for both relevance and coherence. Compared to the strong baseline approach Transformer-XL, MART has similar performance in terms of relevance, but still reasonably better performance in terms of coherence.</p><p>Model Ablation We show model ablation in <ref type="table" target="#tab_8">Table 4</ref>. MART models with recurrence have better overall performance than the variant without, suggesting the effectiveness of our recurrent memory design. We choose to use the model with 2 hidden layers and memory state length 1 as it shows a good balance between performance and computation.</p><p>Qualitative Examples In <ref type="figure" target="#fig_1">Figure 3</ref>, we show paragraph captions generated by vanilla transformer, Transformer-XL, and our method MART. Compared to the two baselines, MART produces more coherent and less redundant paragraphs. In particular, we noticed that vanilla transformer often uses incoherent pronouns/person mentions, while MART and Transformer-XL is able to use suitable pronouns/person mentions across the sentences and thus improve the coherence of the paragraph. Compare with Transformer-XL, we found that the paragraphs generated by MART have much less crosssentence repetitions. We attribute MART's success to its recurrence design -the previous memory states are highly summarized, in which redundant information is removed. While there is less redun-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer</head><p>He is sitting down in a chair. He continues playing the harmonica and ends by looking off into the distance. He continues playing the harmonica and looking off into the distance. He stops playing and looks at the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-XL</head><p>A man is seen speaking to the camera while holding a harmonica. He continues playing the harmonica while looking at the camera. He continues playing the instrument and looking off into the distance. He continues playing and stops playing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART (ours)</head><p>A man is sitting down talking to the camera while holding a camera. He takes a harmonica and begins playing his harmonica. He continues playing the harmonica as he continues playing. He stops and looks at the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>A young man wearing a Cuervo black shirt stares and speaks to the camera as he sits on his chair. He puts a harmonica to his mouth and begins playing. He plays on for about a minute and is very into his song. He then puts the harmonica down and looks into the camera as the video comes to an end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer</head><p>A girl is seen climbing across a set of monkey bars and leads into her climbing across a set of. He jumps off the monkey bars and lands on a bridge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-XL</head><p>A young child is seen climbing across a set of monkey bars and climbing across a set of monkey bars. The boy jumps down and jumps down and jumps down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART (ours)</head><p>A girl is seen speaking to the camera and leads into her climbing across a set of monkey bars. She jumps off the bar and walks back to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>A little girl climbs the monkey bars of a play ground. Then, the little girl jumps to the ground and extend her arms.  dancy between sentences generated by MART, in <ref type="figure" target="#fig_1">Figure 3</ref> (left), we noticed that repetition still exists within a single sentence, suggesting further efforts on reducing the repetition in single sentence generation. More examples are in the appendix.</p><p>Memory Ablation To explore whether the learned memory state could store useful information about the videos and captions, we conducted a video retrieval experiment on ActivityNet Captions train split with 10K videos, where we extract the last step memory state in the first layer of a trained MART model for each video as its representation to perform nearest neighbor search with cosine similarity. Though not explicitly trained for the retrieval task, we observe some positive examples in the experiments. We show an example in <ref type="figure" target="#fig_2">Figure 4</ref>, the neighbors mostly show related activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present a new approach -Memory-Augmented Recurrent Transformer (MART) for video paragraph captioning, where we designed an auxiliary memory module to enable recurrence in transformers. Experimental results on two standard datasets show that MART has better overall performance than the baseline methods. In particular, MART can generate more coherent, less redundant paragraphs without any degradation in relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Additional Qualitative Examples</head><p>We show more caption examples in <ref type="figure" target="#fig_3">Figure 5</ref>. Overall, we see captions generated by models with sentence-level recurrence, i.e., MART and Transformer-XL, tend to be more coherent. Comparing with Transformer-XL, captions generated by MART are usually less repetitive. However, as shown in the two examples at the last row of <ref type="figure" target="#fig_3">Figure 5</ref>, all three models suffer from the content error, where the models are not able to recognize and describe the fine-grained details in the videos, e.g., gender and fine-grained objects/actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer</head><p>He continues speaking while holding the violin and showing how to play his hands. He continues playing the instrument while looking down at the camera. He continues playing the violin and then stops to speak to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-XL</head><p>A man is seen speaking to the camera while holding a violin. The man continues playing the instrument while moving his hands up and down. The man continues playing the instrument and ends by looking back to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART (ours)</head><p>A man is seen speaking to the camera while holding a violin and begins playing the instrument. The man continues to play the instrument while moving his hands up and down. He continues to play and ends by moving his hands up and down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>A man is seen looking to the camera while holding a violin. The man then begins playing the instrument while the camera zooms in on his fingers. The man continues to play and stops to speak to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer</head><p>He is skateboarding down a road. He goes through the streets and goes. He is skateboarding down a road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-XL</head><p>A man is riding a skateboard down a road. He is skateboarding down a road. He is skateboarding down a road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART (ours)</head><p>A man is seen riding down a road with a person walking into frame and speaking to the camera. The man continues riding down the road while looking around to the camera and showing off his movements. The man continues to ride around while looking to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>A camera pans all around an area and leads into a man speaking to the camera. Several shots of the area are shown as well as dogs and leads into a man riding down a hill. The man rides a skateboard continuously around the area and ends by meeting up with the first man.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer</head><p>She continues moving around the room and leads into her speaking to the camera. She continues moving around on the step and ends by speaking to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-XL</head><p>A woman is standing in a gym. She begins to do a step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART (ours)</head><p>A woman is standing in a room talking. She starts working out on the equipment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>A woman is seen speaking to the camera and leads into her walking up and down the board. She then stands on top of the beam while speaking to the camera continuously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer</head><p>Several shots are shown of people riding on the surf board and the people riding along the water. Several shots are shown of people riding around on a surf board and leads into several clips of people riding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-XL</head><p>A large wave is seen followed by several shots of people riding on a surf board and riding along the. The people continue riding along the water while the camera pans around the area and leads into several more shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART (ours)</head><p>A man is seen riding on a surfboard and surfing on the waves. The man continues surfing while the camera captures him from several angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>A man is seen moving along the water on a surf board while another person watches on the side. The person continues riding around and slowing down to demonstrate how to play.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer</head><p>A young girl is seen climbing across a set of monkey bars. A young child is seen climbing across a set of monkey bars. A little girl is standing on a platform in a playground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-XL</head><p>A young child is seen standing before a set of monkey bars and begins climbing across monkey bars. The girl then climbs back and fourth on the bars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART (ours)</head><p>A young child is seen climbing across a set of monkey bars while speaking to the camera. She then climbs down across the bars and begins swinging herself around. She continues to swing down and ends by jumping down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>A boy goes across the monkey bars as a lady watches and cheers him on. At the end he begins to struggle bit, but finally finished. When he is done another little boy comes and stands by him.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer</head><p>The man then holds up a bottle of mouthwash and talks to the camera. The man then puts lotion on her face and begins rubbing it down. The man then begins to blow dry her face and shows off the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-XL</head><p>A man is seen speaking to the camera while holding up a brush. He then rubs lotion all over his face and begins brushing his face. He then puts the lotion on the face and rubs it on the wall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART (ours)</head><p>A man is seen speaking to the camera and leads into him holding up a bottle of water. The man then holds up a can and begins to shave his face. He finishes putting the paper into the mirror and smiles to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>A girl's face is shown in front of the camera. She showed an orange bottle, read the label and squirt the orange content on her palm, showed the cream on the camera, then rub the cream all over her face. She bend down and rinse her face, when her face is visible on the camera her face is clear. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left: Our proposed Memory-Augmented Recurrent Transformer (MART) for video paragraph captioning. Right: Transformer-XL (Dai et al., 2019) model for video paragraph captioning. Relative PE denotes Relative Positional Encoding. SG(·) denotes stop-gradient, denotes Hadamard product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative examples. Red/bold indicates pronoun errors (inappropriate use of pronouns), blue/italic indicates repetitive patterns, underline indicates content errors. Compared to baselines, our model generates more coherent, less repeated paragraphs while maintaining relevance.A girl is giving a small dog a bath. She has an orange bottle in her hand… A man on a diving board walks to the end. The man bounces on the board two times then dives into the water… A young girl is seen walking to the end of a diving board with several other people around her… A little girl stands on a diving board. Then the little girl jumps, flip and dives in the swimming pool…</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Nearest neighbors retrieved using memory states. Top row shows the query, the 3 rows below it are the top-3 nearest neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Additional qualitative examples. Red/bold indicates pronoun errors (inappropriate use of pronouns or person mentions), blue/italic indicates repetitive patterns, underline indicates content errors. Compared to baselines, our model generates more coherent, less repeated paragraphs while maintaining relevance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison with transformer baselines on ActivityNet Captions ae-test split and YouCookII val split. Re.</figDesc><table><row><cell cols="7">indicates whether sentence-level recurrence is used. We report BLEU@4 (B@4), METEOR (M), CIDEr-D (C)</cell></row><row><cell cols="7">and Repetition (R@4). VTransformer denotes vanilla transformer.</cell></row><row><cell></cell><cell cols="3">Det. Re. B@4</cell><cell>M</cell><cell>C</cell><cell>R@4 ↓</cell></row><row><cell>LSTM based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MFT (2018)</cell><cell></cell><cell></cell><cell cols="4">10.29 14.73 19.12 17.71</cell></row><row><cell>HSE (2018)</cell><cell></cell><cell></cell><cell cols="4">9.84 13.78 18.78 13.22</cell></row><row><cell cols="4">LSTM based methods with detection feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GVD (2019)</cell><cell></cell><cell></cell><cell cols="4">11.04 15.71 21.95 8.76</cell></row><row><cell>GVDsup (2019)</cell><cell></cell><cell></cell><cell cols="4">11.30 16.41 22.94 7.04</cell></row><row><cell>AdvInf (2019)</cell><cell></cell><cell></cell><cell cols="4">10.04 16.60 20.97 5.76</cell></row><row><cell cols="2">Transformer based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VTransformer (2018)</cell><cell></cell><cell></cell><cell cols="4">9.75 15.64 22.16 7.79</cell></row><row><cell>Transformer-XL (2019)</cell><cell></cell><cell></cell><cell cols="4">10.39 15.09 21.67 8.54</cell></row><row><cell>Transformer-XLRG</cell><cell></cell><cell></cell><cell cols="4">10.17 14.77 20.40 8.85</cell></row><row><cell>(Ours) MART</cell><cell></cell><cell></cell><cell cols="4">10.33 15.68 23.42 5.18</cell></row><row><cell>Human</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison with baselines on ActivityNet Captions ae-val split. Det. indicates whether the model uses detection feature. Models that use detection features are shown in gray background to indicate they are not in fair comparison with the others. Re. indicates whether sentence-level recurrence is used. VTransformer denotes vanilla transformer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation on ActivityNet Captions aetest set w.r.t. relevance and coherence. Top: MART vs. vanilla transformer (VTransformer). Bottom: MART vs. Transformer-XL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Model ablation on ActivityNet Captions aeval split. Re. indicates whether sentence-level recurrence is used. mem. len. indicates the length of the memory state. MART w/o re. denotes a MART variant without recurrence. Top two scores are highlighted. ers participated the MART vs. vanilla transformer experiments, 47 workers participated the MART vs.</figDesc><table /><note>Transformer-XL experiments. In</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">masked multi-head attention is used to prevent the model from seeing future words<ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their helpful comments and discussions. This work was performed while Jie Lei was an intern at Tencent AI Lab, Seattle, USA. It was later partially supported by NSF Awards CAREER-1846185, 1562098, DARPA KAIROS Grant FA8750-19-2-1004, and ARO-YIP Award W911NF-18-1-0336. The views contained in this article are those of the authors and not of the funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NeurIPS 2016 Deep Learning Symposium</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A dataset for telling the stories of social media videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial inference for multi-sentence video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multitask video captioning with video and entailment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reinforced video captioning with entailment rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Annemarie Friedrich, Manfred Pinkal, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Move forward and tell: A progressive generator of video descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Msrvtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Crossmodal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
