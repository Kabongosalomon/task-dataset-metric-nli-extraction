<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>April 19-23, 2021. April 19-23, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
							<email>bowang19@mails.jlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Jilin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
							<email>tao.shen@student.uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Australian AI Institute</orgName>
								<orgName type="department" key="dep2">School of CS</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
							<email>guodong.long@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Australian AI Institute</orgName>
								<orgName type="department" key="dep2">School of CS</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
							<email>tianyizh@uw.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
							<email>wangying2010@jlu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Jilin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
							<email>yichang@jlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Jilin University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">International Center of Future Science</orgName>
								<orgName type="institution">Jilin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Chang</forename></persName>
						</author>
						<title level="a" type="main">Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<title level="m">Slovenia Â© 2021 IW3C2 (International World Wide Web Conference Committee)</title>
						<meeting> <address><addrLine>Ljubljana; Ljubljana, Slovenia; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">12</biblScope>
							<date type="published">April 19-23, 2021. April 19-23, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3450043</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-curated knowledge graphs provide critical supportive information to various natural language processing tasks, but these graphs are usually incomplete, urging auto-completion of them (a.k.a. knowledge graph completion). Prevalent graph embedding approaches, e.g., TransE, learn structured knowledge via representing graph elements (i.e., entities/relations) into dense embeddings and capturing their triple-level relationship with spatial distance. However, they are hardly generalizable to the elements never visited in training and are intrinsically vulnerable to graph incompleteness. In contrast, textual encoding approaches, e.g., KG-BERT, resort to graph triple's text and triple-level contextualized representations. They are generalizable enough and robust to the incompleteness, especially when coupled with pre-trained encoders. But two major drawbacks limit the performance: (1) high overheads due to the costly scoring of all possible triples in inference, and (2) a lack of structured knowledge in the textual encoder. In this paper, we follow the textual encoding paradigm and aim to alleviate its drawbacks by augmenting it with graph embedding techniques -a complementary hybrid of both paradigms. Specifically, we partition each triple into two asymmetric parts as in translation-based graph embedding approach, and encode both parts into contextualized representations by a Siamese-style textual encoder. Built upon the representations, our model employs both deterministic classifier and spatial measurement for representation and structure learning respectively. It thus reduces the overheads by reusing graph elements' embeddings to avoid combinatorial explosion, and enhances structured knowledge by exploring the spatial characteristics. Moreover, we develop a self-adaptive ensemble scheme to further improve the performance by incorporating triple scores from an existing graph embedding model. In experiments, we achieve state-of-the-art performance on three benchmarks and a zero-shot dataset for link *Joint Corresponding Author. This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge graph (KG) is a ubiquitous format of knowledge base (KB). It is structured as a directed graph whose vertices and edges respectively stand for entities and their relations. It is usually represented as a set of triples in the form of (head entity, relation, tail entity), or (h, r, t) for short. KGs as supporting knowledge play significant roles across a wide range of natural language processing (NLP) tasks, such as dialogue system <ref type="bibr" target="#b17">[15]</ref>, information retrieval <ref type="bibr" target="#b43">[40]</ref>, recommendation system <ref type="bibr" target="#b49">[46]</ref>, etc. However, human-curated knowledge graphs usually suffer from incompleteness <ref type="bibr" target="#b31">[29]</ref>, inevitably limiting their practical applications. To mitigate this issue, knowledge graph completion (KGC) aims to predict the missing triples in a knowledge graph. In this paper, we particularly target link prediction task for KGC, whose goal is to predict the missing head (tail) given the relation and tail (head) in a triple.</p><p>It is noteworthy that KG <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">30,</ref><ref type="bibr" target="#b39">36]</ref> is usually at a scale of billions and the number of involved entities is up to millions, so most graph neural networks (e.g., GCN <ref type="bibr" target="#b18">[16]</ref>) operating on the whole graph are not scalable in computation. Thus, approaches for KGC often operate at triple level, which can be grouped into two paradigms, i.e., graph embedding and textual encoding approaches.</p><p>Graph embedding approaches attempt to learn the representations for graph elements (i.e., entities/relations) as low-dimension vectors by exploring their structured knowledge in a KG. Typically, they directly exploit the spatial relationship of the three embeddings in a triple to learn structured knowledge, which can be classified into two sub-categories. (1) Translation-based approaches, e.g., TransE <ref type="bibr" target="#b2">[3]</ref> and RotatE <ref type="bibr" target="#b34">[31]</ref>, score the plausibility of a triple by applying a translation function to the embeddings of head and relation, and then measuring how close the resulting embedding to the tail embedding, i.e., â|| ( , ) â || ; And (2) semantic matching approaches, e.g., DistMult <ref type="bibr" target="#b47">[44]</ref> and QuatE <ref type="bibr" target="#b50">[47]</ref>, derive the plausibility of a graph triple via a matching function that directly operates on  BERT are state-of-the-art approaches of graph embedding and textual encoding paradigms respectively. StAR is our model, and "StAR (Self-Adp)" is our model plus our proposed self-adaptive ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KG-BERT StAR</head><p>the triple, i.e., <ref type="bibr">( , , )</ref>. Despite their success in structure learning, they completely ignore contextualized information and thus have several drawbacks: <ref type="bibr" target="#b0">(1)</ref> The trained models are not applicable to entities/relations unseen in training; And (2) they are intrinsically vulnerable to the graph incompleteness. These drawbacks severely weaken their generalization capability and prediction quality.</p><p>Textual encoding 1 approaches, e.g., KG-BERT <ref type="bibr" target="#b48">[45]</ref>, predict the missing parts for KGC using the contextualized representation of triples' natural language text. The text can refer to textual contents of entities and relations (e.g., their names or descriptions). Coupled with pre-trained word embedding <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b26">24]</ref> or language model <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b19">17]</ref>, the textual encoder can easily generalize to unseen graph elements and is invulnerable to graph incompleteness issue. However, they are limited by two inherent constraints: <ref type="bibr" target="#b0">(1)</ref> Applying textual encoder to link prediction requires costly inference on all possible triples, causing a combinatorial explosion; <ref type="bibr" target="#b1">(2)</ref> The textual encoder is incompetent in structure learning, leading to a lack of structured knowledge and the entity ambiguity problem <ref type="bibr" target="#b11">[10]</ref>.</p><p>The experiments of these two paradigms also reflect their individual Pros and Cons. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (left), KG-BERT achieves a high Hits@ (i.e., Top-recall) when is slightly large, but fails for small due to entity ambiguity problem. In contrast, RotatE achieves a high Hits@1/@3 since it purely learns from structured knowledge without exposure to the ambiguity problem. But it still underperforms due to a lack of text contextualized information. And in <ref type="figure" target="#fig_0">Figure 1</ref> (right), although KG-BERT outperforms RotatE, it requires much higher overheads due to combinatorial explosion.</p><p>Therefore, it is natural to integrate both the contextualized and structured knowledge into one model, while in previous works they are respectively achieved by textual encoding and graph embedding paradigms. To this end, we start with a textual encoding paradigm with better generalization and then aim at alleviating its intrinsic drawbacks, i.e., overwhelming overheads and insufficient structured knowledge. Specifically, taking the inspiration from translation-based graph embedding approach (e.g., TransE), we first partition each triple into two parts: one with a combination of head and relation, while the other with tail. Then, by applying a Siamese-style textual encoder to their text, we encode each part into separate contextualized representation. Lastly, we concatenate the two representations in an interactive manner <ref type="bibr" target="#b28">[26]</ref> to form the final representation of the triple and train a binary neural classifier upon it. In the meantime, as we encode the triple by separated parts, we can measure their spatial relations like translation function <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">31]</ref> and then conduct a structure learning using contrastive objective.</p><p>Consequently, on the one hand, our model can re-use the same graph elements' embeddings for different triples to avoid evaluating the combinatorial number of triples required in link prediction. On the other hand, it also augments the textual encoding paradigm by modeling structured knowledge, which is essential to graph-related tasks. In addition, our empirical studies on link prediction show that introducing such structured knowledge can effectively reduce false positive predictions and help entity disambiguation. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our model improves the KG-BERT baseline in both performance and efficiency, but given a small (e.g., â¤ 2), the performance is not that satisfactory. Motivated by this, we propose a self-adaptive ensemble scheme that incorporates our model's outputs with the triple scores produced by an existing graph embedding model (e.g., RotatE). Thereby, we can benefit from the advantages of both the graph embedding and textual encoding. Hence, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our model plus the proposed self-adaptive ensemble with RotatE achieves more. Our main contributions are:</p><p>â¢ We propose a hybrid model of textual encoding and graph embedding paradigms to learn both contextualized and structured knowledge for their mutual benefits: A Siamese-style textual encoder generalizes graph embeddings to unseen entities/relations, while augmenting it with structure learning contributes to entity disambiguation and high efficiency.</p><p>â¢ We develop a self-adaptive ensemble scheme to merge scores from graph embedding approach and boost the performance. â¢ We achieve state-of-the-art results on three benchmarks and a zero-shot dataset; We show a remarkable speedup (6.5h vs. 30d on FB15k-237 <ref type="bibr" target="#b36">[33]</ref>) over recent KG-BERT <ref type="bibr" target="#b48">[45]</ref>; We provide a comparative analysis of the two paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We start this section with a formal definition of the link prediction task for KGC. Then, we summarize the pre-trained masked language model and its fine-tuning. And lastly we give a brief introduction to a state-of-the-art textual encoding approach, KG-BERT <ref type="bibr" target="#b48">[45]</ref>.</p><p>Link Prediction. Formally, a KG G = {E, R} consists of a set of triples (h, r, t), where â, â E are head and tail entity respectively while â R is the relation between them. Given a head â (tail ) and a relation , the goal of link prediction is to find the most accurate tail (head â) from E to make a new triple (h, r, t) plausible in G. And during inference, given an incomplete triple (h, r, ?) for example, a trained model is asked to score all candidature triples {(â, , â² )| â² â E} and required to rank the only oracle triple (â, , * ) as high as possible. This is why the combinatorial explosion appears in a computation-intensive model defined at triple level.</p><p>Pre-Trained Masked Language Model. To obtain powerful textual encoders, masked language models (MLMs) pre-trained on largescale raw corpora learn generic contextualized representations in a self-supervised fashion (e.g., BERT <ref type="bibr" target="#b14">[12]</ref> and RoBERTa <ref type="bibr" target="#b19">[17]</ref>). MLMs randomly mask some tokens and predict the masked tokens by considering their contexts on both sides. Specifically, given tokenized text [ 1 , . . . , ], a certain percentage (e.g., 15% in <ref type="bibr" target="#b14">[12]</ref>) of the original tokens are then masked and replaced: of those, 80% with special token [MASK], 10% with a token sampled from the vocabulary V, and the remaining kept unchanged. The masked sequence of embedded tokens [ ( ) 1 , . . . , <ref type="bibr">( )</ref> ] is passed into a Transformer encoder <ref type="bibr" target="#b38">[35]</ref> to produce contextualized representations for the sequence:</p><formula xml:id="formula_0">= Transformer-Enc([ ( ) 1 , . . . , ( ) ]) â R â Ã .<label>(1)</label></formula><p>The pre-training loss is defined as</p><formula xml:id="formula_1">L = â 1 |M| âï¸ âM log ( | :, ),<label>(2)</label></formula><p>where M is the set of masked token indices, and ( | :, ) is the probability of predicting the masked . After pre-trained, they act as initializations of textual encoders, performing very well on various NLP tasks with task-specific modules and fine-tuning <ref type="bibr" target="#b14">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KG-BERT.</head><p>As a recent textual encoding approach <ref type="bibr" target="#b48">[45]</ref> for KGC, instead of using embeddings of entities/relations, it scores a triple upon triple-level contextualized representation. Specifically, a tokenizer with a word2vec <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b26">24]</ref> first transforms the text of each entity/relation to a sequence of word embeddings = [ 1 , . . . , ] â R Ã . So, the text of a triple ( (â) , ( ) , ( ) ) can be denoted as ( (â) , ( ) , ( ) ). Then, KG-BERT applies the Transformer encoder <ref type="bibr" target="#b38">[35]</ref> to a concatenation of the head, relation and tail. The encoder is initialized by a pre-trained MLM, BERT, and the con-</p><formula xml:id="formula_2">catenation isË= [ [CLS] , (â) , [SEP] , ( ) , [SEP] , ( ) , [SEP] ],</formula><p>where [CLS] and [SEP] are special tokens defined by Devlin et al. <ref type="bibr" target="#b14">[12]</ref>. Based on this, KG-BERT produces a contextualized representation for the entire triple, i.e., = Pool(Transformer-Enc(Ë)),</p><p>where Pool(Â·), defined in <ref type="bibr" target="#b14">[12]</ref>, collects the resulting of [CLS] to denote a contextualized representation for the sequence. Next, is passed into a two-way classifier to determine if the triple is plausible or not. Lastly, the model is fine-tuned by minimizing a cross entropy loss. In inference, positive probability (i.e., confidence) of a triple is used as a ranking score. Such a simple approach shows its effectiveness for KGC, highlighting the significance of text representation learning. We thus follow this line and propose our model by avoiding combinatorial explosion and enhancing structure learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>In this section, we first elaborate on a structure-aware triple encoder ( Â§3.1) and a structure-augmented triple scoring module ( Â§3.2), which compose our Structure-Augmented Text Representation (StAR) model to tackle link prediction for KBC (as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>). And we provide the details about training and inference, e.g., training objectives and efficiency, in Â§3.3. Then, we develop a self-adaptive ensemble scheme in Â§3.4 to make the best of an existing graph embedding approach and boost the performance. Lastly, in Â§3.5, we provide comparative analyses between our model and previous text-based approaches for graph-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structure-Aware Triple Encoding</head><p>In this subsection, we aim at encoding a graph triple into vector representation(s) in latent semantic space, with consideration of subsequent structure learning and inference speedup. The representation(s), similar to graph embeddings, can be fed into any downstream objective-specific module to fulfil triple scoring. Recently, to accelerate the inference of a deep Transformer-based model <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b38">35]</ref> in an information retrieval (IR) task, Reimers and Gurevych <ref type="bibr" target="#b28">[26]</ref> adopted a two-branch Siamese architecture <ref type="bibr" target="#b10">[9]</ref> to bypass pairwise input via encoding the query and candidate separately. This enables pre-computing the representations for all candidates and uses a light-weight matching net <ref type="bibr" target="#b28">[26]</ref> to calculate relatedness. We take this inspiration to link prediction to avoid combinatorial explosion, but several open questions arise: (1) How to preserve contextualized knowledge across the entities and relation in a triple;</p><p>(2) How to apply Siamese architecture to a triple with three components; and (3) How to facilitate structure learning in downstream modules.</p><p>These questions can be dispelled by digesting several techniques from translation-based graph embedding approaches, e.g., TransE <ref type="bibr" target="#b2">[3]</ref> and RotatE <ref type="bibr" target="#b34">[31]</ref>. The techniques include applying a translation function to the embeddings of head and relation, and structure learning via exploring spatial relationship (e.g., distance) between the function's output and tail embedding.</p><p>Specifically, TransE and RotatE explicitly define the translation function as real vector addition and complex vector product, respectively. In contrast, as a textual encoding approach, we implicitly formulate the function as applying a Transformer-based encoder to a text concatenation of head and relation. The concatenation is:</p><formula xml:id="formula_4">(â) = [ [CLS] , (â) , [SEP] , ( ) , [SEP] ],<label>(4)</label></formula><p>where <ref type="bibr">[CLS]</ref> and <ref type="bibr">[SEP]</ref> are embedded special tokens defined in <ref type="bibr" target="#b14">[12]</ref>. Refer to KG-BERT in Â§2 for the details about pre-processing. Then, such "contextualizing" translation function is defined as</p><formula xml:id="formula_5">= Pool(Transformer-Enc(Ë( â) )),<label>(5)</label></formula><p>where Transformer-Enc(Â·) denotes the Transformer encoder consisting of multi-head self-attention layers <ref type="bibr" target="#b38">[35]</ref>. We keep using the segment identifier given by Devlin et al. <ref type="bibr" target="#b14">[12]</ref> to mark if a token is from an entity (i.e., 0) or a relation (i.e., 1). And again, Pool(Â·) collects the resulting of [CLS] to denote sequence-level contextualized representation. So, , a contextualized representation across head and relation, can be viewed as the translation function's output.</p><p>On the other side, we also encode tail by applying the Transformer encoder to its text, which is written as = Pool(Transformer-Enc(Ë( ) )),</p><p>where,</p><formula xml:id="formula_7">Ë( ) = [ [CLS] , ( ) , [SEP] ].<label>(7)</label></formula><p>Consequently, , a contextualized representation of tail, is viewed as tail embedding. In our experiment, we keep the two Transformer encoders (i.e., in Eq. <ref type="formula" target="#formula_5">(5)</ref> and <ref type="formula" target="#formula_6">(6)</ref>) parameter-tied for parameter efficiency <ref type="bibr" target="#b28">[26]</ref>. And it is noteworthy that the Transformer encoder can be initialized by a pre-trained language model to further boost its capacity for representation learning, which alternates between BERT <ref type="bibr" target="#b14">[12]</ref> and RoBERTa <ref type="bibr" target="#b19">[17]</ref> in our experiments.</p><p>To sum up, also as answers to the questions above, (1) we trade off the contextualized knowledge with efficiency: keeping context across head and relation, while separating tail for reusable embeddings to avoid combinatorial explosion; (2) we partition each triple into two asymmetric parts as in TransE: a concatenation of head on a corruption of tail entity, and in the same way for the corruption of a head entity or even relation. Note that a notation whose superscript includes " â² " denotes it is derived from a negative example, otherwise from a positive one. and relation, versus tail; and (3) we derive two contextualized embeddings for the two parts respectively, and aim to learn structured knowledge by exploring spatial characteristics between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure-Augmented Scoring Module</head><p>Given and , we present two parallel scoring strategies as at the top of <ref type="figure" target="#fig_1">Figure 2</ref> for both contextualized and structured knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Deterministic Representation Learning.</head><p>Recently, some semantic matching graph embedding approaches for KGC use deterministic strategy <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b25">23]</ref> to learn the representation of entities and relations. This strategy refers to using a binary classifier that determines if a triple is plausible or not. Such representation learning is especially significant to a text-based model, which has been adopted in KG-BERT for KGC and proven effective. But, this strategy cannot be applied to the pair of contextualized representations and produced by our Siamese-style encoder. Fortunately, a common practice in NLP literature is to apply an interactive concatenation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b28">26]</ref> to the pair of representations and then perform a neural binary classifier. Formally, we adopt the interactive concatenation written as</p><formula xml:id="formula_8">= [ ; Ã ; â ; ],<label>(8)</label></formula><p>where is used to represent the semantic relationship between the two parts of a triple. Then, similar to the top layer in KG-BERT, a two-way classifier is then applied to and produces a twodimensional categorical distribution corresponding to the negative and positive probabilities respectively, i.e.,</p><formula xml:id="formula_9">= ( | ; ) â softmax(MLP( ; )) â R 2 ,<label>(9)</label></formula><p>where MLP(Â·) stands for a multi-layer perceptron, and is its learnable parameters. During the inference of link prediction, the 2 dimension of , i.e., the positive probability,</p><formula xml:id="formula_10">= 2<label>(10)</label></formula><p>can serve as a score of the input triple to perform candidate ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Spatial Structure</head><p>Learning. In the meantime, it is possible to augment structured knowledge in the encoder by exploring the spatial characteristics between the two contextualized representations. Typically, translation-based graph embedding approaches conduct structure learning by measuring spatial distance. In particular, TransE <ref type="bibr" target="#b2">[3]</ref> and RotatE <ref type="bibr" target="#b34">[31]</ref> score a triple inversely proportional to the spatial distance between ( , ) and , i.e., â|| ( , ) â ||. And structured knowledge is acquired via maximizing the score margin between a positive triple and its corruptions (i.e., negative triples).</p><p>Here, as a triple is partitioned into two asymmetric parts by imitating the translation-based approaches, we can formulate â (â, ) and â ( ), where (Â·) denotes the textual encoder in Â§3.1. So, to acquire structured knowledge, we can score a triple by</p><formula xml:id="formula_11">= âDistance( (â, ), ( )) â â|| â ||,<label>(11)</label></formula><p>where || Â· || denotes 2-norm, and is the plausible score based on the two contextualized representations, and , of a triple. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference</head><formula xml:id="formula_12">â {(â, , â² )| â² â E â§ (â, , â² ) â G} or â² â {(â â² , , )|â â² â E â§ (â â² , , ) â G},</formula><p>where E denotes the ground set of all unique entities on G. In the remainder, a variable with superscript " â² " means that it is derived from a negative example.</p><p>Triple Classification Objective. Given the resulting confidence score from Eq.(10) in deterministic representation learning ( Â§3.2.1), we employ the following binary cross entropy loss to train the encoder w.r.t this objective, i.e.,</p><formula xml:id="formula_13">L = â 1 |D | âï¸ â D 1 1+|N ( )| log + âï¸ â² âN ( ) log(1â â² ) ,<label>(12)</label></formula><p>where D denotes the training set containing only correct triples, N ( ) denotes a set of wrong triples generated from , denotes positive probability of and (1 â â² ) denotes negative probability of the wrong triple â² . We empirically find such representation learning using the deterministic strategy is critical to the success of textual encoding KGC, consistent with previous works <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b48">45]</ref>. However, might not contain sufficient information for ranking during inference since it is only the confidence for a single triple's correctness that does not take other triple candidates into account. This may cause inconsistency between the model's training and inference. To compromise, loss weights in Eq.(12) must be imbalanced between positive and negative examples, i.e., |N ( )| â« 1, to distinguish the only positive triple among hundreds of thousands of corrupted ones during inference. Nonetheless, over-confident false positive predictions for a corruption (i.e., assigning a corrupted triple with â 1.0) still frequently appear to hurt the performance. These thus emphasize the importance of structure learning.</p><p>Triple Contrastive Objective. Given the distance-based score from Eq.(11) in spatial structure learning ( Â§3.2.1), we also train the encoder by using a contrastive objective. The contrastive objective considers a pairwise ranking between a correct triple and a wrong triple, where the latter is corrupted from the former by negative sampling. Formally, let denote the score derived from a positive triple and â² denote the score derived from a wrong triple â² , we define the loss by using a margin-based hinge loss function, i.e.,</p><formula xml:id="formula_14">L = 1 |D | âï¸ â D 1 |N ( )| âï¸ â² âN ( ) max(0, â + â² ). (13)</formula><p>In experiments, we qualitatively reveal that structure learning is significant to reducing false positive and disambiguating entities, and pushes our model to produce more reliable ranking scores.</p><p>Training and Inference Strategies. The loss L to train the StAR is a sum of the two losses defined in Eq.(12) and Eq.(13), i.e.,</p><formula xml:id="formula_15">L = L + L ,<label>(14)</label></formula><p>where is the weight. After optimizing StAR w.r.t L, , or their integration can be used as ranking basis during inference. We will present a thorough empirical study of the possible options of ranking score based on and in Â§4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Model Efficiency.</head><p>In the following, we analyze why our proposed model is significantly faster than its baseline, KG-BERT.</p><p>Training Efficiency. As overheads are dominated by the computations happening inside the Transformer encoder, we focus on analyzing the complexity of computing the contextualized embeddings by the encoder. In practice, the sequence lengths of the two asymmetric parts of a triple are similar because the length of an entity's text is usually much longer than a relation's text, especially <ref type="table">Table 1</ref>: Inference efficiency comparison. is the length of triple text. | E | and | R | are the numbers of all unique entities and relations in the graph respectively. Usually, | E | exceeds hundreds of thousands and is much greater than | R |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference on Method</head><p>Complexity Speed up</p><p>One Triple</p><formula xml:id="formula_16">KG-BERT ( 2 | E |) â¼ 4Ã StAR ( ( /2) 2 (1 + | E |)) Entire Graph KG-BERT ( 2 | E | 2 | R |) â¼ 4| E |Ã StAR ( ( /2) 2 | E | (1 + | R |))</formula><p>when the entity description is included <ref type="bibr" target="#b41">[38,</ref><ref type="bibr" target="#b48">45]</ref>. Hence, Siamesestyle StAR is 2Ã faster than KG-BERT in training as the complexity of Transformer encoder grows quadratically with sequence length.</p><p>Inference Efficiency. Similarly, we also focus on analyzing the overheads used in the encoder during inference. As shown in <ref type="table">Table  1</ref>, we list the complexities of both KG-BERT baseline and proposed StAR, and analyze the acceleration in two cases. In practice, on the test set of a benchmark, our approach, without combinatorial explosion, is faster than KG-BERT by two orders of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Self-Adaptive Ensemble Scheme</head><p>StAR improves previous textual encoding approaches by introducing structure learning. It reduces those overconfident but false positive predictions and mitigates the entity ambiguity problem. However, compared to graph embedding operating at entity or relation level, our StAR based on the text inherently suffers from entity ambiguity. Fortunately, combining textual encoding with graph embedding paradigms can provide a remedy: Despite entity ambiguity existing, a textual encoding approach achieves a high recall in top-with slightly large (e.g., &gt; 5), whereas a graph embedding approach can then precisely allocate the correct one from the candidates due to robustness to ambiguity. Note, âª |E |.</p><p>To the best of our knowledge, this is the first work to ensemble the two paradigms for mutual benefits. Surprisingly, simple averaging of the scores from the two paradigms significantly improves the performance. This motivates us to take a step further and develop a self-adaptive ensemble scheme.</p><p>Given an incomplete triple (i.e., (h, r, ?) or (?, r, t)), we aim to learn a weight â [0, 1] to generate the final triple-specific score:</p><formula xml:id="formula_17">( ) = Ã ( ) + (1 â ) Ã ( ) ,<label>(15)</label></formula><p>where ( ) is derived from StAR and ( ) is derived from RotatE <ref type="bibr" target="#b34">[31]</ref>. Since ( ) = â [0, 1] from Eq.(10) is normalized, we rescale all candidates' scores of RotatE into [0, 1] to obtain ( ) . Specifically, for an incomplete triple, we first take the top-candidates ranked by StAR and fetch their scores from the two models, which are denoted as ( ) â [0, 1] and ( ) â [0, 1] respectively. Then, we set an unseen indicator to force = 1 if an unseen entity/relation occurs in the incomplete triple. Next, to learn a triple-specific , we build an MLP based upon two kinds of features: ambiguity degree ( ) and score consistency ( ) . Particularly, the ambiguity de-</p><formula xml:id="formula_18">gree ( ) â [Std( ); Mean( )] where "Std( â R Ã ) â R "</formula><p>is the standard deviation of the top-entities' representations, and "Mean( â R Ã100 ) â R " averages the largest 100 cosine similarities between each candidate and all entities in E. Note each entity is denoted by its contextualized representation from Eq. <ref type="bibr" target="#b5">(6)</ref>. And, the score consistency</p><formula xml:id="formula_19">( ) â [| ( ) â ( ) |, ( ) + ( ) , ( ) , ( ) ].</formula><p>Lastly, we pass the features into an MLP with activation , i.e.,</p><formula xml:id="formula_20">= (MLP([ ( ) ; ( ) ]; ( ) )) â [0, 1].<label>(16)</label></formula><p>In training, we fix the parameters of both our model and RotatE, and only optimize ( ) by a margin-based hinge loss. In inference, we use the resulting ( ) to re-rank the top-candidates while keep the remaining unchanged. In experiments, we evaluated two variants of StAR: (1) StAR (Ensemble): â â and â 0.5, equivalent to score average, as our ensemble baseline. (2) StAR (Self-Adp): â 1000 and is learnable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Compared to Prior Text-Based Approach</head><p>Sharing a similar motivation, some previous approaches also use textual information to represent entities and relations. However, they are distinct from textual encoding approaches like KG-BERT or StAR and can be coarsely categorized into two groups:</p><p>Stand-alone Embedding. These approaches <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b31">29]</ref> directly replace an entity/relation embedding in graph with its text representation. The representation is derived from applying a shallow encoder (e.g., CBoW and CNN) to text, regardless of contextual information across entities and relations. But, deep contextualized features are proven effective and critical to text representation for various NLP tasks <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b27">25]</ref>. For KBC, the features are significant for entity disambiguation. Therefore, despite slightly improving generalization, they still deliver an inferior performance. In contrast, our model achieves a better trade-off between deep contextualized features and efficiency by the carefully designed triple encoder.</p><p>Joint Embedding. More similar to our work, some other approaches <ref type="bibr">[13, 33, 37-39, 42, 43]</ref> also bring text representation learning into graph embedding paradigm. Standing opposite our model, they start with graph embeddings and aim at enriching the embeddings with text representations. Typically, they either use text embeddings to represent entities/relations and align heterogeneous representations into the same space <ref type="bibr" target="#b40">[37,</ref><ref type="bibr" target="#b42">39,</ref><ref type="bibr" target="#b46">43]</ref>, or employ largescale raw corpora containing co-occurrence of entities to enrich the graph embeddings <ref type="bibr" target="#b45">[42]</ref>. However, due to graph embeddings involved, they inevitably inherit the generalization problem and incompleteness issue. And same as the above, the representation learning here is also based on shallow networks without deep contextualized knowledge. In contrast, our model, based solely on text's contextualized representations and coupled with structure learning, is able to achieve mutual benefits of the two paradigms for KBC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>In this section 2 , we evaluate StAR on several popular benchmarks ( Â§4.1), and verify the model's efficiency ( Â§4.2) and generalization ( Â§4.3). Then, we conduct an extensive ablation study in Â§4.4 to test various model selections and verify the significance of each proposed module. Lastly, in Â§4.5 we comprehensively analyze the difference between graph embedding approach and textual encoding approach, and assess the self-adaptive ensemble scheme.</p><p>Benchmark Datasets. We assessed the proposed approach on three popular and one zero-shot link prediction benchmarks, whose statistics are listed in <ref type="table" target="#tab_2">Table 2</ref>. First, WN18RR [11] is a link prediction dataset from WordNet <ref type="bibr" target="#b22">[20]</ref>. It consists of English phrases and their semantic relations. Second, FB15k-237 <ref type="bibr" target="#b36">[33]</ref> is a subset of Freebase <ref type="bibr" target="#b1">[2]</ref>, consisting of real-world named entities and their relations. Note, WN18RR and FB15k-237 are updated from WN18 and FB15k <ref type="bibr" target="#b2">[3]</ref> respectively by removing inverse relations and data leakage, which are the most popular benchmarks. <ref type="bibr" target="#b2">3</ref> And third, UMLS [11] is a small KG containing medical semantic entities and their relations. Finally, to verify model's generalization, NELL-One [41] is a few-shot link prediction dataset derived from NELL <ref type="bibr" target="#b5">[6]</ref>, where the relations in dev/test set never appear in train set. We adopted "In-Train" scheme by Chen et al. <ref type="bibr" target="#b7">[8]</ref> and used zero-shot setting. And, in line with prior approaches <ref type="bibr" target="#b41">[38,</ref><ref type="bibr" target="#b48">45]</ref>, we employed entity descriptions as their text for WN18RR and FB15k-237 from synonym definitions and Wikipedia paragraph <ref type="bibr" target="#b42">[39]</ref> respectively. As for the text of relations and other datasets' entities, we directly used their text contents. Please refer to Appendix A for our training setups.</p><p>Evaluation Metrics. In the inference phase, given a test triple of a KG as the correct candidate, all other entities in the KG act as wrong candidates to corrupt its either head or tail entity. The trained model aims at ranking correct triple over corrupted ones with "filtered" setting <ref type="bibr" target="#b2">[3]</ref>. For evaluation metrics, there are two aspects: (1) Mean rank (MR) and mean reciprocal rank (MRR) reflect the absolute ranking; and (2) Hits@ stands for the ratio of test examples whose correct candidate is ranked in top-. And, although there are two ranking scores from Eq.(10) and Eq.(11), only is used for ranking, and other options will be discussed in Â§4.4.</p><p>Evaluation Protocol. We must emphasize that, as stated by Sun et al. <ref type="bibr" target="#b35">[32]</ref>, previous methods (e.g., ConvKB, KBAT and CapsE) use an inappropriate evaluation protocol and thus mistakenly report very high results. The mistake frequently appears in a method whose score is normalized, says [0, 1], due to float precision problem. So, we strictly follow the "RANDOM" protocol proposed by Sun et al. <ref type="bibr" target="#b35">[32]</ref> to evaluate our models, and avoid comparisons with vulnerable methods that have not been re-evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluations on Link Prediction</head><p>The link prediction results of competitive approaches and ours on the three benchmarks are shown in <ref type="table">Table 3</ref>. It is observed our <ref type="table">Table 3</ref>: Link prediction results on WN18RR, FB15k-237 and UMLS. â Resulting numbers are reported by Nathani et al. <ref type="bibr" target="#b23">[21]</ref>, â¦Resulting numbers are re-evaluated by <ref type="bibr" target="#b35">[32]</ref>, and others are taken from the original papers; UMLS results are reported by Yao et al. <ref type="bibr" target="#b48">[45]</ref>, except ConvE from our re-implementation. The bold numbers denote the best results in each genre while the underlined ones are state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head><p>FB15k  proposed StAR is able to achieve state-of-the-art or competitive performance on all these datasets. The improvement is especially significant in terms of MR due to the great generalization performance of textual encoding approach, which will be further analyzed in the section below. And on WN18RR, StAR surpasses all other methods by a large margin in terms of Hits@10. Although it only achieves inferior performance on Hits@1 compared to graph embedding approaches, it still remarkably outperforms KG-BERT from the same genre by introducing structured knowledge. Further, coupled with the proposed self-adaptive scheme, the proposed model delivers new state-of-the-art performance on all metrics. Specifically, our self-adaptive model "StAR (Self-Adp)" significantly surpasses its ensemble baseline "StAR (Ensemble)" on most metrics. And, even if Hits@1 is the main weakness for a textual encoding paradigm, our self-adaptive model is still superior than the best semantic matching graph embedding approach TuckER. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with KG-BERT Baseline</head><p>Since our approach is an update from the non-Siamese-style baseline, says KG-BERT, we compared StAR with KG-BERT on WN18RR in detail, including different initializations. As shown in <ref type="table" target="#tab_4">Table 4</ref>, our proposed StAR consistently achieves superior performance over most metrics. As for empirical efficiency, it is observed our model is faster than KG-BERT despite training or inference, which is roughly consistent with the theoretical analysis in Â§3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalization to Unseen Graph Elements</head><p>Textual encoding approaches are more generalizable to unseen entities/relations than graph embedding ones. This can be more significant when the set of entities or relations is not closed, i.e., unseen graph elements (i.e., entities/relations) appear during inference. For example, 209 out of 3134 and 29 out of 20466 test triples involve unseen entities on WN18RR and FB15k-237 respectively. This inevitably hurts the performance of graph embedding approaches, especially for the unnormalized metric MR. First, we employed a few-shot dataset, NELL-One, to perform a zero-shot evaluation where relations in test never appear in training set. As shown in <ref type="table" target="#tab_5">Table 5</ref>, StAR with zero-shot setting is competitive with graph embedding approaches with one/five-shot setting.</p><p>Then, to verify the generalization to unseen entities, we built two probing settings on WN18RR. The first probing task keeps training set unchanged and makes the test set only consist of the triples with unseen entities. And in the second probing task, we randomly removed 1900 entities from training set to support inductive entity representations <ref type="bibr" target="#b16">[14]</ref> during test for TransE. The setting is detailed in Appendix B. As shown in <ref type="table" target="#tab_6">Table 6</ref>, StAR is competitive across the settings but advanced graph embedding approaches (e.g., RotatE) show a substantial drop in the first task. Even if we used translation formula to inductively complete unseen entities' embeddings in the second probing task, the degeneration of TransE is significant. These verify StAR's promising generalization to unseen elements.</p><p>Lastly, to verify the proposed model is still competitive even if applied to close sets of entities/relations, we built the third probing task as in <ref type="table" target="#tab_6">Table 6</ref>. We only kept the WN18RR test triples with entities/relations visited during training while removed the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To explore each module's contribution, we conducted an extensive ablation study about StAR and the self-adaptive ensemble scheme as shown in <ref type="table" target="#tab_7">Table 7</ref>. For single StAR, (1) Ablating Objective: First, each of the components in Eq. <ref type="bibr" target="#b16">(14)</ref> were removed to estimate the significance of structure and representation learning. (2) Contexts' concatenation: Then, how to concatenate and encode the text from a triple is also non-trivial for learning structured knowledge. Two other options only achieved sub-optimal results. (3) Distance measurement: Two other methods, i.e., Bilinear and Cosine, similarity were also applied to Eq.(11) to measure the distance for structure learning. (4) Ranking Basis: Since two scores can be derived from the two objectives respectively, it is straightforward to integrate them in either additive or multiplicative way. As these ranking bases achieve similar performance, we further calculated the Pearson correlation between and and found the coefficient is 0.939 (p-value=7Ã10 â4 ), which means the two scores are linearly related.  The text above each histogram shows the ranking and for the corresponding un-corrupted (i.e., oracle) triple. Note, interval of [0.0, 0.1] is removed since most negative triples' â² will fall into it.</p><p>For "StAR (Self-Adp)" (in Â§ 3.4), we ablated its features: (1) unseen indicator, (2) ambiguity degree, and (3) score consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further Analyses</head><p>Here, we analyze the effect of structure learning on our textual encoding approach, and compare the proposed model with a graph embedding approach. And we also attempt to qualitatively measure the effectiveness of the proposed self-adaptive ensemble scheme.</p><p>What is the effect of introducing structure learning into a textual encoding approach. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we compared the frequency histograms of ranking score â² derived from classification objective for negative triples from either the "full StAR" or "StAR w/o contrastive objective". It is observed, the textual encoding model augmented with structure learning reduces the number 3, ( , mechanical system, ,mechanism, ) 24, (mechanical system, production line, suspension system, . . . ) of false positive predictions and produces more accurate ranking scores. This also verifies the structured knowledge can alleviate the over-confidence problem ( Â§3.3.1).</p><p>How does StAR bring improvements. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, a detailed comparison regarding different relations is conducted between StAR and RotatE. It is observed StAR achieves more consistent results than RotatE. However, StAR performs worse on several certain relations, e.g., the 8 â relation in <ref type="figure" target="#fig_3">Figure 4</ref>, verb group. After checking the test triples falling into verb group, we found "polysemy" occurs in half of the triples, e.g., (strike , verb group, strike ), which hinders StAR from correctly ranking. These imply that even coupled with structured knowledge, a textual encoding approach is still vulnerable to entity ambiguity or word polysemy, and emphasize the importance of our self-adaptive ensemble scheme.</p><p>Why does StAR achieve better Hits@10 but worse Hits@1 than Ro-tatE. As shown in <ref type="table">Table 3</ref>, it is observed that the textual encoding approach (e.g., KG-BERT, StAR) can outperform graph embedding approach (e.g., TransE, RotatE) by a large margin on Hits@10 but underperform on Hits@1. To dig this out, we conducted a case study based on the inference on WN18RR. In particular, given an oracle test triple, (sensitive , derivationally related form, sense), after corrupting its tail and ranked by our StAR, the top-12 tail candidates are (sensitive , sensitivity, sensibility, sensing, sense impression, sentiency, sensitive , sense, feel, sensory, sensitive , perceptive), where gold tail is only ranked 8 â. It is observed there are many semantically-similar tail entities that can fit the oracle triple, which <ref type="table">Table 9</ref>: An example of polysemy in WordNet: three meanings of "sensitive" are viewed as three separate nodes.</p><p>â¢ sensitive : able to feel or perceive.</p><p>â¢ sensitive : responsive to physical stimuli.</p><p>â¢ sensitive : being susceptible to the attitudes, feelings, or circumstances of others. seem to be false negative labels for a context encoder. But this is not a matter for graph embedding approaches since they only consider graph's structure despite text. It is worth mentioning "polysemy" or "ambiguity" issue usually appears in WN18RR (an example in <ref type="table">Table 9</ref>). The issue is more severe in FB15K-237, which partially explains why StAR only achieves competitive results. Fortunately, this issue can be significantly alleviated by the self-adaptive ensemble scheme. And, it is interesting the oracle head is ranked 1 for tail in this case but self-loop will never appear in WN18RR's test set. Hence, as shown in <ref type="table" target="#tab_9">Table 10</ref>, after filtering self-loop candidates during inference, the performance is improved.</p><p>How does the self-adaptive ensemble scheme bring improvements. As shown in <ref type="table">Table 3</ref>, "StAR (Self-Adp)" improves the performance than "StAR" or RotatE used alone. Intuitively, the improvement is brought from the mutual benefits of representation and structure learning. For further confirmation, we randomly listed some triples in WN18RR test, where the triples experience a certain improvement when applying self-adaptive ensemble scheme. As shown in <ref type="table" target="#tab_8">Table 8</ref>, as demonstrated in the 1 and 3 examples, it is observed that graph structure helps distinguish semantically-similar candidate entities and alleviate the "polysemy" problem. In addition, since the rich contextualized information empowers model with a high top-recall, the self-adaptive ensemble model still achieves a satisfactory ranking result as shown in the 2 example, even if the graph embedding model underperforms. As a result, due to the complementary benefits, the self-adaptive ensemble scheme offers significant improvements over previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Structure Learning for Link Prediction. Previous graph embedding approaches explore structured knowledge through spatial measurement or latent matching in low-dimension vector space. Specifically, on the one hand, translation-based graph embedding approach <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">31]</ref> applies a translation function to head and relation, and compares the resulting with tail via spatial measurement. The most well-known one, TransE <ref type="bibr" target="#b2">[3]</ref>, implements the function and the measurement with real vector addition and 2 norm respectivelyscoring a triple by â||( + ) â ||. However, the graph embeddings defined in real vector space hardly deal with the symmetry relation pattern, and thereby underperform. To remedy this, RotatE <ref type="bibr" target="#b34">[31]</ref> defines the graph embeddings in complex vector space, and implements the translation function with the production of two complex numbers in each dimension. On the other hand, semantic matching graph embedding approach <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">44,</ref><ref type="bibr" target="#b50">47]</ref> uses a matching function ( , , ) operating on whole triple to directly derive its plausibility score. For example, DistMult <ref type="bibr" target="#b47">[44]</ref> applies a bilinear function to each triple's components and uses the latent similarity in vector space as the plausibility score. In spite of their success, the rich text contextualized knowledge is entirely ignored, leading to less generalization.</p><p>Text Representation Learning. In an NLP literature, text representation learning is fundamental to any NLP task, which aims to produce expressively powerful text embedding with contextualized knowledge <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b27">25]</ref>. When applied to KGC, some approaches <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b31">29]</ref> directly replace the graph embeddings with their text embedding. For example, Socher et al. <ref type="bibr" target="#b31">[29]</ref> simply use continuous CBoW as the representation of triple's component, and then proposed a neural tensor network for relation classification. ConMask <ref type="bibr" target="#b30">[28]</ref> learns relationship-dependent entity embeddings of the entity's name and parts of description based on fully CNN. These approaches are not competitive since the deep contextualized representation of a triple is not leveraged. In contrast, KG-BERT <ref type="bibr" target="#b48">[45]</ref>, as a textual encoding approach, applies pre-trained encoder to a concatenation of triples' text for deep contextualized representations. Such a simple method is very effective, but unfortunately suffers from high overheads.</p><p>Jointly Learning Methods. Unlike the approaches above learning either knowledge solely, several works explore jointly learning both text and structured knowledge. Please refer to the end of Â§3.5 for more detail. For example, taking into account the sharing of substructure in the textual relations in a large-scale corpus, Toutanova et al. <ref type="bibr" target="#b36">[33]</ref> applied a CNN to the lexicalized dependency paths of the textual relation, for augmented relation representations. Xie et al. <ref type="bibr" target="#b42">[39]</ref> propose a representation learning method for KGs via embedding entity descriptions, and explored CNN encoder in addition to CBoW. They used the objective across this representation and graph embeddings that a vector integration of head and relation was close to the vector of tail to learn the model, as in translationbased graph embedding approaches <ref type="bibr" target="#b2">[3]</ref>. In contrast, our work only operates on homogeneous textual data and employs the contexts for entities/relations themselves (i.e., only their own text contents or description), rather than acquiring textual knowledge (e.g., textual relations by Toutanova et al. <ref type="bibr" target="#b36">[33]</ref>) from large-scale corpora to enrich traditional graph embeddings via joint embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we propose a structure-augmented text representation (StAR) model to tackle link prediction task for knowledge graph completion. Inspired by translation-based graph embedding designed for structure learning, we first apply a Siamese-style textual encoder to a triple for two contextualized representations. Then, based on the two representations, we present a scoring module where two parallel scoring strategies are used to learn both contextualized and structured knowledge. Moreover, we propose a self-adaptive ensemble scheme with graph embedding approach, to further boost the performance. The empirical evaluations and thorough analyses on several mainstream benchmarks show our approach achieves state-of-the-art performance with high efficiency. <ref type="table">Table 11</ref>: The grid searching of hyperparameters. Note, the hyperparameters in the first part, i.e., Batch Size and , were tuned based on WN18RR benchmark, RoBERTa initialization, learning rate = 10 â5 , number of training epochs = 6. After the first was part tuned, the remaining was tuned subsequently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TRAINING SETUPS</head><p>In training phase, the initialization of Transformer encoder is alternated between BERT and RoBERTa. The model is fine-tuned by Adam optimizer. For the hyperparameters in StAR, based on the best Hits@10 on dev set, we set batch size = 16, learning rate = 10 â5 /5 Ã 10 â5 for the models initialized with RoBERTa and BERT respectively, number of training epochs = 7 on WN18RR and FB15k-237, 8 on NELL-One, 20 on UMLS, |N ( )| = 5, = 1 in Eq.(13), and = 1 in Eq. <ref type="bibr" target="#b16">(14)</ref>. As for grid searching of hyperparameters, we list the searching scopes and the tuned hyperparameters for best in <ref type="table">Table 11</ref>. Note, we sampled 5 negative triples for each positive triple by following Yao et al. <ref type="bibr" target="#b48">[45]</ref> without any tuning, and we also did not tune the random seed while kept the same among the experiments. For the hyperparameters in self-adaptive ensemble scheme, based on the best Hits@10 on WN18RR/FB15k-237 dev set, we set batch size = 32/64, learning rate = 10 â3 /10 â5 , number of training epochs = 1, number of negative samples = 5/10, and margin = 0.60/0.44 in hinge loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROBING TASKS</head><p>The first probing task keeps training set unchanged but makes the test set only consist of the test triples involving unseen entities. And, in second probing task, we conducted a more reasonable comparison by supporting inductive representations <ref type="bibr" target="#b16">[14]</ref> for unseen entities in a translation-based approach, and thus made following changes : (1) 1900 entities were sampled from test set, and only a test triple containing at least one of the sampled entities can be kept, resulting in 1758 test triples in this probing task; (2) Those training triples that do not contain the sampled entities are used as new training set; and (3) Those training triples containing exact one of the sampled entities are used as support set to inductively generate the embedding for the unseen entities via translation formula, such as " + = " in TransE <ref type="bibr" target="#b2">[3]</ref>. Using the second probing setting can assign the unseen entities with competent embeddings, thus leading to a fairer comparison than the first one. Note, if an unseen entity is involved in multiple triple on the support set, an average over the multiple inductive representations is used as its single vector representation. ,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Summary comparisons on WN18RR test. RotatE and KG-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the proposed Structure-Augmented Text Representation (StAR) model for link prediction. This illustration is based</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Three random comparative cases of frequency histogram for â² assigned to a triple's all tail corruptions. -axis denotes â² and -axis denotes frequency over the number of all corruptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>) 2 (34%) 3 (8%) 4 (6%) 5 (4%) 6 (4%) 7 (2%) 8 (1%) 9 (1%) 10 (1%) A comparison between StAR and RotatE regarding different relations on WN18RR test set. Relations corresponding to the indices are 1) hypernym, 2) derivationally related form, 3) member meronym, 4) has part, 5) instance hypernym, 6) synset domain topic of, 7) also see, 8) verb group, 9) member of domain region, 10) member of domain usage. The number in a parenthesis denotes its proportion of test triples with the corresponding relation. Note relation "similar to" is ignored since its proportion is less than 0.1%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2004.14781v2 [cs.CL] 24 Feb 2021</figDesc><table><row><cell>Hits@N on WN18RR test</cell><cell>0.2 0.4 0.6 0.8 1.0</cell><cell>10 0</cell><cell>10 1 N</cell><cell>10 2 RotatE, MR = 3340 KG-BERT, MR = 95 StAR, MR = 51 StAR (Self-Adp), MR = 45</cell><cell>Hits@10 on WN18RR test</cell><cell>0.55 0.60 0.65 0.70 0.75 0.80</cell><cell>10 2 Inference Time (sec) 10 3 10 4 10 5 RotatE StAR (Self-Adp)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3.3.1 TrainingObjectives and Inference Details. Before presenting two training objectives, it is necessary to perform negative sampling and generate wrong triples. In detail, given a correct triple = (â, , ), we corrupt the triple and generate its corresponding wrong triple â² by replacing either the head or tail entity with another entity randomly sampled from the entities E on G during training, which satisfies â²</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary statistics of benchmark datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="5"># Ent # Rel # Train # Dev # Test</cell></row><row><cell cols="2">WN18RR 40,943</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell cols="3">FB15k-237 14,541 237</cell><cell cols="3">272,115 17,535 20,466</cell></row><row><cell>UMLS</cell><cell>135</cell><cell>46</cell><cell>5,216</cell><cell>652</cell><cell>661</cell></row><row><cell cols="3">NELL-One 68,545 822</cell><cell cols="2">189,635 1,004</cell><cell>2,158</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with KG-BERT on WN18RR. "T/Ep" stands for time per training epoch and "Infer" denotes inference time on test set. The time was collected on RTX6000 with mixed precision.</figDesc><table><row><cell></cell><cell>Hits@1 @3 @10 MR MRR T/Ep Infer</cell></row><row><cell>KG-BERT BERT-base</cell><cell>.041 .302 .524 97 .216 40m 32h</cell></row><row><cell>StAR BERT-base</cell><cell>.222 .436 .647 99 .364 20m 0.9h</cell></row><row><cell cols="2">KG-BERT RoBERTa-base .130 .320 .636 84 .278 40m 32h</cell></row><row><cell>StAR RoBERTa-base</cell><cell>.202 .410 .621 71 .343 20m 0.9h</cell></row><row><cell cols="2">KG-BERT RoBERTa-large .119 .387 .698 95 .297 79m 92h</cell></row><row><cell>StAR RoBERTa-large</cell><cell>.243 .491 .709 51 .401 55m 1.0h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Link prediction results on NELL-One. StAR with zero-shot setting is competitive with few-shot GMatching<ref type="bibr" target="#b44">[41]</ref> and MetaR<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="5">-Shot Hits@1 Hits@5 Hits@10 MRR</cell></row><row><cell cols="2">GMatching ComplEx Five-Shot</cell><cell>.14</cell><cell>.26</cell><cell>.31</cell><cell>.20</cell></row><row><cell>MetaR</cell><cell></cell><cell>.17</cell><cell>.35</cell><cell>.44</cell><cell>.26</cell></row><row><cell>GMatching TransE</cell><cell></cell><cell>.12</cell><cell>.21</cell><cell>.26</cell><cell>.17</cell></row><row><cell>GMatching DistMult</cell><cell>One-Shot</cell><cell>.11</cell><cell>.22</cell><cell>.30</cell><cell>.17</cell></row><row><cell>GMatching ComplEx</cell><cell></cell><cell>.12</cell><cell>.26</cell><cell>.31</cell><cell>.19</cell></row><row><cell>MetaR</cell><cell></cell><cell>.17</cell><cell>.34</cell><cell>.40</cell><cell>.25</cell></row><row><cell>StAR BERT-base</cell><cell cols="2">Zero-Shot .17</cell><cell>.35</cell><cell>.45</cell><cell>.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Probing tasks based on WN18RR for analyzing models'</figDesc><table><row><cell cols="3">generalization performance.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Hits@1 Hits@3 Hits@10 MR MRR</cell></row><row><cell></cell><cell>StAR</cell><cell>.243</cell><cell>.491</cell><cell>.709</cell><cell>51 .401</cell></row><row><cell>Original Task</cell><cell cols="2">RotatE .428</cell><cell>.492</cell><cell>.571</cell><cell>3340 .476</cell></row><row><cell></cell><cell cols="2">TransE .042</cell><cell>.441</cell><cell>.532</cell><cell>2300 .243</cell></row><row><cell>First</cell><cell>StAR</cell><cell>.240</cell><cell>.452</cell><cell>.673</cell><cell>71 .384</cell></row><row><cell></cell><cell cols="2">RotatE .005</cell><cell>.007</cell><cell cols="2">.012 17955 .007</cell></row><row><cell>Probing Task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">TransE .000</cell><cell>.007</cell><cell cols="2">.016 20721 .007</cell></row><row><cell>Second</cell><cell>StAR</cell><cell>.301</cell><cell>.497</cell><cell>.676</cell><cell>99 .427</cell></row><row><cell>Probing Task</cell><cell cols="2">TransE .005</cell><cell>.121</cell><cell cols="2">.210 13102 .078</cell></row><row><cell>Third</cell><cell>StAR</cell><cell>.244</cell><cell>.493</cell><cell>.712</cell><cell>49 .402</cell></row><row><cell>Probing Task</cell><cell cols="2">RotatE .455</cell><cell>.523</cell><cell>.612</cell><cell>1657 .507</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on WN18RR. Note that * Full model denotes using two objectives for training, " [h, r] vs. [t]" as concatenation scheme, 2 norm as measurement, and as ranking basis during inference. And Rescale( Â·) denotes scaling all scores to [0, 1].</figDesc><table><row><cell cols="2">Perspective Detail</cell><cell cols="2">Hits@10 MR MRR</cell></row><row><cell cols="3">Single Model: Module Ablation and Selection in "StAR"</cell><cell></cell></row><row><cell>Full model  *</cell><cell>StAR RoBERTa-large</cell><cell>.709</cell><cell>51 .401</cell></row><row><cell>Objective</cell><cell>Â· w/o contrastive obj Â· w/o classification obj</cell><cell>.685 .653</cell><cell>68 .399 67 .337</cell></row><row><cell cols="2">Concatenation, Â· [h, r] vs. [r, t]</cell><cell>.520</cell><cell>106 .204</cell></row><row><cell>e.g., Eq.(4, 7)</cell><cell>Â· [h] vs. [r, t]</cell><cell>.668</cell><cell>51 .402</cell></row><row><cell>Distance</cell><cell>Â· Bilinear</cell><cell>.605</cell><cell>79 .354</cell></row><row><cell>in Eq.(11)</cell><cell>Â· Cosine Similarity</cell><cell>.691</cell><cell>76 .439</cell></row><row><cell></cell><cell>Â·</cell><cell>.701</cell><cell>62 .406</cell></row><row><cell>Ranking Basis</cell><cell>Â· Rescale( ) +</cell><cell>.706</cell><cell>48 .408</cell></row><row><cell></cell><cell>Â· Rescale( ) Ã</cell><cell>.704</cell><cell>51 .408</cell></row><row><cell cols="3">Ensemble Model: Feature Ablation in "StAR (Self-Adp)"</cell><cell></cell></row><row><cell>Full model</cell><cell>StAR (Self-Adp)</cell><cell>.732</cell><cell>46 .551</cell></row><row><cell></cell><cell>Â· w/o hard indicator</cell><cell>.712</cell><cell>50 .540</cell></row><row><cell>Feature</cell><cell>Â· w/o ambiguity degree Â· w/o score consistency</cell><cell>.734 .720</cell><cell>45 .537 45 .540</cell></row><row><cell></cell><cell>Â· w/o self-Adp in Eq.(16)</cell><cell>.675</cell><cell>540 .524</cell></row><row><cell>0.60%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.40%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.20%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.00%</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Top-5 ranking results of candidate entities for different approaches. The first column includes incomplete triples for inference, and their labels. And the others include the ranking position and Top-5 ranked candidates where an underline denotes it is the gold entity.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Positive entity ranking position &amp; Top-5 ranked candidate entities</cell></row><row><cell>Incomplete Triple</cell><cell cols="2">StAR (Self-Adp) [Ensemble]</cell><cell></cell><cell>StAR [Textual Encoding]</cell><cell>RotatE [Graph Embedding]</cell></row><row><cell>(world war ii, has part, ?)</cell><cell cols="3">5, (world war ii, jutland, meuse river,</cell><cell>12, (world war ii, world war i, world</cell><cell>10, (jutland, world war ii,</cell></row><row><cell>â tarawa-makin</cell><cell cols="2">soissons, tarawa-makin)</cell><cell></cell><cell>war, seven years' war, meuse river)</cell><cell>somme river, verdun, soissons)</cell></row><row><cell>(clarify, hypernym, ?)</cell><cell cols="3">2, (clarify, modify, change integrity,</cell><cell>3, (clarify, straighten out, modify,</cell><cell>66, (cook , season, ready,</cell></row><row><cell>â modify</cell><cell cols="2">convert , convert )</cell><cell></cell><cell>alter, transubstantiate)</cell><cell>cook , preserve)</cell></row><row><cell>(mechanical system,</cell><cell cols="2">2, (mechanical system,</cell><cell>,</cell></row><row><cell>hypernym, ?) â</cell><cell>mechanism,</cell><cell cols="2">, machine)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Applying self-loop filter to WN18RR.</figDesc><table><row><cell></cell><cell cols="4">Hits@1 @3 @10 MR MRR</cell></row><row><cell>StAR</cell><cell>.243</cell><cell>.491 .709</cell><cell>51</cell><cell>.401</cell></row><row><cell>+ Self-loop Filter</cell><cell>.328</cell><cell>.533 .719</cell><cell>50</cell><cell>.460</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">"Textual encoding" in this paper refers to capturing contextualized information across entities and relations in a triple<ref type="bibr" target="#b48">[45]</ref>, despite previous works (as detailed in Â§3.5) using the text of a stand-alone entity/relation to enhance corresponding graph embedding.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The source code is available at https://github.com/wangbo9719/StAR_KGC.<ref type="bibr" target="#b2">3</ref> WN18 and FB15k suffer from informative value<ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b36">33]</ref>, which causes &gt; 80% of the test triples ( 1 , 1 , 2 ) can be found in the training set with another relation: ( 1 , 2 , 2 ) or ( 2 , 2 , 1 ). Dettmers et al.<ref type="bibr" target="#b13">[11]</ref> used a rule-based model that learned the inverse relation and achieved state-of-the-art results on the dataset. Thereby it is suggested they should not be used for link prediction evaluation anymore.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1522</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1522" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="5184" to="5193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1145/1376616.1376746</idno>
		<ptr target="https://doi.org/10.1145/1376616.1376746" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<editor>Jason Tsong-Li Wang</editor>
		<meeting>the ACM SIGMOD International Conference on Management of Data<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-06-10" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>GarcÃ­a-DurÃ¡n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<editor>Christopher J. C. Burges, LÃ©on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1075</idno>
		<ptr target="https://doi.org/10.18653/v1/d15-1075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">KBGAN: Adversarial Learning for Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1133</idno>
		<ptr target="https://doi.org/10.18653/v1/n18-1133" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Marilyn A. Walker, Heng Ji, and Amanda Stent</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an Architecture for Never-Ending Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1879" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010</title>
		<editor>Maria Fox and David Poole</editor>
		<meeting>the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2010-07-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-Dimensional Hyperbolic Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>RÃ©</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.617/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion WWW &apos;21</title>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>Ljubljana, Slovenia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<idno type="DOI">10.18653/v1/D19-1431</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1431" />
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4216" to="4225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a Similarity Metric Discriminatively, with Application to Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2005.202</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2005.202" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-Scale Named Entity Disambiguation Based on Wikipedia Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic, Jason Eisner</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-28" />
		</imprint>
	</monogr>
	<note>EMNLP-CoNLL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acl</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D07-1074/" />
		<imprint>
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploring the Combination of Contextual Word Embeddings and Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Dieudonat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phyllicia</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Marquer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08371</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Ulrike von Luxburg, Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anusha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1162</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Long Papers, Regina Barzilay and Min-Yen Kan</editor>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1766" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09090</idno>
		<ptr target="http://arxiv.org/abs/1605.09090" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1466</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1466" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<editor>Long Papers, Anna Korhonen, David R. Traum, and LluÃ­s MÃ rquez</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2053</idno>
		<ptr target="https://doi.org/10.18653/v1/n18-2053" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<editor>Marilyn A. Walker, Heng Ji, and Amanda Stent</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1226</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1226" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2180" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/d14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL. ACL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1202</idno>
		<ptr target="https://doi.org/10.18653/v1/n18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<editor>Marilyn A. Walker, Heng Ji, and Amanda Stent</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldo</forename><surname>Gangemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93417-4_38" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<editor>Maria-Esther Vidal, Pascal Hitzler, RaphaÃ«l Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam</editor>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-06-03" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>Proceedings (Lecture Notes in Computer Science)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Open-World Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16055" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reasoning With Neural Tensor Networks for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<editor>Christopher J. C. Burges, LÃ©on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Carey L</publisher>
			<date type="published" when="2007-05-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Zurko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel-Schneider</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242667</idno>
		<ptr target="https://doi.org/10.1145/1242572.1242667" />
		<editor>Prashant J. Shenoy</editor>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkgEQnRqYQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Re-evaluation of Knowledge Graph Completion Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.489/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5516" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1174</idno>
		<ptr target="https://doi.org/10.18653/v1/d15-1174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>LluÃ­s MÃ rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton</editor>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ThÃ©o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ãric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>KrÃ¶tzsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2629489</idno>
		<ptr target="https://doi.org/10.1145/2629489" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Knowledge Graph and Text Jointly Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1167</idno>
		<ptr target="https://doi.org/10.3115/v1/d14-1167" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Alessandro Moschitti, Bo Pang, and Walter Daelemans</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14306" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>Satinder P. Singh and Shaul Markovitch</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3104" to="3110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Representation Learning of Knowledge Graphs with Entity Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12216" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<editor>Schuurmans and Michael P. Wellman</editor>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA, Dale</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052558</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052558" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<editor>Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy Gabrilovich</editor>
		<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="page" from="1271" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">One-Shot Relational Learning for Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1223</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1223" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun&apos;ichi Tsujii</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Knowledge Graph Representation with Jointly Structural and Textual Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/183</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/183" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<editor>Carles Sierra (Ed.). ijcai.org</editor>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="1318" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k16-1025</idno>
		<ptr target="https://doi.org/10.18653/v1/k16-1025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<editor>Goldberg and Stefan Riezler</editor>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6575" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<ptr target="http://arxiv.org/abs/1909.03193" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Collaborative Knowledge Base Embedding for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939673</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939673" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi</editor>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Quaternion Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8541-quaternion-knowledge-graph-embeddings" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Alina Beygelzimer, Florence d&apos;AlchÃ©-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
