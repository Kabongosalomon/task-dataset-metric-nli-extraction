<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Speech: Scaling up end-to-end speech recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Baidu Research -Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Speech: Scaling up end-to-end speech recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Top speech recognition systems rely on sophisticated pipelines composed of multiple algorithms and hand-engineered processing stages. In this paper, we describe an end-to-end speech system, called "Deep Speech", where deep learning supersedes these processing stages. Combined with a language model, this approach achieves higher performance than traditional methods on hard speech recognition tasks while also being much simpler. These results are made possible by training a large recurrent neural network (RNN) using multiple GPUs and thousands of hours of data. Because this system learns directly from data, we do not require specialized components for speaker adaptation or noise filtering. In fact, in settings where robustness to speaker variation and noise are critical, our system excels: Deep Speech outperforms previously published methods on the Switchboard Hub5'00 corpus, achieving 16.0% error, and performs better than commercial systems in noisy speech recognition tests.</p><p>Traditional speech systems use many heavily engineered processing stages, including specialized input features, acoustic models, and Hidden Markov Models (HMMs). To improve these pipelines, domain experts must invest a great deal of effort tuning their features and models. The introduction of deep learning algorithms <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9]</ref> has improved speech system performance, usually by improving acoustic models. While this improvement has been significant, deep learning still plays only a limited role in traditional speech pipelines. As a result, to improve performance on a task such as recognizing speech in a noisy environment, one must laboriously engineer the rest of the system for robustness. In contrast, our system applies deep learning end-to-end using recurrent neural networks. We take advantage of the capacity provided by deep learning systems to learn from large datasets to improve our overall performance. Our model is trained end-to-end to produce transcriptions and thus, with sufficient data and computing power, can learn robustness to noise or speaker variation on its own.</p><p>Tapping the benefits of end-to-end deep learning, however, poses several challenges: (i) we must find innovative ways to build large, labeled training sets and (ii) we must be able to train networks that are large enough to effectively utilize all of this data. One challenge for handling labeled data in speech systems is finding the alignment of text transcripts with input speech. This problem has been addressed by Graves, Fern√°ndez, Gomez and Schmidhuber <ref type="bibr" target="#b12">[13]</ref>, thus enabling neural networks to easily consume unaligned, transcribed audio during training. Meanwhile, rapid training of large neural networks has been tackled by Coates et al. <ref type="bibr" target="#b6">[7]</ref>, demonstrating the speed advantages of multi-GPU computation. We aim to leverage these insights to fulfill the vision of a generic learning system, based on large speech datasets and scalable RNN training, that can surpass more complicated traditional methods. This vision is inspired partly by the work of Lee et. al. <ref type="bibr" target="#b26">[27]</ref> who applied early unsupervised feature learning techniques to replace hand-built speech features.</p><p>We have chosen our RNN model specifically to map well to GPUs and we use a novel model partition scheme to improve parallelization. Additionally, we propose a process for assembling large quantities of labeled speech data exhibiting the distortions that our system should learn to handle. Using a combination of collected and synthesized data, our system learns robustness to realistic noise and speaker variation (including Lombard Effect <ref type="bibr" target="#b19">[20]</ref>). Taken together, these ideas suffice to build an end-to-end speech system that is at once simpler than traditional pipelines yet also performs better on difficult speech tasks. Deep Speech achieves an error rate of 16.0% on the full Switchboard Hub5'00 test set-the best published result. Further, on a new noisy speech recognition dataset of our own construction, our system achieves a word error rate of 19.1% where the best commercial systems achieve 30.5% error.</p><p>In the remainder of this paper, we will introduce the key ideas behind our speech recognition system. We begin by describing the basic recurrent neural network model and training framework that we use in Section 2, followed by a discussion of GPU optimizations (Section 3), and our data capture and synthesis strategy (Section 4). We conclude with our experimental results demonstrating the state-of-the-art performance of Deep Speech (Section 5), followed by a discussion of related work and our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RNN Training Setup</head><p>The core of our system is a recurrent neural network (RNN) trained to ingest speech spectrograms and generate English text transcriptions. Let a single utterance x and label y be sampled from a training set X = {(x <ref type="bibr" target="#b0">(1)</ref> , y <ref type="bibr" target="#b0">(1)</ref> ), (x <ref type="bibr" target="#b1">(2)</ref> , y <ref type="bibr" target="#b1">(2)</ref> ), . . .}. Each utterance, x (i) , is a time-series of length T (i) where every time-slice is a vector of audio features, x (i) t , t = 1, . . . , T (i) . We use spectrograms as our features, so x (i) t,p denotes the power of the p'th frequency bin in the audio frame at time t. The goal of our RNN is to convert an input sequence x into a sequence of character probabilities for the transcription y, with≈∑ t = P(c t |x), where c t ‚àà {a,b,c, . . . , z, space, apostrophe, blank}.</p><p>Our RNN model is composed of 5 layers of hidden units. For an input x, the hidden units at layer l are denoted h (l) with the convention that h (0) is the input. The first three layers are not recurrent. For the first layer, at each time t, the output depends on the spectrogram frame x t along with a context of C frames on each side. <ref type="bibr" target="#b0">1</ref> The remaining non-recurrent layers operate on independent data for each time step. Thus, for each time t, the first 3 layers are computed by:</p><formula xml:id="formula_0">h (l) t = g(W (l) h (l‚àí1) t + b (l) )</formula><p>where g(z) = min{max{0, z}, 20} is the clipped rectified-linear (ReLu) activation function and W (l) , b (l) are the weight matrix and bias parameters for layer l. <ref type="bibr" target="#b1">2</ref> The fourth layer is a bi-directional recurrent layer <ref type="bibr" target="#b37">[38]</ref>. This layer includes two sets of hidden units: a set with forward recurrence, h (f ) , and a set with backward recurrence h <ref type="bibr">(b)</ref> :</p><formula xml:id="formula_1">h (f ) t = g(W (4) h (3) t + W (f ) r h (f ) t‚àí1 + b (4) ) h (b) t = g(W (4) h (3) t + W (b) r h (b) t+1 + b (4) )</formula><p>Note that h (f ) must be computed sequentially from t = 1 to t = T (i) for the i'th utterance, while the units h (b) must be computed sequentially in reverse from t = T (i) to t = 1.</p><p>The fifth (non-recurrent) layer takes both the forward and backward units as inputs h</p><formula xml:id="formula_2">(5) t = g(W (5) h (4) t + b (5) ) where h (4) t = h (f ) t + h (b)</formula><p>t . The output layer is a standard softmax function that yields the predicted character probabilities for each time slice t and character k in the alphabet:</p><formula xml:id="formula_3">h (6) t,k =≈∑ t,k ‚â° P(c t = k|x) = exp(W (6) k h (5) t + b (6) k ) j exp(W (6) j h (5) t + b (6) j )</formula><p>.</p><p>Here W <ref type="bibr" target="#b5">(6)</ref> k and b <ref type="bibr" target="#b5">(6)</ref> k denote the k'th column of the weight matrix and k'th bias, respectively. Once we have computed a prediction for P(c t |x), we compute the CTC loss <ref type="bibr" target="#b12">[13]</ref> L(≈∑, y) to measure the error in prediction. During training, we can evaluate the gradient ‚àá≈∑L(≈∑, y) with respect to the network outputs given the ground-truth character sequence y. From this point, computing the gradient with respect to all of the model parameters may be done via back-propagation through the rest of the network. We use Nesterov's Accelerated gradient method for training <ref type="bibr" target="#b40">[41]</ref>. 3 <ref type="figure">Figure 1</ref>: Structure of our RNN model and notation. The complete RNN model is illustrated in <ref type="figure">Figure 1</ref>. Note that its structure is considerably simpler than related models from the literature <ref type="bibr" target="#b13">[14]</ref>-we have limited ourselves to a single recurrent layer (which is the hardest to parallelize) and we do not use Long-Short-Term-Memory (LSTM) circuits. One disadvantage of LSTM cells is that they require computing and storing multiple gating neuron responses at each step. Since the forward and backward recurrences are sequential, this small additional cost can become a computational bottleneck. By using a homogeneous model we have made the computation of the recurrent activations as efficient as possible: computing the ReLu outputs involves only a few highly optimized BLAS operations on the GPU and a single point-wise nonlinearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Regularization</head><p>While we have gone to significant lengths to expand our datasets (c.f. Section 4), the recurrent networks we use are still adept at fitting the training data. In order to reduce variance further, we use several techniques.</p><p>During training we apply a dropout <ref type="bibr" target="#b18">[19]</ref> rate between 5% -10%. We apply dropout in the feedforward layers but not to the recurrent hidden activations.</p><p>A commonly employed technique in computer vision during network evaluation is to randomly jitter inputs by translations or reflections, feed each jittered version through the network, and vote or average the results <ref type="bibr" target="#b22">[23]</ref>. Such jittering is not common in ASR, however we found it beneficial to translate the raw audio files by 5ms (half the filter bank step size) to the left and right, then forward propagate the recomputed features and average the output probabilities. At test time we also use an ensemble of several RNNs, averaging their outputs in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Model</head><p>When trained from large quantities of labeled speech data, the RNN model can learn to produce readable character-level transcriptions. Indeed for many of the transcriptions, the most likely character sequence predicted by the RNN is exactly correct without external language constraints. The errors made by the RNN in this case tend to be phonetically plausible renderings of English words- <ref type="table" target="#tab_0">Table 1</ref> shows some examples. Many of the errors occur on words that rarely or never appear in our training set. In practice, this is hard to avoid: training from enough speech data to hear all of the words or language constructions we might need to know is impractical. Therefore, we integrate our system with an N-gram language model since these models are easily trained from huge unlabeled text corpora. For comparison, while our speech datasets typically include up to 3 million utterances, the N-gram language model used for the experiments in Section 5.2 is trained from a corpus of 220 million phrases, supporting a vocabulary of 495,000 words. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN output</head><p>Decoded Transcription what is the weather like in bostin right now what is the weather like in boston right now prime miniter nerenr modi prime minister narendra modi arther n tickets for the game are there any tickets for the game Given the output P(c|x) of our RNN we perform a search to find the sequence of characters c 1 , c 2 , . . . that is most probable according to both the RNN output and the language model (where the language model interprets the string of characters as words). Specifically, we aim to find a sequence c that maximizes the combined objective:</p><formula xml:id="formula_4">Q(c) = log(P(c|x)) + Œ± log(P lm (c)) + Œ≤ word count(c)</formula><p>where Œ± and Œ≤ are tunable parameters (set by cross-validation) that control the trade-off between the RNN, the language model constraint and the length of the sentence. The term P lm denotes the probability of the sequence c according to the N-gram model. We maximize this objective using a highly optimized beam search algorithm, with a typical beam size in the range 1000-8000-similar to the approach described by Hannun et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>and thus efficient computation is critical to make our experiments feasible. We use multi-GPU training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref> to accelerate our experiments, but doing this effectively requires some additional work, as we explain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data parallelism</head><p>In order to process data efficiently, we use two levels of data parallelism. First, each GPU processes many examples in parallel. This is done in the usual way by concatenating many examples into a single matrix. For instance, rather than performing a single matrix-vector multiplication W r h t in the recurrent layer, we prefer to do many in parallel by computing</p><formula xml:id="formula_5">W r H t where H t = [h (i) t , h (i+1) t , . . .] (where h (i) t corresponds to the i'th example x (i) at time t).</formula><p>The GPU is most efficient when H t is relatively wide (e.g., 1000 examples or more) and thus we prefer to process as many examples on one GPU as possible (up to the limit of GPU memory).</p><p>When we wish to use larger minibatches than a single GPU can support on its own we use data parallelism across multiple GPUs, with each GPU processing a separate minibatch of examples and then combining its computed gradient with its peers during each iteration. We typically use 2√ó or 4√ó data parallelism across GPUs.</p><p>Data parallelism is not easily implemented, however, when utterances have different lengths since they cannot be combined into a single matrix multiplication. We resolve the problem by sorting our training examples by length and combining only similarly-sized utterances into minibatches, padding with silence when necessary so that all utterances in a batch have the same length. This solution is inspired by the ITPACK/ELLPACK sparse matrix format <ref type="bibr" target="#b20">[21]</ref>; a similar solution was used by the Sutskever et al. <ref type="bibr" target="#b41">[42]</ref> to accelerate RNNs for text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model parallelism</head><p>Data parallelism yields training speedups for modest multiples of the minibatch size (e.g., 2 to 4), but faces diminishing returns as batching more examples into a single gradient update fails to improve the training convergence rate. That is, processing 2√ó as many examples on 2√ó as many GPUs fails to yield a 2√ó speedup in training. It is also inefficient to fix the total minibatch size but spread out the examples to 2√ó as many GPUs: as the minibatch within each GPU shrinks, most operations become memory-bandwidth limited. To scale further, we parallelize by partitioning the model ("model parallelism" <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>).</p><p>Our model is challenging to parallelize due to the sequential nature of the recurrent layers. Since the bidirectional layer is comprised of a forward computation and a backward computation that are independent, we can perform the two computations in parallel. Unfortunately, naively splitting the RNN to place h (f ) and h (b) on separate GPUs commits us to significant data transfers when we go to compute h <ref type="bibr" target="#b4">(5)</ref> (which depends on both h (f ) and h (b) ). Thus, we have chosen a different partitioning of work that requires less communication for our models: we divide the model in half along the time dimension.</p><p>All layers except the recurrent layer can be trivially decomposed along the time dimension, with the first half of the time-series, from t = 1 to t = T (i) /2, assigned to one GPU and the second half to another GPU. When computing the recurrent layer activations, the first GPU begins computing the forward activations h (f ) , while the second begins computing the backward activations h <ref type="bibr">(b)</ref> . At and swap roles. The first GPU then finishes the backward computation of h (b) and the second GPU finishes the forward computation of h (f ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Striding</head><p>We have worked to minimize the running time of the recurrent layers of our RNN, since these are the hardest to parallelize. As a final optimization, we shorten the recurrent layers by taking "steps" (or strides) of size 2 in the original input so that the unrolled RNN has half as many steps. This is similar to a convolutional network <ref type="bibr" target="#b24">[25]</ref> with a step-size of 2 in the first layer. We use the cuDNN library <ref type="bibr" target="#b1">[2]</ref> to implement this first layer of convolution efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Type</head><p>Hours <ref type="table" target="#tab_1">Speakers   WSJ  read  80  280  Switchboard conversational  300  4000  Fisher  conversational 2000  23000  Baidu</ref> read 5000 9600 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Data</head><p>Large-scale deep learning systems require an abundance of labeled data. For our system we need many recorded utterances and corresponding English transcriptions, but there are few public datasets of sufficient scale. To train our largest models we have thus collected an extensive dataset consisting of 5000 hours of read speech from 9600 speakers. For comparison, we have summarized the labeled datasets available to us in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthesis by superposition</head><p>To expand our potential training data even further we use data synthesis, which has been successfully applied in other contexts to amplify the effective number of training samples <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>. In our work, the goal is primarily to improve performance in noisy environments where existing systems break down. Capturing labeled data (e.g., read speech) from noisy environments is not practical, however, and thus we must find other ways to generate such data.</p><p>To a first order, audio signals are generated through a process of superposition of source signals. We can use this fact to synthesize noisy training data. For example, if we have a speech audio track x (i) and a "noise" audio track Œæ (i) , then we can form the "noisy speech" trackx (i) = x (i) + Œæ (i) to simulate audio captured in a noisy environment. If necessary, we can add reverberations, echoes or other forms of damping to the power spectrum of Œæ (i) or x (i) and then simply add them together to make fairly realistic audio scenes.</p><p>There are, however, some risks in this approach. For example, in order to take 1000 hours of clean speech and create 1000 hours of noisy speech, we will need unique noise tracks spanning roughly 1000 hours. We cannot settle for, say, 10 hours of repeating noise, since it may become possible for the recurrent network to memorize the noise track and "subtract" it out of the synthesized data. Thus, instead of using a single noise source Œæ (i) with a length of 1000 hours, we use a large number of shorter clips (which are easier to collect from public video sources) and treat them as separate sources of noise before superimposing all of them:</p><formula xml:id="formula_6">x (i) = x (i) + Œæ (i) 1 + Œæ (i) 2 + . . .</formula><p>. When superimposing many signals collected from video clips, we can end up with "noise" sounds that are different from the kinds of noise recorded in real environments. To ensure a good match between our synthetic data and real data, we rejected any candidate noise clips where the average power in each frequency band differed significantly from the average power observed in real noisy recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Capturing Lombard Effect</head><p>One challenging effect encountered by speech recognition systems in noisy environments is the "Lombard Effect" <ref type="bibr" target="#b19">[20]</ref>: speakers actively change the pitch or inflections of their voice to overcome noise around them. This (involuntary) effect does not show up in recorded speech datasets since they are collected in quiet environments. To ensure that the effect is represented in our training data we induce the Lombard effect intentionally during data collection by playing loud background noise through headphones worn by a person as they record an utterance. The noise induces them to inflect their voice, thus allowing us to capture the Lombard effect in our training data. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We performed two sets of experiments to evaluate our system. In both cases we use the model described in Section 2 trained from a selection of the datasets in <ref type="table" target="#tab_1">Table 2</ref> to predict character-level transcriptions. The predicted probability vectors and language model are then fed into our decoder to yield a word-level transcription, which is compared with the ground truth transcription to yield the word error rate (WER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Conversational speech: Switchboard Hub5'00 (full)</head><p>To compare our system to prior research we use an accepted but highly challenging test set, Hub5'00 (LDC2002S23). Some researchers split this set into "easy" (Switchboard) and "hard" (CallHome) instances, often reporting new results on the easier portion alone. We use the full set, which is the most challenging case and report the overall word error rate. We evaluate our system trained on only the 300 hour Switchboard conversational telephone speech dataset and trained on both Switchboard (SWB) and Fisher (FSH) <ref type="bibr" target="#b2">[3]</ref>, a 2000 hour corpus collected in a similar manner as Switchboard. Many researchers evaluate models trained only with 300 hours from Switchboard conversational telephone speech when testing on Hub5'00. In part this is because training on the full 2000 hour Fisher corpus is computationally difficult. Using the techniques mentioned in Section 3 our system is able perform a full pass over the 2300 hours of data in just a few hours.</p><p>Since the Switchboard and Fisher corpora are distributed at a sample rate of 8kHz, we compute spectrograms of 80 linearly spaced log filter banks and an energy term. The filter banks are computed over windows of 20ms strided by 10ms. We did not evaluate more sophisticated features such as the mel-scale log filter banks or the mel-frequency cepstral coefficients.</p><p>Speaker adaptation is critical to the success of current ASR systems <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b35">36]</ref>, particularly when trained on 300 hour Switchboard. For the models we test on Hub5'00, we apply a simple form of speaker adaptation by normalizing the spectral features on a per speaker basis. Other than this, we do not modify the input features in any way.</p><p>For decoding, we use a 4-gram language model with a 30,000 word vocabulary trained on the Fisher and Switchboard transcriptions. Again, hyperparameters for the decoding objective are chosen via cross-validation on a held-out development set.</p><p>The Deep Speech SWB model is a network of 5 hidden layers each with 2048 neurons trained on only 300 hour switchboard. The Deep Speech SWB + FSH model is an ensemble of 4 RNNs each with 5 hidden layers of 2304 neurons trained on the full 2300 hour combined corpus. All networks are trained on inputs of +/-9 frames of context.</p><p>We report results in  <ref type="table" target="#tab_2">Table 3</ref>: Published error rates (%WER) on Switchboard dataset splits. The columns labeled "SWB" and "CH" are respectively the easy and hard subsets of Hub5'00.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Noisy speech</head><p>Few standards exist for testing noisy speech performance, so we constructed our own evaluation set of 100 noisy and 100 noise-free utterances from 10 speakers. The noise environments included a background radio or TV; washing dishes in a sink; a crowded cafeteria; a restaurant; and inside a car driving in the rain. The utterance text came primarily from web search queries and text messages, as well as news clippings, phone conversations, Internet comments, public speeches, and movie scripts. We did not have precise control over the signal-to-noise ratio (SNR) of the noisy samples, but we aimed for an SNR between 2 and 6 dB.</p><p>For the following experiments, we train our RNNs on all the datasets (more than 7000 hours) listed in <ref type="table" target="#tab_1">Table 2</ref>. Since we train for 15 to 20 epochs with newly synthesized noise in each pass, our model learns from over 100,000 hours of novel data. We use an ensemble of 6 networks each with 5 hidden layers of 2560 neurons. No form of speaker adaptation is applied to the training or evaluation sets. We normalize training examples on a per utterance basis in order to make the total power of each example consistent. The features are 160 linearly spaced log filter banks computed over windows of 20ms strided by 10ms and an energy term. Audio files are resampled to 16kHz prior to the featurization. Finally, from each frequency bin we remove the global mean over the training set and divide by the global standard deviation, primarily so the inputs are well scaled during the early stages of training.</p><p>As described in Section 2.2, we use a 5-gram language model for the decoding. We train the language model on 220 million phrases of the Common Crawl 6 , selected such that at least 95% of the characters of each phrase are in the alphabet. Only the most common 495,000 words are kept, the rest remapped to an UNKNOWN token.</p><p>We compared the Deep Speech system to several commercial speech systems: (1) wit.ai, (2) Google Speech API, (3) Bing Speech and (4) Apple Dictation. <ref type="bibr" target="#b6">7</ref> Our test is designed to benchmark performance in noisy environments. This situation creates challenges for evaluating the web speech APIs: these systems will give no result at all when the SNR is too low or in some cases when the utterance is too long. Therefore we restrict our comparison to the subset of utterances for which all systems returned a non-empty result. <ref type="bibr" target="#b7">8</ref> The results of evaluating each system on our test files appear in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>To evaluate the efficacy of the noise synthesis techniques described in Section 4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Several parts of our work are inspired by previous results. Neural network acoustic models and other connectionist approaches were first introduced to speech pipelines in the early 1990s <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b10">11]</ref>. These systems, similar to DNN acoustic models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9]</ref>, replace only one stage of the speech recognition pipeline. Mechanically, our system is similar to other efforts to build end-to-end speech systems from deep learning algorithms. For example, Graves et al. <ref type="bibr" target="#b12">[13]</ref> have previously introduced the "Connectionist Temporal Classification" (CTC) loss function for scoring transcriptions produced by RNNs and, with LSTM networks, have previously applied this approach to speech <ref type="bibr" target="#b13">[14]</ref>. We similarly adopt the CTC loss for part of our training procedure but use much simpler recurrent networks with rectified-linear activations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. Our recurrent network is similar to the bidirectional RNN used by Hannun et al. <ref type="bibr" target="#b15">[16]</ref>, but with multiple changes to enhance its scalability. By focusing on scalability, we have shown that these simpler networks can be effective even without the more complex LSTM machinery.</p><p>Our work is certainly not the first to exploit scalability to improve performance of DL algorithms. The value of scalability in deep learning is well-studied <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref> and the use of parallel processors (including GPUs) has been instrumental to recent large-scale DL results <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b23">24]</ref>. Early ports of DL algorithms to GPUs revealed significant speed gains <ref type="bibr" target="#b32">[33]</ref>. Researchers have also begun choosing designs that map well to GPU hardware to gain even more efficiency, including convolutional <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref> and locally connected <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref> networks, especially when optimized libraries like cuDNN <ref type="bibr" target="#b1">[2]</ref> and BLAS are available. Indeed, using high-performance computing infrastructure, it is possible today to train neural networks with more than 10 billion connections <ref type="bibr" target="#b6">[7]</ref> using clusters of GPUs. These results inspired us to focus first on making scalable design choices to efficiently utilize many GPUs before trying to engineer the algorithms and models themselves.</p><p>With the potential to train large models, there is a need for large training sets as well. In other fields, such as computer vision, large labeled training sets have enabled significant leaps in performance as they are used to feed larger and larger DL systems <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b22">23]</ref>. In speech recognition, however, such large training sets are less common, with typical benchmarks having training sets ranging from tens of hours (e.g. the Wall Street Journal corpus with 80 hours) to several hundreds of hours (e.g. Switchboard and Broadcast News). Larger benchmark datasets, such as the Fisher corpus <ref type="bibr" target="#b2">[3]</ref> with 2000 hours of transcribed speech, are rare and only recently being studied. To fully utilize the expressive power of the recurrent networks available to us, we rely not only on large sets of labeled utterances, but also on synthesis techniques to generate novel examples. This approach is well known in computer vision <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref> but we have found this especially convenient and effective for speech when done properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented an end-to-end deep learning-based speech system capable of outperforming existing state-of-the-art recognition pipelines in two challenging scenarios: clear, conversational speech and speech in noisy environments. Our approach is enabled particularly by multi-GPU training and by data collection and synthesis strategies to build large training sets exhibiting the distortions our system must handle (such as background noise and Lombard effect). Combined, these solutions enable us to build a data-driven speech system that is at once better performing than existing methods while no longer relying on the complex processing stages that had stymied further progress. We believe this approach will continue to improve as we capitalize on increased computing power and dataset sizes in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the mid-point (t = T (i) /2), the two GPUs exchange the intermediate activations, h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of transcriptions directly from the RNN (left) with errors that are fixed by addition of a language model (right).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>A summary of the datasets used to train Deep Speech. The Wall Street Journal, Switchboard and Fisher<ref type="bibr" target="#b2">[3]</ref> corpora are all published by the Linguistic Data Consortium.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The model from Vesely et al. (DNN-GMM sMBR) [44] uses a sequence based loss function on top of a DNN after using a typical hybrid DNN-HMM system to realign the training set. The performance of this model on the combined Hub5'00 test set is the best previously published result. When trained on the combined 2300 hours of data the Deep Speech system improves upon this baseline by 2.4% absolute WER and 13.0% relative. The model from Maas et al. (DNN-HMM FSH) [28] achieves 19.9% WER when trained on the Fisher 2000 hour corpus.That system was built using Kaldi<ref type="bibr" target="#b31">[32]</ref>, state-of-the-art open source speech recognition software. We include this result to demonstrate that Deep Speech, when trained on a comparable amount of data is competitive with the best existing ASR systems.</figDesc><table><row><cell>Model</cell><cell cols="3">SWB CH Full</cell></row><row><cell>Vesely et al. (GMM-HMM BMMI) [44]</cell><cell cols="3">18.6 33.0 25.8</cell></row><row><cell>Vesely et al. (DNN-HMM sMBR) [44]</cell><cell cols="3">12.6 24.1 18.4</cell></row><row><cell>Maas et al. (DNN-HMM SWB) [28]</cell><cell cols="3">14.6 26.3 20.5</cell></row><row><cell>Maas et al. (DNN-HMM FSH) [28]</cell><cell cols="3">16.0 23.7 19.9</cell></row><row><cell>Seide et al. (CD-DNN) [39]</cell><cell>16.1</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell cols="2">Kingsbury et al. (DNN-HMM sMBR HF) [22] 13.3</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>Sainath et al. (CNN-HMM) [36]</cell><cell>11.5</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>Soltau et al. (MLP/CNN+I-Vector) [40]</cell><cell>10.4</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>Deep Speech SWB</cell><cell cols="3">20.0 31.8 25.9</cell></row><row><cell>Deep Speech SWB + FSH</cell><cell cols="3">12.6 19.3 16.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1, we trained two RNNs, one on 5000 hours of raw data and the other trained on the same 5000 hours plus noise. On the 100 clean utterances both models perform about the same, 9.2% WER and 9.0% WER for the clean trained model and the noise trained model respectively. However, on the 100 noisy utterances the noisy model achieves 22.6% WER over the clean model's 28.7% WER, a 6.1% absolute and 21.3% relative improvement.</figDesc><table><row><cell>System</cell><cell cols="3">Clean (94) Noisy (82) Combined (176)</cell></row><row><cell>Apple Dictation</cell><cell>14.24</cell><cell>43.76</cell><cell>26.73</cell></row><row><cell>Bing Speech</cell><cell>11.73</cell><cell>36.12</cell><cell>22.05</cell></row><row><cell>Google API</cell><cell>6.64</cell><cell>30.47</cell><cell>16.72</cell></row><row><cell>wit.ai</cell><cell>7.94</cell><cell>35.06</cell><cell>19.41</cell></row><row><cell>Deep Speech</cell><cell>6.56</cell><cell>19.06</cell><cell>11.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results (%WER) for 5 systems evaluated on the original audio. Scores are reported only for utterances with predictions given by all systems. The number in parentheses next to each dataset, e.g. Clean (94), is the number of utterances scored.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We typically use C ‚àà {5, 7, 9} for our experiments.<ref type="bibr" target="#b1">2</ref> The ReLu units are clipped in order to keep the activations in the recurrent layer from exploding; in practice the units rarely saturate at the upper bound.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use momentum of 0.99 and anneal the learning rate by a constant factor, chosen to yield the fastest convergence, after each epoch through the data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">OptimizationsAs noted above, we have made several design decisions to make our networks amenable to highspeed execution (and thus fast training). For example, we have opted for homogeneous rectifiedlinear networks that are simple to implement and depend on just a few highly-optimized BLAS calls. When fully unrolled, our networks include almost 5 billion connections for a typical utterance<ref type="bibr" target="#b3">4</ref> We use the KenLM toolkit<ref type="bibr" target="#b16">[17]</ref> to train the N-gram language models in our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We have experimented with noise played through headphones as well as through computer speakers. Using headphones has the advantage that we obtain "clean" recordings without the background noise included and can add our own synthetic noise afterward.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">commoncrawl.org<ref type="bibr" target="#b6">7</ref> wit.ai and Google Speech each have HTTP-based APIs. To test Apple Dictation and Bing Speech, we used a kernel extension to loop audio output back to audio input in conjunction with the OS X Dictation service and the Windows 8 Bing speech recognition API.<ref type="bibr" target="#b7">8</ref> This leads to much higher accuracies than would be reported if we attributed 100% error in cases where an API failed to respond.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Jia Lei, whose work on DL for speech at Baidu has spurred us forward, for his advice and support throughout this project. We also thank Ian Lane, Dan Povey, Dan Jurafsky, Dario Amodei, Andrew Maas, Calisa Cole and Li Wei for helpful conversations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Connectionist Speech Recognition: A Hybrid Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Fisher corpus: a resource for the next generations of speech-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="69" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text detection and character recognition in scene images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning with COTS HPC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on AI and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Size matters: An empirical study of neural network training for large vocabulary continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1013" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern√°ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shift-invariance sparse coding for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.5241</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1408.2873</idno>
		<ptr target="http://arxiv.org/abs/1408.2873" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors. abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1406.7806" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Lombard reflex and its role on human listeners and automatic speech recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Junqua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="510" to="524" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Itpackv 2d users guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Oppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scalable minimum Bayes risk training of deep neural network acoustic models using distributed hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Increasing deep neural network acoustic model size for large vocabulary continuous speech recognition. abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Lengerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1406.7806" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesel√Ω</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-scale deep unsupervised learning using graphics processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Connectionist probability estimators in HMM speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="174" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improvements to deep convolutional neural networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aravkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A fast data collection and augmentation procedure for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Twenty-Third Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature engineering in context-dependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint training of convolutional and non-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the importance of momentum and initialization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.3215" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence-discriminative training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
