<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Merge and Label: A novel neural network architecture for nested NER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Fisher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Economics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
							<email>andreas.vlachos@cst.cam.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Merge and Label: A novel neural network architecture for nested NER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition (NER) is one of the best studied tasks in natural language processing. However, most approaches are not capable of handling nested structures which are common in many applications. In this paper we introduce a novel neural network architecture that first merges tokens and/or entities into entities forming nested structures, and then labels each of them independently. Unlike previous work, our merge and label approach predicts real-valued instead of discrete segmentation structures, which allow it to combine word and nested entity embeddings while maintaining differentiability. We evaluate our approach using the ACE 2005 Corpus, where it achieves state-of-the-art F1 of 74.6, further improved with contextual embeddings (BERT) to 82.4, an overall improvement of close to 8 F1 points over previous approaches trained on the same data. Additionally we compare it against BiLSTM-CRFs, the dominant approach for flat NER structures, demonstrating that its ability to predict nested structures does not impact performance in simpler cases. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of nested named entity recognition (NER) focuses on recognizing and classifying entities that can be nested within each other, such as "United Kingdom" and "The Prime Minister of the United Kingdom" in <ref type="figure">Figure 1</ref>. Such entity structures, while very commonly occurring, cannot be handled by the predominant variant of NER models <ref type="bibr" target="#b14">(McCallum and Li, 2003;</ref><ref type="bibr" target="#b11">Lample et al., 2016)</ref>, which can only tag non-overlapping entities.</p><p>A number of approaches have been proposed for nested NER. <ref type="bibr" target="#b12">Lu and Roth (2015)</ref> introduced a hypergraph representation which can represent 1 Code available at https://github.com/ fishjh2/merge_label overlapping mentions, which was further improved by <ref type="bibr" target="#b15">Muis and Lu (2017)</ref>, by assigning tags between each pair of consecutive words, preventing the model from learning spurious structures (overlapping entity structures which are gramatically impossible). More recently, <ref type="bibr" target="#b8">Katiyar and Cardie (2018)</ref> built on this approach, adapting an LSTM <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref> to learn the hypergraph directly, and  introduced a segmental hypergraph approach, which is able to incorporate a larger number of span based features, by encoding each span with an LSTM.</p><p>Our approach decomposes nested NER into two stages. First tokens are merged into entities (Level 1 in <ref type="figure">Figure 1</ref>), which are merged with other tokens or entities in higher levels. These merges are encoded as real-valued decisions, which enables a parameterized combination of word embeddings into entity embeddings at different levels. These entity embeddings are used to label the entities identified. The model itself consists of feedforward neural network layers and is fully differentiable, thus it is straightforward to train with backpropagation.</p><p>Unlike methods such as <ref type="bibr" target="#b8">Katiyar and Cardie (2018)</ref>, it does not predict entity segmentation at each layer as discrete 0-1 labels, thus allowing the model to flexibly aggregate information across layers. Furthermore inference is greedy, without attempting to score all possible entity spans as in , which results in faster decoding (decoding requires simply a single forward pass of the network).</p><p>To test our approach on nested NER, we evaluate it on the ACE 2005 corpus (LDC2006T06) where it achieves a state-of-the-art F1 score of 74.6. This is further improved with contextual embeddings <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> to 82.4, an overall improvement of close to 8 F1 points against the Figure 1: Trained model's representation of nested entities, after thresholding the merge values, M (see section 2.1). Note that the merging of ", to" is a mistake by the model. previous best approach trained on the same data, . Our approach is also 60 times faster than its closest competitor. Additionally, we compare it against BiLSTM-CRFs <ref type="bibr" target="#b6">(Huang et al., 2015)</ref>, the dominant flat NER paradigm, on Ontonotes (LDC2013T19) and demonstrate that its ability to predict nested structures does not impact performance in flat NER tasks as it achieves comparable results to the state of the art on this dataset.</p><p>2 Network Architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The model decomposes nested NER into two stages. Firstly, it identifies the boundaries of the named entities at all levels of nesting; the tensor M in <ref type="figure" target="#fig_0">Figure 2</ref>, which is composed of real values between 0 and 1 (these real values are used to infer discrete split/merge decisions at test time, giving the nested structure of entities shown in <ref type="figure">Figure 1</ref>). We refer to this as predicting the "structure" of the NER output for the sentence. Secondly, given this structure, it produces embeddings for each entity, by combining the embeddings of smaller entities/tokens from previous levels (i.e. there will be an embedding for each rectangle in <ref type="figure">Figure 1</ref>). These entity embeddings are used to label the entities identified.</p><p>An overview of the architecture used to predict the structure and labels is shown in <ref type="figure" target="#fig_0">Figure  2</ref>. The dimensions of each tensor are shown in square brackets in the figure. The input tensor, X, holds the word embeddings of dimension e, for every word in the input of sequence length, s. The first dimension, b, is the batch size. The Static Layer updates the token embeddings using contextual information, giving tensor X s of the same dimension, [b, s, e].</p><p>Next, for u repetitions, we go through a series of building the structure using the Structure Layer, and then use this structure to continue updating the individual token embeddings using the Update Layer, giving an output X u .</p><p>The updated token embeddings X u are passed through the Structure Layer one last time, to give the final entity embeddings, T and structure, M . A feedforward Output Layer then gives the predictions of the label of each entity.</p><p>The structure is represented by the tensor M , of dimensions [b, s − 1, L]. M holds, for every pair of adjacent words (s − 1 given input length s) and every output level (L levels), a value between 0 and 1. A value close to 0 denotes that the two (adjacent) tokens/entities from the previous level are likely to be merged on this level to form an entity; nested entities emerge when entities from lower levels are used. Note that for each individual application of the Structure Layer, we are building multiple levels (L) of nested entities. That is, within each Structure Layer there is a loop of length L. By building the structure before the Update Layer, the updates to the token embeddings can utilize information about which entities each token is in, as well as neighbouring entities, as opposed to just using information about neighbouring tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preliminaries</head><p>Before analysing each of the main layers of the network, we introduce two building blocks, which are used multiple times throughout the architecture. The first one is the Unfold operators. Given that we process whole news articles in one batch (often giving a sequence length (s) of 500 or greater) we do not allow each token in the sequence to consider every other token. Instead, we define a kernel of size k around each token, similar to convolutional neural networks <ref type="bibr" target="#b9">(Kim, 2014)</ref>, allowing it to consider the k/2 prior tokens and the k/2 following tokens. The unfold operators create kernels transforming tensors holding the word embeddings of shape [b, s, e] to shape <ref type="bibr">[b, s, k, e]</ref>. unfold <ref type="bibr">[from]</ref> simply tiles the embedding x of each token k times, and unfold <ref type="bibr">[to]</ref> generates the k/2 token embeddings either side, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, for a kernel size k of 4. The first row of the unfold <ref type="bibr">[to]</ref> tensor holds the two tokens before and the two tokens after the word "The", the second row the two before and after "President" etc. As we process whole articles, the unfold operators allow tokens to consider tokens from previous/following sentences.</p><p>The second building block is the Embed Update layer, shown in <ref type="figure">Figure 4</ref>. This layer is used to update embeddings within the model, and as such, can be thought of as equivalent in function to the residual update mechanism in Transformer <ref type="bibr">(Vaswani et al., 2017)</ref>. It is used in each of the Static Layer, Update Layer and Structure Layer from the main network architecture in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>It takes an input I of size <ref type="bibr">[b, s, k, in]</ref>, formed using the unfold ops described above, where the last dimension in varies depending on the point in the architecture at which the layer is used. It passes this input through the feedforward NN <ref type="figure">Figure 4</ref>: Embed Update layer F F EU , giving an output of dimension [b, s, k, e + 1] (the network broadcasts over the last three dimensions of the input tensor). The output is split into two. Firstly, a tensor E of shape [b, s, k, e], which holds, for each word in the sequence, k predictions of an updated word vector based on the k/2 words either side. Secondly, a weighting tensor C of shape <ref type="bibr">[b, s, k, 1]</ref>, which is scaled between 0 and 1 using the sigmoid function, and denotes how "confident" each of the k predictions is about its update to the word embedding. This works similar to an attention mechanism, allowing each token to focus on updates from the most relevant neighbouring tokens. <ref type="bibr">2</ref> The output, U is then a weighted average of E :</p><formula xml:id="formula_0">U = sum 2 (sigmoid(C ) * E )</formula><p>where sum 2 denotes summing across the second dimension of size k. U therefore has dimensions [b, s, e] and contains the updated embedding for each word.</p><p>During training we initialize the weights of the network using the identity function. As a result, the default behaviour of F F EU prior to training is to pass on the word embedding unchanged, which is then updated during via backpropagation. An example of the effect of the identity initialization is provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Static Layer</head><p>The static layer is a simple preliminary layer to update the embeddings for each word based on contextual information, and as such, is very similar to a Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> layer. Following the unfold ops, a positional encoding P of dimension e (we use a learned encoding) is added, giving tensor I s :</p><formula xml:id="formula_1">I s = concat(U nf old [f rom] (X), U nf old [to] (X)+P )</formula><p>I s is then passed through the Embed Update layer. In our experiments, we use a single static layer. There is no merging of embeddings into entities in the static layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Structure Layer</head><p>The Structure Layer is responsible for three tasks. Firstly, deciding which token embeddings should be merged at each level, expressed as real values between 0 and 1, and denoted M . Secondly, given these merge values M , deciding how the separate token embeddings should be combined in order to give the embeddings for each entity, T . Finally, for each token and entity, providing directional vectors D to the k/2 tokens either side, which are used to update each token embedding in the Update Layer based on its context. Intuitively, the directional vectors D can be thought of as encoding relations between entities -such as the relation between an organization and its leader, or that between a country and its capital city (see Section 6.2 for an analysis of these relation embeddings). <ref type="figure" target="#fig_3">Figure 6</ref> shows a minimal example of the calculation of D, M and T , with word embedding and directional vector dimensions e = d = 2, and kernel size, k = 4. We pass the embeddings (X) of each pair of adjacent words through a feedforward NN F F S to give directions D [b, s-1, d] and merge values M [b, s-1, 1] between each pair. If F F S predicts M (1,2) to be close to 0, this indicates that tokens 1 and 2 are part of the same entity on this level. The unfold <ref type="bibr">[to]</ref> op gives, for each word (we show only the unfolded tensors for the word "Kingdom" in <ref type="figure" target="#fig_3">Figure 6</ref> for simplicity), D and M for pairs of words up to k/2 either side. for direction of the two cumsum ops), we get directional vectors and merge values from the word "Kingdom" to the words before and after it in the phrase, D 3,i and M 3,i for i = (1, 2, 4, 5). Note that we take the inverse of vectors D (1,2) and D (2,3) prior to the cumsum, as we are interested in the directions from the token "Kingdom" backwards to the tokens "United" and "The". The values M 3,i are converted to weights W of dimension [b, s, k, 1] using the formula W = max(0, 1 − M ) 3 , with the max operation ensuring the model puts a weight of zero on tokens in separate entities (see the reduction of the value of 1.7 in M in <ref type="figure" target="#fig_3">Figure 6</ref> to a weighting of 0.0). The weights are normalized to sum to 1, and multiplied with the unfolded token embeddings X to give the entity embeddings T , of dimension [b, s, e]</p><formula xml:id="formula_2">T = W sum 2 (W ) * X</formula><p>Consequently, the embeddings at the end of level 1 for the words "The", "United" and "Kingdom"</p><p>(T 1 1 , T 1 2 and T 1 3 respectively) are all now close to equal, and all have been formed from a weighted average of the three separate token embeddings. If M (1,2) and M (2,3) were precisely zero, and M (3,4 ) was precisely 1.0, then all three would be identical. In addition, on higher levels, the directions from other words to each of these three tokens will also be identical. In other words, the use of "directions" 4 allows the network to represent entities as a single embedding in a fully differentiable fashion, whilst keeping the sequence length constant. <ref type="figure" target="#fig_3">Figure 6</ref> shows just a single level from within the Structure Layer. The embeddings T are then passed onto the next level, allowing progressively larger entities to be formed by combining smaller entities from the previous levels. The full architecture of the Structure Layer is shown in <ref type="figure">Figure 7</ref>. The main difference to <ref type="figure" target="#fig_3">Figure  6</ref> is the additional use of Embed Update Layer, to decide how individual token/entity embeddings are combined together into a single entity. The reason for this is that if we are joining the words "The", "United" and "Kingdom" into a single entity, it makes sense that the joint vector should be based largely on the embeddings of "United" and "Kingdom", as "The" should add little information. The embeddings are unfolded (using the unfold <ref type="bibr">[from]</ref> op) to shape [b, s, k, e] and concatenated with the directions between words, D , to give the tensor of shape <ref type="bibr">[b, s, k, e + d]</ref>. This is passed through the Embed Update layer, giving, for each word, a weighted and updated embedding, ready to be combined into a single entity (for unimportant words like "The", this embedding will have been reduced to close to zero). We use this tensor in place of tensor X in <ref type="figure" target="#fig_3">Figure 6</ref>, and multiply with the weights W to give the new entity embeddings, T .</p><p>There are four separate outputs from the Structure Layer. The first, denoted by T , is the entity embeddings from each of the levels concatenated together, giving a tensor of size <ref type="bibr">[b, s, e, L]</ref>. The second output, R , is a weighted average of the embeddings from different layers, of shape <ref type="bibr">[b, s, k, e]</ref>. This will be used in the place of the unfold [to] tensor described above as an input the the Update Layer. It holds, for each token in the sequence, embeddings of entities up to k/2 tokens either side. The third output, D , will also be used by the Update Layer. It holds the directions of each token/entity to the k/2 tokens/entities either side. It is formed using the cumsum op, as shown in <ref type="figure" target="#fig_3">Figure 6</ref>. Finally, the fourth output, M , stores the merge values for every level. It is used in the loss function, to directly incentivize the correct merge decisions at the correct levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Update Layer</head><p>The Update Layer is responsible for updating the individual word vectors, using the contextual information derived from outputs R and D of the Structure Layer. It concatenates the two outputs together, along with the output of the unfold <ref type="bibr">[from]</ref> op, X s , and with an article theme embedding A tensor, giving tensor Z of dimension [b, s, k, (e*2 + d + a)]. The article theme embedding is formed by passing every word in the article through a feedforward NN, and taking a weighted average of the outputs, giving a tensor of dimension <ref type="bibr">[b, a]</ref>. This is then tiled 5 to dimension [b, s, k, a], giving tensor A. A allows the network to adjust its contextual understanding of each token based on whether the article is on finance, sports, etc. Z is then passed through an Embed Update layer, giving an output X u of shape [b, s, e]. X u = Embed U pdate(concat(X s , R, D, A))</p><p>We therefore update each word vector using four pieces of information. The original word embedding, a direction to a different token/entity, the embedding of that different token/entity, and the article theme. The use of directional vectors D in the Update Layer can be thought of as an alternative to the positional encodings in Transformer <ref type="bibr">(Vaswani et al., 2017)</ref>. That is, instead of updating each token embedding using neighbouring tokens embeddings with a positional encoding, we update using neighbouring token embeddings, and the directions to those tokens.</p><p>3 Implementation Details 3.1 Data Preprocessing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">ACE 2005</head><p>ACE 2005 is a corpus of around 180K tokens, with 7 distinct entity labels. The corpus labels include nested entities, allowing us to compare our model to the nested NER literature. The dataset is not pre-tokenized, so we carry out sentence and word tokenization using NLTK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">OntoNotes</head><p>OntoNotes v5.0 is the largest corpus available for NER, comprised of around 1.3M tokens, and 19 different entity labels. Although the labelling of the entities is not nested in OntoNotes, the corpus also includes labels for all noun phrases, which we train the network to identify concurrently. For training, we copy entities which are not contained within a larger nested entity onto higher levels, as shown in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Labelling</head><p>For both datasets, during training, we replace all "B-" labels with their corresponding "I-" label. At evaluation, all predictions which are the first word in a merged entity have the "B-" added back on. As the trained model's merging weights, M , can take any value between 0 and 1, we have to set a <ref type="figure">Figure 9</ref>: OntoNotes Labelling cutoff at eval time when deciding which words are in the same entity. We perform a grid search over cutoff values using the dev set, with a value of 0.75 proving optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss function</head><p>The model is trained to predict the correct merge decisions, held in the tensor M of dimension [b, s-1, L] and the correct class labels given these decisions, C. The merge decisions are trained directly using the mean absolute error (MAE):</p><formula xml:id="formula_3">M AE M = sum(|M −M |) (b * s * L)</formula><p>This is then weighted by a scalar w M , and added to the usual Cross Entropy (CE) loss from the predictions of the classes, CE C , giving a final loss function of the form:</p><formula xml:id="formula_4">Loss = (w M * M AE M ) + CE C</formula><p>In experiments we set the weight on the merge loss, w M to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>Following previous literature, for both the ACE and OntoNotes datasets, we use a strict F1 measure, where an entity is only considered correct if both the label and the span are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">ACE 2005</head><p>For the ACE corpus, the default metric in the literature <ref type="bibr" target="#b7">Ju et al., 2018;</ref> does not include sequential ordering of nested entities (as many architectures do not have a concept of ordered nested outputs). As a result, an entity is considered correct if it is present in the target labels, regardless of which layer the model predicts it on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">OntoNotes</head><p>NER models evaluated on OntoNotes are trained to label the 19 entities, and not noun phrases (NP).</p><p>To provide as fair as possible a comparison, we consequently flatten all labelled entities into a single column. As 96.5% of labelled entities in OntoNotes do not contain a NP nested inside, this applies to only 3.5% of the dataset. The method used to flatten the targets is shown in <ref type="figure">Figure 10</ref>. The OntoNotes labels include a named entity (TIME), in the second column, with the NP "twenty-four" minutes nested inside. Consequently, we take the model's prediction from the second column as our prediction for this entity. This provides a fair comparison to existing NER models, as all entities are included, and if anything, disadvantages our model, as it not only has to predict the correct entity, but do so on the correct level. That said, the NP labels provide additional information during training, which may give our model an advantage over flat NER models, which do not have access to these labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and HyperParameters</head><p>We performed a small amount of hyperparameter tuning across dropout, learning rate, distance embedding size d, and number of update layers u. We set dropout at 0.1, the learning rate to 0.0005, d to 200, and u to 3. For full hyperparameter details see the supplementary materials. The number of levels, L, is set to 3, with a kernel size k of 10 on the first level, 20 on the second, and 30 on the third (we increase the kernel size gradually for computational efficiency as first level entities are extremely unlikely to be composed of more than 10 tokens, whereas higher level nested entities may be larger). Training took around 10 hours for OntoNotes, and around 6 hours for ACE 2005, on an Nvidia 1080 Ti.</p><p>For experiments without language model (LM) embeddings, we used pretrained Glove embeddings <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> of dimension 300.</p><p>Following <ref type="bibr" target="#b18">(Strubell et al., 2017)</ref>, we added a "CAP features" embedding of dimension 20, denoting if each word started with a capital letter, was all capital letters, or had no capital letters. For the experiments with LM embeddings, we used the implementations of the BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and ELMO <ref type="bibr" target="#b17">(Peters et al., 2018)</ref> models from the Flair <ref type="bibr" target="#b0">(Akbik et al., 2018</ref>) project 6 . We do not finetune the BERT and ELMO models, but take their embeddings as given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ACE 2005</head><p>On the ACE 2005 corpus, we begin our analysis of our model's performance by comparing to models which do not use the POS tags as additional features, and which use non-contextual word embeddings. These are shown in the top section of <ref type="table" target="#tab_1">Table  1</ref>. The previous state-of-the-art F1 of 72.2 was set by <ref type="bibr" target="#b7">Ju et al. (2018)</ref>, using a series of stacked BiL-STM layers, with CRF decoders on top of each of them. Our model improves this result with an F1 of 74.6 (avg. over 5 runs with std. dev. of 0.4). This also brings the performance into line with  and , which concatenate embeddings of POS tags with word embeddings as an additional input feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Pr. Rec. F1</p><p>Multigraph + MS (Muis and Lu, 2017) 69.1 58.1 63.1 RNN + hyp <ref type="bibr">(Katiyar and Cardie, 2018) 70.6 70.4 70.5</ref> BiLSTM-CRF stacked <ref type="bibr" target="#b7">(Ju et al., 2018)</ref> 74.2 70.3 72.2 LSTM + forest [POS]    Given the recent success on many tasks using contextual word embeddings, we also evaluate performance using the output of pre-trained BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and ELMO <ref type="bibr" target="#b17">(Peters et al., 2018)</ref> models as input embeddings. This leads to a significant jump in performance to 78.9 with ELMO, and 82.4 with BERT (both avg. over 5 runs with 0.4 and 0.3 std. dev. respectively), an overall increase of 8 F1 points from the previous state-of-the-art. Finally, we report the concurrently published result of <ref type="bibr" target="#b13">Luan et al. (2019)</ref>, in which they use ELMO embeddings, and additional labelled data (used to train the coreference part of their model and the entity boundaries) from the larger OntoNotes dataset.</p><p>A secondary advantage of our architecture relative to those models which require construction of a hypergraph or CRF layer is its decoding speed, as decoding requires only a single forward pass of the network. As such it achieves a speed of 9468 words per second (w/s) on an Nvidia 1080 Ti GPU, relative to a reported speed of 157 w/s for the closest competitor model of , a sixty fold advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">OntoNotes</head><p>As mentioned previously, given the caveats that our model is trained to label all NPs as well as entities, and must also predict the correct layer of an entity, the results in <ref type="table" target="#tab_3">Table 2</ref> should be seen as indicative comparisons only. Using non-contextual embeddings, our model achieves a test F1 of 87.59. To our knowledge, this is the first time that a nested NER architecture has performed comparably to BiLSTM-CRFs <ref type="bibr" target="#b6">(Huang et al., 2015)</ref> (which have dominated the named entity literature for the last few years) on a flat NER task.</p><p>Given the larger size of the OntoNotes dataset, we report results from a single iteration, as opposed to the average of 5 runs as in the case of ACE05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model F1</head><p>BiLSTM-CRF (Chiu and Nichols, 2016) 86.28 ID-CNN <ref type="bibr" target="#b18">(Strubell et al., 2017)</ref> 86.84 BiLSTM-CRF <ref type="bibr" target="#b18">(Strubell et al., 2017)</ref> 86 89.20 BiLSTM-CRF Flair <ref type="bibr" target="#b0">(Akbik et al., 2018)</ref> 89.71 We also see a performance boost from using BERT embeddings, pushing the F1 up to 89.20. This falls slightly short of the state-of-the-art on this dataset, achieved using character-based Flair <ref type="bibr" target="#b0">(Akbik et al., 2018)</ref> contextual embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablations</head><p>To better understand the results, we conducted a small ablation study. The affect of including the Static Layer in the architecture is consistent across both datasets, yielding an improvement of around 2 F1 points; the updating of the token embeddings based on context seems to allow better merge decisions for each pair of tokens. Next, we look at the method used to update entity embeddings prior to combination into larger entities in the Structure Layer. In the described architecture, we use the Embed Update mechanism (see <ref type="figure">Figure 7)</ref>, allowing embeddings to be changed dependent on which other embeddings they are about to be combined with. We see that this yields a significant improvement on both tasks of around 4 F1 points, relative to passing each embedding through a linear layer.</p><p>The inclusion of an "article theme" embedding, used in the Update Layer, has little effect on the ACE05 data. but gives a notable improvement for OntoNotes. Given that the distribution of types of articles is similar for both datasets, we suggest this is due to the larger size of the OntoNotes set allowing the model to learn an informative article theme embedding without overfitting.</p><p>Next, we investigate the impact of allowing the model to attend to tokens in neighbouring sentences (we use a set kernel size of 30, allowing each token to consider up to 15 tokens prior and 15 after, regardless of sentence boundaries). Ignoring sentence boundaries boosts the results on ACE05 by around 4 F1 points, whilst having a smaller affect on OntoNotes. We hypothesize that this is due to the ACE05 task requiring the labelling of pronominal entities, such as "he" and "it", which is not required for OntoNotes. The coreference needed to correctly label their type is likely to require context beyond the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Entity Embeddings</head><p>As our architecture merges multi-word entities, it not only outputs vectors of each word, but also for all entities -the tensor T . To demonstrate this, <ref type="table" target="#tab_5">Table 3</ref> shows the ten closest entity vectors in the OntoNotes test data to the phrases "the United Kingdom", "Arab Foreign Ministers" and "Israeli   Prime Minister Ehud Barak". 7 Given that the OntoNotes NER task considers countries and cities as GPE (Geo-Political Entities), the nearest neighbours in the left hand column are expected. The nearest neighbours of "Arab Foreign Ministers" and "Israeli Prime Minister Ehud Barak" are more interesting, as there is no label for groups of people or jobs for the task. <ref type="bibr">8</ref> Despite this, the model produces good embedding-based representations of these complex higher level entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Directional Embeddings</head><p>The representation of the relationship between each pair of words/entities as a vector is primarily a mechanism used by the model to update the word/entity vectors. However, the resulting vectors, corresponding to output D of the Structure Layer, may also provide useful information for 7 Note that we exclude from the 10 nearest neighbours identical entities from higher levels. I.e. if "the United Kingdom" is kept as a three token entity, and not merged into a larger entity on higher levels, we do not report the same phrase from all levels in the nearest neighbours. <ref type="bibr">8</ref> The phrase "Israeli Prime Minister Ehud Barak" would have "Israeli" labelled as NORP, and "Ehud Barak" labelled as PERSON in the OntoNotes corpus. downstream tasks such as knowledge base population.</p><p>To demonstrate the directional embeddings, Table 5 shows the ten closest matches for the direction between "the president" and "the People's Bank of China". The network has clearly picked up on the relationship of an employee to an organisation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a novel neural network architecture for smoothly merging token embeddings in a sentence into entity embeddings, across multiple levels. The architecture performs strongly on the task of nested NER, setting a new state-of-the-art F1 score by close to 8 F1 points, and is also competitive at flat NER. Despite being trained only for NER, the architecture provides intuitive embeddings for a variety of multi-word entities, a step which we suggest could prove useful for a variety of downstream tasks, including entity linking and coreference resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 HyperParameters</head><p>In addition to the hyperparameters recorded in the main paper, there are a large number of additional hyperparameters which we kept constant throughout experiments. The feedforward NN in the Static Layer, F F s , has two hidden layers each of dimension 200. The NN in the Embed Update layer, F F EU has two hidden layers, each of dimension 320. The output NN has one hidden layer of dimension 200. Aside from F F EU , which is initialized using the identity function as described in Supplementary section A.2, all parameters of networks are initialized from the uniform distribution between -0.1 and 0.1. The article theme size, a, is set to 50. All network layers use the SELU activation function of <ref type="bibr" target="#b10">(Klambauer et al., 2017)</ref>. The kernel size k for the Static Layer is set to 6, allowing each token to attend the 3 tokens either side.</p><p>On the OntoNotes Corpus, we train for 60 epochs, and half the learning rate every 12 epochs. On ACE 2005, we train for 150 epochs, and half the learning rate every 30 epochs. We train with a maximum batch dimension of 900 tokens. Articles longer than length 900 are split and processed in separate batches. We train using the Adam Optimizer, and, in addition to the dropout of 0.1, we apply a dropout to the Glove/LM embeddings of 0.2. <ref type="figure" target="#fig_7">Figure 11</ref> gives a minimum working example of identity initialization of F F EU . The embedding for "The" is [1.1, 0.5], and that for "President" is [1.1, -0.3]. Through the unfold ops, we'll end up with the two embeddings concatenated together. <ref type="figure" target="#fig_7">Figure 11</ref> shows F F EU as having just one layer with no activation function to demonstrate the effect of the identity initialization. The first two dimensions of the output are the embedding for "The" with no changes. The final output (in light green) is the weighting. In reality, the zeros in the weights tensor are initialized to very small random numbers (we use a uniform initialization between -0.01 and 0.01), so that during training F F EU learns to update the embedding for "The" using the information that it is one step before the word "President".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Identity initialization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Formation of outputs R and D in Structure Layer</head><p>Outputs R and D of the Structure Layer have dimensions <ref type="bibr">[b,s, k, e]</ref> and <ref type="bibr">[b, s, k, d]</ref> respectively. These outputs are a weighted average of the directional and embedding outputs from the L levels of the structure layer. We use the weights, W , (see <ref type="figure" target="#fig_3">Figure 6</ref>) to form the weighted average:</p><formula xml:id="formula_5">D = L l=1 W l D l</formula><p>In the case of the weighted average for the embedding tensor, R, we use the weights from the next level.</p><formula xml:id="formula_6">R = L l=1 W l+1 R l</formula><p>As a result, when updating, each token "sees" information from tokens/entities on other levels dependent on whether or not they are in the same entity. For the intuition behind this, we use the example phrase "The United Kingdom government" from <ref type="figure" target="#fig_3">Figure 6</ref>. The model should output merge values M which group the tokens "The United Kingdom" on the first level, and then group all the tokens on the second level. If this is the case, then for the token "United", R and D will hold the embedding of/directions to the tokens "The" and "Kingdom" in their disaggregated (unmerged) form. However, for the token "government", R and D will hold embeddings of/ directions to the combined entity "the United Kingdom" in each of the three slots for "The", "United" and "Kingdom". Because "government" is not in the same entity as "The United Kingdom" on the first level, it "sees" the aggregated embedding of this entity.</p><p>Intuitively, this allows the token "government" to update in the model based on the information that it has a country one step to the left of it, as opposed to having three separate tokens, one, two and three steps to the left respectively. Note that as with the entity merging, there are no hard decisions during training, with this effect based on the real valued merge tensor M , to allow differentiability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Unfold Operators for the passage "... yesterday. The President of France met with ...". Each row in the matrices corresponds to the words "The", "President", "of" and "France" (top to bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>Figure 5: Static Layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Calculation of merging weight, directions and entities in Structure Layer By taking both the left and right cumulative sum (cumsum) of the resulting two tensors from the center out (see grey dashed arrows in Figure 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure 7: Structure Layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure 8: Update Layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Figure 10: OntoNotes Targets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Update mechanism</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>ACE 2005</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>OntoNotes NER</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Entity Embeddings Nearest Neighbours</figDesc><table><row><cell></cell><cell cols="2">ACE05 OntoNotes</cell></row><row><cell>Static Layer</cell><cell></cell><cell></cell></row><row><cell>with</cell><cell>74.6</cell><cell>87.59</cell></row><row><cell>without</cell><cell>73.1</cell><cell>85.22</cell></row><row><cell>Embed Combination</cell><cell></cell><cell></cell></row><row><cell>Linear</cell><cell>70.2</cell><cell>83.96</cell></row><row><cell>Embed Update</cell><cell>74.6</cell><cell>87.59</cell></row><row><cell>Article Embedding</cell><cell></cell><cell></cell></row><row><cell>with</cell><cell>74.5</cell><cell>87.59</cell></row><row><cell>without</cell><cell>74.6</cell><cell>85.60</cell></row><row><cell>Sentence boundaries</cell><cell></cell><cell></cell></row><row><cell>with</cell><cell>70.8</cell><cell>86.30</cell></row><row><cell>without</cell><cell>74.6</cell><cell>87.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Architecture Ablations</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Directional Embeddings Nearest Neighbours</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>also provides further examples of the network merging and providing intuitive embeddings for multi-word entities.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The difference being that the weightings are generated using a sigmoid rather than a softmax layer, allowing the attention values to be close to one for multiple tokens.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use the notation D to denote the unfolded version of tensor D, i.e. D = U nf old[to]  (D)   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the term "directions" as we inverse the vectors to get the reverse direction, and cumsum them to get directions between tokens multiple steps away.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Tiling refers to simply repeating the tensor across both the sequence length s and kernel size k dimensions</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/zalandoresearch/flair/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Andreas Vlachos is supported by the EPSRC grant eNeMILP (EP/R021643/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Robust lexical features for improved neural network namedentity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
		<idno>abs/1806.03489</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A neural layered model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meizhi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1446" to="1459" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nested named entity recognition revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="861" to="871" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1706.02515</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint mention extraction and classification with mention hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="867" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>abs/1904.03296</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="188" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Labeling gaps between words: Recognizing overlapping mentions with mention separators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldrian</forename><surname>Obaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2608" to="2618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and accurate sequence labeling with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. CoRR, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural segmental hypergraphs for overlapping mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A neural transition-based model for nested mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1011" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
