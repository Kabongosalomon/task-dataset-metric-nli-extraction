<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Meftah</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LASTI</orgName>
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">LIST</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Tamaazousti</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">MIT</orgName>
								<orgName type="institution" key="instit2">CSAIL</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasredine</forename><surname>Semmar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LASTI</orgName>
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">LIST</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassane</forename><surname>Essafi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LASTI</orgName>
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">LIST</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatiha</forename><surname>Sadat</surname></persName>
							<email>sadat.fatiha@uqam.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">UQÀM</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-tuning neural networks is widely used to transfer valuable knowledge from highresource to low-resource domains. In a standard fine-tuning scheme, source and target problems are trained using the same architecture. Although capable of adapting to new domains, pre-trained units struggle with learning uncommon target-specific patterns. In this paper, we propose to augment the target-network with normalised, weighted and randomly initialised units that beget a better adaptation while maintaining the valuable source knowledge. Our experiments on POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>POS tagging is a sequence labelling problem, that consists on assigning to each sentence' word, its disambiguated POS tag (e.g., Pronoun, Noun) in the phrasal context in which the word is used. Such information is useful for higher-level applications, such as machine-translation <ref type="bibr" target="#b7">(Niehues and Cho, 2017)</ref> or cross-lingual information retrieval <ref type="bibr" target="#b12">(Semmar et al., 2008)</ref>.</p><p>One of the best approaches for POS tagging of social media text <ref type="bibr" target="#b4">(Meftah et al., 2018a)</ref>, is transfer-learning, which relies on a neuralnetwork learned on a source-dataset with sufficient annotated data, then further adapted to the problem of interest <ref type="bibr">(target-dataset)</ref>. While this approach is known to be very effective <ref type="bibr" target="#b16">(Zennaki et al., 2019)</ref>, because it takes benefit from pretrained neurons, it has one main drawback by design. Indeed, it has been shown in computervision <ref type="bibr" target="#b17">(Zhou et al., 2018a</ref>) that, when fine-tuning on scenes a model pre-trained on objects, it is the neuron firing on the white dog object that became highly sensitive to the white waterfall scene. Simply said, pre-trained neurons are biased by what they have learned in the source-dataset. This is <ref type="figure">Figure 1</ref>: Given a word representation x i , a BiLSTM (Φ) models the sequence, and a FC layer (Ψ) performs classification. In standard fine-tuning, the units are pre-trained on a large source-dataset then adapted to the target one. In this work, we propose to add randomly initialised units (green branch) and jointly adapt them with pre-trained ones (gray branch). An elementwise sum is further applied to merge the two branches. Before merging, we balance the different behaviours of pre-trained and random units, using an independent normalisation (N ). Finally we let the network learn which of pre-trained or random neurons are more suited for every class, by adding learnable weighting vectors (u and v initialised with 1-values) on the FC layers. also the case on NLP (see experiments). Consequently, pre-trained units struggle with learning patterns specific to the target-dataset (e.g., "wanna" or "gonna" in the Tweets domain). This last is non-desirable, since it has been shown recently <ref type="bibr" target="#b18">(Zhou et al., 2018b)</ref> that such specific units are important for performance. To overcome this drawback, one can propose to take benefit from randomly initialised units, that are by design nonbiased. However, it is common to face small target-datasets that contain too few data to learn such neurons from scratch. Hence, in such setting, it is hard to learn random units that fire on specific patterns and generalise well.</p><p>In this article, we propose a hybrid method that takes benefit from both worlds, without their drawbacks. It consists in augmenting the source-network (set of pre-trained units) with randomly initialised units and jointly learn them. We call our method PretRand (Pretrained and Random units) and illustrate it in <ref type="figure">Fig. 1</ref>. The main difficulty is forcing the network to consider random units, because they have different behaviours than pretrained ones. Indeed, while these last strongly fire discriminatively on many words, these first do not fire on any word at the initial stage of fine-tuning. Therefore, random units do not significantly contribute to the computation of gradients and are thus slowly updated. To overcome this problem, we proposed to independently normalise pre-trained and random layers. This last balances their range of activations and thus forces the network to consider them, both. Last but not least, we do not know which of pre-trained and random units are the best for every class-predictor, thus we propose to learn weighting vectors on top of each branch.</p><p>Evaluation was carried on 3 POS tagging Tweets datasets in a transfer-learning setting. Our method outperforms SOTA methods and significantly surpasses fairly comparable baselines.</p><p>2 Proposed Method: PretRand 2.1 Base Model Given an input sentence S = [w 1 , . . . , w n ] of n successive tokens w i , the goal of a POS tagger is to predict the POS-tag c i ∈ C of every w i , with C ∈ R C being the tag-set. Hence, for our base model, we used a common sequence labelling model which first, computes for each token w i , a word-level embedding (denoted Υ w ) and character-level embedding using biLSTM encoder (Υ c ), and concatenates them to get a final representation x i . Second, it feeds the later representation into a biLSTM features extractor (denoted Φ) that outputs a hidden representation, that is itself fed into a fully-connected (FC) layer (denoted Ψ) for classification. Formally, given w i , the logits are obtained using:</p><formula xml:id="formula_0">ŷ w i = Ψ • Φ • Υ(w i ), with</formula><p>Υ being the concatenation of the output of Υ c and Υ w for w i . In a standard fine-tuning scheme <ref type="bibr" target="#b5">(Meftah et al., 2018b)</ref>, Υ and Φ are pre-trained on the source-task and Ψ is randomly initialised. Then, the three modules are further jointly trained on the target-task by minimising a Softmax Cross-Entropy (SCE) loss using the SGD algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adding Random Branch</head><p>As mentioned in the introduction, pre-trained neurons are biased by design, thus limited. This mo-tivated our proposal to augment the pre-trained branch with additional random units (as illustrated in <ref type="figure">Fig. 1</ref>). To do so, theoretically one can add the new units in any layer of the base model. However in practice, we have to make a trade-off between performances and the number of parameters (model complexity). Thus, given that deep layers are more task-specific than shallow ones <ref type="bibr" target="#b10">(Peters et al., 2018;</ref><ref type="bibr" target="#b6">Mou et al., 2016)</ref>, and that word embeddings (shallow layers) contain the majority of parameters, we choose to expand only the top layers. With this choice, we desirably increase the complexity of the model only by 1.02× compared to the base one. In terms of the layers expanded, we specifically add k units to Φ resulting in an extra biLSTM layer: Φ r (r for rand); and C units in Ψ resulting in an extra FC layer: Ψ r . Hence, for every w i , the additional random branch predicts class-probabilities following:</p><formula xml:id="formula_1">ŷ r w i = Ψ r • Φ r (x i ) (with x i = Υ(w i ))</formula><p>. Note that, having two FC layers obviously outputs two predictions per class (one from the pre-trained FCŷ p w i and one from the randomŷ r w i ), that thus need to be merged. Hence, to get the final predictions, we simply apply an element-wise sum between the output of both branches:ŷ w i =ŷ p w i ⊕ŷ r w i . As in the classical scheme, SCE is minimised but here, both branches are trained jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Independent Normalisation</head><p>Nevertheless, while at the initial stage of finetuning, the pre-trained units are strongly firing on many words, the random ones are firing very weakly. As stated in some computervision works <ref type="bibr" target="#b2">(Liu et al., 2015;</ref><ref type="bibr" target="#b15">Tamaazousti et al., 2018)</ref>, the later setting causes an absorption of the weights, outputs and thus gradients of the random units by the pre-trained ones, which thus makes them useless at the end. We encountered the same problem with textual data on the POStagging problem. Indeed, as illustrated in the left plot of <ref type="figure" target="#fig_0">Fig.2</ref>, at the end of training, the distribution of the random units' weights is still absorbed (closer to zero) by that of the pre-trained ones.</p><p>To prompt the two classifiers to work cooperatively, we normalise (using an p -norm) both of them independently before merging them. Formally, we apply N p (x) = x ||x||p onŷ p w i andŷ r w i . The normalisation is desirably solving the weights absorption problem since at the end of the training, the distributions of the pre-trained and random weights become very similar (right of <ref type="figure" target="#fig_0">Fig. 2</ref>). Furthermore, we have observed that despite the normalisation, the performances of the pre-trained classifiers were still much better than the randomly initialised ones. Thus, to make them more competitive, we propose to start with optimising only the randomly initialised units while freezing the pretrained ones, then, launch the joint training. This is called random++ in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learnable Weighting Vectors</head><p>Back to the extra predictor (FC layer of random branch), it is important to note that both branches are equally important for making a decision for every class, i.e., no weight is applied on the dimensions ofŷ p w i andŷ r w i . However, this latter is sub-optimal since we, a priori, do not know which kind of units (random or pre-trained) is better for making a decision. Consequently, we propose to weight the contribution of the predictions for each class. For this end, instead of simply performing an element-wise sum between the random and pre-trained predictions, we first weight each of them with learnable weighting vectors, then compute a Hadamard product with their associated normalised predictions; the learnable vectors u ∈ R C and v ∈ R C , respectively corresponding to the pre-trained and random branch, are initialised with 1-values and are learned by SGD. Formally, the final predictions are computed following:</p><formula xml:id="formula_2">ŷ w i = u N p (ŷ p w i ) ⊕ v N p (ŷ r w i ).</formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>In the word-level embeddings, tokens are lowercased while the character-level component still retains access to the capitalisation information. We set the character embedding dimension at 50, the dimension of hidden states of the character-level biLSTM at 100 and used 300-dimensional wordlevel embeddings. The latter were pre-loaded from publicly available Glove pre-trained vectors on 42 billions words from a web crawling and contain-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison Methods</head><p>To assess the POS tagging performances of our PretRand model, we compared it to 5 baselines: Random-200 and Random-400: randomly initialised neural model with 200 and 400 biLSTM's units; Fine-tuning: pre-trained neural model, fine-tuned with the standard scheme; Ensemble (2 rand): averaging the predictions of two base models randomly initialised and learned independently (with different random initialisation) on Tweets datasets; and Ensemble (1 pret + 1 rand): same as the previous but with one pre-trained on WSJ and the other randomly initialised. We also compared it to the 3 best SOTA methods: <ref type="bibr" target="#b0">Derczynski et al. (2013)</ref>   <ref type="bibr" target="#b0">(Derczynski et al., 2013)</ref> n/a 89.37 88.69 n/a n/a n/a n/a GATE-bootstrap <ref type="bibr" target="#b0">(Derczynski et al., 2013)</ref> n/a n/a 90.54 n/a n/a n/a n/a ARK <ref type="bibr" target="#b8">(Owoputi et al., 2013)</ref> n/a n/a 90.40 93.2 n/a 94.6 n/a TPANN <ref type="bibr" target="#b1">(Gui et al., 2017)</ref> n/a 91.08 90.92 92.8 n/a n/a n/a   Markov Model (MEMM) with greedy decoding and using brown clustering and careful handengineered features. Recently, <ref type="bibr" target="#b1">Gui et al. (2017)</ref> proposed TPANN that uses adversarial training to leverage huge amounts of unlabelled Tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>From the results given in <ref type="table" target="#tab_4">Table 2</ref>, one can first see that our approach outperforms the SOTA and baseline methods on all the datasets. More interestingly, PretRand significantly outperforms the popular fine-tuning baseline by +1.4% absolute point on average and is better on all classes (see perclass improvement on <ref type="figure" target="#fig_1">Fig. 4</ref>). PretRand also outperforms the challenging Ensemble Model by a large margin (+2.2%), while using much less parameters. This clearly highlights the difference of our method with ensemble methods and the importance of having a shared word representation as well as our normalisation and weighting learnable vectors during training. A key asset of Pre-tRand, is that it uses only 0.02% more parameters compared to the fine-tuning baseline. An interesting experiment is to evaluate the gain of performance of PretRand compared to finetuning, according different target-datasets' sizes. From the results in <ref type="figure">Fig. 3</ref>, PretRand has desirably a bigger gain with bigger target-task datasets, which clearly means that the more target training-data, the more interesting our method will be.</p><p>To assess the contribution of different components of PretRand, we performed an ablation study. Specifically, we successively ablated the main components of PretRand, namely, the learnable vectors (learnVect), the longer training for random units (random++) and the normalisation ( 2 -norm). From the results in <ref type="table" target="#tab_5">Table 3</ref>, we can observe that the performances are only marginally better than standard fine-tuning when ablating the three components from PretRand. More importantly, adding each of them successively, makes the performances significantly better, which highlights the importance of every component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>Bias when fine-tuning pre-trained units Here our goal is to highlight that as in <ref type="bibr" target="#b17">(Zhou et al., 2018a)</ref>, pre-trained units can be biased in the standard fine-tuning scheme. To do so, we follow <ref type="bibr" target="#b14">(Tamaazousti et al., 2017)</ref> and analyse the units of Φ (biLSTM layer) before and after finetuning. Specifically, we compute the Pearson's correlation between all the units of the layer before and after fine-tuning. Here, a unit is represented by the random variable being the concatenation of its output activations from all the validation samples of the TweeBank dataset. From the resulting correlation matrix illustrated in <ref type="figure" target="#fig_2">Fig. 5</ref>, one can clearly observe the white diagonal, highlighting the fact that, every unit after fine-tuning is more correlated with itself before fine-tuning than with any other unit. This clearly confirms our initial motivation that pre-trained units are highly biased to what they have learned in the source-dataset, making them limited to learn patterns specific to the target-dataset.</p><p>Additionally, we visualise in <ref type="figure" target="#fig_3">Fig. 6</ref> a concrete example of a biased neuron when transferring from newswire to Tweets domain. Specifically, we show the top-10 words activating unit-169 of Φ (from the standard fine-tuning baseline), before fine-tuning (at this stage, the model is trained on the source-dataset WSJ) and during fine-tuning on the TweeBank dataset. We can observe that this unit is highly sensitive to proper nouns (e.g., George and Washington) before fine-tuning, and to words with capitalised first-letter whether the word is a proper noun or not (e.g., Man and Father) during fine-tuning on TweeBank dataset. Indeed, we found that most of tokens with upper-cased first letter are mistakenly predicted as proper nouns (PROPN) in the standard fine-tuning scheme. In fact, in standard English, inside sentences, only proper nouns start with upper-cased letter thus fine-tuning the pre-trained model fails to slough this pattern which is not always respected in Tweets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unique units emerge in random branch</head><p>Finally, we highlight the ability of randomly initialised units to learn patterns specific to the targetdataset and not learned by the pre-trained ones because of their bias problem. To do so, we visualise unique units -i.e., random units having a correlation lower than 0.4 with pre-trained ones -emerging in the random branch. While only one shown in <ref type="figure" target="#fig_4">Fig. 7</ref>, many unique units have been learned by the random branch of our PretRand model: 37.5% of the 200 random units have correlation lower than 0.4 with the pre-trained ones. Regarding unit-99, it is highly discriminative to tokens "na", "ta" and "n't". Indeed, in TweeBank, words like "gonna" (going to) are tokenized into two tokens: "gon" and "na", with the later annotated as a particle and the former as a verb. Importantly, not even one unit from the standard finetuning scheme has been found firing on the same important and target-dataset specific pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduced a method to improve fine-tuning using 3 main ideas: adding random units and jointly learn them with pre-trained ones; normalising the activations of both to balance their different behaviours; applying learnable weights on both predictors to let the network learn which of random or pre-trained one is better for every class. We have demonstrated its effectiveness on domain adaptation from newswire domain to three commonly used Tweets-datasets for POS tagging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Distributions of learned weight-values for the randomly initialised (green) and pre-trained (gray) fully-connected layers after joint training. Left: without normalisation, right: with normalisation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Sorted class-accuracy improvement (%) on TweeBank of PretRand compared to fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Correlation between units' activations before fine-tuning (columns) and after fine-tuning (rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Top-10 words activating unit-169 of standard fine-tuning scheme, before fine-tuning (Final-WSJ) and during fine-tuning on TweeBank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Top-10 words activating unit-99 of the random branch of PretRand, before and during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of tokens in every used dataset.</figDesc><table><row><cell>ing 1.9M words (Pennington et al., 2014). Note</cell></row><row><cell>that, these embeddings are also updated during</cell></row><row><cell>fine-tuning. For biLSTM (token-level feature ex-</cell></row><row><cell>tractor), we set the number of units of the pre-</cell></row><row><cell>trained branch to 200 and experimented our ap-</cell></row><row><cell>proach with k added random-units, with k ∈</cell></row><row><cell>{50, 100, 150, 200}. For the normalisation, we</cell></row><row><cell>used 2 -norm. Finally, in all experiments, training</cell></row><row><cell>was performed using SGD with momentum and</cell></row><row><cell>mini-batches of 8 sentences. Evidently, all the hy-</cell></row><row><cell>perparameters have been cross-validated.</cell></row><row><cell>3.2 Datasets</cell></row><row><cell>For the source-dataset, we used the Penn-Tree-</cell></row><row><cell>Bank (PTB) of Wall Street Journal (WSJ), a large</cell></row><row><cell>English dataset containing 1.2M+ tokens from the</cell></row><row><cell>newswire domain annotated with the PTB tag-</cell></row></table><note>set. Regarding the target-datasets, we used three datasets in the Tweets domain: TPoS (Ritter et al., 2011), annotated with 40 tags ; ARK (Owoputi et al., 2013) containing 25 coarse tags; and the re- cent TweeBank (Liu et al., 2018) containing 17 tags (PTB universal tag-set). The number of to- kens in the datasets are given in Table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Random-200 1× 88.32 87.76 90.67 91.20 91.56 89.90 Random-400 1.03× 89.01 88.89 90.99 91.38 91.63 90.38</figDesc><table><row><cell>Standard fine-tuning</cell><cell>1×</cell><cell>90.96</cell><cell>90.7</cell><cell>91.72 92.59 92.99 91.79</cell></row><row><cell>Ensemble Model (2 rand)</cell><cell>2×</cell><cell>89.20</cell><cell>88.8</cell><cell>91.36 91.73 92.05 90.62</cell></row><row><cell>Ensemble Model (1 pret + 1 rand)</cell><cell>2×</cell><cell cols="3">89.77 88.61 91.41 92.57 92.85 91.04</cell></row><row><cell>PretRand (Ours)</cell><cell>1.02×</cell><cell cols="3">91.56 91.46 93.77 94.51 94.95 93.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our method to state-of-the-art (top) and baselines (bottom) in terms of token-level accuracy (in %) on 3 Tweets datasets. Note that, baselines are more fairly comparable to our method. In the second and last columns, we respectively highlighted the number of parameters and the average performance on the 3 datasets.</figDesc><table><row><cell cols="4">Figure 3: Performances (on dev-set of TweeBank)</cell></row><row><cell cols="4">according different training-set sizes for the target-</cell></row><row><cell cols="4">dataset. Transparent green highlights the difference be-</cell></row><row><cell cols="3">tween our PretRand and standard fine-tuning.</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">TPoS ArK TweeBank</cell><cell>Avg</cell></row><row><cell>PretRand</cell><cell>91.46 93.77</cell><cell>94.95</cell><cell>93.39</cell></row><row><cell>-learnVect</cell><cell>91.25 93.46</cell><cell>94.59</cell><cell>93.10</cell></row><row><cell cols="2">-random ++ 90.97 93.11</cell><cell>94.13</cell><cell>92.73</cell></row><row><cell>-l2 norm</cell><cell>90.76 92.11</cell><cell>93.38</cell><cell>92.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study, Token level accuracy (in %) when progressively ablating PretRand components.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Twitter part-of-speech tagging for all: Overcoming sparse and noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter with adversarial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2411" to="2420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parsing tweets into universal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural network model for part-of-speech tagging of social media texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Meftah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasredine</forename><surname>Semmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatiha</forename><surname>Sadat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Using neural transfer learning for morpho-syntactic tagging of southslavic languages tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Meftah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasredine</forename><surname>Semmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatiha</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Raaijmakers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<title level="m">How transferable are neural networks in nlp applications? In EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting linguistic resources for neural machine translation using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="380" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Evaluating a natural language processing approach in arabic information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasredine</forename><surname>Semmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laib</forename><surname>Meriama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fluhr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">ELRA Workshop on Evaluation Looking into the Future of Evaluation: When automatic metrics meet task-based and performance-based approaches</title>
		<imprint>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mucale-net: Multi categorical-level networks to generate more discriminating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Tamaazousti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Hervé Le Borgne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hudelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Céline Hudelot, Mohamed El Amine Seddik, and Mohamed Tamaazousti</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Tamaazousti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hervé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09708</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Learning more universal representations for transfer-learning</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural approach for inducing multilingual resources and natural language processing tools for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O Zennaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Semmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="67" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpreting deep visual representations via network dissection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Revisiting the importance of individual units in cnns via ablation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02891</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
