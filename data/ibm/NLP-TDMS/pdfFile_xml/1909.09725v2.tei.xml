<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Image Matting for Simultaneous Foreground and Alpha Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Portland State University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
							<email>fliu@cs.pdx.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Portland State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Image Matting for Simultaneous Foreground and Alpha Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural image matting is an important problem in computer vision and graphics. It is an ill-posed problem when only an input image is available without any external information. While the recent deep learning approaches have shown promising results, they only estimate the alpha matte. This paper presents a context-aware natural image matting method for simultaneous foreground and alpha matte estimation. Our method employs two encoder networks to extract essential information for matting. Particularly, we use a matting encoder to learn local features and a context encoder to obtain more global context information. We concatenate the outputs from these two encoders and feed them into decoder networks to simultaneously estimate the foreground and alpha matte. To train this whole deep neural network, we employ both the standard Laplacian loss and the feature loss: the former helps to achieve high numerical performance while the latter leads to more perceptually plausible results. We also report several data augmentation strategies that greatly improve the network's generalization performance. Our qualitative and quantitative experiments show that our method enables high-quality matting for a single natural image. Our inference codes and models have been made publicly available at https://github. com/hqqxyy/Context-Aware-Matting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Natural image matting is the problem of estimating the foreground image and the corresponding alpha matte from an input image. It is a critical step of image composition, which is widely used in image and video production. Without any external information, matting is a seriously ill-posed problem. In practice, most existing matting methods take a trimap as input; however, matting is still underconstrained in the undefined area in the trimap.</p><p>Traditional methods solve the matting problem by inferring the alpha matte information in the undefined area from those in the defined areas <ref type="bibr" target="#b50">[51]</ref>. For instance, the matte values in the undefined areas can be propagated from the known areas according to the spatial and appearance affinity between them <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">46]</ref>. Alternatively, the undefined matte values can be computed by sampling the color or texture distribution of the known foreground and background areas and optimizing a carefully defined metric, such as the likelihood of the foreground, background, and alpha values <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. While these methods provide promising results and some of them are incorporated into commercial tools, single natural image matting is still a challenging problem as these methods rely on the distinctive appearance of the foreground and background areas, such as their local or global color distribution.</p><p>Our research is inspired by the recent deep learning approaches to image matting. These deep matting approaches, such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b51">52]</ref> take an input image and the corresponding user-provided trimap as input and output an alpha map. They are shown robust for many challenging scenarios. These methods, however, only output the alpha map without the foreground or background image. This paper presents a deep image matting method that simultaneously estimate the alpha map and the foreground image. Our method explores both local image and global context information for high-quality matting. This is inspired by the success of non-deep learning-based matting approaches that combines the global sampling and local propagation strategies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref>. Specifically, we designed a two-encoder-two-decoder fully convolutional neural network for context-aware simultaneous foreground image and alpha map estimation. The matting encoder learns to extract the local features while the context encoder learns more global features. We concatenate the features from these two encoders and feed them to an alpha decoder and a foreground decoder to estimate the alpha map and the corresponding foreground image simultaneously.</p><p>We explore a Laplacian loss and the feature loss to train our deep matting neural network. We found that the Laplacian loss enables our network to achieve the state-of-the-art numerical performance while the feature loss leads to more perceptually plausible matting results. We also found that some data augmentation methods are particularly helpful Input image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trimap</head><p>Our alpha map and composition results Results from Closed-form <ref type="bibr" target="#b26">[27]</ref>  <ref type="figure">Figure 1</ref>. Real-world image matting. Our method is able to simultaneously estimate high-quality foreground images and alpha maps from real-world images although trained on a synthetic dataset. Our results keep final structures (the top example) while being free from the common color bleeding problem (the bottom example).</p><p>for our neural network to generalize to real-world images although our network is trained on a synthetic dataset provided by Xu et al. <ref type="bibr" target="#b51">[52]</ref>.</p><p>To our best knowledge, this paper contributes the first deep matting method that enables simultaneous foreground and alpha estimation. Both our qualitative and quantitative experiments demonstrate that our method is able to generate state-of-the-art matting results on challenging real-world examples, as shown in <ref type="figure">Figure 1</ref>. We attribute the success of our method to 1) the integration of local visual features and global context information, 2) the combination of the Laplacian and feature loss, and 3) various effective data augmentation strategies that help generalizing our method to a wide variety of challenging real-world images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image matting assumes that an image I is a linear composition of a foreground image F and a background image B according to an alpha map α α α as follows <ref type="bibr" target="#b44">[45]</ref>.</p><formula xml:id="formula_0">I = α α αF + (1 − α α α)B<label>(1)</label></formula><p>Given the input image I, image matting aims to recover F, B and α α α. Most of existing matting methods require a user-provided trimap that specifies known foreground and background areas, as well as an undefined area. In this way, matting is reduced to solving for the foreground, background, and alpha values in the undefined area. Given only the input image I and the trimap, matting is a seriously illposed problem. A rich literature exists for matting. These methods infer the matte information for the undefined area from the known foreground and background according to the trimap. They either propagate the matte information from the neighboring foreground or background areas to the unknown areas <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">46]</ref>, or more globally sample the appearance information of the known foreground and background and use them to optimize for the matting in the unknown area <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. There are also methods that combine the local propagation strategy and the global sampling strategy to achieve more reliable results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref>. Wang and Cohen provided a good survey on these traditional image matting algorithms <ref type="bibr" target="#b50">[51]</ref>. Our design of a double-encoder-double-decoder network to learn to estimate local and global context information is inspired by these hybrid methods.</p><p>Our work is most relevant to the recent deep learning approaches to image matting. Shen et al. trained a dedicated deep convolutional neural network for portrait matting <ref type="bibr" target="#b42">[43]</ref>. Their method first employs a deep neural network to generate the trimap of a portrait image and then feeds it to an offthe-shelf matting method, namely the Closed-form Matting algorithm <ref type="bibr" target="#b26">[27]</ref>, to obtain the final matting result. Cho et al. developed a deep matting method that takes the matting results from the Closed-form Matting algorithm <ref type="bibr" target="#b26">[27]</ref> and the KNN Matting algorithm <ref type="bibr" target="#b5">[6]</ref> as input, and refine it using a deep neural network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Xu et al. developed a large-scale synthetic image matting dataset and used it to train a twostage deep neural network for alpha matting. Their method produced high-quality matting results for both synthetic and real-world images <ref type="bibr" target="#b51">[52]</ref>. Lutz et al. explores generative adversarial networks to achieve high-quality natural image matting <ref type="bibr" target="#b32">[33]</ref>. In their recent work, Chen et al. addressed a difficult case of image matting, transparent object matting. By considering transparent object matting as a refractive flow estimation problem, they developed a two-stage neural network to estimate the refractive flow from only one input image for transparent object matting <ref type="bibr" target="#b3">[4]</ref>. While these methods are able to estimate high-quality alpha maps, they do not generate the foreground component. Our work builds upon these deep learning methods and simultaneously estimate the foreground image and the alpha map, thus providing a complete solution to image matting. Our network learns to extract both local visual features and global context information to obtain high-quality image matting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Context-Aware Image Matting</head><p>Our method takes an image I and a user-specified trimap T as input and aims to estimate the foreground F and the corresponding alpha map α α α, thus providing a full solution to matting. With the foreground and the alpha map, we can directly compute the background according to Equation 1.</p><p>We design a context-aware two-encoder-two-decoder deep neural network to simultaneously estimate the foreground and the alpha map, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The outputs of the two encoders are concatenated and fed to the two decoder to generate the foreground and the alpha map, respectively. The two-encoder design of the network is inspired by the success of traditional matting algorithms that combine the local propagation and global sampling strategies for robust image matting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Specifically, the matting encoder is designed to learn to extract local features that are required to capture final image structures, such as hairs, while the context encoder learns to estimate more global context information that is helpful to disambiguate the foreground and background when they are similar to each other locally. Below we describe the encoders and decoders in more detail.</p><p>Matting encoder. We adopt the modified version of the Xception 65 architecture <ref type="bibr" target="#b9">[10]</ref> from the deeplab v3+ <ref type="bibr" target="#b4">[5]</ref> and set the down-sampling factor as 4 by setting the entroy flow's block2 and block3's stride as 1. This modification enables the middle flow to have a big spatial resolution. While traditional classification models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref> more aggressively compromise the spatial resolution to have a large valid receptive field, we use such a smaller down-sampling factor to retain sufficient spatial information that is important for the task of matting to capture fine image structures. Meanwhile, there is a trade-off between the computation/memory cost and spatial resolution. We empirically find that the down-sampling factor of 4 can get good matting results and cost a relatively small amount of computation and memory. We use skip connections to use features from the earlier layers as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Context encoder. We also adopt the Xception 65 architecture <ref type="bibr" target="#b9">[10]</ref> from <ref type="bibr" target="#b4">[5]</ref>. Compared to the matting encoder, we use a much larger down-sampling factor of 16 to capture more global contextual information. We bilinearly upsample the final features by a factor of 4 so that the context features are of the same size as the local matting features from the matting encoder.</p><p>Alpha decoder and foreground decoder have the same network architecture. Specifically, we first bilinearly upsample the concatenated features from the encoders by a factor of 2 and then combine them with the intermediate features from the context encoder using a skip connection as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. This is followed by two 3 × 3 convolutional layers with 64 channels. We repeat this process twice so that each decoder outputs the foreground image and the alpha map with the same size as the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Loss functions</head><p>We compute the loss over both the alpha map and the foreground image. We explore a range of loss functions to train our network. Below we describe them one by one.</p><p>We use a Laplacian loss <ref type="bibr" target="#b34">[35]</ref> to measure the difference between the predicated alpha map α α α and its ground truthα α α.</p><formula xml:id="formula_1">L α lap = 5 i=1 2 i−1 L i (α α α) − L i (α α α) 1 ,<label>(2)</label></formula><p>where L i (α α α) indicates the i th level of the Laplacian pyramid of the alpha map. This loss function measures the differences of two Laplacian pyramid representations and captures the local and global difference. We scale the contribution of a Laplacian level according to its spatial size. We also use the feature loss to measure the perceptual quality of the alpha map. The feature loss, based on the differences between the high-level features extracted from a pre-trained convolutional neural network, has been shown effective in generating perceptually high-quality images in many image enhancement and synthesis tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref>. However, it is difficult to directly measure the perceptual quality of an alpha map. Our solution is to composite the ground-truth foreground image onto the black background using the alpha map and then measure the perceptual quality of the composition result as follows.</p><formula xml:id="formula_2">L α F = layer φ layer (α α α * F) − φ layer (α α α * F) 2 2 ,<label>(3)</label></formula><p>whereF indicates the ground truth foreground and φ layer indicates the features output by the layer in a pre-trained VGG16 network <ref type="bibr" target="#b43">[44]</ref>. Our method uses [conv1 2, conv2 2, conv3 3, conv4 3] to compute the features. We follow the same setting to calculate the feature loss for the predicated foreground image. Here the feature loss L c F is computed on the composition result using the groundtruth alpha map with the foreground image as follows.</p><formula xml:id="formula_3">L c F = layer φ layer (α α α * F) − φ layer (α α α * F) 2 2 ,<label>(4)</label></formula><p>We also use the standard 1 loss for the predicted foreground F. We only calculate the loss where the foreground is visible, in other words, the ground truth alpha matte is bigger than 0,</p><formula xml:id="formula_4">L c 1 = 1(α α α &gt; 0) * (F − F) 1 ,<label>(5)</label></formula><p>where 1 is an indicator function that takes 1 if the statement is true and 0 otherwise.</p><p>Finally, we apply the standard 2 regularization loss to all the convolutional layers. We will examine these loss functions in our experiments (Section 4). <ref type="figure">Figure 3</ref>. Image patch selection. The alpha map is illustrated using the color map, with yellow and blue indicating the foreground and background, respectively. The patches are selected to cover the unknown region but with relatively small overlaps among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>We initialize our neural network with pre-trained models from <ref type="bibr" target="#b4">[5]</ref>. We use TensorFlow to train our neural network. Similar to <ref type="bibr" target="#b4">[5]</ref>, we use the "poly" learning rate policy to train our network, where lr = lr init (1 − iter max iter ) power with lr init = 7 × 10 −4 and power = 0.9. We use a minibatch size of 6 and train the neural network for 1 million iterations for models <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref> in <ref type="table" target="#tab_0">Table 1</ref>. We fine-tune models (4-9) based on the pretrained model (3) with 10 5 iterations with lr init = 10 −4 . Training dataset. We train our network using the matting dataset shared by Xu et al. <ref type="bibr" target="#b51">[52]</ref>. This dataset contains 431 training images with the corresponding alpha maps and the foreground images. We create the training samples in a similar way to Xu et al. Specifically, we composite the foreground image onto a randomly selected background image from MS-COCO dataset <ref type="bibr" target="#b29">[30]</ref>. We down-sample the foreground image gradually by a factor of 0.9 until the short side is 600 pixels. If the source image's short side is less than 600 pixels, we first scale it up to 780. In total, we generate 1957 scaled foreground image. Then we select image patches that contain unknown regions in the trimap. Specially, we slide windows of size 600 × 600 on the full image with a stride of 5 pixels to get a large amount of candidate windows and remove patches where less than 10% pixels are unknown. Furthermore, since many patches overlap with each other significantly, we employ non-maximum suppression(NMS) to remove overlapping patches. Specifically, we set the NMS threshold as 0.3 and only keep the top 30 image patches with the highest unknown pixel percentages in each image. <ref type="figure">Figure 3</ref> shows an example of selected image patches. In total, we obtain 9,507 600 × 600 foreground image patches. Finally, we create training samples of size 225 × 225 by randomly cropping the composited  image with the following data augmentation operators. Data augmentation. Following Xu et al. <ref type="bibr" target="#b51">[52]</ref>, our training samples are obtained by compositing a foreground image and a background image using an alpha map. As reported in many papers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref>, many subtle artifacts, such as misaligned JPEG blocks, compression quantization artifacts, and resampling artifacts, can sometimes affect their methods a lot despite that the images look plausible to the human eyes. Some splice detection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref> even build their algorithms based on such an observation. Directly training the network on the composited images without special augmentation may suffer from a similar problem and thus compromises the generalization capability of the trained network. Therefore, besides the resizing augmentation used in Xu et al. <ref type="bibr" target="#b51">[52]</ref>, we follow the post processing steps in the image splice detection methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref> and use re-JPEGing and Gaussian blur to augment our training samples. These operators introduce subtle artifacts that are not visually noticeable but can make the network less bias to the small difference between the foreground and the background. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the original background is smoother than the original foreground image. Therefore, it is possible that the network relies on this bias to differentiate the foreground from the background. Re-JPEGing and Gaussian blur can relieve this problem by introducing artifacts or remove these artifacts. For re-JPEGing, we keep 70% quality of the composited images. For Gaussian blur, we on-the-fly generate a Gaussian kernel with standard deviation in the range of [0, 3] and the kernel size in the range of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, and apply it to the composited image. We also randomly resize the composited image with a rate of between 0.5 and 1.</p><p>Besides, we also use some standard data augmentation operators. Specifically, we employ the gamma transforms to increase the color diversity. The gamma value is randomly selected from [0.2, 2]. We randomly flip the images horizontally. The trimap for each image is automatically generated by randomly dilating its corresponding ground truth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We experiment with our methods on the synthetic Composition-1K dataset and a real-world matting image dataset, both of which are provided by Xu et al. <ref type="bibr" target="#b51">[52]</ref>. As discussed in Section 3.2, our neural networks are all trained on the synthetic Composition-1K training set. We evaluate our models and compare to the state of the art methods on the Composition-1K testing set and the real-world matting image set. Specifically, the Composition-1K testing dataset contains 1000 composited images. They were generated by compositing 50 unique foreground images onto each of the 20 images from the PASCAL VOC 2012 dataset <ref type="bibr" target="#b14">[15]</ref>. We used the code provided by Xu et al. <ref type="bibr" target="#b51">[52]</ref> to generate these testing images. The real world image dataset contains 31 real world images pulled from the internet <ref type="bibr" target="#b51">[52]</ref>. We conduct our user study on the real world images.</p><p>Since not all the methods produce both the foreground images and the alpha maps as the final matting results, we compare our methods to the state of the art on the alpha maps and the foreground images separately. Besides, we also report our user study and our ablation studies to more thoroughly evaluate our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on alpha maps</head><p>We compare our methods to both the state of the art non-deep learning methods, including Shared Matting <ref type="bibr" target="#b15">[16]</ref>, Learning Based Matting <ref type="bibr" target="#b53">[54]</ref>, Comprehensive Sampling <ref type="bibr" target="#b41">[42]</ref>, Global Matting <ref type="bibr" target="#b18">[19]</ref>, Closed-form Mat-   <ref type="bibr" target="#b51">[52]</ref> and AlphaGan <ref type="bibr" target="#b32">[33]</ref>. <ref type="table" target="#tab_0">Table 1</ref> reports the results on these methods as well as ours on the Composition-1K dataset. The results of the comparing methods are obtained either from their papers or from the recent studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>To evaluate these methods, we use various metrics, including SAD, MSE, Gradient (Grad) and Connectivity (Conn) <ref type="bibr" target="#b38">[39]</ref>. Note that the Conn metric fails on some results, which are denoted as "-". For the ablation analysis of our work, we reported our results on nine versions of our networks with different components. We use "ME", "CE", "DA" to indicate the matting encoder, the context encoder, and data augmentation, respectively.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, our two-encoder-two-decoder models (model (3-9)) generate matting results with significantly smaller errors than the state of the art methods. To understand what contributes to this improvement, we evaluated on a baseline method (model <ref type="bibr" target="#b1">(2)</ref>) that removes the context encoder and found that this baseline model performs much worse according to all the four metrics. Therefore, the improvement can be mainly attribute to the use of our two encoders to capture both local visual features for fine structures and more global contextual information to disambiguate the locally similar foreground and background. Besides these numerical scores, our methods produce visually more plausible results as shown in <ref type="figure">Figure 6</ref>. For example, the last example has a strand of long hair. The results from existing methods either miss it entirely or the hair is broken into pieces while our methods better preserve it. Number of parameters. We make model <ref type="bibr" target="#b1">(2)</ref> deeper so that its number of parameters roughly match model <ref type="bibr" target="#b2">(3)</ref>. As shown in <ref type="table">Table 3</ref>, while this deeper version of model <ref type="bibr" target="#b1">(2)</ref> improves over the original one w.r.t SAD and MSE, it performs worse than our model (3) (ME + CE). Sensitivity of trimap. Following the same process of the Deep Matting work <ref type="bibr" target="#b51">[52]</ref>, we examine our method's sensitivity to the trimap size by dilating the ground-truth to a range of sizes. As illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>, our method is stable to the trimap sizes. Note, the scores of comparing methods were obtained from <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on foreground images</head><p>As existing deep learning methods only output alpha maps, we compare to three representative non-deep learning matting methods, namely Global Matting <ref type="bibr" target="#b18">[19]</ref>, Closed-Form Matting <ref type="bibr" target="#b26">[27]</ref> and KNN Matting <ref type="bibr" target="#b5">[6]</ref>, on how well foreground images can be extracted from single input images on the Composition-1K dataset. We calculate the SAD and MSE of α α α * F following the previous work <ref type="bibr" target="#b37">[38]</ref>. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our method reduces the error by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>As discussed in Section 4.1, our two-encoder structure brings in the major performance improvement. Besides, we found that proper loss functions and data augmentations are also important to obtain high-quality matting results and help generalizing to real-world images. Loss functions. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our model (2) with the Laplacian loss L α lap generates more numerically accurate results than our model (1) with the loss used in Deep Matting <ref type="bibr" target="#b51">[52]</ref>. Our model (3) generates better result compared to the model (4) with both the Laplacian loss and the feature loss L α F . On the other hand, the feature loss enables our model (4) to generate visually better results that keep more final structures than our model (3), as shown in the last example in <ref type="figure">Figure 6</ref>. This is consistent with many other works on image synthesis tasks that the feature loss tends to produce perceptually better results (often at the expense of the numerical performance) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref>.  <ref type="figure">Figure 6</ref>. Comparison of the alpha matte on the real world images dataset <ref type="bibr" target="#b51">[52]</ref>.</p><p>When training our network with both the foreground decoder and the alpha decoder, color loss functions, namely L c 1 and L c F , are naturally needed. By comparing models (4) and (6), (5) and <ref type="bibr" target="#b6">(7)</ref> in <ref type="table" target="#tab_0">Table 1</ref>, we can find that these color losses can improve the alpha map estimation slightly. This is in part because the color and the alpha decoders share the same learned features, and the tasks of foreground color estimation and alpha map estimation are relevant. Data augmentation. As shown in <ref type="table" target="#tab_0">Table 1</ref>, data augmentations, such as ReJPEGing and Gaussian blur, can greatly increases the errors of our methods on the Composition-1k testing dataset. On the other hand, we found that these data augmentations can greatly improve the generalization of our trained networks on the real world images. As shown in <ref type="figure">Figure 6</ref>, when trained with these data augmentation strategies, our models can maintain more fine details, such as hairs. Since these real world examples do not have ground truth, to obtain objective scores of these results, we evaluate the quality of composition results using our matting results. Specifically, we composite the foreground objects in source images onto some external background images using our matting results and then measure the visual quality of the composition results using the NIMA quality assessment algorithm <ref type="bibr" target="#b47">[48]</ref>. As reported in <ref type="table" target="#tab_2">Table 4</ref>, our data augmentation algorithms are helpful. We also test our methods on the Spectral Matting dataset <ref type="bibr" target="#b27">[28]</ref> with the known ground truth. This dataset is generated by photographing dolls in front of a computer monitor displaying seven different background images. The trimap is generated by dilating the alpha map by 20 pixels by alpha map denoising. Our method with DA outperforms our method without DA significantly according to most of the metrics: 3.58 vs 4.28 (SAD), 6.64 vs 9.05 (MSE), and 2.57 vs 3.19 (Conn), and slightly reduces the performance according to Grad: 2.04 vs 1.92.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">User study</head><p>To further evaluate the quality of our results, we conducted a user study. We compared our method (model <ref type="formula">(7)</ref>) with three representative methods, including Deep Matting <ref type="bibr" target="#b51">[52]</ref> and two state-of-the-art non-deep learning methods, Close-form Matting <ref type="bibr" target="#b26">[27]</ref> and Global matting <ref type="bibr" target="#b18">[19]</ref>.</p><p>Our study used all the 31 real-world images from Xu et al. <ref type="bibr" target="#b51">[52]</ref>. We used a similar protocol to Xu et al. <ref type="bibr" target="#b51">[52]</ref> to produce the results for the study. For the methods except Deep Matting, we composite the predicted foreground and alpha map onto a blank background image. We use the black background or the white background randomly with the exceptions that for certain foreground images, a particular background color is not appropriate. For example, it is meaningless to composite the black hair onto a black background image, so for such an example, we choose to use the white background. Since Deep Matting does not output the foreground image, we composite the input image using the estimated alpha map as suggested in their paper. Therefore, the comparison between our results with those from Deep Matting should be interpreted with a grain of salt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image Trimap</head><p>Closed-form Matting Global Matting Deep Matting Ours <ref type="figure">Figure 7</ref>. Comparison of the composite results on the real world image dataset <ref type="bibr" target="#b51">[52]</ref>.</p><p>Our user study recruited 42 students with different backgrounds. None of them have previous experience with the matting task. Therefore, we conducted a training session for each participant before the formal study. Specifically, each of them was shown two real-world images. For each image, we showed two matting results from different methods without revealing which methods were used to generate these results. We then explained the differences between two results to the participant. This training session is helpful as the subtle difference in matting results was often difficult to spot for people with no prior matting experience.</p><p>In our study, we divided the 42 participants into three groups. Each group evaluated how our results compared to one of the three existing methods. In each trial, a participant was presented with a screen that only shows a source image and two corresponding matting results at a time. The participant could select which image to view by clicking the corresponding button or using the left or right key on the keyboard. In this way, the participant can flip between different images to examine the quality or compare the difference. In each trial, the participant was asked to choose a more accurate and realistic result between the two results. Each participant conducted 31 trials so that the results for all the 31 testing images are evaluated.</p><p>We calculated the percentage of the times that our results were preferred by the participants and then calculated the average and the standard deviation for each group. As reported in <ref type="table">Table 5</ref>, more of our results are preferred by the participants than all the comparing methods. <ref type="figure">Figure 7</ref> shows some examples in our study. They show that our method can better capture very fine structures like the hair in the first example even when the hair shares a similar color to the background. In the last example, our result not only keeps the delicate edge of the lace, which is lost in the other <ref type="table">Table 5</ref>. The user study in the real world image dataset <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours vs</head><p>Mean preference rate Std Global Matting <ref type="bibr" target="#b18">[19]</ref> 85.48% 0.21 Closed-form Matting <ref type="bibr" target="#b26">[27]</ref> 84.11% 0.19 Deep Matting <ref type="bibr" target="#b51">[52]</ref> 77.67% 0.24</p><p>results, but also is free from the color bleeding problem where the blue background color contaminated the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presented a context-aware deep matting method for simultaneously estimating the foreground and the alpha map from a single natural image. We developed a two-encoder-two-decoder neural network for this task. The two encoders were designed to capture both the local fine structures and the more global context information to disambiguate the foreground and background with a similar appearance. The two decoders output the foreground and the alpha map respectively. Our experiments showed that using the feature loss helps to obtain visually more pleasant matting results while the Laplacian loss tends to optimize the numerical performance. Our experiments also showed that dedicated data augmentation methods, such as Re-JPEGING and Gaussian blurring, are helpful to generalize the neural network trained on a synthetic dataset to handle real-world challenging matting tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of our matting network. We design a two-encoder-two-decoder network. The matting encoder and the context encoder capture both visual features and more global context information. The features from these two encoders are concatenated and feed to the foreground and the alpha decoder to output the foreground image and the alpha map of the input image simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Data augmentation. In the composited image without any data augmentation (a), the foreground image contains some JPEG artifacts while the background is smooth, which produces a bias that will compromises the training of the network. Re-JPEGing introduces the artifacts to the foreground and the background to reduce the bias while Gaussian Blur does so by smoothing the high-frequency artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Sensitivity test with respect to trimap sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Alpha map results on the Composition-1K testing set.</figDesc><table><row><cell>Methods</cell><cell>SAD</cell><cell>MSE(10 3 )</cell><cell>Grad</cell><cell>Conn</cell></row><row><cell>Shared Matting[16]</cell><cell>128.9</cell><cell>91</cell><cell cols="2">126.5 135.3</cell></row><row><cell>Learning Based Matting [54]</cell><cell>113.9</cell><cell>48</cell><cell>91.6</cell><cell>122.2</cell></row><row><cell>Comprehensive Sampling [42]</cell><cell>143.8</cell><cell>71</cell><cell cols="2">102.2 142.7</cell></row><row><cell>Global Matting [19]</cell><cell>133.6</cell><cell>68</cell><cell>97.6</cell><cell>133.3</cell></row><row><cell>Closed-Form Matting [27]</cell><cell>168.1</cell><cell>91</cell><cell cols="2">126.9 167.9</cell></row><row><cell>KNN Matting [6]</cell><cell>175.4</cell><cell>103</cell><cell cols="2">124.1 176.4</cell></row><row><cell>DCNN Matting [8]</cell><cell>161.4</cell><cell>87</cell><cell cols="2">115.1 161.9</cell></row><row><cell>Three-layer Graph [29]</cell><cell>106.4</cell><cell>66</cell><cell>70.0</cell><cell>-</cell></row><row><cell>Deep Matting [52]</cell><cell>50.4</cell><cell>14</cell><cell>31.0</cell><cell>50.8</cell></row><row><cell>Information-flow Matting [2]</cell><cell>75.4</cell><cell>66</cell><cell>63.0</cell><cell>-</cell></row><row><cell>AlphaGan-Best 1 [33]</cell><cell>52.4</cell><cell>30</cell><cell>38.0</cell><cell>-</cell></row><row><cell>(1) ME + L deepmatting</cell><cell>49.1</cell><cell>13.4</cell><cell>26.7</cell><cell>49.8</cell></row><row><cell>(2) ME + L α lap (3) ME + CE + L α lap (4) ME + CE + L α lap + L α F (5) ME + CE + L α lap + L α F + DA</cell><cell>43.9 35.8 38.8 71.3</cell><cell>11.8 8.2 9.0 23.6</cell><cell>20.6 17.3 19.0 38.8</cell><cell>41.6 33.2 36.0 72.0</cell></row><row><cell>(6) ME + CE + L α lap + L α F + L c 1 + L c F (7) ME + CE + L α lap + L α F + L c 1 + L c F + DA (8) ME + CE + L α lap + L α F + L c 1 + L c F + DA -ReJPEGing (9) ME + CE + L α lap + L α F + L c 1 + L c F + DA -GaussianBlur</cell><cell>38.0 84.1 55.1 69.1</cell><cell>8.8 29.1 15.5 23.5</cell><cell>16.9 39.2 24.6 39.6</cell><cell>35.4 -54.7 69.1</cell></row><row><cell cols="2">alpha map in the range of [4, 25].</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The foreground result on the Composition-1k dataset.</figDesc><table><row><cell>Methods</cell><cell></cell><cell></cell><cell>SAD</cell><cell>MSE(10 3 )</cell></row><row><cell>Global Matting [19]</cell><cell></cell><cell></cell><cell>220.39</cell><cell>36.29</cell></row><row><cell cols="2">Closed-Form Matting [27]</cell><cell></cell><cell>254.15</cell><cell>40.89</cell></row><row><cell>KNN Matting [6]</cell><cell></cell><cell></cell><cell>281.92</cell><cell>36.29</cell></row><row><cell cols="3">(6) ME + CE + L α lap + L α F + L c 1 + L c F</cell><cell>61.72</cell><cell>3.24</cell></row><row><cell cols="3">(7) ME + CE + L α lap + L α F + L c 1 + L c F + DA</cell><cell>94.41</cell><cell>8.67</cell></row><row><cell cols="3">(8) ME + CE + L α lap + L α F + L c 1 + L c F + DA -ReJPEGing</cell><cell>73.79</cell><cell>4.96</cell></row><row><cell cols="3">(9) ME + CE + L α lap + L α F + L c 1 + L c F + DA -GaussianBlur</cell><cell>85.8</cell><cell>7.10</cell></row><row><cell cols="5">Table 3. Parameter numbers of our models and their performance</cell></row><row><cell cols="2">on the Composition-1K dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4"># of Parameters SAD MSE(10 3 ) Grad Conn</cell></row><row><cell>ME (model 2)</cell><cell>54.0 M</cell><cell>43.9</cell><cell>11.8</cell><cell>20.6 41.6</cell></row><row><cell>ME (deeper model 2)</cell><cell>117.0 M</cell><cell>43.7</cell><cell>11.0</cell><cell>21.2 42.6</cell></row><row><cell>ME + CE (model 3)</cell><cell>107.5 M</cell><cell>35.8</cell><cell>8.2</cell><cell>17.3 33.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison of visual quality on the real-world dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">Mean score Std</cell></row><row><cell>ME + CE + L lap</cell><cell>4.64</cell><cell>0.42</cell></row><row><cell>ME + CE + L lap + L F</cell><cell>4.69</cell><cell>0.40</cell></row><row><cell>ME + CE + L lap + L F + DA</cell><cell>5.03</cell><cell>0.25</cell></row><row><cell cols="3">ting [27], KNN Matting [6], Three-layer Graph [29],</cell></row><row><cell cols="3">Information-flow Matting [2], and recent deep learning mat-</cell></row><row><cell cols="3">ting approaches, including DCNN Matting [8], Deep Mat-</cell></row><row><cell>ting</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The source images in <ref type="figure">Figure 1</ref> are used under a Creative Commons license from Flickr users Robbie Sproule, MEGA PISTOLO and Jeff Latimer. The background images in <ref type="figure">Figure 1</ref> are from the MS-COCO dataset <ref type="bibr" target="#b29">[30]</ref>. Source images used in <ref type="figure">Figure 2, 3, 4, 6, and 7</ref> are from the matting dataset shared by Xu et al. <ref type="bibr" target="#b51">[52]</ref>. We thank Nvidia for their GPU donation and Google for their cloud credits.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Photo forensics from jpeg dimples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tunc Ozan Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Transparent Object Matting from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom-Net</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9233" to="9241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">KNN matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image matting with local and nonlocal smooth priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">Zhiying</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1902" to="1907" />
		</imprint>
	</monogr>
	<note>Qinping Zhao, and Ping Tan</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inso</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for natural image matting using initial alpha mattes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1054" to="1067" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">264</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exposing digital image forgeries by illumination color classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago José De</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elli</forename><surname>Angelopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helio</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>De Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1182" to="1194" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning classification with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virginia R De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detection of metadata tampering through discrepancy between image content and metadata using multi-task deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallabi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bor-Chun Is Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iterative transductive learning for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanwu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4282" to="4286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast matting using large kernel matting laplacian matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2165" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting double jpeg compression with the same quantization matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangjun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun Qing</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="848" to="856" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fighting fake news: Image splice detection via learned self-consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlocal matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2193" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="228" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<title level="m">Spectral matting. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1699" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Threelayer graph framework with the sumd feature for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huali</forename><surname>Pi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detection of misaligned cropping and recompression with the same quantization matrix and relevant forgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingzhong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd international ACM workshop on Multimedia in forensics and intelligence</title>
		<meeting>the 3rd international ACM workshop on Multimedia in forensics and intelligence</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jpeg error analysis and its applications to digital image forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="491" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alphagan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10088</idno>
		<title level="m">Generative adversarial networks for natural image matting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A data set of authentic and spliced image blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ADVENT Technical Report</title>
		<imprint>
			<biblScope unit="page" from="203" to="2004" />
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exposing digital forgeries by detecting traces of resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on signal processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="758" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simultaneous foreground, background, and alpha estimation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2157" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1826" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving image matting using comprehensive sampling sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Blue screen matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvy Ray</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Blinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poisson matting. In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Digital image forensics via intrinsic fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kj Ray</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information forensics and security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="117" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nima: Neural image assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3998" to="4011" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An iterative optimization approach for unified image segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="936" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image and video matting: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="175" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning based digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
