<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Texas State University</orgName>
								<address>
									<settlement>San Marcos</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Huawei Technologies Ireland</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Texas State University</orgName>
								<address>
									<settlement>San Marcos</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-view image translation is challenging because it involves images with drastically different views and severe deformation. In this paper, we propose a novel approach named Multi-Channel Attention SelectionGAN (Selection-GAN) that makes it possible to generate images of natural scenes in arbitrary viewpoints, based on an image of the scene and a novel semantic map. The proposed SelectionGAN explicitly utilizes the semantic information and consists of two stages. In the first stage, the condition image and the target semantic map are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using a multi-channel attention selection mechanism. Moreover, uncertainty maps automatically learned from attentions are used to guide the pixel loss for better network optimization. Extensive experiments on Dayton <ref type="bibr" target="#b41">[42]</ref>, CVUSA [44]  and Ego2Top [1]  datasets show that our model is able to generate significantly better results than the state-of-the-art methods. The source code, data and trained models are available at https://github. com/Ha0Tang/SelectionGAN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cross-view image translation is a task that aims at synthesizing new images from one viewpoint to another. It has been gaining a lot interest especially from computer vision and virtual reality communities, and has been widely investigated in recent years <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46]</ref>. Earlier works studied this problem using encoder-decoder Convolutional Neural Networks (CNNs) by involving viewpoint codes in the bottle-neck representations for city scene synthesis <ref type="bibr" target="#b52">[53]</ref> and 3D object translation <ref type="bibr" target="#b45">[46]</ref>. There also exist some works exploring Generative Adversarial Net-* Equal contribution. <ref type="figure">Figure 1</ref>: Examples of our cross-view translation results on two public benchmarks i.e. Dayton <ref type="bibr" target="#b41">[42]</ref> and CVUSA <ref type="bibr" target="#b43">[44]</ref>, and on our self-created large-scale benchmark based on Ego2Top <ref type="bibr" target="#b0">[1]</ref>. works (GAN) for similar tasks <ref type="bibr" target="#b30">[31]</ref>. However, these existing works consider an application scenario in which the objects and the scenes have a large degree of overlapping in appearances and views.</p><p>Different from previous works, in this paper, we focus on a more challenging setting in which fields of views have little or even no overlap, leading to significantly distinct structures and appearance distributions for the input source and the output target views, as illustrated in <ref type="figure">Fig. 1</ref>. To tackle this challenging problem, Regmi and Borji <ref type="bibr" target="#b33">[34]</ref> recently proposed a conditional GAN model which jointly learns the generation in both the image domain and the corresponding semantic domain, and the semantic predictions are further utilized to supervise the image generation. Although this approach performed an interesting exploration, we observe unsatisfactory aspects mainly in the generated scene structure and details, which are due to different reasons. First, since it is always costly to obtain manually annotated semantic labels, the label maps are usually produced from pretrained semantic models from other large-scale segmentation datasets, leading to insufficiently accurate predictions for all the pixels, and thus misguiding the image generation. Second, we argue that the translation with a single <ref type="figure">Figure 2</ref>: Overview of the proposed SelectionGAN. Stage I presents a cycled semantic-guided generation sub-network which accepts images from one view and conditional semantic maps and simultaneously synthesizes images and semantic maps in another view. Stage II takes the coarse predictions and the learned deep semantic features from stage I, and performs a fine-grained generation using the proposed multi-channel attention selection module. phase generation network is not able to capture the complex scene structural relationships between the two views. Third, a three-channel generation space may not be suitable enough for learning a good mapping for this complex synthesis problem. Given these problems, could we enlarge the generation space and learn an automatic selection mechanism to synthesize more fine-grained generation results?</p><p>Based on these observations, in this paper, we propose a novel Multi-Channel Attention Selection Generative Adversarial Network (SelectionGAN), which contains two generation stages. The overall framework of the proposed Selec-tionGAN is shown in <ref type="figure">Fig. 2</ref>. In this first stage, we learn a cycled image-semantic generation sub-network, which accepts a pair consisting of an image and the target semantic map, and generates images for the other view, which further fed into a semantic generation network to reconstruct the input semantic maps. This cycled generation adds more strong supervision between the image and semantic domains, facilitating the optimization of the network.</p><p>The coarse outputs from the first generation network, including the input image, together with the deep feature maps from the last layer, are input into the second stage networks. Several intermediate outputs are produced, and simultaneously we learn a set of multi-channel attention maps with the same number as the intermediate generations.</p><p>These attention maps are used to spatially select from the intermediate generations, and are combined to synthesize a final output. Finally, to overcome the inaccurate semantic label issue, the multi-channel attention maps are further used to generate uncertainty maps to guide the reconstruction loss. Through extensive experimental evaluations, we demonstrate that SelectionGAN produces remarkably better results than the baselines such as Pix2pix <ref type="bibr" target="#b15">[16]</ref>, Zhai et al. <ref type="bibr" target="#b47">[48]</ref>, X-Fork <ref type="bibr" target="#b33">[34]</ref> and X-Seq <ref type="bibr" target="#b33">[34]</ref>. Moreover, we establish state-of-the-art results on three different datasets for the arbitrary cross-view image synthesis task.</p><p>Overall, the contributions of this paper are as follows: • A novel multi-channel attention selection GAN framework (SelectionGAN) for the cross-view image translation task is presented. It explores cascaded semantic guidance with a coarse-to-fine inference, and aims at producing a more detailed synthesis from richer and more diverse multiple intermediate generations. • A novel multi-channel attention selection module is proposed, which is utilized to attentively select interested intermediate generations and is able to significantly boost the quality of the final output. The multi-channel attention module also effectively learns uncertainty maps to guide the pixel loss for more robust optimization. • Extensive experiments clearly demonstrate the effectiveness of the proposed SelectionGAN, and show state-ofthe-art results on two public benchmarks, i.e. Dayton <ref type="bibr" target="#b41">[42]</ref> and CVUSA <ref type="bibr" target="#b43">[44]</ref>. Meanwhile, we also create a largerscale cross-view synthesis benchmark using the data from Ego2Top <ref type="bibr" target="#b0">[1]</ref>, and present results of multiple baseline models for the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">[11]</ref> have shown the capability of generating better high-quality images <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36]</ref>, compared to existing methods such as Restricted Boltzmann Machines <ref type="bibr" target="#b12">[13]</ref> and Deep Boltzmann Machines <ref type="bibr" target="#b13">[14]</ref>. A vanilla GAN model <ref type="bibr" target="#b10">[11]</ref> has two important components, i.e. a generator G and a discriminator D.</p><p>The goal of G is to generate photo-realistic images from a noise vector, while D is trying to distinguish between a real image and the image generated by G. Although it is successfully used in generating images of high visual fidelity <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b31">32]</ref>, there are still some challenges, i.e. how to generate images in a controlled setting. To generate domain-specific images, Conditional GAN (CGAN) <ref type="bibr" target="#b26">[27]</ref> has been proposed. CGAN usually combines a vanilla GAN and some external information, such as class labels or tags <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37]</ref>, text descriptions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref>, human pose <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref> and reference images <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref>. Image-to-Image Translation frameworks adopt inputoutput data to learn a parametric mapping between inputs and outputs. For example, Isola et al. <ref type="bibr" target="#b15">[16]</ref> propose Pix2pix, which is a supervised model and uses a CGAN to learn a translation function from input to output image domains. Zhu et al. <ref type="bibr" target="#b53">[54]</ref> introduce CycleGAN, which targets unpaired image translation using the cycle-consistency loss.</p><p>To further improve the generation performance, the attention mechanism has been recently investigated in image translation, such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. However, to the best of our knowledge, our model is the first attempt to incorporate a multi-channel attention selection module within a GAN framework for image-to-image translation task. Learning Viewpoint Transformations. Most existing works on viewpoint transformation have been conducted to synthesize novel views of the same object, such as cars, chairs and tables <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5]</ref>. Another group of works explore the cross-view scene image generation, such as <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b52">53]</ref>. However, these works focus on the scenario in which the objects and the scenes have a large degree of overlapping in both appearances and views. Recently, several works started investigating image translation problems with drastically different views and generating a novel scene from a given arbitrary one. This is a more challenging task since different views have little or no overlap. To tackle this problem, Zhai et al. <ref type="bibr" target="#b47">[48]</ref> try to generate panoramic ground-level images from aerial images of the same location by using a convolutional neural network. Krishna and Ali <ref type="bibr" target="#b33">[34]</ref> propose a X-Fork and a X-Seq GAN-based structure to address the aerial to street view image translation task using an extra semantic segmentation map. However, these methods are not able to generate satisfactory results due to the drastic difference between source and target views and their model design. To overcome these issues, we aim at a more effective network design, and propose a novel multi-channel attention selection GAN, which allows to automatically select from multiple diverse and rich intermediate generations and thus significantly improves the generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Channel Attention Selection GAN</head><p>In this section we present the details of the proposed multi-channel attention selection GAN. An illustration of the overall network structure is depicted in <ref type="figure">Fig. 2</ref>. In the first stage, we present a cascade semantic-guided generation sub-network, which utilizes the images from one view and conditional semantic maps from another view as inputs, and reconstruct images in another view. These images are further input into a semantic generator to recover the input semantic map forming a generation cycle. In the sec-ond stage, the coarse synthesis and the deep features from the first stage are combined, and then are passed to the proposed multi-channel attention selection module, which aims at producing more fine-grained synthesis from a larger generation space and also at generating uncertainty maps to guide multiple optimization losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cascade Semantic-guided Generation</head><p>Semantic-guided Generation. Cross-view synthesis is a challenging task, especially when the two views have little overlapping as in our study case, which apparently leads to ambiguity issues in the generation process. To alleviate this problem, we use semantic maps as conditional guidance. Since it is always costly to obtain annotated semantic maps, following <ref type="bibr" target="#b33">[34]</ref> we generate the maps using segmentation deep models pretrained from large-scale scene parsing datasets such as Cityscapes <ref type="bibr" target="#b5">[6]</ref>. However, <ref type="bibr" target="#b33">[34]</ref> uses semantic maps only in the reconstruction loss to guide the generation of semantics, which actually provides a weak guidance. Different from theirs, we apply the semantic maps not only in the output loss but also as part of the network's input. Specifically, as shown in <ref type="figure">Fig. 2</ref>, we concatenate the input image I a from the source view and the semantic map S g from a target view, and input them into the image generator G i and synthesize the target view image I g as I g =G i (I a , S g ). In this way, the ground-truth semantic maps provide stronger supervision to guide the cross-view translation in the deep network. Semantic-guided Cycle. Regmi and Borji <ref type="bibr" target="#b33">[34]</ref> observed that the simultaneous generation of both the images and the semantic maps improves the generation performance. Along the same line, we propose a cycled semantic generation network to benefit more the semantic information in learning. The conditional semantic map S g together with the input image I a are input into the image generator G i , and produce the synthesized image I g . Then I g is further fed into the semantic generator G s which reconstructs a new semantic map S g . We can formalize the process as</p><formula xml:id="formula_0">S g =G s (I g )=G s (G i (I a , S g )).</formula><p>Then the optimization objective is to make S g as close as possible to S g , which naturally forms a semantic generation cycle, i.e. [I a , S g ]</p><formula xml:id="formula_1">Gi → I g Gs → S g ≈S g .</formula><p>The two generators are explicitly connected by the ground-truth semantic maps, which in this way provide extra constraints on the generators to learn better the semantic structure consistency. Cascade Generation. Due to the complexity of the task, after the first stage, we observe that the image generator G i outputs a coarse synthesis, which yields blurred scene details and high pixel-level dis-similarity with the targetview images. This inspires us to explore a coarse-to-fine generation strategy in order to boost the synthesis performance based on the coarse predictions. Cascade models have been used in several other computer vision tasks such as object detection <ref type="bibr" target="#b1">[2]</ref> and semantic segmentation <ref type="bibr" target="#b6">[7]</ref>, and have shown great effectiveness. In this paper, we introduce the cascade strategy to deal with the complex cross-view translation problem. In both stages we have a basic cycled semantic guided generation sub-network, while in the second stage, we propose a novel multi-channel attention selection module to better utilize the coarse outputs from the first stage and produce fine-grained final outputs. We observed significant improvement by using the proposed cascade strategy, illustrated in the experimental part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Channel Attention Selection</head><p>An overview of the proposed multi-channel attention selection module G a is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. The module consists of a multi-scale spatial pooling and a multi-channel attention selection component. Multi-Scale Spatial Pooling. Since there exists a large object/scene deformation between the source view and the target view, a single-scale feature may not be able to capture all the necessary spatial information for a fine-grained generation. Thus we propose a multi-scale spatial pooling scheme, which uses a set of different kernel size and stride to perform a global average pooling on the same input features. By so doing, we obtain multi-scale features with different receptive fields to perceive a different spatial context. More specifically, given the coarse inputs and the deep semantic features produced from the stage I, we first concatenate all of them as new features denoted as F c for the stage II as:</p><formula xml:id="formula_2">F c = concat(I a , I g , F i , F s )<label>(1)</label></formula><p>where concat(·) is a function for channel-wise concatenation operation; F i and F s are features from the last con-volution layers of the generators G i and G s , respectively. We apply a set of M spatial scales {s i } M i=1 in pooling, resulting in pooled features with different spatial resolution. Different from the pooling scheme used in <ref type="bibr" target="#b50">[51]</ref> which directly combines all the features after pooling, we first select each pooled feature via an element-wise multiplication with the input feature. Since in our task the input features are from different sources, highly correlated features would preserve more useful information for the generation. Let us denote pl up s (·) as pooling at a scale s followed by an up-sampling operation to rescale the pooled feature at the same resolution, and ⊗ as element-wise multiplication, we can formalize the whole process as follows:</p><formula xml:id="formula_3">F c ← concat F c ⊗ pl up 1 (F c ), . . . , F c ⊗ pl up M (F c ))</formula><p>(2) Then the features F c are fed into a convolutional layer, which produces new multi-scale features F c for the use in the multi-channel selection module. Multi-Channel Attention Selection. In previous crossview image synthesis works, the image is generated only in a three-channel RGB space. We argue that this is not enough for the complex translation problem we are dealing with, and thus we explore using a larger generation space to have a richer synthesis via constructing multiple intermediate generations. Accordingly, we design a multi-channel attention mechanism to automatically perform spatial and temporal selection from the generations to synthesize a finegrained final output.</p><p>Given the multi-scale feature volume F c ∈R h×w×c , where h and w are width and height of the features, and c is the number of channels, we consider two directions. One is for the generation of multiple intermediate image syn-thesis, and the other is for the generation of multi-channel attention maps. To produce N different intermediate gener-</p><formula xml:id="formula_4">ations I G ={I i G } N i=1 , a convolution operation is performed with N convolutional filters {W i G , b i G } N i=1</formula><p>followed by a tanh(·) non-linear activation operation. For the generation of corresponding N attention maps, the other group of fil-</p><formula xml:id="formula_5">ters {W i A , b i A } N i=1</formula><p>is applied. Then the intermediate generations and the attention maps are calculated as follows:</p><formula xml:id="formula_6">I i G = tanh(F c W i G + b i G ), for i = 1, . . . , N I i A = Softmax(F c W i A + b i A ), for i = 1, . . . , N<label>(3)</label></formula><p>where Softmax(·) is a channel-wise softmax function used for the normalization. Finally, the learned attention maps are utilized to perform channel-wise selection from each intermediate generation as follows:</p><formula xml:id="formula_7">I g = (I 1 A ⊗ I 1 G ) ⊕ · · · ⊕ (I N A ⊗ I N G )<label>(4)</label></formula><p>where I g represents the final synthesized generation selected from the multiple diverse results, and the symbol ⊕ denotes the element-wise addition. We also generate a final semantic map in the second stage as in the first stage, i.e. S g =G s (I g ). Due to the same purpose of the two semantic generators, we use a single G s twice by sharing the parameters in both stages to reduce the network capacity. Uncertainty-guided Pixel Loss. As we discussed in the introduction, the semantic maps obtained from the pretrained model are not accurate for all the pixels, which leads to a wrong guidance during training. To tackle this issue, we propose the generated attention maps to learn uncertainty maps to control the optimization loss. The uncertainty learning has been investigated in <ref type="bibr" target="#b18">[19]</ref> for multi-task learning, and here we introduce it for solving the noisy semantic label problem. Assume that we have K different loss maps which need a guidance. The multiple generated attention maps are first concatenated and passed to a convolution layer with K filters {W i u } K i=1 to produce a set of K uncertainty maps. The reason of using the attention maps to generate uncertainty maps is that the attention maps directly affect the final generation leading to a close connection with the loss. Let L i p denote a pixel-level loss map and U i denote the i-th uncertainty map, we have:</p><formula xml:id="formula_8">U i = σ W i u (concat(I 1 A , . . . , I N A ) + b i u L i p ← L i p U i + log U i , for i = 1, . . . , K<label>(5)</label></formula><p>where σ(·) is a Sigmoid function for pixel-level normalization. The uncertainty map is automatically learned and acts as a weighting scheme to control the optimization loss.</p><p>Parameter-Sharing Discriminator. We extend the vanilla discriminator in <ref type="bibr" target="#b15">[16]</ref> to a parameter-sharing structure. In the first stage, this structure takes the real image I a and the generated image I g or the ground-truth image I g as input.</p><p>The discriminator D learns to tell whether a pair of images from different domains is associated with each other or not.</p><p>In the second stage, it accepts the real image I a and the generated image I g or the real image I g as input. This pairwise input encourages D to discriminate the diversity of image structure and capture the local-aware information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overall Optimization Objective</head><p>Adversarial Loss. In the first stage, the adversarial loss of D for distinguishing synthesized image pairs [I a , I g ] from real image pairs [I a , I g ] is formulated as follows,</p><formula xml:id="formula_9">L cGAN (I a , I g ) =E Ia,Ig [log D(I a , I g )] + E Ia,I g log(1 − D(I a , I g )) .<label>(6)</label></formula><p>In the second stage, the adversarial loss of D for distinguishing synthesized image pairs [I a , I g ] from real image pairs [I a , I g ] is formulated as follows:</p><formula xml:id="formula_10">L cGAN (I a , I g )=E Ia,Ig [log D(I a , I g )] + E Ia,I g log(1 − D(I a , I g )) .<label>(7)</label></formula><p>Both losses aim to preserve the local structure information and produce visually pleasing synthesized images. Thus, the adversarial loss of the proposed SelectionGAN is the sum of Eq. (6) and <ref type="formula" target="#formula_10">(7)</ref>,</p><formula xml:id="formula_11">L cGAN = L cGAN (I a , I g ) + λL cGAN (I a , I g ). (8)</formula><p>Overall Loss. The total optimization loss is a weighted sum of the above losses. Generators G i , G s , attention selection network G a and discriminator D are trained in an end-toend fashion optimizing the following min-max function, min {Gi,Gs,Ga}</p><formula xml:id="formula_12">max {D} L = 4 i=1 λ i L i p + L cGAN + λ tv L tv .<label>(9)</label></formula><p>where L i p uses the L1 reconstruction to separately calculate the pixel loss between the generated images I g , S g , I g and S g and the corresponding real images. L tv is the total variation regularization <ref type="bibr" target="#b16">[17]</ref> on the final synthesized image I g . λ i and λ tv are the trade-off parameters to control the relative importance of different objectives. The training is performed by solving the min-max optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Network Architecture. For a fair comparison, we employ U-Net <ref type="bibr" target="#b15">[16]</ref> as our generator architectures G i and G s . U-Net is a network with skip connections between a downsampling encoder and an up-sampling decoder. Such architecture comprehensively retains contextual and textural information, which is crucial for removing artifacts and padding textures. Since our focus is on the cross-view image generation task, G i is more important than G s . Thus we use a deeper network for G i and a shallow network for <ref type="figure">Figure 4</ref>: Results generated by different methods in 256×256 resolution in a2g and g2a directions on Dayton dataset. <ref type="table">Table 1</ref>: SSIM, PSNR, Sharpness Difference (SD) and KL score (KL) of different methods. For these metrics except KL score, higher is better. (*) These results are reported in <ref type="bibr" target="#b33">[34]</ref>. Training Details. Following <ref type="bibr" target="#b33">[34]</ref>, we use RefineNet <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b51">[52]</ref> to generate segmentation maps on Dayton and Ego2Top datasets as training data, respectively. We follow the optimization method in <ref type="bibr" target="#b10">[11]</ref> to optimize the proposed SelectionGAN, i.e. one gradient descent step on discriminator and generators alternately. We first train G i , G s , G a with D fixed, and then train D with G i , G s , G a fixed. The proposed SelectionGAN is trained and optimized in an end-to-end fashion. We employ Adam <ref type="bibr" target="#b20">[21]</ref> with momentum terms β 1 =0.5 and β 2 =0.999 as our solver. The initial learning rate for Adam is 0.0002. The network initialization strategy is Xavier <ref type="bibr" target="#b9">[10]</ref>, weights are initialized from a Gaussian distribution with standard deviation 0.2 and mean 0.  <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b33">34]</ref>, the aerial images are center-cropped to 224×224 and resized to 256×256. For the ground level images and corresponding segmentation maps, we take the first quarter of both and resize them to 256×256; (iii) The Ego2Top dataset <ref type="bibr" target="#b0">[1]</ref> is more challenging and contains different indoor and outdoor conditions. Each case contains one top-view video and several egocentric videos captured by the people visible in the top-view camera. This dataset has more than 230,000 frames. For training data, we randomly select 386,357 pairs and each pair is composed of two images of the same scene but different viewpoints. We randomly select 25,600 pairs for evaluation.</p><p>Parameter Settings. For a fair comparison, we adopt the same training setup as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref>. All images are scaled to 256×256, and we enabled image flipping and random crops for data augmentation. Similar to <ref type="bibr" target="#b33">[34]</ref> Evaluation Protocol. Similar to <ref type="bibr" target="#b33">[34]</ref>, we employ Inception Score, top-k prediction accuracy and KL score for the quantitative analysis. These metrics evaluate the generated   images from a high-level feature space. We also employ pixel-level similarity metrics to evaluate our method, i.e. Structural-Similarity (SSIM), Peak Signal-to-Noise Ratio (PSNR) and Sharpness Difference (SD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>Baseline Models. We conduct ablation study in a2g (aerialto-ground) direction on Dayton dataset. To reduce the training time, we randomly select 1/3 samples from the whole 55,000/21,048 samples i.e. around 18,334 samples for training and 7,017 samples for testing. The proposed SelectionGAN considers eight baselines (A, B, C, D, E, F, G, H) as shown in <ref type="table" target="#tab_5">Table 4</ref>. Baseline A uses a Pix2pix structure <ref type="bibr" target="#b15">[16]</ref> and generates I g using a single image I a . Baseline B uses the same Pix2pix model and generates I g using the corresponding semantic map S g . Baseline C also uses the Pix2pix structure, and inputs the combination of a conditional image I a and the target semantic map S g to the generator G i . Baseline D uses the proposed cycled semantic generation upon Baseline C. Baseline E represents the <ref type="figure">Figure 5</ref>: Qualitative results of coarse-to-fine generation on CVUSA dataset. pixel loss guided by the learned uncertainty maps. Baseline F employs the proposed multi-channel attention selection module to generate multiple intermediate generations, and to make the neural network attentively select which part is more important for generating a scene image with a new viewpoint. Baseline G adds the total variation regularization on the final result I g . Baseline H employs the proposed multi-scale spatial pooling module to refine the features F c from stage I. All the baseline models are trained and tested on the same data using the configuration. Ablation Analysis. The results of ablation study are shown in <ref type="table" target="#tab_5">Table 4</ref>. We observe that Baseline B is better than baseline A since S g contains more structural information  <ref type="figure">Figure 6</ref>: Results generated by different methods in 256×256 resolution in a2g direction on CVUSA dataset. than I a . By comparison Baseline A with Baseline C, the semantic-guided generation improves SSIM, PSNR and SD by 8.19, 3.1771 and 0.3205, respectively, which confirms the importance of the conditional semantic information; By using the proposed cycled semantic generation, Baseline D further improves over C, meaning that the proposed semantic cycle structure indeed utilizes the semantic information in a more effective way, confirming our design motivation; Baseline E outperforms D showing the importance of using the uncertainty maps to guide the pixel loss map which contains an inaccurate reconstruction loss due to the wrong semantic labels produced from the pretrained segmentation model; Baseline F significantly outperforms E with around 4.67 points gain on the SSIM metric, clearly demonstrating the effectiveness of the proposed multi-channel attention selection scheme; We can also observe from <ref type="table" target="#tab_5">Table 4</ref> that, by adding the proposed multi-scale spatial pool scheme and the TV regularization, the overall performance is further boosted. Finally, we demonstrate the advantage of the proposed two-stage strategy over the one-stage method. Several examples are shown in <ref type="figure">Fig. 5</ref>. It is obvious that the coarse-to-fine generation model is able to generate sharper results and contains more details than the one-stage model. State-of-the-art Comparisons. We compare our Selec-tionGAN with four recently proposed state-of-the-art methods, which are Pix2pix <ref type="bibr" target="#b15">[16]</ref>, Zhai et al. <ref type="bibr" target="#b47">[48]</ref>, X-Fork <ref type="bibr" target="#b33">[34]</ref> and X-Seq <ref type="bibr" target="#b33">[34]</ref>. The comparison results are shown in Tables 1, 2, 3, and 5. We can observe the significant improvement of SelectionGAN in these tables. SelectionGAN consistently outperforms Pix2pix, Zhai et al., X-Fork and X-Seq on all the metrics except for Inception Score. In some cases in <ref type="table" target="#tab_4">Table 3</ref> we achieve a slightly lower performance as compared with X-Seq. However, we generate much more photo-realistic results than X-Seq as shown in <ref type="figure">Fig. 4 and 6</ref>.</p><p>Qualitative Evaluation. The qualitative results in higher It can be seen that our method generates more clear details on objects/scenes such as road, tress, clouds, car than the other comparison methods in the generated ground level images. For the generated aerial images, we can observe that grass, trees and house roofs are well rendered compared to others. Moreover, the results generated by our method are closer to the ground truths in layout and structure, such as the results in a2g direction in <ref type="figure">Fig. 4 and 6</ref>.</p><p>Arbitrary Cross-View Image Translation. Since Dayton and CVUSA datasets only contain two views in one scene, i.e. aerial and ground views. We further use the Ego2Top dataset to conduct the arbitrary cross-view image translation experiments. The quantitative and qualitative results are shown in <ref type="table" target="#tab_6">Table 5</ref> and <ref type="figure" target="#fig_1">Fig. 7</ref>, respectively. Given an image and some novel semantic maps, SelectionGAN is able to generate the same scene but with different viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose the Multi-Channel Attention Selection GAN (SelectionGAN) to address a novel image synthesizing task by conditioning on a reference image and a target semantic map. In particular, we adopt a cascade strategy to divide the generation procedure into two stages. Stage I aims to capture the semantic structure of the scene and Stage II focus on more appearance details via the proposed multi-channel attention selection module. We also propose an uncertainty map-guided pixel loss to solve the inaccurate semantic labels issue for better optimization. Extensive experimental results on three public datasets demonstrate that our method obtains much better results than the state-of-the-art.</p><p>This supplementary document provides additional results supporting the claims of the main paper. First, we provide detailed experimental results about the influence of the number of attention channels (Sec. 6). Additionally, we compare our two-stage model with one-stage model (Sec. 7). We also provide the visualization results of the generated uncertainty maps (Sec. 8) and the arbitrary cross-view image translation experiments on Ego2Top dataset <ref type="bibr" target="#b0">[1]</ref> (Sec. 9). Finally, we compare our SelectionGAN with the state-ofthe-arts methods, i.e. Pix2pix <ref type="bibr" target="#b15">[16]</ref>, X-Fork <ref type="bibr" target="#b33">[34]</ref> and X-Seq <ref type="bibr" target="#b33">[34]</ref>. Specifically, we compare the results of the generated segmentation maps (Sec. 10), and visualize the comparison results on Dayton <ref type="bibr" target="#b41">[42]</ref>, CVUSA <ref type="bibr" target="#b43">[44]</ref> and Ego2Top <ref type="bibr" target="#b0">[1]</ref> datasets (Sec. 11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Influence of the Number of Attention Channels N</head><p>We investigate the influence of the number of attention channels N in Equation 3 in the main paper. Results are shown in <ref type="table" target="#tab_7">Table 6</ref>. We observe that the performance tends to be stable after N = 10. Thus, taking both performance and training speed into consideration, we have set N = 10 in all our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Coarse-to-Fine Generation</head><p>We provide more comparison results of coarse-to-fine generation in <ref type="table" target="#tab_8">Table 7</ref> and Figures 8, 9 and 10. We observe that our two-stage method generate much visually better results than the one-stage model, which further confirms our motivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Visualization of Uncertainty Map</head><p>In <ref type="figure" target="#fig_2">Figures 8, 9</ref>, 10 and 11, we show some samples of the generated uncertainty maps. We can see that the generated uncertainty maps learn the layout and structure of the target images. Note that most textured regions are similar in our generation images, while the junction/edge of different regions is uncertain, and thus the model learns to highlight these parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Arbitrary Cross-View Image Translation</head><p>We also conducted the arbitrary cross-view image translation experiments on Ego2Top dataset. As we can see from <ref type="figure" target="#fig_5">Figure 11</ref>, given an image and some novel semantic maps, SelectionGAN is able to generate the same scene but with different viewpoints in both outdoor and indoor environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Generated Segmentation Maps</head><p>Since the proposed SelectionGAN can generate segmentation maps, we also compare it with X-Fork <ref type="bibr" target="#b33">[34]</ref> and X-Seq [34] on  <ref type="table">Table 8</ref>: Per-class accuracy and mean IOU for the generated segmentation maps on Dayton dataset. For both metric, higher is better. (*) These results are reported in <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method a2g</head><p>Per-Class Acc. mIOU X-Fork <ref type="bibr" target="#b33">[34]</ref> 0.6262* 0.4163* X-Seq <ref type="bibr" target="#b33">[34]</ref> 0.4783* 0.3187* SelectionGAN (Ours) 0.6415 0.5455</p><p>Dayton dataset. Following <ref type="bibr" target="#b33">[34]</ref>, we compute per-class accuracies and mean IOU for the most common classes in this dataset: "vegetation", "road", "building" and "sky" in ground segmentation maps. Results are shown in <ref type="table">Table 8</ref>. We can see that the proposed SelectionGAN achieves better results than X-Fork <ref type="bibr" target="#b33">[34]</ref> and X-Seq [34] on both metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">State-of-the-art Comparisons</head><p>In <ref type="figure" target="#fig_0">Figures 12, 13</ref>, 14, 15 and 16, we show more image generation results on Dayton, CVUSA and Ego2Top datasets compared with the state-of-the-art methods i.e., Pix2pix <ref type="bibr" target="#b15">[16]</ref>, X-Fork <ref type="bibr" target="#b33">[34]</ref> and X-Seq <ref type="bibr" target="#b33">[34]</ref>. For <ref type="figure" target="#fig_0">Figures 12, 13, 14, 15</ref>, we reproduced the results of Pix2pix <ref type="bibr" target="#b15">[16]</ref>, X-Fork <ref type="bibr" target="#b33">[34]</ref> and X-Seq <ref type="bibr" target="#b33">[34]</ref> using the pretrained models provided by the authors 1 . As we can see from all these figures, the proposed SelectionGAN achieves significantly visually better results than the competing methods.         </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the proposed multi-channel attention selection module. The multi-scale spatial pooling pools features in different receptive fields in order to have better generation of scene details; the multi-channel attention selection aims at automatically select from a set of intermediate diverse generations in a larger generation space to improve the generation quality. The symbols ⊕, ⊗, c and ↑ denote element-wise addition, element-wise multiplication, concatenation, and upsampling operation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Arbitrary cross-view image translation on Ego2Top dataset. resolution on Dayton and CVUSA datasets are shown in Fig. 4 and 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>1 https://github.com/kregmi/cross-view-image-synthesis Results generated by our SelectionGAN in 256×256 resolution in a2g direction on Dayton dataset. These samples were randomly selected for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Results generated by our SelectionGAN in 256×256 resolution in g2a direction on Dayton dataset. These samples were randomly selected for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Results generated by our SelectionGAN in 256×256 resolution in a2g direction on CVUSA dataset. These samples were randomly selected for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Arbitrary cross-view image translation on Ego2Top dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :</head><label>12</label><figDesc>Results generated by different methods in 64×64 resolution in both a2g (Top) and g2a (Bottom) directions on Dayton dataset. These samples were randomly selected for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>Results generated by different methods in 256×256 resolution in a2g direction on Dayton dataset. These samples were randomly selected for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Results generated by different methods in 256×256 resolution in g2a direction on Dayton dataset. These samples were randomly selected for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 :</head><label>15</label><figDesc>Results generated by different methods in 256×256 resolution in a2g direction on CVUSA dataset. These samples were randomly selected for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 16 :</head><label>16</label><figDesc>Results generated by different methods in 256×256 resolution on Ego2Top dataset. These samples were randomly selected for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, the low resolution (64×64) experiments on Dayton dataset are carried out for 100 epochs with batch size of 16, whereas the high resolution (256×256) experiments for this dataset are trained for 35 epochs with batch size of 4. For the CVUSA dataset, we follow the same setup as in<ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b33">34]</ref>, and train our network for 30 epochs with batch size of 4. For the Ego2Top dataset, all models are trained with 10 epochs using batch size 8. In our experiment, we set λ tv =1e−6, λ 1 =100, λ 2 =1, λ 3 =200 and λ 4 =2 in Eq. (9), and λ=4 in Eq.<ref type="bibr" target="#b7">(8)</ref>. The number of attention channels N in Eq. (3) is set to 10. The proposed SelectionGAN is implemented in PyTorch. We perform our experiments on Nvidia GeForce GTX 1080 Ti GPU with 11GB memory to accelerate both training and inference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracies of different methods. For this metric, higher is better. (*) These results are reported in<ref type="bibr" target="#b33">[34]</ref>.</figDesc><table><row><cell>Dir.</cell><cell>Method</cell><cell></cell><cell cols="2">Dayton (64×64)</cell><cell></cell><cell></cell><cell cols="2">Dayton (256×256)</cell><cell></cell><cell></cell><cell cols="2">CVUSA</cell></row><row><cell></cell><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell></row><row><cell></cell><cell>Zhai et al. [48]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">13.97* 14.03* 42.09* 52.29*</cell></row><row><cell></cell><cell>Pix2pix [16]</cell><cell>7.90*</cell><cell cols="3">15.33* 27.61* 39.07*</cell><cell>6.80*</cell><cell>9.15*</cell><cell cols="2">23.55* 27.00*</cell><cell>7.33*</cell><cell>9.25*</cell><cell cols="2">25.81* 32.67*</cell></row><row><cell>a2g</cell><cell>X-Fork [34]</cell><cell cols="12">16.63* 34.73* 46.35* 70.01* 30.00* 48.68* 61.57* 78.84* 20.58* 31.24* 50.51* 63.66*</cell></row><row><cell></cell><cell>X-Seq [34]</cell><cell>4.83*</cell><cell>5.56*</cell><cell cols="10">19.55* 24.96* 30.16* 49.85* 62.59* 80.70* 15.98* 24.14* 42.91* 54.41*</cell></row><row><cell></cell><cell>SelectionGAN (Ours)</cell><cell>45.37</cell><cell>79.00</cell><cell>83.48</cell><cell>97.74</cell><cell>42.11</cell><cell>68.12</cell><cell>77.74</cell><cell>92.89</cell><cell>41.52</cell><cell>65.51</cell><cell>74.32</cell><cell>89.66</cell></row><row><cell></cell><cell>Pix2pix [16]</cell><cell>1.65*</cell><cell>2.24*</cell><cell>7.49*</cell><cell cols="5">12.68* 10.23* 16.02* 30.90* 40.49*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>g2a</cell><cell>X-Fork [34] X-Seq [34]</cell><cell>4.00* 1.55*</cell><cell cols="7">16.41* 15.42* 35.82* 10.54* 15.29* 30.76* 37.32* 2.99* 6.27* 8.96* 12.30* 19.62* 35.95* 45.94*</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>-</cell></row><row><cell></cell><cell>SelectionGAN (Ours)</cell><cell>14.12</cell><cell>51.81</cell><cell>39.45</cell><cell>74.70</cell><cell>20.66</cell><cell>33.70</cell><cell>51.01</cell><cell>63.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Inception Score of different methods. For this metric, higher is better. (*) These results are reported in<ref type="bibr" target="#b33">[34]</ref>.</figDesc><table><row><cell>Dir.</cell><cell>Method</cell><cell cols="3">Dayton (64×64)</cell><cell cols="3">Dayton (256×256)</cell><cell></cell><cell>CVUSA</cell></row><row><cell></cell><cell></cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell></cell><cell>classes</cell><cell>class</cell><cell>classes</cell><cell>classes</cell><cell>class</cell><cell>classes</cell><cell>classes</cell><cell>class</cell><cell>classes</cell></row><row><cell></cell><cell>Zhai et al. [48]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">1.8434* 1.5171* 1.8666*</cell></row><row><cell></cell><cell>Pix2pix [16]</cell><cell cols="9">1.8029* 1.5014* 1.9300* 2.8515* 1.9342* 2.9083* 3.2771* 2.2219* 3.4312*</cell></row><row><cell></cell><cell>X-Fork [34]</cell><cell cols="9">1.9600* 1.5908* 2.0348* 3.0720* 2.2402* 3.0932* 3.4432* 2.5447* 3.5567*</cell></row><row><cell>a2g</cell><cell>X-Seq [34]</cell><cell cols="9">1.8503* 1.4850* 1.9623* 2.7384* 2.1304* 2.7674* 3.8151* 2.6738* 4.0077*</cell></row><row><cell></cell><cell>SelectionGAN (Ours)</cell><cell>2.1606</cell><cell>1.7213</cell><cell>2.1323</cell><cell>3.0613</cell><cell>2.2707</cell><cell>3.1336</cell><cell>3.8074</cell><cell>2.7181</cell><cell>3.9197</cell></row><row><cell></cell><cell>Real Data</cell><cell>2.3534</cell><cell>1.8135</cell><cell>2.3250</cell><cell>3.8319</cell><cell>2.5753</cell><cell>3.9222</cell><cell>4.8741</cell><cell>3.2959</cell><cell>4.9943</cell></row><row><cell></cell><cell>Pix2pix [16]</cell><cell cols="6">1.7970* 1.3029* 1.6101* 3.5676* 2.0325* 2.8141*</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-Fork [34]</cell><cell cols="6">1.8557* 1.3162* 1.6521* 3.1342* 1.8656* 2.5599*</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>g2a</cell><cell>X-Seq [34]</cell><cell cols="6">1.7854* 1.3189* 1.6219* 3.5849* 2.0489* 2.8414*</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SelectionGAN (Ours)</cell><cell>2.1571</cell><cell>1.4441</cell><cell>2.0828</cell><cell>3.2446</cell><cell>2.1331</cell><cell>3.4091</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Real Data</cell><cell>2.3015</cell><cell>1.5056</cell><cell>2.2095</cell><cell>3.7196</cell><cell>2.3626</cell><cell>3.8998</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablations study of the proposed SelectionGAN.</figDesc><table><row><cell>Baseline</cell><cell cols="4">Setup</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SD</cell></row><row><cell>A</cell><cell>Ia</cell><cell></cell><cell cols="2">Gi → I g</cell><cell>0.4555 19.6574 18.8870</cell></row><row><cell>B</cell><cell cols="2">Sg</cell><cell cols="3">Gi → I g</cell><cell>0.5223 22.4961 19.2648</cell></row><row><cell>C</cell><cell cols="3">[Ia, Sg]</cell><cell cols="2">Gi → I g</cell><cell>0.5374 22.8345 19.2075</cell></row><row><cell>D</cell><cell>[Ia, Sg]</cell><cell cols="3">Gi → I g</cell><cell>Gs → S g</cell><cell>0.5438 22.9773 19.4568</cell></row><row><cell>E</cell><cell cols="5">D + Uncertainty-Guided Pixel Loss</cell><cell>0.5522 23.0317 19.5127</cell></row><row><cell>F</cell><cell cols="5">E + Multi-Channel Attention Selection 0.5989 23.7562 20.0000</cell></row><row><cell>G</cell><cell cols="5">F + Total Variation Regularization</cell><cell>0.6047 23.7956 20.0830</cell></row><row><cell>H</cell><cell cols="5">G + Multi-Scale Spatial Pooling</cell><cell>0.6167 23.9310 20.1214</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results on Ego2Top dataset. For all metrics except KL score, higher is better.</figDesc><table><row><cell>Method</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SD</cell><cell></cell><cell>Inception Score</cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell><cell></cell><cell>KL Score</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">all classes Top-1 class Top-5 classes</cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell></cell></row><row><cell>Pix2pix [16]</cell><cell cols="3">0.2213 15.7197 16.5949</cell><cell>2.5418</cell><cell>1.6797</cell><cell>2.4947</cell><cell>1.22</cell><cell>1.57</cell><cell>5.33</cell><cell>6.86</cell><cell>120.46 ± 1.94</cell></row><row><cell>X-Fork [34]</cell><cell cols="3">0.2740 16.3709 17.3509</cell><cell>4.6447</cell><cell>2.1386</cell><cell>3.8417</cell><cell>5.91</cell><cell cols="3">10.22 20.98 30.29</cell><cell>22.12 ± 1.65</cell></row><row><cell>X-Seq [34]</cell><cell cols="3">0.2738 16.3788 17.2624</cell><cell>4.5094</cell><cell>2.0276</cell><cell>3.6756</cell><cell>4.78</cell><cell>8.96</cell><cell cols="2">17.04 24.40</cell><cell>25.19 ± 1.73</cell></row><row><cell cols="4">SelectionGAN (Ours) 0.6024 26.6565 19.7755</cell><cell>5.6200</cell><cell>2.5328</cell><cell>4.7648</cell><cell cols="4">28.31 54.56 62.97 76.30</cell><cell>3.05 ± 0.91</cell></row><row><cell>Real Data</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.4523</cell><cell>2.8507</cell><cell>5.4662</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Influence of the number of attention channels N .</figDesc><table><row><cell>n</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SD</cell></row><row><cell>0</cell><cell cols="3">0.5438 22.9773 19.4568</cell></row><row><cell>1</cell><cell cols="3">0.5522 23.0317 19.5127</cell></row><row><cell>5</cell><cell cols="3">0.5901 23.8068 20.0033</cell></row><row><cell cols="4">10 0.5986 23.7336 19.9993</cell></row><row><cell cols="4">32 0.5950 23.8265 19.9086</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results of coarse-to-fine generation. The best results are marked in blue color.</figDesc><table><row><cell>Baseline Stage I Stage II F √ F √ G √ G √ H √ H √</cell><cell>SSIM 0.5551 23.1919 19.6311 PSNR SD 0.5989 23.7562 20.0000 0.5680 23.2574 19.7371 0.6047 23.7956 20.0830 0.5567 23.1545 19.6034 0.6167 23.9310 20.1214</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ego2top: Matching viewers in egocentric and top-view videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Ardeshir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention-gan for object transfiguration in wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Soft-gated warping-gan for pose-guided person image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="692" to="705" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAIS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press Neural computation</publisher>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A better way to pretrain deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pose guided human image synthesis by view disentanglement and enhanced weighting loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ilyes</forename><surname>Lakhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Da-gan: Instance-level image translation by deep attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised attentionguided image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Youssef Alami Mejjati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang In</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Riza Alp Guler, and Iasonas Kokkinos. Dense pose transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformation-grounded image generation network for novel 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Scott E Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-view image synthesis using conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Regmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Animating arbitrary objects via deep motion transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Segey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Whitening and coloring batch transform for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attribute-guided sketch generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gesturegan for hand gesture-to-gesture translation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attentionguided generative adversarial networks for unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dual generator generative adversarial networks for multidomain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Localizing and orienting street views using overhead imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weakly-supervised disentangling with recurrent transformations for 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Novel view synthesis for large-scale scene using adversarial loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07064</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Predicting ground-level scene layout from aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Bessinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Generative adversarial frontal view to bird view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
