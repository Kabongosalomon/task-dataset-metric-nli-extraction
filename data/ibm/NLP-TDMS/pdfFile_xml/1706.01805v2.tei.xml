<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SegAN: Adversarial Network with Multi-scale L 1 Loss for Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<email>han.zhang@cs.rutgers.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">L Rodney</forename><surname>Long</surname></persName>
							<email>rlong@mail.nih.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Xiaolei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Rodney</forename><surname>Long</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Lehigh University</orgName>
								<address>
									<settlement>Bethlehem</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">National Library of Medicine</orgName>
								<orgName type="institution">National Institutes of Health</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:lang="en">SegAN: Adversarial Network with Multi-scale L 1 Loss for Medical Image Segmentation</title>
						<title level="a" xml:lang="en">are Co-first Authors.</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by classic generative adversarial networks (GAN), we propose a novel end-to-end adversarial neural network, called SegAN, for the task of medical image segmentation. Since image segmentation requires dense, pixellevel labeling, the single scalar real/fake output of a classic GAN's discriminator may be ineffective in producing stable and sufficient gradient feedback to the networks. Instead, we use a fully convolutional neural network as the segmentor to generate segmentation label maps, and propose a novel adversarial critic network with a multi-scale L 1 loss function to force the critic and segmentor to learn both global and local features that capture long-and short-range spatial relationships between pixels. In our SegAN framework, the segmentor and critic networks are trained in an alternating fashion in a min-max game: The critic takes as input a pair of images, (original image * predicted label map, original image * ground truth label map), and then is trained by maximizing a multi-scale loss function; The segmentor is trained with only gradients passed along by the critic, with the aim to minimize the multi-scale loss function. We show that such a SegAN framework is more effective and stable for the segmentation task, and it leads to better performance than the state-of-the-art U-net segmentation method.</p><p>We tested our SegAN method using datasets from the MIC-CAI BRATS brain tumor segmentation challenge. Extensive experimental results demonstrate the effectiveness of the proposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance comparable to the state-of-the-art for whole tumor and tumor core segmentation while achieves better precision and sensitivity for Gd-enhance tumor core segmentation; on BRATS 2015 SegAN achieves better performance than the state-of-the-art in both dice score and precision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advances in a wide range of medical imaging technologies have revolutionized how we view functional and pathological events in the body and define anatomical structures in which these events take place. X-ray, CAT, MRI, Ultrasound, nuclear medicine, among other medical imaging technologies, enable 2D or tomographic 3D images to capture invivo structural and functional information inside the body for diagnosis, prognosis, treatment planning and other purposes.</p><p>One fundamental problem in medical image analysis is image segmentation, which identifies the boundaries of objects such as organs or abnormal regions (e.g. tumors) in images. Since manually annotation can be very time-consuming and subjective, an accurate and reliable automatic segmentation method is valuable for both clinical and research purpose. Having the segmentation result makes it possible for shape analysis, detecting volume change, and making a precise radiation therapy treatment plan.</p><p>In the literature of image processing and computer vision, various theoretical frameworks have been proposed for automatic segmentation. Traditional unsupervised methods such as thresholding <ref type="bibr" target="#b24">[25]</ref>, region growing <ref type="bibr" target="#b0">[1]</ref>, edge detec-tion and grouping <ref type="bibr" target="#b2">[3]</ref>, Markov Random Fields (MRFs) <ref type="bibr" target="#b20">[21]</ref>, active contour models <ref type="bibr" target="#b13">[14]</ref>, Mumford-Shah functional based frame partition <ref type="bibr" target="#b22">[23]</ref>, level sets <ref type="bibr" target="#b19">[20]</ref>, graph cut <ref type="bibr" target="#b28">[29]</ref>, mean shift <ref type="bibr" target="#b5">[6]</ref>, and their extensions and integrations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> usually utilize constraints about image intensity or object appearance. Supervised methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11]</ref>, on the other hand, directly learn from labeled training samples, extract features and context information in order to perform a dense pixel (or voxel)-wise classification.</p><p>Convolutional Neural Networks (CNNs) have been widely applied to visual recognition problems in recent years, and they are shown effective in learning a hierarchy of features at multiple scales from data. For pixel-wise semantic segmentation, CNNs have also achieved remarkable success. In <ref type="bibr" target="#b17">[18]</ref>, Long et al. first proposed a fully convolutional networks (FCNs) for semantic segmentation. The authors replaced conventional fully connected layers in CNNs with convolutional layers to obtain a coarse label map, and then upsampled the label map with deconvolutional layers to get per pixel classification results. Noh et al. <ref type="bibr" target="#b23">[24]</ref> used an encoderdecoder structure to get more fine details about segmented objects. With multiple unpooling and deconvolutional layers in their architecture, they avoided the coarse-to-fine stage in <ref type="bibr" target="#b17">[18]</ref>. However, they still needed to ensemble with FCNs in their method to capture local dependencies between labels. Lin et al. <ref type="bibr" target="#b16">[17]</ref> combined Conditional Random Fields (CRFs) and CNNs to better explore spatial correlations between pixels, but they also needed to implement a dense CRF to refine their CNN output.</p><p>In the field of medical image segmentation, deep CNNs have also been applied with promising results. Ronneberger et al. <ref type="bibr" target="#b26">[27]</ref> presented a FCN, namely U-net, for segmenting neuronal structures in electron microscopic stacks. With the idea of skip-connection from <ref type="bibr" target="#b17">[18]</ref>, the U-net achieved very good performance and has since been applied to many different tasks such as image translation <ref type="bibr" target="#b11">[12]</ref>. In addition, Havaei et al. <ref type="bibr" target="#b10">[11]</ref> obtained good performance for medical image segmentation with their InputCascadeCNN. The In-putCascadeCNN has image patches as inputs and uses a cascade of CNNs in which the output probabilities of a firststage CNN are taken as additional inputs to a second-stage CNN. Pereira et al. <ref type="bibr" target="#b25">[26]</ref> applied deep CNNs with small kernels for brain tumor segmentation. They proposed different architectures for segmenting high grade and low grade tumors, respectively. Kamnitsas et al. <ref type="bibr" target="#b12">[13]</ref> proposed a 3D CNN using two pathways with inputs of different resolutions. 3D CRFs were also needed to refine their results.</p><p>Although these previous approaches using CNNs for segmentation have achieved promising results, they still have limitations. All above methods utilize a pixel-wise loss, such as softmax, in the last layer of their networks, which is insufficient to learn both local and global contextual relations between pixels. Hence they always need models such as CRFs <ref type="bibr" target="#b3">[4]</ref> as a refinement to enforce spatial contiguity in the output label maps. Many previous methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> address this issue by training CNNs on image patches and using multiscale, multi-path CNNs with different input resolutions or different CNN architectures. Using patches and multi-scale inputs could capture spatial context information to some extent. Nevertheless, as described in U-net <ref type="bibr" target="#b26">[27]</ref>, the computational cost for patch training is very high and there is a trade-off between localization accuracy and the patch size. Instead of training on small image patches, current stateof-the-art CNN architectures such as U-net are trained on whole images or large image patches and use skip connections to combine hierarchical features for generating the label map. They have shown potential to implicitly learn some local dependencies between pixels. However, these methods are still limited by their pixel-wise loss function, which lacks the ability to enforce the learning of multi-scale spatial constraints directly in the end-to-end training process. Compared with patch training, an issue for CNNs trained on entire images is label or class imbalance. While patch training methods can sample a balanced number of patches from each class, the numbers of pixels belonging to different classes in full-image training methods are usually imbalanced. To mitigate this problem, U-net uses a weighted cross-entropy loss to balance the class frequencies. However, the choice of weights in their loss function is taskspecific and is hard to optimize. In contract to the weighted loss in U-net, a general loss that could avoid class imbalance as well as extra hyper-parameters would be more desirable.</p><p>In this paper, we propose a novel end-to-end Adversarial Network architecture, called SegAN, with a multi-scale L 1 loss function, for semantic segmentation. Inspired by the original GAN <ref type="bibr" target="#b8">[9]</ref>, the training procedure for SegAN is similar to a two-player min-max game in which a segmentor network (S) and a critic network (C) are trained in an alternating fashion to respectively minimize and maximize an objective function. However, there are several major differences between our SegAN and the original GAN that make SegAN significantly better for the task of image segmentation.</p><p>-In contrast to classic GAN with separate losses for generator and discriminator, we propose a novel multi-scale loss function for both segmentor and critic. Our critic is trained to maximize a novel multi-scale L 1 objective function that takes into account CNN feature differences between the predicted segmentation and the ground truth segmentation at multiple scales (i.e. at multiple layers). -We use a fully convolutional neural network (FCN) as the segmentor S, which is trained with only gradients flowing through the critic, and with the objective of minimizing the same loss function as for the critic. -Our SegAN is an end-to-end architecture trained on whole images, with no requirements for patches, or inputs of multiple resolutions, or further smoothing of the predicted label maps using CRFs. By training the entire system end-to-end with back propagation and alternating the optimization of S and C, SegAN can directly learn spatial pixel dependencies at multiple scales. Compared with previous methods that learn hierarchical features with multi-scale multi-path CNNs <ref type="bibr" target="#b6">[7]</ref>, our SegAN network applies a novel multi-scale loss to enforce the learning of hierarchical features in a more straightforward and efficient manner. Extensive experimental results demonstrate that the proposed SegAN achieves comparable or better results than the state-of-the-art CNN-based architectures including U-net.</p><p>The rest of this paper is organized as follows. Section 2 introduces our SegAN architecture and methodology. Experimental results are presented in Section 3. Finally, we conclude this paper in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>As illustrated in <ref type="figure">Figure 1</ref>, the proposed SegAN consists of two parts: the segmentor network S and the critic network C. The segmentor is a fully convolutional encoder-decoder network that generates a probability label map from input images. The critic network is fed with two inputs: original images masked by ground truth label maps, and original images masked by predicted label maps from S. The S and C networks are alternately trained in an adversarial fashion: the training of S aims to minimize our proposed multi-scale L 1 loss, while the training of C aims to maximize the same loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The multi-scale L 1 loss</head><p>The conventional GANs <ref type="bibr" target="#b8">[9]</ref> have an objective loss function defined as:</p><formula xml:id="formula_0">min θ G max θ D L(θ G , θ D ) = E x∼P data [log D(x)] + E z∼Pz log(1 − D(G(z)))] .<label>(1)</label></formula><p>In this objective function, x is the real image from an unknown distribution P data , and z is a random input for the generator, drawn from a probability distribution (such as Gaussion) P z . θ G and θ D represent the parameters for the generator and discriminator in GAN, respectively.</p><p>In our proposed SegAN, given a dataset with N training images x n and corresponding ground truth label maps y n , the multi-scale objective loss function L is defined as:</p><formula xml:id="formula_1">min θ S max θ C L(θ S , θ C ) = 1 N N n=1 mae (f C (x n • S(x n )), f C (x n • y n )) ,<label>(2)</label></formula><p>where mae is the Mean Absolute Error (MAE) or L 1 distance; x n • S(x n ) is the input image masked by segmentorpredicted label map (i.e., pixel-wise multiplication of predicted label map and original image); x n • y n is the input image masked by its ground truth label map (i.e., pixel-wise multiplication of ground truth label map and original image); and f C (x) represents the hierarchical features extracted from image x by the critic network. More specifically, the mae function is defined as:</p><formula xml:id="formula_2">mae (f C (x), f C (x )) = 1 L L i=1 ||f i C (x) − f i C (x )|| 1 ,<label>(3)</label></formula><p>where L is the total number of layers (i.e. scales) in the critic network, and f i C (x) is the extracted feature map of image x at the ith layer of C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SegAN Architecture</head><p>Segmentor. We use a fully convolutional encoder-decoder structure for the segmentor S network. We use the convolutional layer with kernel size 4 × 4 and stride 2 for downsampling, and perform upsampling by image resize layer with a factor of 2 and convolutional layer with kernel size 3 × 3 stride 1. We also follow the U-net and add skip connections between corresponding layers in the encoder and the decoder. Critic. The critic C has the similar structure as the decoder in S. Hierarchical features are extracted from multiple layers of C and used to compute the multi-scale L 1 loss. This loss can capture long-and short-range spatial relations between pixels by using these hierarchical features, i.e., pixel-level features, low-level (e.g. superpixels) features, and middlelevel (e.g. patches) features.</p><p>More details including activation layers (e.g., leaky ReLU), batch normalization layer and the number of feature maps used in each convolutional layers can be found in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training SegAN</head><p>The segmentor S and critic C in SegAN are trained by backpropagation from the proposed multi-scale L 1 loss. In an alternating fashion, we first fix S and train C for one step using gradients computed from the loss function, and then fix C and train S for one step using gradients computed from the same loss function passed to S from C. As shown in <ref type="bibr" target="#b1">(2)</ref>, the training of S and C is like playing a min-max game: while G aims to minimize the multi-scale feature loss, C tries to maximize it. As training progresses, both the S and C networks become more and more powerful. And eventually, the segmentor will be able to produce predicted label maps that are very close to the ground truth as labeled by Segmentor Conv3x3 N64 S1 R2 ReLU - <ref type="figure">Fig. 1</ref> The architecture of the proposed SegAN with segmentor and critic networks. 4 × 4 convolutional layers with stride 2 (S2) and the corresponding number of feature maps (e.g., N64) are used for encoding, while image resize layers with a factor of 2 (R2) and 3 × 3 convolutional layers with stride 1 are used for decoding. Masked images are calculated by pixel-wise multiplication of a label map and (the multiple channels of) an input image. Note that, although only one label map (for whole tumor segmentation) is illustrated here, multiple label maps (e.g. also for tumor core and Gd-enhanced tumor core) can be generated by the segmentor in one path.</p><p>human experts. We also find that the S-predicted label maps are smoother and contain less noise than manually-obtained ground truth label maps. We trained all networks using RMSProp solver with batch size 64 and learning rate 0.00002. We used a grid search method to select the best values for the number of up-sampling blocks and the number of down-sampling blocks for the segmentor (four, in both cases), and for the number of downsampling blocks for the critic (three).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Proof of training stability and convergence</head><p>Having introduced the multi-scale L 1 loss, we next prove that our training is stable and finally reaches an equilibrium. First, we introduce some notations.</p><p>Let f : X → X be the mapping between an input medical image and its corresponding ground truth segmentation, where X represents the compact space of medical images 1 and X represents the compact space of ground truth segmentations. We approximate this ground truth mapping f with a segmentor neural network g θ : X → X parameterized by vector θ which takes an input image, and generates a segmentation result. Assume the best approximation to the ground truth mapping by a neural network is the network gθ with optimal parameter vectorθ.</p><p>Second, we introduce a lemma about the Lipschitz continuity of either the segmentor or the critic neural network in our framework.</p><p>Lemma 1 Let g θ be a neural network parameterized by θ, and x be some input in space X , then g θ is Lipschitz continuous with a bounded Lipschitz constants K(θ) such that</p><formula xml:id="formula_3">||g θ (x 1 ) − g θ (x 2 )|| 1 K(θ)(||x 1 − x 2 || 1 ) ,<label>(4)</label></formula><p>and for different parameters with same input we have</p><formula xml:id="formula_4">||g θ1 (x) − g θ2 (x)|| 1 K(x)||θ 1 − θ 2 || 1 ,<label>(5)</label></formula><p>Now we prove Lemma 1.</p><p>Proof Note that the neural network consists of several affine transformations and pointwise nonlinear activation functions such as leaky ReLU (see <ref type="figure">Figure 1</ref>). All these functions are Lipschitz continuous because all their gradient magnitudes are within certain ranges. To prove Lemma 1, it's equivalent to prove the gradient magnitudes of g θ with respect to x and θ are bounded. We start with a neural network with only one layer: g θ (x) = A 1 <ref type="figure">(W 1 x)</ref> where A 1 and W 1 represent the activation and weight matrix in the first layer. We have ∇ x g θ (x) = W 1 D 1 where D 1 is the diagonal Jacobian of the activation, and we have ∇ θ g θ (x) = D 1 x where θ represents the parameters in the first layer. Then we consider the neural network with L layers. We apply the chain rule of the gradient and we have ∇ x g θ (x) = L k=1 W k D k where k represent the k-th layer of the network. Then we have</p><formula xml:id="formula_5">||∇ x g θ (x)|| 1 = || L k=1 W k D k || 1 .<label>(6)</label></formula><p>Due to the fact that all parameters and inputs are bounded, we have proved (4). Let's denote the first i layers of the neural network by g i (which is another neural network with less layers), we can compute the gradient with respect to the parameters in i-th</p><formula xml:id="formula_6">layer as ∇ θi g θ (x) = ( L k=i+1 W k D k )D i g i−1 (x).</formula><p>Then we sum parameters in all layers and get</p><formula xml:id="formula_7">||∇ θ g θ (x)|| 1 = || L i=1 ( L k=i+1 W k D k )D i g i−1 (x)|| 1 L i=1 ||(( L k=i+1 W k D k )D i )g i−1 (x)|| 1 .<label>(7)</label></formula><p>Since we have proved that g(x) is bounded, we finish the proof of (5).</p><p>Based on Lemma 1, we then prove that our multi-scale loss is bounded and won't become arbitrarily large during the training, and it will finally converge.</p><p>Theorem 1 Let L t (x) denote the multi-scale loss of our SegAN at training time t for input image x, then there exists a small constant C so that</p><formula xml:id="formula_8">lim t→+∞ E x∈X L t (x) C .<label>(8)</label></formula><p>Proof Let g and d represent the segmentor and critic neural network, θ and w be the parameter vector for the segmentor and critic, respectively. Without loss of generality, we omit the masked input for the critic and rephrase <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> as</p><formula xml:id="formula_9">min θ max w L t = E x∈X 1 L L i=1 ||d i (g θ (x)) − d i (gθ(x))|| 1 ,<label>(9)</label></formula><p>recall that gθ is the ground truth segmentor network and d i is the critic network with only first i layers. Let's firstly focus on the critic. To make sure our multi-scale loss won't become arbitrarily large, inspired by <ref type="bibr" target="#b1">[2]</ref>, we clamp the weights of our critic network to some certain range (e.g., [−0.01, 0.01] for all dimensions of parameter) every time we update the weights through gradient descent. That is to say, we have a compact parameter space W such that all functions in the critic network are in a parameterized family of functions {d w } w∈W . From Lemma 1, we know that ||d w (</p><formula xml:id="formula_10">x 1 ) − d w (x 2 )|| 1 K(w)(||x 1 − x 2 || 1 ).</formula><p>Due to the fact that W is compact, we can find a maximum value for K(w), K, and we have</p><formula xml:id="formula_11">||d(x 1 ) − d(x 2 )|| 1 K||x 1 − x 2 || 1 .<label>(10)</label></formula><p>Note that this constant K only depends on the space W and is irrelevant to individual weights, so it is true for any parameter vector w after we fix the vector space W. Since Lemma 1 applies for the critic network with any number of layers, we have</p><formula xml:id="formula_12">1 L L i=1 ||d i (g θ (x)) − d i (gθ(x))|| 1 K||g θ (x) − gθ(x)|| 1 .<label>(11)</label></formula><p>Now let's move to the segmentor. According to Lemma 1, we have ||g θ (x)−gθ(x)|| 1 K(x)||θ −θ|| 1 , then combined with <ref type="bibr" target="#b10">(11)</ref> we have</p><formula xml:id="formula_13">1 L L i=1 ||d i (g θ (x))−d i (gθ(x))|| 1 K(x)K||θ−θ|| 1 . (12)</formula><p>We know X is compact, so there's a maximal value for K(x) and it only depends on the difference between the ground truth parameter vectorθ and the parameter vector of the segmentor θ. Since we don't update weights in the segmentor when we update weights in the critic, there's an upper bound for L t when we update the critic network and it won't be arbitrarily large during the min-max game.</p><p>When we update the parameters in the segmentor, we want to decrease the loss. This makes sense because smaller loss indicates smaller difference betweenθ and θ. When θ →θ, L t converges to zero because the upper bound of L becomes zero. However, we may not be able to find the global optimum for θ. Now let us denote a reachable local optimum for θ in the segmentor by θ 0 , we will keep updating parameters in the segmentor through gradient descent and gradually approaches θ 0 . Based on (9) and (12), we denote the maximum of K(x) by K and have</p><formula xml:id="formula_14">lim t→+∞ L t (x) KK ||θ − θ 0 || 1 = C .<label>(13)</label></formula><p>Since the constant C does not depend on input x, we have proved Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we evaluated our system on the fully-annotated MICCAI BRATS datasets <ref type="bibr" target="#b21">[22]</ref>. Specifically, we trained and validated our models using the BRATS 2015 training dataset, which consists of 220 high grade subjects and 54 low grade subjects with four modalities: T1, T1c, T2 and Flair. We randomly split the BRATS 2015 training data with the ratio 9 : 1 into a training set and a validation set. We did such split for the high grade and low grade subjects separately, and then re-combined the resulting sets for training and validation. Each subject in BRATS 2015 dataset is a 3D brain MRI volume with size 240 × 240 × 155. We center cropped each subject into a subvolume of 180 × 180 × 128, to remove the border black regions while still keep the entire brain regions. We did our final evaluation and comparison on the BRATS 2015 test set using the BRATS online evaluation system, which has Dice, Precision and Sensitivity as the evaluation metrics. The Dice score is is identical to the F-score which normalizes the number of true positives to the average size of the two segmented regions:</p><formula xml:id="formula_15">Dice = 2|P ∩ T | |P | + |T |<label>(14)</label></formula><p>where P and T represent the predicted region and the ground truth region, respectively. Since the BRATS 2013 dataset is a subset of BRATS 2015, we also present our results on BRATS 2013 leaderboard set.</p><p>Although some work with 3D patching CNNs have been done for medical image segmentation, due to the limitation of our hardware memory and for the reason that brain images in BRATS dataset are inconsistent in third dimension, we built a 2D SegAN network to generate the label map for each axial slice of a 3D volume and then restack these 2D label maps to produce the 3D label map for brain tumor. Since each subject was center cropped to be a 180×180×128 volume, it yields 128 axial slices each with the size 180 × 180. These axial slices were further randomly cropped to size 160 × 160 during training for the purpose of data augmentation. They were centered cropped to size 160 × 160 during validation and testing.</p><p>We used three modalities of these MRI images: T1c, T2, FLAIR. Corresponding slices of T1c, T2, FLAIR modalities are concatenated along the channel dimension and used as the multi-channel input to our SegAN model, as shown in <ref type="figure">Figure 1</ref>. The segmentor of SegAN outputs label maps with the same dimensions as the input images. As required by the BRATS challenge <ref type="bibr" target="#b21">[22]</ref>, we did experiments with the objective to generate label maps for three types of tumor regions: whole tumor, tumor core and Gd-enhanced tumor core. In this section, we compare different implementations of the proposed SegAN architecture and also evaluate the effectiveness of the proposed multi-scale L 1 loss on the BRATS validation set for the brain tumor segmentation task. Specifically, we compare the following implementations:</p><p>-S1-1C. A separate SegAN is built for every label class, i.e. one segmentor (S1) and one critic (1C) per label. -S3-1C: A SegAN is built with one segmentor and one critic, but the segmentor generates a three-channel label map, one channel for each label. Therefore, each 3channel label map produces three masked images (one for each class), which are then concatenated in the channel dimension and fed into the critic. -S3-3C. A SegAN is built with one segmentor that generates a three-channel (i.e. three-class) label map, but three separate critics, one for each label class. The networks, one S and three Cs, are then trained end-to-end using the average loss computed from all three Cs. -S3-3C single-scale loss models. For comparison, we also built two single-scale loss models: S3-3C-s0 and S3-3C-s3. S3-3C-s0 computes the loss using features from only the input layers (i.e., layer 0) of the critics, and S3-3C-s3 calculates the loss using features from only the output layers (i.e., layer 3) of the critics.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, models S1-1C and S3-3C give similar performance which is the best among all models. Since the computational cost for S1-1C is higher than S3-3C, S3-3C is more favorable and we use it to compare our SegAN model with other methods in Section 3. In contrast, while model S3-1C is the simplest requiring the least computational cost, it sacrifices some performance; but by using the multi-scale loss, it still performs better than any of the two single-scale loss models especially for segmenting tumor core and Gdenhanced tumor core regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison to state-of-the-art</head><p>In this subsection, we compare the proposed method, our S3-3C SegAN model, with other state-of-the-art methods on the BRATS 2013 Leaderboard <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref> Test and the BRATS 2015 Test <ref type="bibr" target="#b12">[13]</ref>. We also implemented a U-net model <ref type="bibr" target="#b26">[27]</ref> for comparison. This U-net model has the exact same architecture as our SegAN segmentor except that the multiscale SegAN loss is replaced with the softmax loss in the U-net.  <ref type="figure">Figure 3</ref> illustrates some example results of our SegAN; in the figure, the segmented regions of the three classes (whole tumor, tumor core, and Gd-enhanced tumor core) are shown in yellow, blue, and red, respectively. One possible reason behind this phenomenon is that the proposed multi-scale L 1 loss from our adversarial critic network encourages the segmentor to learn both global and local features that capture long-and short-range spatial relations between pixels, resulting fewer noises and smoother results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>To the best of our knowledge, our proposed SegAN is the first GAN-inspired framework adapted specifically for the segmentation task that produces superior segmentation accuracy. While conventional GANs have been successfully applied to many unsupervised learning tasks (e.g., image synthesis <ref type="bibr" target="#b30">[31]</ref>) and semi-supervised classification <ref type="bibr" target="#b27">[28]</ref>, there are very few works that apply adversarial learning to semantic segmentation. One such work that we found by Luc et al. <ref type="bibr" target="#b18">[19]</ref> used both the conventional adversarial loss of GAN and pixel-wise softmax loss against ground truth. They showed small but consistent gains on both the Stanford Background dataset and the PASCAL VOC 2012 dataset; the authors observed that pre-training only the adversarial network was unstable and suggested an alternating scheme for updating the segmenting networks and the adversarial networks weights. We believe that the main reason contributing to the unstable training of their framework is: the conventional adversarial loss is based on a single scalar output by the discriminator that classifies a whole input image into real or fake category. When inputs to the discriminator are generated vs. ground truth dense pixel-wise label maps as in the segmentation task, the real/fake classification task is too easy for the discriminator and a trivial solution is found quickly. As a result, no sufficient gradients can flow through the discriminator to improve the training of generator. In comparison, our SegAN uses a multi-scale feature loss that measures the difference between generated segmentation and ground truth segmentation at multiple layers in the critic, forcing both the segmentor and critic to learn hierarchical features that capture long-and short-range spatial relationships between pixels. Using the same loss function for both S and C, the training of SegAN is end-to-end and stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a novel end-to-end Adversarial Network architecture, namely SegAN, with a new multiscale loss for semantic segmentation. Experimental evaluation on the BRATS brain tumor segmentation dataset shows that the proposed multi-scale loss in an adversarial training framework is very effective and leads to more superior performance when compared with single-scale loss or the conventional pixel-wise softmax loss.</p><p>As a general framework, our SegAN is not limited to medical image segmentation applications. In our future work, we plan to investigate the potential of SegAN for general semantic segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Average dice scores of different architectures on BRATS validation set 3.1 Choice of components in SegAN architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Comparison to previous methods and a baseline implementation of U-net with softmax loss for segmenting three classes of brain tumor regions: whole, core and Gd-enhanced (Enha.)MethodsDice Precision Sensitivity Whole Core Enha. Whole Core Enha. Whole Core Enha. Example results of our SegGAN (right) with corresponding T2 slices (left) and ground truth (middle) on BRATS validation set.</figDesc><table><row><cell>BRATS 2013 Leaderboard</cell><cell>Havaei [11] Pereira [26] SegAN</cell><cell>0.84 0.84 0.84</cell><cell>0.71 0.72 0.70</cell><cell>0.57 0.62 0.65</cell><cell>0.88 0.85 0.87</cell><cell>0.79 0.82 0.80</cell><cell>0.54 0.60 0.68</cell><cell>0.84 0.86 0.83</cell><cell>0.72 0.76 0.74</cell><cell>0.68 0.68 0.72</cell></row><row><cell>BRATS 2015 Test</cell><cell>Kamnitsas [13] U-net SegAN</cell><cell>0.85 0.80 0.85</cell><cell>0.67 0.63 0.70</cell><cell>0.63 0.64 0.66</cell><cell>0.85 0.83 0.92</cell><cell>0.86 0.81 0.80</cell><cell>0.63 0.78 0.69</cell><cell>0.88 0.80 0.80</cell><cell>0.60 0.58 0.65</cell><cell>0.67 0.60 0.62</cell></row><row><cell>Fig. 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>gives all comparison results. From the table, one can see that our SegAN compares favorably to the existing state-of-the-art on BRATS 2013 while achieves better performance on BRATS 2015. Moreover, the dice scores of our SegAN outperform the U-net baseline for segmenting all three types of tumor regions. Another observation is that our SegAN-produced label maps are smooth with little noise.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although the pixel value ranges of medical images can vary, one can always normalize them to a certain value range such as [0,1], so it is compact.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This research was supported in part by the Intramural Research Program of the National Institutes of Health (NIH), National Library of Medicine (NLM), and Lister Hill National Center for Biomedical Communications (LHNCBC), under Contract HHSN276201500692P.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="647" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>telligence</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>arXiv:170107875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.7062" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d variational brain tumor segmentation using a high dimensional feature set. In: Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cobzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murtha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial decision forests for ms lesion segmentation in multi-channel magnetic resonance images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geremia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Clatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="378" to="390" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable registration of glioma images using em algorithm and diffusion reaction modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="375" to="390" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>arXiv:161107004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmenting brain tumors using pseudoconditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image computing and computer-assisted intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive, gpu-based level sets for 3d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="564" to="572" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno>arXiv:161108408</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Shape modeling with front propagation: A level set approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Sethian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Vemuri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="158" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised texture segmentation using markov random field models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="478" to="482" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal approximations by piecewise smooth functions and associated variational problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on pure and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="685" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A threshold selection method from graylevel histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on systems, man, and cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using convolutional neural networks in mri images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discriminative modelconstrained graph cuts approach to fully automated pediatric brain tumor segmentation in 3-d mri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>arXiv:161203242</idno>
		<title level="m">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
