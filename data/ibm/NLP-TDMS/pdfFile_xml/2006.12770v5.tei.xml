<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Feature Alignment: Improving Transferability of Unsupervised Domain Adaptation by Gaussian-guided Latent Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhe</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clarence</forename><forename type="middle">W</forename><surname>De Silva</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Feature Alignment: Improving Transferability of Unsupervised Domain Adaptation by Gaussian-guided Latent Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Domain adaptation Transfer learning Computer vision Distribution alignment Encoder-decoder Information theory</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>In this paper, we focus on the unsupervised domain adaptation problem where an approximate inference model is to be learned from a labeled data domain and expected to generalize well to an unlabeled data domain. The success of unsupervised domain adaptation largely relies on the cross-domain feature alignment. Previous work has attempted to directly align latent features by the classifier-induced discrepancies. Nevertheless, a common feature space cannot always be learned via this direct feature alignment especially when a large domain gap exists. To solve this problem, we introduce a Gaussianguided latent alignment approach to align the latent feature distributions of the two domains under the guidance of the prior distribution. In such an indirect way, the distributions over the samples from the two domains will be constructed on a common feature space, i.e., the space of the prior, which promotes better feature alignment. To effectively align the target latent distribution with this prior distribution, we also propose a novel unpaired L1-distance by taking advantage of the formulation of the encoder-decoder. The extensive evaluations on nine benchmark datasets validate the superior knowledge transferability through outperforming state-of-the-art methods and the versatility of the proposed method by improving the existing work significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The performance of computer vision models has been improved significantly by deep neural networks that take advantage of large quantities of labeled data. However, the models trained on one dataset typically perform poorly on another, different, but related, dataset <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b29">30]</ref>. This shortcoming calls for adaptation strategies that help transfer knowledge from a label-rich source domain to a label-scarce target domain. Among such adaptation strategies, unsupervised domain adaptation (UDA) aims at mitigating domain shift in a way that does not use the target dataset labels, while attempting to maximize the performance of the classifier on them. Existing UDA algorithms attempt to mitigate domain shifts by only considering the classifier-induced discrepancy between the two domains, which can reduce the domain divergence <ref type="bibr" target="#b0">[1]</ref>. Both adversarial <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref> and nonadversarial domain adaptation (DA) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref> methods work under the guidance of convergence learning bounds <ref type="bibr" target="#b0">[1]</ref>. The main idea behind these bounds is that concurrently minimizing the source domain classification error and the classifierinduced discrepancy between the source domain and the target domain, inadvertently aligning the two latent feature spaces in which classification is done. In particular, adversarial DA attempts to align the feature spaces by minimizing the <ref type="figure">Figure 1</ref>: (Best viewed in color.) Existing UDA methods try to align the feature distributions of the two domains by the classifier-induced discrepancies. However, it might be difficult for them to construct the two feature distributions in a single distribution space or align arbitrarily complex feature distributions in that space. Our method attempts to indirectly align the features of the two domains under the guidance of the Gaussian prior distribution. Our method can encourage the features of the two domains to be constructed in a common feature space, i.e., the space of the Gaussian prior, where the target samples can maximally take advantage of the discriminative source features for their own classification tasks.</p><p>classifier-induced discrepancy with adversarial objectives.</p><p>However, as shown in <ref type="figure">Figure 1</ref>, adaptation in this manner alone cannot effectively learn a common feature space for the classification in the two domains. This claim is empirically validated in Section 5.1. To address this problem, we propose a discriminative feature alignment (DFA) to align the two latent feature distributions of the source dataset and the target dataset under the guidance of the Gaussian prior (sim-ilar to VAE <ref type="bibr" target="#b17">[18]</ref>). Because the classification takes place in the latent space, the latent space itself is discriminative, in turn, making alignment focus on the discriminative feature distributions. Our approach is built on the encoder-decoder (autoencoder) formulation with an implicitly shared discriminative latent space (see <ref type="figure" target="#fig_3">Figure 3</ref>). Specifically, we define a feature extractor which takes and encodes input samples into a latent space; similarly we define a decoder which takes a latent feature vector, or a random vector sampled from a Gaussian prior, and decodes it back to the image. Both the encoder ( ) and the decoder ( ) are shared by the samples from the source domain and the target domain; and one can consider as a form of regularization. We utilize a KL-divergence penalty to encourage the latent distribution over the source samples to be close to the Gaussian prior. While we can similarly encourage the target distribution in the feature space to be close to the Gaussian prior, thereby achieving the desired alignment, this turns out less effective in practice. Instead, the alignment between the source and target distributions in the latent space is achieved by a novel unpaired L1-distance between the reconstructed samples from the decoder, i.e., minimizing the distance between ( ( )) and ( ( )) among all pairs of samples from the source domain (s) and the target domain (t). The proposed regularization for the distribution alignment is named distribution alignment loss. We further find that instead of aligning the latent distributions directly, we get better results by aligning the target latent distribution to the Gaussian prior, i.e., minimizing the distance between ( ( )) and the decoded samples from the prior in the feature space. The sampling also serves as data augmentation and could be useful in scenarios where the source dataset itself maybe limited.</p><p>Moreover, the proposed DFA can be incorporated into other UDA frameworks, either adversarial or non-adversarial, to improve results via a better feature alignment. To validate the versatility of DFA, we demonstrate it using an adversarial framework for the digit classification and a nonadversarial framework for the object classification. The two frameworks are developed based on the existing techniques, mainly: maximum classifier discrepancy (MCD) <ref type="bibr" target="#b36">[37]</ref> and stepwise adaptive feature norm (SAFN) <ref type="bibr" target="#b47">[48]</ref>, since they are state-of-the-art for the digit classification and the object classification, respectively. In all settings, our DFA significantly improves the performance of the original frameworks and outperforms other existing frameworks by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>• We propose a novel model for unsupervised domain adaptation, which utilizes an indirect latent alignment process to construct a common feature space under the guidance of a Gaussian prior.</p><p>• We introduce a new method to align two distributions, which, instead of minimizing discriminator error using a GAN, minimizes the direct L1-distance between the decoded samples.</p><p>• We evaluate the proposed frameworks and the versa-tility of the proposed DFA on both digit and object classification tasks by adapting it into existing UDA approaches, and achieve state-of-the-art performance on the benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Existing UDA methods can be divided into two major types: adversarial and non-adversarial domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Adversarial Domain Adaptation</head><p>Motivated by generative adversarial nets (GANs) <ref type="bibr" target="#b10">[11]</ref>, adversarial DA methods, which stem from the technique proposed in <ref type="bibr" target="#b9">[10]</ref>, are widely explored by the DA community. The goal is for the latent feature distributions of the two domains to be aligned, such that domain classifier is unable to recognize domain from which the features originate. In early works, such alignment was realized by simple batch normalization statistics, which aligned the data distributions from the two domains to a canonical form <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>. Introducing an adversarial loss makes it more difficult for the domain classifier to classify the domains correctly <ref type="bibr" target="#b37">[38]</ref>, producing better alignment. Further advances in adversarial DA can be found in recent works. Long et al. propose to measure the domain divergence by considering the distribution correlations for each class of objects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. Domain separation network <ref type="bibr" target="#b3">[4]</ref> is also proposed to better preserve the component that is private to each domain before aligning the latent feature distributions.</p><p>However, the mechanism concerns constructing adversarial learning between the feature extractor and the domain classifier, which does not consider the relationship between the decision boundary and the target samples. Maximum classifier discrepancy (MCD), instead, involves an adversarial mechanism between its image classifiers and the feature extractor <ref type="bibr" target="#b36">[37]</ref>. This method can align the latent feature distributions of the two domains by considering the decision divergence on predicting the target samples between the two image classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Non-adversarial Domain Adaptation</head><p>Existing non-adversarial DA methods attempt to quantify domain shifts by designing specific statistical distances between the two domains. Correlation alignment <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> utilizes the difference of the mean and the covariance between the two datasets as the domain divergence, and attempts to match them during the training. The methods based on maximum mean discrepancy (MMD) <ref type="bibr" target="#b1">[2]</ref> such as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> measure the variance between the latent feature distributions of the two domains. Some studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50]</ref> also propose to learn the discriminative representations by pseudo-labels and aligning the output class distributions. However, they still consider classifier-induced discrepancies for the latent alignment, which cannot guarantee the safe transfer of the discriminative features across domains. Moreover, stepwise adaptive feature norm (SAFN) <ref type="bibr" target="#b47">[48]</ref> identifies that domain shifts rely on the less-informative features with small norms for the target-specific task, and the knowledge across domains can be safely transferred by placing the target features far away from these small-norm regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, the details of the proposed method are presented. First, we discuss the preliminary of the UDA problem in Section 3.1. Second, we explain about the way to achieve knowledge transfer by taking advantage of the formulation of the encoder-decoder in Section 3.2. Third, we discuss the overall idea of the proposed model in Section 3.3. Fourth, we give details about the loss functions that are used in the proposed method in Section 3.4. Finally, we demonstrate the versatility of the proposed method by incorporating it into the existing UDA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>Under the setting of UDA, we sample labeled images from the source space { , } to form the source domain = {( ( ) , ( ) )} =1 , as well as unlabeled images from the target space { , } to form the target domain = {( ( ) )} =1 . The objective of UDA is to obtain a feature extractor that generates a target distribution in the feature space that can maximize the performance of classifying without accessing its label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Knowledge Transfer via Encoder-Decoder</head><p>The proposed work is under the assumption that every neural-network-based UDA framework should consist of a feature extractor and an image classifier . The goal of the proposed method is not only to align the latent distributions of the two domains but also to make learn the representation from the target samples under the guidance of the discriminative source representation. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the decoder is specifically used for the proposed distribution alignment loss to align the target latent distribution with the prior distribution. Thus, is also an encoder that learns the hidden representations for both and in our setting. As continuously shares its learning parameters with during the training, our model can also be viewed as a weight-tied autoencoder. The proposed distribution alignment loss, which is different from the reconstruction loss used in the existing work on autoencoder, is an L1-distance between the reconstructed target samples and the decoded samples from the prior in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Knowledge Transfer via Distribution Alignment</head><p>The objective of unsupervised domain adaptation is to retain sufficient knowledge about the source domain in the target latent space. In a single-domain problem, the information about the input domain can be retained in its latent space by reconstructing the input samples <ref type="bibr" target="#b44">[45]</ref>. Motivated by this, we argue that minimizing the difference between the reconstructed target samples and the source input samples can encourage the target latent space to cover sufficient information about the source domain. To be specific, minimizing the  proposed distribution alignment loss on the premise of constructing the source feature space on the space of the prior is equivalent to maximizing the lower bound of the mutual information between the latent space of the target domain and the input space of the source domain .</p><p>In the setting of UDA, we are interested in learning the correspondence between the samples from the target latent space and the samples from the source input space :</p><formula xml:id="formula_0">← ← ← ← ← ← ← ← ← ← ← ← → ← ← ← ← ← ← ← ← ← ← ← ← →̂ ← ← ← ← ← ← ← ← ← ← ← ← → ← ← ← ← ← ← ← ← ← ← ← ← →̂ ,<label>(1)</label></formula><p>where the encoder shares it learning parameters with the decoder . The mutual information between the source input space and the target latent space can be expressed as</p><formula xml:id="formula_1">( ; ) = ( ) − ( | ),<label>(2)</label></formula><p>where (⋅) is the mutual information; (⋅) is the entropy.</p><p>( ) is an unknown constant since the source input space is from a fixed distribution that will not be affected by . Hence, the information maximization process can be reduced according to Equation 2:</p><formula xml:id="formula_2">max ( ; ) = max − ( | ) = max ( , ) [log ( | ; )].<label>(3)</label></formula><p>Normally, the reconstructed target samplê = ( ) is not exactly the same as a corresponding source sample . However, in probabilistic terms, the parameters of a distribution ( | ) may producê with high probability as they share the same object feature. Therefore, the lower bound of the mutual information can be maximized by minimizing</p><formula xml:id="formula_3">1 ( ,̂ ) ∝ − log ( | ),<label>(4)</label></formula><p>where 1 is the L1 distance.</p><p>However, this objective cannot be achieved because of the lack of the correspondence between the reconstructed samples from the target domain and the input samples from the source domain.</p><p>To tackle this problem, we define a prior distribution ( ) and construct the discriminative source features on the space of the prior . If there exists ( ( )|| ( )) = 0, ≈ , Equation 1 becomes</p><formula xml:id="formula_4">← ← ← ← ← ← ← ← ← ← ← ← → ← ← ← ← ← ← ← ← ← ← ← ← →̂ ← ← ← ← ← ← ← ← ← ← ← ← → ≈ ← ← ← ← ← ← ← ← ← ← ← ← →̂ ≈̂ ,<label>(5)</label></formula><p>Now, we define a distribution (̂ | ) for the following inequality:</p><formula xml:id="formula_5">( , ) [log ( | )] ≥ (̂ , ) [log (̂ | )], (6) where ( || ) ≥ 0. The left-hand side of Equation 6</formula><p>is the lower bound of the mutual information between the source input space and the target latent space. We thus have a new lower bound for the mutual information:</p><formula xml:id="formula_6">max ( ; ) ≥ max (̂ , ) [log (̂ | ; )]. (7)</formula><p>Considering the parametric distribution (̂ | ; ), the lower bound shown in Equation 7 can be maximized by</p><formula xml:id="formula_7">max (̂ , ) [log (̂ | ; )].<label>(8)</label></formula><p>Therefore, the mutual information ( ; ) can be max- </p><p>Then, we consider the distribution alignment error:</p><formula xml:id="formula_9">1 (̂ ,̂ ) ≈ 1 (̂ ,̂ ) ∝ − log (̂ | ),<label>(10)</label></formula><p>We thus have the following minimization that is equivalent to the maximization of the lower bound of the mutual information:</p><formula xml:id="formula_10">min (̂ ,̂ ) [ 1 (̂ ,̂ )] ⇒ min ( , ) [ 1 ( ( ), ( ( )))],<label>(11)</label></formula><p>which can be rewritten according to <ref type="bibr">Equation 4</ref> and Equation 10:</p><formula xml:id="formula_11">max ( ; ) ≥ max (̂ , ) [log (̂ | ; )] ≈ max (̂ , ) [log (̂ | ; )] = max ( , ) [log ( ( )| ( ))] = min ( , ) [ 1 ( ( ), ( ( )))]<label>(12)</label></formula><p>At this point, we can conclude that the lower bound of the mutual information between the source input space and the target latent space can be maximized by minimizing the proposed distribution alignment error 1 (̂ ,̂ ) on the premise that the source latent distribution is close enough to the prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Decoder</head><p>The proposed regularization has two functionalities in our model: 1) distribution alignment; 2) discriminative feature extraction. The distribution alignment mechanism alone cannot guarantee the produced latent distribution ( ) is adequately discriminative for to generalize well to the target domain. To further enforce to focus on the cross-domain classification discriminative characteristics of the target samples, we let the weight matrices of and be symmetric. The choice of weight tying for the proposed encoder-decoder is motivated by the denoising autoencoder (DAE) <ref type="bibr" target="#b44">[45]</ref>. DAE shows that the tying weight makes it more difficult for an encoder to stay in the linear regime of its nonlinearity.</p><p>We denote a mapping layer of followed by a nonlinearity by</p><formula xml:id="formula_12">(x) = (W x + b )<label>(13)</label></formula><p>with learning parameters = (W , b ), where W is the weight matrix for the convolutional layer and b is its bias matrix. Similarly, we define a mapping layer of followed by the same nonlinearity as</p><formula xml:id="formula_13">(y) = (W y + b )<label>(14)</label></formula><p>with learning parameters = (W , b ), where W is the weight matrix for the 2-D transposed convolutional layer and b is its bias matrix. Therefore, without considering the pooling, unpooling and batch normalization, our 2 -layer autoencoder with tying weight can be denoted bŷ</p><formula xml:id="formula_14">x = 1 (W 1 (… (W ( (W (… 1 (W 1 x + b 1 ) + … ) + b ) + b ) + … ) + b 1 ),<label>(15)</label></formula><p>Then, with the support of a task-specific classifier, the less representative features can be placed in the nonlinear regime of the encoder and, therefore, rejected. As our objective is to encourage ( ) to be as discriminative as possible, it is straightforward to take advantage of this property of weight tying. The layers with different functionalities of the proposed decoder are listed below: 2-D Transposed Convolution A convolutional layer can be represented as a sparse matrix W, and has W for its backward propagation. Thus for , we have a transposed convolutional layer W that utilizes W and W for its forward and backward propagations, respectively.</p><p>Max Unpooling The max unpooling used for takes the output, i.e., the maximum value, of the corresponding max pooling of and the indices of this output as its input. Then, the output of the max unpooling is appropriately sized by setting all non-maximal values to zero. While this type of operation is not a good inverse of the max pooling, it is perfectly suitable for our objective. This is because we only want to retain the features extracted by for the proposed distribution alignment loss.</p><p>Average Unpooling The average unpooling utilized for takes the output of the corresponding average pooling of as its input and sets other values to this average. Similar to the max unpooling, this operation only maintains the information of the features extracted by .</p><p>Nonlinearity We observed from our experiments that the nonlinearity term retained a significant amount of features that were extracted by . Therefore, we assume that the impact of the nonlinearity is limited to the reconstruction of the hidden representation extracted from the target domain to achieve the distribution alignment. In this study, we use the same activation function for as that of , i.e., ReLU activation, without considering the reversibility of the proposed encoder-decoder.</p><p>The average unpooling utilized for the decoder is the upsampling using the nearest-neighbor interpolation. The max unpooling used for the decoder is torch.nn.MaxUnpool2d 1 implemented by Pytorch. The transposed convolution utilized for the decoder is torch.nn.functional.conv_transpose2d 2 implemented by Pytorch. The tying weight is achieved by sharing the weight matrix of the corresponding convolution with the transposed convolution. Our decoder for the object classification tasks can be viewed as an inverted version of the feature extractor of ResNet-50 with 2-D transposed convolution and upsampling. The detailed architecture and configuration of the proposed ResNet-50-based decoder are presented in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Framework of Discriminative Feature Alignment</head><p>In this section, we will discuss how to construct the latent distributions of the two domains on the space of the prior using the proposed regularization.</p><p>Our model, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, consists of a feature extractor and a decoder that share the learning parameters . To predict the categories of the input samples, the framework developed based on our model should also have an image classifier . We represent a mapping function from the input data, either or , to its latent feature vector or as ( ; ). Meanwhile, we denote a mapping function from a latent feature vector or the Gaussian prior vector to an image by ( ; ).</p><p>As the source dataset labels are accessible, we can make a reasonable assumption that the feature space of the source domain is discriminative. Therefore, the goal of our model is to learn a latent feature distribution ( ) from the target domain that can maximally take advantage of the discriminative features of the source domain for its own classification. To achieve this, we need to design a feature alignment approach that can ultimately construct the two feature spaces in a common distribution space. The problem is how to define such distribution space and effectively project the features of the two domains into this space.</p><p>For this objective, we propose to indirectly align the source features and the target features under the guidance of the Gaussian prior. As the first step of our model, we define the Gaussian prior distribution ( ) ∼  (0, 1) where we will construct the two feature spaces on. To encourage the discriminative feature space of the source domain to be constructed on the space of the prior, we regularize and by softmax cross-entropy loss on the labeled source samples, and enforce the distribution over the source samples ( ) to be close to the Gaussian prior ( ) via the KL-divergence penalty on . Meanwhile, the latent feature distribution of the target domain ( ) should be similarly close to the Gaussian prior. In preliminary experiments, we tried to use the same KL-divergence penalty to achieve such alignment, but it turned out to be not as effective as we expected. Therefore, to effectively align ( ) with the prior distribution ( ), we propose a novel L1-distance between the reconstructed samples from the decoder, i.e., minimizing the distance between ( ( )) and ( ), to regularize . Once the training of our model converges, the three distributions, i.e., the source and the target distributions in the feature space and the Gaussian prior distribution, can be properly aligned. In other words, our method can effectively construct the feature spaces of the two domains in the same distribution space, i.e., the space of the Gaussian prior. We also include different ways to achieve such latent-space alignment in Section 5 and compare them with our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Softmax Cross-entropy Loss</head><p>We use softmax cross-entropy loss to handle the classification task on the labeled source domain. This objective can ensure that the discriminative feature space of the source domain can be properly constructed on the space of the prior. We train both and to minimize the objective function:</p><formula xml:id="formula_15"> ( , ) = − 1 ∑ =1 ( = ( ) ) log ( ( ) ),<label>(16)</label></formula><p>where ( = ( ) ) is a binary indicator which is 1 when equals ( ) ; is the mapping function for the classification scores, i.e., = • • .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Kullback-Leibler Divergence</head><p>To encourage the latent feature distribution of the source domain to be close to the Gaussian prior, we apply the KLdivergence penalty between ( ) and ( ) to regularize . We express this objective as:</p><formula xml:id="formula_16"> ( ) = 1 ∑ =1 ( ( ) ) log ( ( ) ) ( ( ) ) ,<label>(17)</label></formula><p>where G seeks to generate the discriminative features of the source domain in the space of the prior under the support of  . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Share Weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target flow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Distribution Alignment Loss</head><p>Regularizing and by  and  , respectively, makes the discriminative feature space of the source domain be constructed on the space of the prior. Therefore, by encouraging ( ) to be defined in the same distribution space, tasks on the target domain can maximally take advantage of the knowledge learned from the source labels. To achieve this, we propose a simple yet effective method to align the target latent distribution with the prior distribution, namely, distribution alignment loss (DAL). DAL is applied to regularize both and . We utilize the absolute difference between the two data distributions produced by and formulate the proposed DAL as:</p><formula xml:id="formula_17"> ( ) = 1 ∑ =1 || ( ( ( ) ); ) − ( ( ) ; )|| 1 ,<label>(18)</label></formula><p>where ||•|| 1 is the L1-norm. In Section 4.1, we present a detailed analysis of the proposed DAL, and empirically verify that it serves as a distribution alignment mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4.">Entropy Loss</head><p>In the proposed framework DFA-ENT, the latent feature vector is fed into to produce predictions for the target input samples. To control the contribution of the target pre-dictions in the generalization of an image classifier, we employ a low-density separation technique entropy minimization (ENT) <ref type="bibr" target="#b11">[12]</ref> to measure the class overlap of the target samples:</p><formula xml:id="formula_18"> ( ) = 1 ∑ =1 − ( ( ( ) )) log ( ( ( ) )). (19)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5.">Full Objective</head><p>The full objective function of the proposed framework DFA-ENT is a linear combination of softmax cross-entropy loss, KL-divergence penalty, distribution alignment loss and the entropy loss:</p><formula xml:id="formula_19"> =  +  +  +  ,<label>(20)</label></formula><p>where and are the weights for the KL-divergence penalty and DAL,respectively, to control the relative importance of the proposed regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Versatility</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Adversarial Domain Adaptation</head><p>Maximum classifier discrepancy <ref type="bibr" target="#b36">[37]</ref> achieves state-ofthe-art on digit and traffic-sign classification. It has one feature extractor and two image classifiers 1 and 2 . It regards the disagreement between 1 and 2 as its classifier-induced discrepancy. It uses a three-step adversarial training strategy to avoid the input target samples that are outside the support of the source domain: first, minimizing softmax cross-entropy loss  ; second, minimizing the difference between  and the L1-loss between the outputs of the two image classifiers on the target samples  ( ); and third, minimizing  ( ).</p><p>The proposed DFA-MCD is developed based on MCD. Our objective  is integrated into the first and the second training steps of MCD; and the proposed  is combined with the objective function of its last training step. To better clarify DFA-MCD, we include the details of the training procedures in Algorithm 1 and highlight our method in red. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Non-adversarial Domain Adaptation</head><p>Stepwise adaptive feature norm <ref type="bibr" target="#b47">[48]</ref> is state-of-the-art approach on non-adversarial DA and object classification. It follows the standard DA setting with a feature extractor and a -layer image classifier . It denotes the first − 1 layers of its image classifier as , and utilizes the intermediate features from</p><p>to calculate its classifier-induced discrepancy:</p><formula xml:id="formula_20">( ) = 2 (ℎ( ; ) + , ℎ( ; )),<label>(21)</label></formula><p>where 2 is the L2-distance; ℎ( ) is the L2-norm of ( ( )); and represent the learning parameters in the previous and the current iterations, respectively; and is a constant to control the feature-norm enlargement. Thus, SAFN can mitigate domain shifts by minimizing the following loss:</p><formula xml:id="formula_21"> ( , , ) = ( , ) +  ( ) + ∈( ∪ ) [ ( )],<label>(22)</label></formula><p>where is a trade-off among the objectives. Our DFA-SAFN is developed based on SAFN. We implement a ResNet-50-based decoder to generate ( ) and ( ) for the proposed DAL. We integrate all of our objective functions into the final loss of SAFN. The details of DFA-SAFN are shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We implemented all experiments on the PyTorch 3 platform. We reported the results of the benchmark algorithms 3 https://pytorch.org/ Get ( ) and ( ) using the current for the next iteration;  under their optimal hyper-parameter settings. To better validate the versatility of our model, we followed the same settings and the hyper-parameters that were utilized in MCD <ref type="bibr" target="#b36">[37]</ref> and SAFN <ref type="bibr" target="#b47">[48]</ref> for evaluating DFA-MCD and DFA-SAFN, and did not fine-tune the two frameworks. To be specific, we used Adam <ref type="bibr" target="#b16">[17]</ref> optimizer, and set the learning rate and the batch size to 2.0 × 10 −4 and 128, respectively, in all experiments for the evaluation on the digit and traffic-sign recognition datasets; we utilized SGD optimizer, and set the learning rate and the batch size to 1.0 × 10 −3 and 32, respectively, in all experiments for the evaluation on the object recoginition benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on Synthetic Datasets</head><p>In this section, we empirically verified the distribution alignment mechanism of the proposed distribution alignment loss (DAL) on three synthetic datasets, namely, 2D Gaussian distributions with different mean or covariance, moons dataset and blobs dataset. For each experiment, we generated 500 samples for each domain. We employed the same networks and for all synthetic experiments. The encoder is a 3-layer MLP that maps a 2D distribution to a higher dimensional space. The deocder , which is also a 3-layer MLP, maps the higher dimensional latent distribution back to the input distribution space. The architectures for the two MLPs are shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>The samples from the target input distribution are fed into the encoder and the decoder to generate their predictions ( ( )). The outputs of , which are the predicted target samples, and the samples from the source input distribution are utilized for the proposed DAL. We tested the same covariance case and the same mean case for the 2D Gaussian distributions. For the same covariance case, the green points (source) were sampled from a 2D Gaussian with mean 5 5 and covariance 4 2 2 2 ; and the blue points (target) indicate the samples from a 2D Gaussian with the same covariance but different mean 1 1 . For the same mean case, the two 2D Gaussian distributions have the same mean 1 1 but different covariance, i.e., 0.3 0.2 0.2 0.2 for the source input distribution and 4 2 2 2 for the target input distribution. We used scikit-learn <ref type="bibr" target="#b30">[31]</ref> to generate moons and blobs datasets. For moons dataset, we made two interleaving half circles for the two domains and add a Gaussian noise with standard deviation 0.1 to the data. For blobs dataset, we generated two isotropic Gaussian blobs with centers at 11 11 and 9 9 for the source input distribution and the target input distribution, respectively. As shown in <ref type="figure" target="#fig_8">Figure 4</ref>, the predicted target samples (blue points) successfully align with the source samples (green points) after optimizing by DAL alone in all synthetic experiments. Therefore, we can claim that the proposed DAL serves as the distribution alignment mechanism in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Same Covariance</head><p>Before Optimizing Optimized by DAL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Same Mean</head><p>Before Optimizing Optimized by DAL (a) 2D Gaussian.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Digit Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Setup</head><p>In this section, we evaluated the adaptation of our two frameworks DFA-ENT and DFA-MCD on five digit and trafficsign recognition datasets. For each adaptation scenario, we employed the same network architectures utilized in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37]</ref>, and implemented the decoder accordingly. To evaluate DFA-ENT, we used the SGD optimizer with a mini-batch size of 256 in all digit and traffic-sign recognition experiments. We set the learning rate to 0.1 in the adaptation from SVHN to MNIST and 0.02 in other adaptation scenarios for evaluating DFA-ENT. Our hyper-parameters and were  <ref type="bibr" target="#b28">[29]</ref> and MNIST <ref type="bibr" target="#b19">[20]</ref> datasets were used as the source domain and the target domain, respectively. The two datasets consist of images of digit from 0 to 9. However, SVHN <ref type="bibr" target="#b28">[29]</ref> has significant variations in the colored background, contrast, rotation, scale, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST (MN) ↔ USPS (US):</head><p>We evaluated two adaptation scenarios on USPS <ref type="bibr" target="#b14">[15]</ref> and MNIST <ref type="bibr" target="#b19">[20]</ref> datasets. We used the same setup provided by <ref type="bibr" target="#b36">[37]</ref> for the two adaptation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYN SIGNS (SY) → GTSRB (GT):</head><p>We also evaluated the proposed frameworks on a more complex scenario, from synthetic traffic signs dataset (SYN SIGNS) <ref type="bibr" target="#b27">[28]</ref> to the realworld German Traffic Signs Recognition Benchmark (GT-SRB) <ref type="bibr" target="#b38">[39]</ref>. This domain adaptation scenario has 43 different traffic signs (classes). We split the datasets based on <ref type="bibr" target="#b36">[37]</ref>. <ref type="table" target="#tab_0">Table 2</ref> lists the results for the target domain classification. { } * denotes that all of the training samples are used for training the frameworks. We used the same networks for the source only evaluation. The average and the standard deviation of the accuracy on each DA scenario are reported by repeating each experiment 5 times. The results indicate that our model significantly improves the adaptation performance of MCD on all digit and traffic-sign datasets. The standard deviations of DFA-MCD are much lower than those of MCD, which indicates that our model can result in more robust performance. The visualizations of the learned feature representations are shown in <ref type="figure" target="#fig_9">Figure 5</ref>. The comparison is conducted between DFA-MCD and MCD. The better feature clustering indicates that our model significantly improves the adaptation performance of MCD through better </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Setup</head><p>We extensively evaluated the adaptation performance of DFA-ENT and DFA-SAFN on five benchmark datasets for object recognition, namely, VisDA2017, Office-31, ImageCLEF-DA and Office-Home. For each adaptation scenario, we employed ResNet-50 <ref type="bibr" target="#b12">[13]</ref> that was fine-tuned from the Ima-geNet <ref type="bibr" target="#b8">[9]</ref> pre-trained model. We implemented our decoder as an inverted version of the feature extractor of ResNet-50. To evaluate DFA-ENT, we used the SGD optimizer with a learning rate of 1 × 10 −3 , and set the batch size to 32 on all benchmark datasets. Our hyper-parameters and were set to 0.1 and 10, respectively, for both frameworks.</p><p>VisDA2017 <ref type="bibr" target="#b32">[33]</ref> is a large-scale benchmark dataset used for the 2017 visual domain adaptation challenge. The goal of the dataset is trying to bridge the domain gap between the synthetic objects and the real obbjects. It has over 280K images across 12 object categories. The source domain consists of 152,397 synthetic images that are generated by rendering the 3D models of a certain object categories. The target domain contains 55,388 images of the real objects, which are collected from Microsoft COCO dataset <ref type="bibr" target="#b21">[22]</ref>. This could be the most challenging benchmark dataset for UDA.</p><p>Office-Home <ref type="bibr" target="#b43">[44]</ref> has images of everyday objects from four different domains: Artistic (Ar), Clipart (Cl), Product (Pr) and Real-World (Rw). The dataset has around 15,500 images. Each domain contains 65 object classes. Notably, Ar consists of the images from the different forms of artistic depictions of objects, while a regular camera takes the images of Rw. Some image samples from this dataset are shown in <ref type="figure" target="#fig_11">Figure 6</ref>.</p><p>ImageCLEF-DA 5 is a dataset used for the 2014 Image-CLEF domain adaptation challenge. This dataset selects 12 common object classes from three public datasets: Caltech-256 (C), ImageNet ILSVRC2012 (I) and Pascal VOC 2012  (P). The dataset organizers selected 50 images per class and 600 images in total for each domain. Office-31 <ref type="bibr" target="#b34">[35]</ref> is a standard benchmark dataset for evaluating visual DA algorithms. It has three different domains: Amazon (A), Webcam (W), and DSLR (D). Amazon consists of images from amazon.com. Webcam and DSLR contain images for the office environment captured by a web camera and a digital SLR camera, respectively. It consists of 4,652 images of 31 object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Results</head><p>The results of DFA-ENT and DFA-SAFN on VisDA2017, ImageCLEF-DA, Office-31 and Office-Home are listed in <ref type="table" target="#tab_3">Table 3</ref>, 4, 5 and 6, respectively. {Method} * indicates that tencrop images are used in the evaluation phase with its bestperforming models. We repeated each experiment 3 times and reported the average and the standard deviation of the accuracy for evaluating the datasets Office-Home, ImageCLEF-DA and Office-31. We reported the accuracy of the evaluation on VisDA2017 after 20 epochs with no repeated experiments. The results illustrate that the proposed frameworks significantly outperform the benchmark algorithms on object classification. The robustness of SAFN is also improved by DFA with lower variance among each repeated experiments.</p><p>Results on VisDA2017 show that the proposed DFA can significantly help the existing methods to better bridge the synthetic-to-real domain gap, which improves the performance of the baseline methods by at least 3.9% (6.2% for SAFN and 3.9% for MCD). Notably, the proposed DFA-MCD achieves state-of-the-art performance on this large-scale dataset. Besides, our simplified framework DFA-ENT achieves the competitive performance in all four beachmark datasets for the object recognition task, which suggests the effectiveness of the latent alignment in transfer learning. Moreover, the outstanding improvement on the adaptation scenarios (Office-31, Office-Home) with significant nuisance image variations suggests that our model can improve other frameworks' knowledge transferability remarkably in the adaptation scenario with significant variations. One interesting observation can be revealed from these results that the transfer gains of the existing approaches, which mitigate the domain gap by classifier-induced discrepancies, can be further improved by improving the alignment in the feature spaces. One limitation of our research is that we only consider the way to better construct the feature spaces for the DA problem and directly incorporate the proposed method into the classifier-induced discrepancy based methods. Therefore, we believe that the transfer gains can be more significantly improved by explicitly considering the relationship between the features induced from the feature extractor and the feature induced from the classifier. But how to trade off the alignment of the latent distributions against the alignment of the output class distributions is still a big challenge for the DA community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The Shape of The Latent Distribution</head><p>In this ablation study, we validated the claim that the proposed regularization could construct the latent distributions of the two domains on a common distribution space. In our setting, the common distribution space is the space of the Gaussian prior. The best-performing models that were trained previously were used in the study. We selected a vector from the source latent distribution and one corresponding vector from the target latent distribution, and plotted their histograms for demonstration. Note that the selected vectors from the source latent distribution and the target latent distribution fall under the same category so that they should share the discriminative features. <ref type="figure" target="#fig_13">Figure 7</ref> demonstrates that the existing UDA methods (take SAFN <ref type="bibr" target="#b47">[48]</ref> as an example) cannot effectively construct the feature spaces of the two domains on a common distribution space. This could make the classification tasks on the target samples hard to make the most use of the discriminative source features. By contrast, as shown in <ref type="figure" target="#fig_15">Figure 8</ref> and <ref type="figure" target="#fig_16">Figure 9</ref>, the proposed regularization can encourage the source discriminative features to be projected into the space of the Gaussian prior, and construct the target feature space on this prior distribution space. This indicates that the proposed DFA can encourage the latent distributions of the two domains to be closed to a common distribution in the feature space, i.e., the Gaussian prior, which promotes better feature alignment. Note that, the latent vectors are observed from the layer before the last ReLU activation of the encoder for better demonstration.   </p><formula xml:id="formula_22">± 0.1 ± 0.4 ± 0.2 ± 0.1 ± 0.3 ± 0.3 ± 0.2 ± 0.1 ± 0.0 ± 0.3 ± 0.0 ± 0.0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of the Proposed Regularization</head><p>In this ablation study, we validated that our method could effectively align the feature spaces of the two domains. We conducted a case study on the adaptation scenario from SVHN to MNIST as its significant domain variation. We randomly selected 100 images per class from both domains and 2000 images in total. We utilized the best-performing models that were trained in the previous experiments. By measuring the distance between the feature spaces, the effectiveness of the feature alignment can be examined. We computed the average L2-distances between the feature space of SVHN and the feature space of MNIST after the adaptation with and without our model, as shown in <ref type="table" target="#tab_5">Table 7</ref>. As expected, the   feature-space distance of DFA-MCD is much shorter than that of MCD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">How to Effectively Align Feature Spaces</head><p>We investigated the most effective method for the latent alignment in this ablation study. We conducted a case study on the adaptation scenario from MNIST to UPSP. To better illustrate this study, we first define some loss functions. We formulate the paired reconstruction loss of an autoencoder as:</p><formula xml:id="formula_23"> ( ) = 1 ∑ =1</formula><p>[|| ( ( ( ) ); ) − ( ) )|| 1 ]. (23) Average L2-distance between the SVHN feature space and the MNIST feature space. The numbers (0-9) denote the digit labels, and All indicates evaluating by all samples.  We define a KL-divergence penalty to encourage ( ) to be close to ( ) as  . To validate the effect of weight tying, we further define the learning parameters for the decoder in the case where the tying weight is not applied. We explored six different ways to align the two latent feature distributions ( ) and ( ): 1) the proposed DFA-ENT framework; 2) DFA-ENT but the encoder and the decoder do not share their weights ( ≠ ); 3) instead of using our DAL to align the target latent distribution with the Gaussian prior, utilizing a KL-divergence to make ( ) close to the prior; 4) the direct latent alignment via an unpaired L1-distance between the reconstructed samples from the two domains, i.e., minimizing the distance between ( ( )) and ( ( )) ( ); 5) the direct latent alignment using  ; and 6) further regularizing Case 5) by two reconstruction losses  ( ) +  ( ) ( ) with our weight-tied encoder-decoder formulation. The results, which are shown in <ref type="table" target="#tab_6">Table 8</ref>, indicate that the proposed DFA is the most effective approach to align the latent distributions of the two domains. The ablation study validates that all of the Gaussian-guided alignment, unpaired L1-distance and weight tying are of necessity for the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Parameter Sensitivity</head><p>To quantify the impact of our discriminative feature alignment (DFA) on the UDA frameworks, we investigated the sensitivity of our hyper-parameters, i.e., and , in DFA-MCD and DFA-SAFN. We selected adaptation scenarios from MNIST to USPS and from Amazon to DSLR for demonstration. The results are shown in <ref type="figure" target="#fig_18">Figure 10</ref>(a)(b). For each case study, and were varied from 0.001 to 100. As shown in both figures, DFA can stably improve the performance of adversarial and non-adversarial UDA frameworks with different values of and . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Computational Complexity Analysis</head><p>We investigated the computational efficiency of our model as it could be combined with other UDA frameworks. We conducted a case study on the adaptation scenario from SVHN to MNIST. Although the time spent on training one epoch for DFA-MCD is 1.21 times MCD (NVIDIA GeForce RTX 2070), DFA-MCD requires fewer epochs to converge, as shown in <ref type="figure" target="#fig_19">Figure 11</ref>. Therefore, we can say that our model can efficiently improve the performance of various UDA frameworks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduced a novel model for UDA to better align the source and the target features, which could improve the adaptation performance of the UDA framework. We proposed an indirect latent alignment process to encourage the features of the two domains to be constructed on a common feature space, i.e., the space of the Gaussian prior. To better align two distributions, we also proposed a novel unpaired L1-distance in the decoder space, and empirically confirmed that it served as a distribution alignment mechanism. Our frameworks outperformed state-of-the-arts in most experiments. The results of the extensive experiments have validated the importance and the versatility of our research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Aligning two distributions by taking advantage of the formulation of the encoder-decoder. It contains an encoding function and a decoding function . The mapping function ( (•)) can be regarded as a weight-tied autoencoder that can put the less representative features into the nonlinear regime of 's nonlinearity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>imized when ∃ s.t. (̂ | ; ) = ( | ; ). Combining Equation 5 and Equation 8, we have the lower bound of the mutual information between and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(Best viewed in color.) The overall architecture of the proposed framework. The feature extractor G maps the input data to their latent feature vectors. The decoder D, which can be viewed as an inverted version of , maps a latent feature vector or Gaussian prior vector to an image that has the same dimensions as the input samples. Our model can encourage the discriminative features of the two domains to be projected into the space of the prior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 : 3 for ℎ ← 1 to N do 4 Step 1 : 5 Update , 1 7 Step 2 : 2 [ 9 Step 3 :</head><label>13415172293</label><figDesc>DFA-MCD 1 Input image normalization; initialize the Gaussian prior ( ) ∼  (0, 1); 2 while epoch ≤ max epoch do Sample minibatch of samples from the Gaussian prior ( ); and 2 to min , 1 , 2 [ ( , )+  ( )]; 6 Fix ; and update 1 and 2 to min 1 ,  ( , ) −  ( )+  ( )] ; 8 Fix 1 and 2 . Calculate  ( ) using the current . Then update and to min , [ ( )+  ( )].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>(Best viewed in color.) Green and blue points indicate the samples from the source distribution and the target distribution, respectively. The predicted target distribution well aligns with the source distribution after the proposed distribution alignment loss converges, which validates the distribution alignment mechanism of distribution alignment loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>(Best viewed in color.) t-SNE<ref type="bibr" target="#b26">[27]</ref> visualizations of the learned feature representations for two different adaptation scenarios. Blue and red points indicate the latent features from the source domain and the target domain, respectively. feature alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>5 https://www.imageclef.org/2014/adaptation (a) Artistic. (b) Clipart. (c) Product. (d) Real-World.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Example images for alarm clock from the four different domains of Office-Home.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Source Latent (Office-31).(b) Target Latent (Office-31).(c) Source Latent (Home). (d) Target Latent (Home).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Histograms of the source latent distribution and the target latent distribution after the training of SAFN converges. Top: the adaptation scenario from Amazon to DSLR (Office-31 ). Bottom: the adaptation scenario from Clipart to Product (Office-Home).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 8 :</head><label>8</label><figDesc>Histograms of the source latent distribution and the target latent distribution after the training of the proposed DFA-SAFN on the adaptation scenario from Amazon to DSLR (Office-31 ) converges. (a) Gaussian Prior. (b) Source Latent. (c) Target Latent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9 :</head><label>9</label><figDesc>Histograms of the source latent distribution and the target latent distribution after the training of the proposed DFA-SAFN on the adaptation scenario from Clipart to Product (Office-Home) converges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>0.1596 0.1472 0.1517 0.0564 DFA-MCD 0.0800 0.0829 0.0692 0.0756 0.0266</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 10 :</head><label>10</label><figDesc>Sensitivity analysis of the hyper-parameters and for DFA-MCD and DFA-SAFN (orange lines indicate DFA-SAFN; blue lines indicate DFA-MCD). was set to 0.1 when evaluating . was set to 10 when evaluating .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 11 :</head><label>11</label><figDesc>Relationship between the training epoch and the accuracy (orange line indicates the proposed DFA-MCD; blue line indicates MCD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 2 :</head><label>2</label><figDesc>DFA-SAFN Input image normalization; initialize tensors for storing ℎ( ; ), ( ) and ( ); initialize the Gaussian prior ( ) ∼  (0, 1); 2 while epoch ≤ max epoch do Calculate  using ( ) and ( ) from the previous iteration;</figDesc><table><row><cell>3</cell><cell>for</cell><cell>ℎ ← 1 to N do</cell><cell></cell></row><row><cell>4</cell><cell></cell><cell cols="2">Sample minibatch of</cell><cell>samples from the Gaussian prior ( );</cell></row><row><cell>5</cell><cell></cell><cell>Calculate (</cell><cell>∪</cell><cell>) using ℎ( ; ) and ℎ( ; );</cell></row><row><cell>7</cell><cell></cell><cell cols="3">Update , and to minimize [</cell><cell>+  +  ];</cell></row></table><note>168 Calculate ℎ( ; ) and store it as ℎ( ; ) for the next iteration; 9</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 Network</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Architectures of the encoder and the decoder for the</cell></row><row><cell cols="2">synthetic experiments to validate the distribution alignment</cell></row><row><cell cols="2">mechanism of the proposed regularization. FC-x represents</cell></row><row><cell cols="2">fully-connected layer with x hidden neurons. ReLU denotes</cell></row><row><cell cols="2">the ReLU activation. BatchNorm represents the batch nor-</cell></row><row><cell>malization.</cell><cell></cell></row><row><cell>Model</cell><cell>Architecture</cell></row><row><cell cols="2">Encoder G FC-56, ReLU, FC-128, ReLU,</cell></row><row><cell></cell><cell>FC-256, ReLU, BatchNorm</cell></row><row><cell>Decoder D</cell><cell>FC-128, ReLU, BatchNorm,</cell></row><row><cell></cell><cell>FC-56, ReLU, FC-2, ReLU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Accuracy(%) of the proposed frameworks on the benchmark datasets for digit and traffic-sign recognition.Method SV )MN SY )GT MN )US MN * )US * US )MN</figDesc><table><row><cell cols="2">Source Only 67.1</cell><cell>85.1</cell><cell>76.7</cell><cell>79.4</cell><cell>63.4</cell></row><row><cell cols="2">DANN[10] 71.1</cell><cell>88.7</cell><cell>77.1</cell><cell>85.1</cell><cell>73.0</cell></row><row><cell>DSN[4]</cell><cell>82.7</cell><cell>93.1</cell><cell>91.3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ADDA[43] 76.0</cell><cell>-</cell><cell>89.4</cell><cell>-</cell><cell>90.1</cell></row><row><cell cols="2">MSTN[47] 91.7</cell><cell>-</cell><cell>-</cell><cell>92.9</cell><cell>-</cell></row><row><cell>GTA[38]</cell><cell>92.4</cell><cell>-</cell><cell>92.8</cell><cell>95.3</cell><cell>90.8</cell></row><row><cell>DEV[49]</cell><cell>93.2</cell><cell>-</cell><cell>-</cell><cell>92.5</cell><cell>96.9</cell></row><row><cell cols="2">GPDA 4 [16] 98.2</cell><cell>96.2</cell><cell>96.4</cell><cell>98.1</cell><cell>96.4</cell></row><row><cell>MCD[37]</cell><cell>96.2</cell><cell>94.4</cell><cell>94.2</cell><cell>96.5</cell><cell>94.1</cell></row><row><cell>(n = 4)</cell><cell cols="3">± 0.4 ± 0.3 ± 0.7</cell><cell>± 0.3</cell><cell>± 0.3</cell></row><row><cell cols="2">DFA-ENT 98.2</cell><cell>96.8</cell><cell>96.5</cell><cell>97.9</cell><cell>96.2</cell></row><row><cell>(Ours)</cell><cell cols="3">± 0.3 ± 0.2 ± 0.4</cell><cell>± 0.2</cell><cell>± 0.1</cell></row><row><cell cols="2">DFA-MCD 98.9</cell><cell>97.5</cell><cell>97.3</cell><cell>98.6</cell><cell>96.6</cell></row><row><cell>(Ours)</cell><cell cols="3">± 0.2 ± 0.2 ± 0.1</cell><cell>± 0.1</cell><cell>± 0.2</cell></row></table><note>set to 0.01 and 10, respectively, in all adaptation scenarios for both frameworks. SVHN (SV) → MNIST (MN): Street-View House Num- ber (SVHN)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Accuracy(%) of the proposed frameworks on VisDA2017 (ResNet-50). plane bcycl bus car horse knife mcycl person plant sktbrd train truck Per-class</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet-50 [13]</cell><cell>60.2 10.3 54.7 54.5 42.9</cell><cell>2.1</cell><cell>78.9</cell><cell>4.5</cell><cell>45.5</cell><cell>29.5</cell><cell>89.0 12.4</cell><cell>40.4</cell></row><row><cell cols="2">SAFN [48]</cell><cell cols="3">90.5 55.9 80.3 64.6 88.8 31.8 92.7</cell><cell>70.4</cell><cell>93.2</cell><cell>49.6</cell><cell>87.7 23.2</cell><cell>69.1</cell></row><row><cell cols="2">MCD [37]</cell><cell cols="3">90.3 62.6 84.8 71.7 85.9 72.9 93.7</cell><cell>71.9</cell><cell>86.8</cell><cell>79.1</cell><cell>81.6 14.3</cell><cell>74.6</cell></row><row><cell cols="5">DFA-ENT (Ours) 88.3 55.1 81.0 72.9 91.4 94.4 91.1</cell><cell>75.1</cell><cell>80.6</cell><cell>45.7</cell><cell>88.2 15.8</cell><cell>73.3</cell></row><row><cell cols="5">DFA-SAFN (Ours) 93.1 58.4 85.8 69.9 89.8 96.1 90.3</cell><cell>77.5</cell><cell>87.4</cell><cell>48.9</cell><cell>85.1 21.1</cell><cell>75.3</cell></row><row><cell cols="5">DFA-MCD (Ours) 91.2 77.4 80.5 63.3 87.1 85.4 86.4</cell><cell>79.5</cell><cell>90.3</cell><cell cols="2">79.7 89.2 31.6</cell><cell>78.5</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Accuracy(%) of the proposed frameworks on ImageCLEF-DA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(ResNet-50).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">I→P P→I I→C C→I C→P P→C Avg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ResNet-50 [13] 74.8 83.9 91.5 78.0 65.5 91.2 80.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DANN [10]</cell><cell cols="2">75.0 86.0 96.2 87.0 74.3 91.5 85.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDAN  *  [25]</cell><cell cols="2">76.7 90.6 97.0 90.5 74.5 93.5 87.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CADA[19]</cell><cell cols="2">78.0 90.5 96.7 92.0 77.2 95.5 88.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CDAN+TN [46] 78.3 90.8 96.7 92.3 78.0 94.8 88.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HAFN [48]</cell><cell cols="2">76.9 89.0 94.4 89.6 74.9 92.9 86.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAFN [48]</cell><cell cols="2">79.3 93.3 96.3 91.7 77.6 95.3 88.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">± 0.1 ± 0.4 ± 0.4 ± 0.0 ± 0.1 ± 0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DFA-ENT</cell><cell cols="2">79.5 93.0 96.4 92.5 77.2 95.8 89.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Ours)</cell><cell cols="2">± 0.0 ± 0.3 ± 0.2 ± 0.2 ± 0.1 ± 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DFA-SAFN</cell><cell cols="2">80.0 94.2 97.5 93.8 78.7 96.7 90.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Ours)</cell><cell cols="2">± 0.1 ± 0.3 ± 0.2 ± 0.0 ± 0.1 ± 0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Accuracy(%) of the proposed frameworks on Office-31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(ResNet-50).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">A→W D→W W→D A→D D→A W→A Avg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ResNet-50 [13] 68.4 96.7 99.3 68.9 62.5 60.7 76.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DANN [10]</cell><cell cols="2">82.0 96.9 99.1 79.7 68.2 67.4 82.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GTA [38]</cell><cell cols="2">89.5 97.9 99.8 87.7 72.8 71.4 86.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDAN  *  [25]</cell><cell cols="2">93.1 98.2 100.0 89.8 70.1 68.0 86.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DSBN[7]</cell><cell cols="2">93.3 99.1 100.0 90.8 72.7 73.9 88.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TAT[23]</cell><cell cols="2">92.5 99.3 100.0 93.2 73.1 72.1 88.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HAFN [48]</cell><cell cols="2">83.4 98.3 99.7 84.4 69.4 68.5 83.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAFN [48]</cell><cell cols="2">90.1 98.6 99.8 90.7 73.0 70.2 87.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">± 0.8 ± 0.2 ± 0.0 ± 0.5 ± 0.2 ± 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DFA-ENT</cell><cell cols="2">90.5 99.0 100.0 94.3 72.1 67.8 87.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Ours)</cell><cell cols="2">± 0.7 ± 0.1 ± 0.0 ± 0.4 ± 0.2 ± 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DFA-SAFN 93.5 99.4 100.0 94.8 73.8 71.0 88.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Ours)</cell><cell cols="2">± 0.5 ± 0.1 ± 0.0 ± 0.3 ± 0.1 ± 0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Accuracy(%) of the proposed frameworks on Office-Home (ResNet-50). Ar )Cl Ar )Pr Ar )Rw Cl )Ar Cl )Pr Cl )Rw Pr )Ar Pr )Cl Pr )Rw Rw )Ar Rw )Cl Rw )Pr Avg</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 [13]</cell><cell>34.9</cell><cell>50.0</cell><cell>58.0</cell><cell>37.4</cell><cell>41.9</cell><cell>46.2</cell><cell>38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9</cell><cell>46.1</cell></row><row><cell>DANN[10]</cell><cell>45.6</cell><cell>59.3</cell><cell>70.1</cell><cell>47.0</cell><cell>58.5</cell><cell>60.9</cell><cell>46.1</cell><cell>43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8</cell><cell>57.6</cell></row><row><cell>CDAN  *  [25]</cell><cell>49.0</cell><cell>69.3</cell><cell>74.5</cell><cell>54.4</cell><cell>66.0</cell><cell>68.4</cell><cell>55.6</cell><cell>48.3</cell><cell>75.9</cell><cell>68.4</cell><cell>55.4</cell><cell>80.5</cell><cell>63.8</cell></row><row><cell>DWT-MEC[34]</cell><cell>50.3</cell><cell>72.1</cell><cell>77.0</cell><cell>59.6</cell><cell>69.3</cell><cell>70.2</cell><cell>58.3</cell><cell>48.1</cell><cell>77.3</cell><cell>69.3</cell><cell>53.6</cell><cell>82.0</cell><cell>65.6</cell></row><row><cell>TAT[23]</cell><cell>51.6</cell><cell>69.5</cell><cell>75.4</cell><cell>59.4</cell><cell>69.5</cell><cell>68.6</cell><cell>59.5</cell><cell>50.5</cell><cell>76.8</cell><cell>70.9</cell><cell>56.6</cell><cell>81.6</cell><cell>65.8</cell></row><row><cell cols="2">CDAN+TN [46] 50.2</cell><cell>71.4</cell><cell>77.4</cell><cell>59.3</cell><cell>72.7</cell><cell>73.1</cell><cell>61.0</cell><cell>53.1</cell><cell>79.5</cell><cell>71.9</cell><cell>59.0</cell><cell>82.9</cell><cell>67.6</cell></row><row><cell>HAFN [48]</cell><cell>50.2</cell><cell>70.1</cell><cell>76.6</cell><cell>61.1</cell><cell>68.0</cell><cell>70.7</cell><cell>59.5</cell><cell>48.4</cell><cell>77.3</cell><cell>69.4</cell><cell>53.0</cell><cell>80.2</cell><cell>65.4</cell></row><row><cell>SAFN [48]</cell><cell>52.0</cell><cell>71.7</cell><cell>76.3</cell><cell>64.2</cell><cell>69.9</cell><cell>71.9</cell><cell>63.7</cell><cell>51.4</cell><cell>77.1</cell><cell>70.9</cell><cell>57.1</cell><cell>81.5</cell><cell>67.3</cell></row><row><cell></cell><cell cols="9">± 0.1 ± 0.6 ± 0.3 ± 0.3 ± 0.6 ± 0.6 ± 0.4 ± 0.2 ± 0.0</cell><cell cols="3">± 0.4 ± 0.1 ± 0.0</cell><cell></cell></row><row><cell>DFA-ENT</cell><cell>50.6</cell><cell>74.8</cell><cell>79.3</cell><cell>65.2</cell><cell>73.8</cell><cell>74.5</cell><cell>63.5</cell><cell>51.4</cell><cell>81.4</cell><cell>73.9</cell><cell>58.2</cell><cell>83.3</cell><cell>69.2</cell></row><row><cell>(Ours)</cell><cell cols="9">± 0.1 ± 0.3 ± 0.2 ± 0.2 ± 0.3 ± 0.4 ± 0.4 ± 0.3 ± 0.0</cell><cell cols="3">± 0.4 ± 0.0 ± 0.0</cell><cell></cell></row><row><cell>DFA-SAFN</cell><cell>52.8</cell><cell>73.9</cell><cell>77.4</cell><cell>66.5</cell><cell>72.9</cell><cell>73.6</cell><cell>64.9</cell><cell>53.1</cell><cell>78.7</cell><cell>74.5</cell><cell>58.1</cell><cell>82.4</cell><cell>69.1</cell></row><row><cell>(Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>Accuracy(%) of different latent-alignment methods on the adaptation scenario from MNIST to USPS. Note that all methods utilize  and  for classification.</figDesc><table><row><cell></cell><cell cols="2"> +  (Ours) </cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>97.3</cell><cell>93.1</cell><cell></cell><cell>87.9</cell></row><row><cell></cell><cell> +  , ≠</cell><cell></cell><cell></cell><cell>+ </cell></row><row><cell>Accuracy</cell><cell>95.8</cell><cell>89.2</cell><cell></cell><cell>83.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://pytorch.org/docs/stable/nn.html 2 https://pytorch.org/docs/stable/nn.functional</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This framework is developed based on MCD.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with genrative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2724" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5067" to="5075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domainspecific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A dataset for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation: A deep max-margin gaussian process approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4380" to="4390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attending to discriminative certainty for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of the IEEE</title>
		<meeting>eeding of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transferable adversarial training: A general approach to adapting deep classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluation of traffic sign recognition methods trained on synthetically generated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reading digits in neural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation using feature-whitening and consensus loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9471" to="9480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8503" to="8512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transferable normalization: Towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5423" to="5432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards accurate model selection in deep unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7124" to="7133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
