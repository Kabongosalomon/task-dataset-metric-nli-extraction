<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Disentangled Representation for Robust Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Eom</surname></persName>
							<email>cheom@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
							<email>bumsub.ham@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Disentangled Representation for Robust Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons can have the same attribute and the same person's appearance looks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations (e.g., human pose), which requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to disentangle identity-related and -unrelated features from person images. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose, scale changes). To this end, we introduce a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN), that factorizes these features using identification labels without any auxiliary information. We also propose an identity-shuffling technique to regularize the disentangled features. Experimental results demonstrate the effectiveness of IS-GAN, significantly outperforming the state of the art on standard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID. Our code and models are available online: https://cvlab-yonsei.github.io/projects/ISGAN/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (reID) aims at retrieving person images of the same identity as a query from a large dataset, which is particularly important for finding/tracking missing persons or criminals in a surveillance system. This can be thought of as a fine-grained retrieval task in that 1) the data set contains images of the same object class (i.e., person) but with different background clutter and intra-class variations (e.g., pose, scale changes), and 2) they are typically captured with different illumination conditions across multiple cameras possibly with different characteristics and viewpoints. To tackle these problems, reID methods have focused on learning metric space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref> and discriminative person representations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, robust to intra-class variations and distracting scene details.</p><p>Convolutional neural networks (CNNs) have allowed significant advances in person reID in the past few years. Recent methods using CNNs add few more layers for aggregating body parts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> and/or computing an attention map <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47]</ref>, on the top of e.g., a (cropped) ResNet <ref type="bibr" target="#b7">[8]</ref> trained for ImageNet classification <ref type="bibr" target="#b16">[17]</ref>. They give state-of-the-art results, but finding person representations robust to various factors is still very challenging. More recent methods exploit generative adversarial networks (GANs) <ref type="bibr" target="#b6">[7]</ref> to learn feature representations robust to a particular factor. For example, conditioned on a target pose map and a person image, they generate a new person image of the same identity but with the target pose <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, and the generated image is then used as an additional training data. This allows to learn pose-invariant features, and also has an effect of data augmentation for regularization.</p><p>(a) Interpolation between identity-related features (b) Interpolation between identity-unrelated features <ref type="figure">Figure 1</ref>: Visual comparison of identity-related and -unrelated features. We generate new person images by interpolating (a) identity-related features and (b) identity-unrelated ones between two images, while fixing the other ones. We can see that identity-related features encode e.g., clothing and color, and identity-unrelated ones involve e.g., human pose and scale changes. Note that we disentangle these features using identification labels only. (Best viewed in color.)</p><p>In this paper, we introduce a novel framework, dubbed identity shuffle GAN (IS-GAN), that disentangles identity-related and -unrelated features from input person images, without any auxiliary supervisory signals except identification labels. Identity-related features contain information useful for identifying a particular person (e.g., gender, clothing, hair), while identity-unrelated ones hold all other information (e.g., human pose, background clutter, occlusion, scale changes). See <ref type="figure">Fig. 1</ref> for example. To this end, we propose an identity shuffling technique to disentangle these features using identification labels only within our framework, regularizing the disentangled features. At training time, IS-GAN inputs person images of the same identity and extracts identity-related and -unrelated features. In particular, we divide person images into horizontal parts, and disentangle these features in both image-and part-levels. We then learn to generate new images of the same identity by shuffling identity-related features between the person images. We use the identity-related features only to retrieve person images at test time. We set a new state of the art on standard benchmarks for person reID, and show an extensive experimental analysis with ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Person representations. Recent reID methods provide person representations robust to a particular factor of variations such as human pose, occlusion, and background clutter. Part-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> represent a person image as a combination of body parts either explicitly or implicitly. Explicit part-based methods use off-the-shelf pose estimators, and extract body parts (e.g., head, torso, legs) with corresponding features <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46]</ref>. This makes it possible to obtain pose-invariant representations, but off-the-shelf pose estimators often give incorrect pose maps, especially for occluded parts. Instead of using human pose explicitly, a person image is sliced into different horizontal parts of multiple scales in implicit part-based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>. They can exploit various partial information of the image, and provide a feature representation robust to occlusion. Hard <ref type="bibr" target="#b46">[47]</ref> or soft <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref> attention techniques are also widely exploited in person reID to focus more on discriminative parts while discarding background clutter. GAN for person reID. Recent reID methods leverage GANs to fill the domain gap between source and target datasets <ref type="bibr">[44? ]</ref> or to obtain pose-invariant features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>. In <ref type="bibr" target="#b43">[44]</ref>, CycleGAN <ref type="bibr" target="#b53">[54]</ref> is used to transform pedestrian images from a source domain to a target one. Similarly, Liu et al.</p><p>[? ] use StarGAN <ref type="bibr" target="#b3">[4]</ref> to match the camera style of images between source and target domains. Two typical ways of obtaining person representations robust to human pose are to fuse all features extracted from the person images of different poses and to distill pose-relevant information from the images. In <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, new images are generated using GANs conditioned on target pose maps and input person images. Person representations for the generated images are then fused. This approach gives pose-invariant features, but requires auxiliary pose information at test time. It is thus not applicable to new images without pose information. To address this problem, Ge et al. <ref type="bibr" target="#b5">[6]</ref> introduce FD-GAN that generates a new person image of the same identity as the input with the target pose. Different from the works of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, it distills identity-related and pose-unrelated features from the input image, getting rid of pose-related information disturbing the reID task. It also does not require additional human pose information during inference. Disentangled representations. Disentangling the factor of variations in CNN features has been widely used to learn the style of a specified factor in order to synthesize new images or extract discriminative features. Mathieu et al. <ref type="bibr" target="#b28">[29]</ref> introduce a conditional generative model that extracts class-related and -independent features for image retrieval. Liu et al. <ref type="bibr" target="#b25">[26]</ref> and Bao et al.  To regularize the disentanglement process, it learns to generate the same images as the inputs while preserving the identities, using (b) disentangled features and (c) disentangled and identity shuffled ones. We train the encoders, E R and E U , the generator G, the discriminators, D D and D C , end-to-end. We denote by ⊕ a concatenation of features. See text for details. the identity and attributes of a face to generate new face images. Denton et al. <ref type="bibr" target="#b4">[5]</ref> represent videos as stationary and temporally varying components for the prediction of future frames. Unlike these methods, DR-GAN <ref type="bibr" target="#b39">[40]</ref> and FD-GAN <ref type="bibr" target="#b5">[6]</ref> use a side information (i.e., pose labels) to learn identityrelated and pose-unrelated features explicitly for face recognition and person reID, respectively. Other applications of disentangled features include image-to-image translation for producing diverse outputs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> and domain-specific image deblurring for text restoration <ref type="bibr" target="#b26">[27]</ref>.</p><p>Most similar to ours is FD-GAN <ref type="bibr" target="#b5">[6]</ref> that extracts pose-invariant features for person reID. It, however, offers limited feature representations, in that they are not robust to other factors of variations such as scale changes, background clutter and occlusion. Disentangling features with respect to these factors is not feasible within the FD-GAN framework, as this requires corresponding supervisory signals describing the factors (e.g., foreground masks for background clutter). In contrast, IS-GAN factorizes identity-related and -unrelated features without any auxiliary supervisory signals. We also propose to shuffle identity-related features in both image-and part-levels. We empirically find that this is helpful for robust person representations, especially in the case of occlusion and large pose variations that can be seen frequently in person images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We denote by I and y ∈ {1, 2, ..., C} a person image and an identification label, respectively. C is the number of identities in a dataset. We denote by I a and I p anchor and positive images, respectively, that share the same identification label. At training time, we input pairs of I a and I p with the corresponding labels, and train our model to learn identity-related/-unrelated features, φ R (I) and φ U (I), respectively. At testing time, we compute the Euclidean distance between identity-related features of person images to distinguish whether the identities of them are the same or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>IS-GAN mainly consists of five components ( <ref type="figure" target="#fig_1">Fig. 2)</ref>: An identity-related encoder E R , an identityunrelated encoder E U , a generator G, a domain discriminator D D , and a class discriminator D C . Given pairs of I a and I p , the encoders, E R and E U , learn identity-related features, φ R (I a ) and φ R (I p ), and identity-unrelated ones, φ U (I a ) and φ U (I p ), respectively ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>). To encourage identity-related and -unrelated encoders to disentangle these features from the input images, we train the generator G, such that it synthesizes the same images as</p><formula xml:id="formula_0">I a from φ R (I a ) ⊕ φ U (I a ) and φ R (I p ) ⊕ φ U (I a )</formula><p>, where we denote by ⊕ a concatenation of features ( <ref type="figure" target="#fig_1">Fig. 2(b-c)</ref>). Similarly, it generates the same images as</p><formula xml:id="formula_1">I p from φ R (I p ) ⊕ φ U (I p ) and φ R (I a ) ⊕ φ U (I p ).</formula><p>Since I a and I p have the same identity but with e.g. different poses, scales, and illumination, this identity shuffling encourages the identity-related encoder E R to extract features robust to such variations, focusing on the shared information between I a and I p , while enforcing the identity-unrelated encoder E U to capture other factors. We also perform the feature disentanglement and identity shuffling in a part-level by dividing the input images into multiple horizontal regions ( <ref type="figure" target="#fig_2">Fig. 3)</ref>. Given the generated images, the class discriminator D C determines their identification labels as either that of I a or I p , and the domain discriminator D D tries to distinguish real and fake images. IS-GAN is trained end-to-end using identification labels without any auxiliary supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline model</head><p>We exploit a network architecture similar to <ref type="bibr" target="#b41">[42]</ref> for the encoder E R . It has three branches on top of a backbone network, where each branch has the same structure but different parameters. We call them as part-1, part-2, and part-3 branches, that slice a feature map from the network equally into one, two, and three horizontal regions, respectively. The part-1 branch provides a global feature of the entire person image. Other branches give both global and local features describing body parts, where the local features are extracted from corresponding horizontal regions. For example, the part-3 branch outputs three local features and a single global one. Accordingly, we extract K features from the encoder E R in total, where K = 8 in our case. Without loss of generality, we can use additional branches to consider different horizontal regions of multiple scales.</p><p>ID loss. We denote by I k and φ k R (k = 1 . . . K) horizontal regions of multiple scales and corresponding embedding functions that extract identity-related features, respectively. Following other reID methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>, we formulate the reID problem as a multi-class classification task, and train the encoder E R with a cross-entropy loss. Concretely, a loss function L R to learn the embedding function φ k R is defined as follows:</p><formula xml:id="formula_2">L R = − C c=1 K k=1 q k c log p(c|w k c φ k R (I k )),<label>(1)</label></formula><p>where w k c is the classifier parameters associated with the identification label c and the region I k . q k c is the index label with q k c = 1 if the label c corresponds to the identity of the image I k (i.e., c = y) and q k c = 0 otherwise. The probability of I k with the label c is defined using a softmax function as</p><formula xml:id="formula_3">p(c|w k c φ k R (I k )) = exp(w k c φ k R (I k )) C i=1 exp(w k c φ k R (I k ))</formula><p>.</p><p>(</p><p>We concatenate all features from three branches, and use it as an identity-related feature φ R (I) for</p><formula xml:id="formula_5">the image I, that is, φ R (I) = φ 1 R (I 1 ) ⊕ ... ⊕ φ K R (I K ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IS-GAN</head><p>The identity-related feature φ R (I) from the encoder E R contains information useful for person reID, such as clothing, texture, and gender. However, the feature φ R (I) learned using the classification loss in (1) only may have other information that is not related to or even distracts specifying a person (e.g., human pose, background clutter, scale), and thus it is not enough to handle these factors of variations. To address this problem, we use an additional encoder E U to extract the identity-unrelated feature φ U (I), and train the encoders such that they give disentangled feature representations for identifying a person. The key idea behind the feature disentanglement is to distill identity-unrelated information from the identity-related feature, and vice versa. To this end, we propose to leverage image synthesis using an identity shuffling technique. Applying this to the whole body and its parts regularizes the disentangled features. Two discriminators allow to generate realistic person images of particular identities, further regularizing the disentanglement process.</p><p>Identity-shuffling loss. We assume that the disentangled person representation satisfies the following conditions: 1) An original image should be reconstructed from its identity-related and -unrelated features; 2) The shared information between different images of the same identity corresponds to the identity-related feature. To implement this, the generator G is required to reconstruct an anchor image I a from φ R (I a ) ⊕ φ U (I a ) and φ R (I p ) ⊕ φ U (I a ) while synthesizing a positive image I p from  <ref type="figure" target="#fig_1">Fig. 2(b-c)</ref>). We define an identity-shuffling loss as follows:</p><formula xml:id="formula_6">φ R (I p ) ⊕ φ U (I p ) and φ R (I a ) ⊕ φ U (I p ) (</formula><formula xml:id="formula_7">L S = i,j∈{a,p} I i − G(φ R (I j ) ⊕ φ U (I i )) 1 .<label>(3)</label></formula><p>The generator acts as an auto-encoder when i = j, enforcing the combination of identity-related and -unrelated features from the same image to contain all information in order to reconstruct the original image. When i = j, it encourages the encoder E R to extract the same identity-related features, φ R (I a ) and φ R (I p ) from a pair of I a and I p , focusing on the consistent information between them. Other factors, not shared by I a and I p , are encoded into the identity-unrelated features, φ U (I a ) and φ U (I p ).</p><p>Part-level shuffling loss. We also apply the identity shuffling technique to part-level features ( <ref type="figure" target="#fig_2">Fig. 3</ref>). We randomly choose local features from φ R (I a ), and swap them with corresponding ones from φ R (I p ) at the same locations, and vice versa ( <ref type="figure" target="#fig_2">Fig. 3(a)</ref>). This assumes that horizontal regions in a person image contain discriminative body parts sufficient for distinguishing its identity. Similar to (3), we compute the discrepancy between the original image and its reconstruction from the identity-related features shuffled in a part-level and the identity-unrelated ones ( <ref type="figure" target="#fig_2">Fig. 3(b)</ref>), and define a part-level shuffling loss as</p><formula xml:id="formula_8">L PS = i,j∈{a,p} i =j I i − G(S(φ R (I i ), φ R (I j )) ⊕ φ U (I i )) 1 ,<label>(4)</label></formula><p>where we denote by S a region-wise shuffling operator. The part-level identity shuffling has the following advantages: (1) It enables our model to see various combinations of identity-related features for individual body parts, regularizing a feature disentanglement process; (2) It imposes feature consistency between corresponding parts of the images.</p><p>KL divergence loss. We disentangle the identity-related and -unrelated features using identification labels only. Although we train the encoders separately to extract these features, where they share a backbone network with different heads, the generator G may largely rely on the identity-unrelated features to synthesize new person images in <ref type="formula" target="#formula_7">(3)</ref> and <ref type="formula" target="#formula_8">(4)</ref>, while ignoring the identity-related ones, which distracts the feature disentanglement process. To circumvent this issue, we encourage the identity-unrelated features to have the normal distribution N (0, 1) with zero mean and unit variance, and formulate this using a KL divergence loss as follows:</p><formula xml:id="formula_9">L U = K k=1 D KL φ k U (I k )||N (0, 1)<label>(5)</label></formula><p>where D KL (p||q) = − p(z)log p(z) q(z) . The KL divergence loss regularizes the identity-unrelated features by limiting the distribution range, such that they do not contain much identity-related information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>. This enforces the generator G to use the identity-related features when synthesizing new person images, facilitating the disentanglement process.</p><p>Domain and class losses. To train the generator G in <ref type="formula" target="#formula_7">(3)</ref> and <ref type="formula" target="#formula_8">(4)</ref>, we use two discriminators D D and D C . The domain discriminator D D <ref type="bibr" target="#b6">[7]</ref> helps the generator G to synthesize more realistic person images, and the class discriminator D C <ref type="bibr" target="#b29">[30]</ref> encourages the synthesized images to have the identification labels of anchor and positive images, further regularizing the feature learning process. Concretely, we define a domain loss L D as</p><formula xml:id="formula_10">L D = max DD i∈{a,p} log D D (I i ) + i,j∈{a,p} log(1 − D D (G(φ R (I j ) ⊕ φ U (I i )))) (6) + i,j∈{a,p} i =j log(1 − D D (G(S(φ R (I i ), φ R (I j )) ⊕ φ U (I i )))).</formula><p>The domain discriminator D D is trained, such that it distinguishes real and fake images while the generator G tries to synthesize more realistic images to fool D D . A class loss L C is defined as</p><formula xml:id="formula_11">L C = − i∈{a,p} log D C (I i ) − i,j∈{a,p} log(D C (G(φ R (I j ) ⊕ φ U (I i )))) (7) − i,j∈{a,p} i =j log(D C (G(S(φ R (I i ), φ R (I j )) ⊕ φ U (I i )))).</formula><p>The class discriminator D C classifies the identification labels of generated and input person images. When the generator G synthesizes a hard-to-classify image without sufficient identity-related information, the class discriminator D C would be confused to determine the identification label of the generated image. The generator G thus tries to synthesize a person image of the particular identity associated with the identity-related features, φ R (I j ) and S(φ R (I i ), φ R (I j )).</p><p>Training loss. The overall objective is a weighted sum of all loss functions defined as:</p><formula xml:id="formula_12">L(E R , E U , G, D D , D C ) = λ R L R + λ U L U + λ S L S + λ PS L PS + λ D L D + λ C L C ,<label>(8)</label></formula><p>where λ R , λ U , λ S , λ PS , λ D , λ C are the weighting factors for each loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Network architecture. We exploit a ResNet-50 <ref type="bibr" target="#b7">[8]</ref> trained for ImageNet classification <ref type="bibr" target="#b16">[17]</ref>. Specifically, we use the network cropped at conv4-1 as our backbone to extract CNN features. On top of that, we add two heads for the identity-related and -unrelated encoders. Each encoder has part-1, part-2, and part-3 branches that consist of two convolutional, global max pooling, and bottleneck layers but with different number of channels and network parameters. The part-1, part-2, and part-3 branches in the encoders give feature maps of size 1 × 1 × p, 1 × 1 × 3p, and 1 × 1 × 4p, respectively. See Section 3.2 for details. We set the size of p (i.e., the number of channels) to 256 and 64 for the identity-related and -unrelated encoders, respectively. We concatenate all features from three branches for each encoder, and obtain the identity-related and -unrelated features. The generator consists of a series of six transposed convolutional layers with batch normalization <ref type="bibr" target="#b11">[12]</ref>, Leaky ReLU <ref type="bibr" target="#b27">[28]</ref> and Dropout <ref type="bibr" target="#b34">[35]</ref>. It inputs identity-related and -unrelated features, a noise vector, and a one-hot vector encoding an identification label whose dimensions are 2048, 512, 128 and C, respectively. The domain and class discriminators share five blocks consisting of a convolutional layer with stride 2 with instance normalization <ref type="bibr" target="#b40">[41]</ref> and Leaky ReLU <ref type="bibr" target="#b27">[28]</ref>, but have different heads. For the domain discriminator, we add two more blocks, resulting in a features map of size 12 × 4. We then use this as an input to PatchGAN <ref type="bibr" target="#b12">[13]</ref>. For the class discriminator, we add one more block followed by a fully connected layer.</p><p>Dataset and evaluation metric. We compare our model to the state of the art on person reID with the following benchmark datasets: Market-1501 <ref type="bibr" target="#b47">[48]</ref>, CUHK03 <ref type="bibr" target="#b19">[20]</ref> and DukeMTMC-reID <ref type="bibr" target="#b49">[50]</ref>. The Market-1501 dataset <ref type="bibr" target="#b47">[48]</ref> contains 1, 501 pedestrian images captured from six viewpoints. Following the standard split <ref type="bibr" target="#b47">[48]</ref>, we use 12, 936 images of 751 identities for training and 19, 732 images of Training. To train the encoders and the generator, we use the Adam <ref type="bibr" target="#b14">[15]</ref> optimizer with β 1 = 0.9 and β 2 = 0.999. For the discriminators, we use the stochastic gradient descent with momentum of 0.9. Similar to the training scheme in <ref type="bibr" target="#b5">[6]</ref>, we train IS-GAN in three stages: In the first stage, we train the identity-related encoder E R using the loss function L R , which corresponds to the baseline model, for 300 epochs over the training data. A learning rate is set to 2e-4. In the second stage, we fix the baseline, and train the identity-unrelated encoder E U , the generator G, and the discriminators D D and D C with the corresponding losses L U , L S , L PS , L D , and L C . This process iterates for 200 epochs with the learning rate of 2e-4. Finally, we train the whole network end-to-end with the learning rate of 2e-5 for 100 epochs. Following [? ], we resize all image into 384 × 128. We augment the datasets with horizontal flipping and random erasing <ref type="bibr" target="#b51">[52]</ref>. Note that random erasing is used only in the first stage, as we empirically find that it hinders the disentanglement process. For mini-batch, we randomly select 4 different identities, and sample a set of 4 images for each identity.</p><p>Hyperparameter. We empirically find that training with a large value of λ U is unstable. We thus set λ U to 0.001 in the second stage, and increase it to 0.01 in the third stage to regularize the disentanglement. Following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>, we fix λ S and λ D to 10 and 1, respectively. To set other parameters, we randomly split IDs in the training dataset of Market-1501 <ref type="bibr" target="#b47">[48]</ref> into 651/100 and used corresponding images as training/validation sets. We use a grid search to set the parameters (λ R = 20, λ PS = 10, λ C = 2) with λ R ∈ {5, 10, 20}, λ PS ∈ {5, 10, 20}, and λ C ∈ {1, 2} on the validation split. We fix all parameters and train our models on Market-1501 <ref type="bibr" target="#b47">[48]</ref>, CUHK03 <ref type="bibr" target="#b19">[20]</ref> and DukeMTMC-reID <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Quantitative Comparison with the state of the art. We show in <ref type="table" target="#tab_0">Table 1</ref> rank-1 accuracy and mAP for Market-1501 <ref type="bibr" target="#b47">[48]</ref>, CUHK03 <ref type="bibr" target="#b19">[20]</ref> and DukeMTMC-reID <ref type="bibr" target="#b49">[50]</ref>, and compare IS-GAN with the state of the art including FD-GAN <ref type="bibr" target="#b5">[6]</ref>, PCB+RPP <ref type="bibr" target="#b38">[39]</ref>, DG-Net <ref type="bibr" target="#b50">[51]</ref>, and MGN <ref type="bibr" target="#b41">[42]</ref>. We  use a single query, and do not use any post-processing techniques (e.g., a re-ranking method <ref type="bibr" target="#b52">[53]</ref>). We achieve 95.2% rank-1 accuracy and 87.1% mAP on Market-1501 <ref type="bibr" target="#b47">[48]</ref>, 74.1%/72.3% rank-1 accuracy and 72.5%/68.8% mAP with labeled/detected images on CUHK03 <ref type="bibr" target="#b19">[20]</ref>, and 90.0% rank-1 accuracy and 79.5% mAP on DukeMTMC-reID <ref type="bibr" target="#b49">[50]</ref>, setting a new state of the art on CUHK03 and DukeMTMC-reID. Note that IS-GAN is the first model we are aware of that achieves more 90% rank-1 accuracy on DukeMTMC-reID <ref type="bibr" target="#b49">[50]</ref>. DG-Net <ref type="bibr" target="#b50">[51]</ref> also use a feature distillation technique, but appearance/structure features in DG-Net are completely different from identity-related/-unrelated ones in IS-GAN. DG-Net computes the features by AdaIN <ref type="bibr" target="#b9">[10]</ref>, widely used in image stylization, and thus they contain style/content information, rather than identity-related/-unrelated one. <ref type="figure">Figure 9</ref> in Appendix of <ref type="bibr" target="#b50">[51]</ref> visualizes generated person images when structure features (analogous to identity-unrelated features of IS-GAN) are changed only. We can see that DG-Net even changes the entire attributes (e.g., gender) except the color information, suggesting that the structure features also contain identity-related cues. As a result, IS-GAN outperforms DG-Net for all benchmarks by a large margin.</p><p>MGN <ref type="bibr" target="#b41">[42]</ref> uses the same backbone network as IS-GAN to extract initial part-level features. As it is trained with a hard-triplet loss, the part-level features of MGN capture discriminative attributes of person images well. For Market-1501, MGN shows the reID performance comparable with IS-GAN, and performs slightly better in terms of rank-1 accuracy. Note that, compared to other datasets, it contains person images of less pose and attribute variations. The reID performance of MGN, however, drops significantly on other datasets, especially for CUHK03, where the same person is captured with different poses, viewpoints, background, and occlusion, demonstrating that the person representations for MGN are not robust to such factors of variations.</p><p>Qualitative Comparison with the state of the art. <ref type="figure" target="#fig_3">Figure 4</ref> shows person retrieval results of PCB <ref type="bibr" target="#b38">[39]</ref>, FD-GAN <ref type="bibr" target="#b5">[6]</ref>, and ours on Market-1501 <ref type="bibr" target="#b47">[48]</ref>. We can see that PCB mainly focuses on clothing color, retrieving many person images of different identities from the query. FD-GAN using the identity-related and pose-unrelated features shows the robustness to pose variations. It, however, largely relies on color information. For example, FD-GAN even retrieves person images of different genders, just because the persons carry a red bag and put on a white top. In contrast, IS-GAN retrieves person images of the same identity as the query correctly. We can see that identity-related features in IS-GAN are robust to large pose variations, occlusion, background clutter, and scale changes. <ref type="table">Table 2</ref>: Ablation studies of IS-GAN on Market-1501 <ref type="bibr" target="#b47">[48]</ref>, CUHK03 <ref type="bibr" target="#b19">[20]</ref> and DukeMTMC-reID <ref type="bibr" target="#b49">[50]</ref> in terms of rank-1 accuracy(%) and mAP(%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Losses</head><p>Market-1501 CUHK03-labeled DukeMTMC-reID Ablation study. We show an ablation analysis on different losses in IS-GAN. We measure rank-1 accuracy and mAP, and report results on Market-1501 <ref type="bibr" target="#b47">[48]</ref>, CUHK03 <ref type="bibr" target="#b19">[20]</ref> and DukeMTMC-reID <ref type="bibr" target="#b49">[50]</ref> in <ref type="table">Table 2</ref>. From the first and second rows, we can clearly see that disentangling identity-related and -unrelated features using an identity shuffling technique gives better results on all datasets, but the performance gain for the CUHK03 <ref type="bibr" target="#b19">[20]</ref>, which typically contains person images of large pose variations and similar attributes, is more significant. The third row shows that applying the identity shuffling technique in a part-level further boosts the reID performance. The last three rows demonstrate that domain and class discriminators are complementary, and combining all losses gives the best results. Part-level shuffling loss. We show in <ref type="table" target="#tab_2">Table 3</ref> the effect of the partlevel shuffling loss for different numbers of body parts. We can see that 1) the part-level shuffling loss generalizes well across different numbers of body parts, and 2) IS-GAN shows better performance as more body parts are used. To further evaluate the generalization ability of our model, we use PCB <ref type="bibr" target="#b38">[39]</ref> as our baseline and add IS-GAN on top of that. We modify the network architecture such that each part-level feature has the size of 1 × 1 × 256 for an efficient computation. Note that the original PCB also gives six part-level features, but with the size of 1 × 1 × 2, 048.  <ref type="figure" target="#fig_4">Figure 5</ref> visualizes the ability of IS-GAN to disentangle identity-related and -unrelated features in a part-level. We show an example of generated images using a part-level identity shuffling technique. Specifically, we shuffle the identity-related/-unrelated features for upper/lower parts between person images of different identities. When identity-related features are shuffled e.g., in the upper left picture, we can see that IS-GAN changes colors of T-shirts between persons but with the same pose and background. This suggests that the identity-related features do not contain pose and background information. Interestingly, when identity-unrelated features are shuffled, IS-GAN generates new images where background and pose information for the corresponding parts are changed. For example in the upper right picture, the person looking at the front side now sees the left side and vice versa when shuffling the features between upper parts, while preserving the shapes of the legs in the lower parts.</p><formula xml:id="formula_13">L R L U L S L PS L D L C R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel framework, IS-GAN, to learn disentangled representations for robust person reID. In particular, we have proposed a feature disentanglement method using an identity shuffling technique, which regularizes identity-related and -unrelated features and allows to factorize them without any auxiliary supervisory signals. We achieve a new state of the art on standard reID benchmarks in terms of rank-1 accuracy and mAP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Image synthesis using identity shuffled features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of IS-GAN. (a) IS-GAN disentangles identity-related and -unrelated features from person images. (b-c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>, 2 ( * )) ⨁ % ( * )) (b) Image synthesis using identity shuffled features in a part-level (a) We randomly swap local features between anchor and positive images. (b) Similar toFig. 2(c), we generate person images with identity-related features but shuffled in a part-level and identity-unrelated ones. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparison of retrieval results on Market-1501 [48]. Results with green boxes have the same identity as the query, while those with red boxes do not. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>An example of generated images using a part-level identity shuffling technique. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FD-GAN [ 6 ]</head><label>6</label><figDesc>is similar to IS-GAN in that both use a GAN-based distillation technique for person reID. It extracts identity-related and pose-unrelated features using extra pose labels. Distilling other factors except for human pose is not feasible. IS-GAN on the other hand disentangles identity-related and -unrelated features through identity shuffling, factorizing other factors irrelevant to person reID, such as pose, scale, background clutter, without supervisory signals for them. Accordingly, the identity-related feature of IS-GAN is much more robust to such factors of variations than the identityrelated and pose-unrelated one of FD-GAN, showing the better performance on Market-1501 and DukeMTMC-reID. Note that the results of FD-GAN on CUHK03 are excluded, as it uses a different training/test split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The rank-1/mAP results of PCB, PCB+IS-GAN (w/o L PS ), and PCB+IS-GAN are 91.0/74.2, 92.1/78.3, and 92.6/78.5, respectively, showing that our model improves the performance of PCB consistently. Visual analysis for disentangled features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison with the state of the art on Market-1501<ref type="bibr" target="#b47">[48]</ref>, CUHK03<ref type="bibr" target="#b19">[20]</ref> and DukeMTMC-reID<ref type="bibr" target="#b49">[50]</ref> in terms of rank-1 accuracy(%) and mAP(%). Numbers in bold indicate the best performance and underscored ones are the second best. †: ReID methods trained using both classification and (hard) triplet losses; * : Our implementation.750 identities for testing. The CUHK03 dataset<ref type="bibr" target="#b19">[20]</ref> contains 14, 096 images of 1, 467 identities captured by two cameras. For the training/testing split, we follow the experimental protocol in<ref type="bibr" target="#b52">[53]</ref>. The DukeMTMC-reID dataset<ref type="bibr" target="#b49">[50]</ref>, a subset of the DukeMTMC<ref type="bibr" target="#b31">[32]</ref>, provides 36, 411 images of 1, 812 identities captured by eight cameras, including 408 identities (distractor IDs) that appear in only one camera. We use the training/test split provided by<ref type="bibr" target="#b49">[50]</ref> corresponding 16, 522 images of 702 identities for training and 2, 228 query and 17, 661 gallery images of 702 identities for testing.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Market-1501</cell><cell></cell><cell cols="3">CUHK03 labeled detected</cell><cell cols="2">DukeMTMC-reID</cell></row><row><cell></cell><cell>f-dim</cell><cell cols="6">R-1 mAP R-1 mAP R-1 mAP R-1</cell><cell>mAP</cell></row><row><cell>IDE [49]</cell><cell cols="6">2,048 73.9 47.8 22.2 21.0 21.3 19.7</cell><cell>-</cell><cell>-</cell></row><row><cell>SVDNet [38]</cell><cell cols="7">2,048 82.3 62.1 40.9 37.8 41.5 37.3 76.7</cell><cell>56.8</cell></row><row><cell>DaRe  † [43]</cell><cell cols="7">128 86.4 69.3 58.1 53.7 55.1 51.3 75.2</cell><cell>57.4</cell></row><row><cell>PN-GAN [31]</cell><cell cols="2">1,024 89.4 72.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.6</cell><cell>53.2</cell></row><row><cell>MLFN [2]</cell><cell cols="7">1,024 90.0 74.3 54.7 49.2 52.8 47.8 81.0</cell><cell>62.8</cell></row><row><cell>FD-GAN [6]</cell><cell cols="2">2,048 90.5 77.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.0</cell><cell>64.5</cell></row><row><cell>HA-CNN [21]</cell><cell cols="7">1,024 91.2 75.7 44.4 41.0 41.7 38.6 80.5</cell><cell>63.8</cell></row><row><cell>Part-Aligned  † [37]</cell><cell cols="2">512 91.7 79.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.4</cell><cell>69.3</cell></row><row><cell>PCB [39]</cell><cell cols="2">12,288 92.3 77.4</cell><cell>-</cell><cell>-</cell><cell cols="3">59.7 53.2 81.7</cell><cell>66.1</cell></row><row><cell>PCB+RPP [39]</cell><cell cols="2">12,288 93.8 81.6</cell><cell>-</cell><cell>-</cell><cell cols="3">62.8 56.7 83.3</cell><cell>69.2</cell></row><row><cell>HPM [? ]</cell><cell cols="2">3,840 94.2 82.7</cell><cell>-</cell><cell>-</cell><cell cols="3">63.9 57.5 86.6</cell><cell>74.3</cell></row><row><cell>DG-Net [51]</cell><cell cols="2">1,024 94.8 86.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.6</cell><cell>74.8</cell></row><row><cell>MGN  † [42]</cell><cell cols="7">2,048 95.7 86.9 68.0 67.4 66.8 66.0 88.7</cell><cell>78.4</cell></row><row><cell>MGN  †, *  [42]</cell><cell cols="7">2,048 94.5 84.8 69.2 67.6 65.7 62.1 88.2</cell><cell>76.7</cell></row><row><cell>IS-GAN</cell><cell cols="7">2,048 95.2 87.1 74.1 72.5 72.3 68.8 90.0</cell><cell>79.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>We</cell></row><row><cell cols="9">measure mean average precision (mAP) and cumulative matching characteristics (CMC) at rank-1</cell></row><row><cell>for evaluation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of different numbers of body parts on Market-1501<ref type="bibr" target="#b47">[48]</ref>.</figDesc><table><row><cell></cell><cell>L PS R-1 mAP</cell></row><row><cell>part-2</cell><cell>X 93.6 82.6 93.9 82.9</cell></row><row><cell>part-3</cell><cell>X 94.1 82.9 94.4 83.0</cell></row><row><cell>part-1,2</cell><cell>X 94.4 84.4 94.7 84.5</cell></row><row><cell>part-1,3</cell><cell>X 94.5 84.9 94.7 85.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by R&amp;D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) funded by Ministry of Science and ICT (NRF-2018M3E3A1057289).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards open-set identity preserving face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Houqiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gang</surname></persName>
		</author>
		<idno>6713-6722 of: CVPR</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno>2109-2118 of: CVPR</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaotang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno>403-412 of: CVPR</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">-</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sunghun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<idno>8789-8797 of: CVPR</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<idno>4414-4423 of: NIPS</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">FD-GAN: Pose-guided feature distilling GAN for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhuowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guojun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaogang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pages 1222-1233 of: NIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>2672-2680 of: NIPS</idno>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>770-778 of: CVPR</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>1501-1510 of: ICCV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multimodal unsupervised image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
		<idno>172-189 of: ECCV</idno>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jun-Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tinghui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>1125-1134 of: CVPR</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muhittin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno>1062-1071 of: CVPR</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<idno>2288-2295 of: CVPR</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pages 1097-1105 of: NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">-</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ming-Hsuan</surname></persName>
		</author>
		<idno>35-51 of: ECCV</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaotang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno>384-393 of: CVPR</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno>152-159 of: CVPR</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaogang</surname></persName>
		</author>
		<idno>2285-2294 of: CVPR</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A Unified Generative Adversarial Framework for Image Generation and Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tianzhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lingyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<idno>163-172 of: ACM MM</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<idno>3685-3693 of: ICCV</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shengcai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<idno>2197-2206 of: CVPR</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno>350-359 of: ICCV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring disentangled feature representation beyond face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fangyin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pages 2080-2089 of: CVPR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised domain-specific deblurring via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jun-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rama</surname></persName>
		</author>
		<idno>10225-10234 of: CVPR</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<title level="m">Rectifier nonlinearities improve neural network acoustic models. Page 3 of: ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Pages 5040-5048 of: NIPS</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>2642-2651 of: ICML</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pose-normalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yanwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wenxuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu-Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pages 650-667 of: ECCV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francesco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<idno>17-35 of: ECCV</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">End-to-end deep kronecker-product matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hongsheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pages 6886-6895 of: CVPR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hongsheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dapeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pages 486-504 of: ECCV</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jianing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Pages 3960-3969 of: ICCV</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jingdong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pages 402-419 of: ECCV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weijian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shengjin</surname></persName>
		</author>
		<idno>3800-3808 of: ICCV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shengjin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pages 480-496 of: ECCV</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<idno>1415-1424 of: CVPR</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yufeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<idno>274-282 of: ACM MM</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Resource aware person re-identification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lequn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yurong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<idno>8042-8051 of: CVPR</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno>79-88 of: CVPR</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weilai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yixiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaogang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaoou</surname></persName>
		</author>
		<idno>1077-1085 of: CVPR</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno>3219-3228 of: ICCV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liyue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shengjin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
		<idno>1116-1124 of: ICCV</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>3754-3762 of: ICCV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhiding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Kautz</surname></persName>
		</author>
		<idno>2138-2147 of: CVPR</idno>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guoliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<title level="m">Random erasing data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaozi</surname></persName>
		</author>
		<idno>1318-1327 of: CVPR</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taesung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>2223-2232 of: ICCV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
