<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DECOUPLING LOCALIZATION AND CLASSIFICATION IN SINGLE SHOT TEMPORAL ACTION DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lu</surname></persName>
							<email>yutong.lu@nscc-gz.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DECOUPLING LOCALIZATION AND CLASSIFICATION IN SINGLE SHOT TEMPORAL ACTION DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Analysis</term>
					<term>Temporal Action Detec- tion</term>
					<term>Action Proposal</term>
					<term>Action Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video temporal action detection aims to temporally localize and recognize the action in untrimmed videos. Existing onestage approaches mostly focus on unifying two subtasks, i.e., localization of action proposals and classification of each proposal through a fully shared backbone. However, such design of encapsulating all components of two subtasks in one single network might restrict the training by ignoring the specialized characteristic of each subtask. In this paper, we propose a novel Decoupled Single Shot temporal Action Detection (Decouple-SSAD) method to mitigate such problem by decoupling the localization and classification in a one-stage scheme. Particularly, two separate branches are designed in parallel to enable each component to own representations privately for accurate localization or classification. Each branch produces a set of action anchor layers by applying deconvolution to the feature maps of the main stream. High-level semantic information from deeper layers is thus incorporated to enhance the feature representations. We conduct extensive experiments on THUMOS14 dataset and demonstrate superior performance over state-of-the-art methods. Our code is available online 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the tremendous increase in online and personal media archives, people are generating, storing, and consuming a large collection of videos. This trend encourages the development of effective and efficient algorithms to intelligently parse video data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> and discover semantic information <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. One fundamental challenge underlying the success of these advances is action detection from videos in both temporal <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and spatio-temporal aspects <ref type="bibr" target="#b10">[11]</ref>. In this study, we focus on the temporal action detection task, which aims to find the exact time stamps of an action's start and end time, and recognize the category of the action.</p><p>Recent works have been inspired by object detection, which can be categorized as either two-stage approach <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> or one-stage approach <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. The two-stage approaches follow the framework of first generating action proposals and then classifying them. The main drawback of these two-stage approaches is the indirect optimization strategy, which may result in a sub-optimal solution. Contrarily, the one-stage approaches directly produce and classify the action proposals in only one step, which is an end-toend framework. However, such design of encapsulating all components of two subtasks in one single network might restrict the training by ignoring the specialized characteristic of each subtask. For example, given two different proposals near the same ground truth action instance, the classification optimization aims to pull their feature representations together, while the localization optimization would separate them since they have different offset values. Both categories of methods would produce unsatisfactory performance.</p><p>To solve the above problems, we propose a novel Decoupled Single Shot temporal Action Detection (Decouple-SSAD) method to inherit the advantages from both directions. Decouple-SSAD improves the one-stage action detection methods by additionally involving two branches, i.e., the refined classification branch and the refined proposal branch. The two branches manually separate the proposal generation and classification process, preserving the learning of specialized features useful for optimizing the performance of each component. Specially, each branch produces a set of action anchor layers by applying deconvolution to the feature maps of the main stream. High-level semantic information from deeper layers is thus incorporated to enhance the features. The whole framework is trained in an end-to-end manner. With such a one-stage framework, Decouple-SSAD could not only optimize the proposal and recognition parts jointly but also provide additional refinement for each part.</p><p>The main contribution of this paper is the proposed Decouple-SSAD for addressing the issue of temporal action detection. The method provides an elegant solution by incor-arXiv:1904.07442v1 [cs.CV] 16 Apr 2019 porating the separate optimization for proposal and classification into a one-stage action detection framework. Decouple-SSAD achieves state-of-the-art performance on the standard benchmark THUMOS14, demonstrating its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>We summarize recent works related to our approach into two categories: object detection and temporal action detection.</p><p>Object Detection. Recent works on temporal action detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref> get inspiration from the works on object detection. These works can be divided into two categories: twostage approaches (e.g., RCNN <ref type="bibr" target="#b18">[19]</ref> and Faster RCNN <ref type="bibr" target="#b19">[20]</ref>) and one-stage approaches (e.g., YOLO <ref type="bibr" target="#b20">[21]</ref> and SSD <ref type="bibr" target="#b21">[22]</ref>). Recently, RefineDet <ref type="bibr" target="#b22">[23]</ref> proposes to refine the object localization in the one-stage framework by transferring information from deep anchor layers to shallow refined layers to predict accurate locations and classes, which is similar to our Decouple-SSAD. However, Decouple-SSAD is more robust as it learns specialized features separately for localization and classification, whereas RefineDet refines the detection with coarse results in each anchor layer.</p><p>Temporal Action Detection. As mentioned previously, existing temporal action detection methods can be divided into two categories: two-stage approaches and one-stage approaches. The two-stage approaches first generate action instances and then classify them. Some works focus on proposal generation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> while others concentrate on classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. The one-stage approaches integrate proposal and classification into a single step. SSAD <ref type="bibr" target="#b15">[16]</ref> utilizes 1D convolution to generate multiple temporal anchor action instances for action classification and boundary box regression. The Single Stream Temporal Action Detection (SS-TAD) <ref type="bibr" target="#b17">[18]</ref> utilizes the RNN-based architecture to learn action proposal and classification jointly. Yeung et al. <ref type="bibr" target="#b16">[17]</ref> explored RNN to learn a glimpse policy for predicting the starting and ending points of actions in an endto-end manner. Nevertheless, both two-stage and one-stage approaches have drawbacks that the former adopts the indirect optimization and the later usually generates inaccurate proposals. In this research, we leverage the advantages of both directions and overcome their weaknesses by involving two separate branches into the one-stage framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPROACH</head><p>In this section, we present the proposed Decoupled Single Shot temporal Action Detection (Decouple-SSAD) in detail. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of our architecture, which consists of three main components: the base feature network, the anchor network, and the classification and regression module. The base feature network extracts representations of each frame or clip for later action detection. Then a multibranch anchor network is utilized to localize the proposals and classify them accurately. Specifically, we separate proposal generation and category recognition process by additionally adding two branches to perform proposal and classification refinement respectively. Finally, we exploit the action classification and temporal boundary box regression module for accurate detection. Different types of optimization are employed for each branch. The whole framework can be trained in an end-to-end manner. The above design not only optimizes the proposal and recognition parts jointly but also provides additional refinement for each part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Base Feature Network</head><p>Our goal is to detect action segments in 1d temporal space. To achieve this, given the video frame sequence, we first extract a 1d feature map for each frame/clip, typically via a 2d/3d ConvNet. We then pass the sequence of feature maps to the 1d ConvNet, converting it into a detection problem in 1d domain. In particular, a sequence of frame/clip level CNN fea-</p><formula xml:id="formula_0">tures {f i } T −1</formula><p>i=0 are extracted, where T is temporal length. We further feed the features into two 1d convolutional layers plus one max-pooling layer to increase its receptive field for better detection. We use the output of the base feature network for proposal generation and feature pooling in the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>Given the 1d sequence of feature maps, the common onestage action detection solution stacks 1d anchor convolutional layers of different scales to generate anchor proposals for classification and boundary regression <ref type="bibr" target="#b15">[16]</ref>. However, such one-step scheme encapsulates all components of two subtasks (classification and localization) in a single network, restricting the training by ignoring the specialized characteristic of each subtask. To this end, we propose to additionally involve refined proposal and classification branches, which focus on learning specialized features useful for optimizing the performance of each subtask. Our anchor network thus consists of three parts: the main stream, the classification branch, and the proposal branch.</p><p>Main Stream. The main stream predicts the locations and categories of action instances simultaneously. Based on the feature representations from the base network, it generates a set of anchor layers with different scales by temporal pooling. Particularly, N l convolutional layers are cascaded, each with one anchor layer to produce action proposals. Denote the feature map of j-th convolutional layer in main stream as f j m , 1 ≤ j ≤ N l . In each anchor layer, we produce a set of action proposals with various aspect ratios at each temporal location. With the feature representation of each anchor action proposal (a c , a w ) where a c and a w are the default center position and width, we utilize 1d convolutional layers to output three predictions for action recognition and localization: i) action classification scores p = [p 0 , p 1 , ..., p C ] indicating the probabilities belong to C action categories plus one "background"  ConvNet. A multi-branch anchor network is designed to produce multiple sets of feature maps. Particularly, it consists of a main stream and two branches derived from it, i.e., the refined classification branch and proposal branch. The Main Stream performs classification and regression simultaneously. The Classification and Proposal Branches focus on learning specialized features useful for themselves, which incorporate the semantic information of deep layers from main stream through deconvolution. The action classification loss and location regression loss are utilized to optimize different branches. The whole network is trained in an end-to-end manner. class, ii) the overlap value p ov representing the IoU between the default proposal (a c , a w ) and its closest ground truth proposal, and iii) proposal regression parameters (∆c, ∆w) denoting the center and width offsets for (a c , a w ). Finally, we obtain the predicted proposal (ϕ c , ϕ w ) by ϕc = ac + α1aw∆c and ϕw = aw exp (α2∆w) ,</p><p>where ϕ c , ϕ w are the center position and width, and α 1 , α 2 are the parameters for adjusting the effect of ∆c, ∆w. The detection results of the above main stream network are usually unsatisfactory. On the one hand, it ignores the specialized characteristic of each subtask. On the other hand, the lower anchor layers lack the rich semantic clues from deep layers for accurate detection. To address these issues, we introduce two additional branches, that is, the classification branch and the proposal branch.</p><p>Classification Branch. This branch focuses on the classification of proposals. To enhance its performance in lower anchor layers, we propose to incorporate the rich semantic information in deeper layers through deconvolution. Instead of the direct deconvolution on the main stream to obtain the classification branch, we combine the deconvolutional feature map with one of the shallower layers in the main stream to generate the feature map {f j c } N l j=1 in this branch. Particularly, the feature map f j c in j-th layer is given by</p><formula xml:id="formula_2">f j c = C1(f j m ) , if j = N l C2(S(C3(f j m ), D(f j+1 c ))), if 1 ≤ j &lt; N l ,<label>(2)</label></formula><p>where C 1 (·), C 2 (·), C 3 (·) are the combinations of convolution and ReLU operations for increasing the discriminative ability, D(·) indicates the deconvolution, and S(·, ·) denotes the element-wise summation, S(a, b) = ρ · a + (1 − ρ) · b, ρ is the hyper-parameter. We produce a set of action proposals at each temporal location of anchor layers, as done similarly in the main stream, and predict their action classification scores p c = [p 0 ,p 1 , ...,p C ]. In addition, we adopt an average fusion to fuse the scores of the classification branch and the main stream, obtaining new scores p c for classification branch as</p><formula xml:id="formula_3">p c = (p + pc)/2.<label>(3)</label></formula><p>p c is then used for training and testing classification branch.</p><p>With the classification branch, we take advantages of both main stream and refined classification branch to recognize actions from coarse to fine. The main stream first conducts predictions on both location and category scores while the classification branch performs a second refinement on recognition.</p><p>Proposal Branch. This branch concentrates on providing the exact start and end time stamps of action instances in each video. We propose to involve high-level semantic information into the proposal branch, as done similarly in the classification branch. In detail, the feature map f j p of j-th layer in proposal branch is formulated as follows,</p><formula xml:id="formula_4">f j p = C4(f j m ) , if j = N l C5(S(C6(f j m ), D(f j+1 p ))), if 1 ≤ j &lt; N l ,<label>(4)</label></formula><p>where C 4 (·), C 5 (·), C 6 (·) are also the combinations of convo-lution and ReLU operations. We then generate a set of anchor layers, in which action proposals are produced by outputting two predictions: the overlap value p ov_p and proposal offsets (∆c p , ∆w p ). The average fusion strategy is also adopted to fuse the two predictions of the proposal branch and the main stream. The overlap prediction p ov and the offset prediction </p><p>The average fusion strategy incorporates the complementary information from the main stream into the proposal branch and classification branch and could make an improvement of about 3% mAP@0.5 on THUMOS14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>Loss Function. The proposed network is optimized via three types of supervision: classification loss, location regression loss, and overlap loss. Particularly, the classification branch is optimized with classification loss, while the proposal branch is optimized with the other two losses. The main stream is optimized with all three losses. For classification, we utilize the standard softmax loss, which is formulated as</p><formula xml:id="formula_6">L cls = − C n=0</formula><p>In=c log(pn), <ref type="bibr" target="#b5">(6)</ref> where I n=c is an indicator function which equals to 1 if n is the ground truth class label c, otherwise 0. For location regression, we employ the Smooth L1 loss (S L1 ) to force the proposal (ϕ c , ϕ w ) to move towards its closest ground truth proposal (g c , g w ). The loss is computed by</p><formula xml:id="formula_7">Lreg = SL1(ϕc − gc) + SL1(ϕw − gw).<label>(7)</label></formula><p>The IoU overlap loss is the Mean Square Error (MSE) loss:</p><formula xml:id="formula_8">Lov = (pov − giou) 2 ,<label>(8)</label></formula><p>where g iou is the ground truth IoU value between the proposal and its closest ground truth. Finally, we accumulate the losses from three branches. The overall training objective is defined as follows</p><formula xml:id="formula_9">L = α · L cls + β · Lreg + γ · Lov,<label>(9)</label></formula><p>where L cls = ω ·L cls,m +(1−ω)·L cls,c , L reg = ω ·L reg,m + (1−ω)·L reg,p , L ov = ω·L ov,m +(1−ω)·L ov,p , and α, β, γ, ω are the parameters for balancing different tasks and branches. Inference. During testing, the classification scores from classification branch are utilized as the final prediction scores. For localization, as aforementioned, we exploit the fusion of proposal branch and main stream to obtain the final offsets (∆c , ∆w ), which are further used to compute the proposals. Finally, we apply the Non-Maximum Suppression (NMS) with Jaccard overlap of 0.2 to produce the detection results. Hard Negative Mining. Following <ref type="bibr" target="#b15">[16]</ref>, we adopt the hard negative mining strategy to handle the positive-negative class imbalance problem. Negative samples are defined as the instances whose Jaccard overlap values with their closest ground truth instances are smaller than 0.5, while positive samples are the opposite. Then the hard negative samples are instances with predicted overlaps larger than 0.5. We select all hard negative samples and let the ratio between negative and positive be 1:1.</p><p>Network Details. We utilize two-stream networks <ref type="bibr" target="#b0">[1]</ref> pre-trained on UCF101 <ref type="bibr" target="#b31">[32]</ref> to extract spatial and temporal feature representations for each clip with length 512, and feed the feature sequence into our network. We also evaluate the feature extractor pre-trained on Kinetics <ref type="bibr" target="#b32">[33]</ref> when compared with the state-of-the-art. The number of anchor layers N l is 3. The aspect ratios at each temporal location are {0.5, 0.75, 1, 1.5, 2}. We set α 1 , α 2 to 0.1. When producing anchor feature maps of the two branches, C 1 (·), C 4 (·) consist of three "Conv-ReLU" units, C 2 (·), C 5 (·) denote the "Conv-ReLU-Conv" unit, and C 3 (·), C 6 (·) are the "ReLU-Conv-ReLU" unit. We set ρ = 2/3, ω = 2/3, α = 1, and β = γ = 10 through the cross validation. In all of our experiments, our network is trained by utilizing Adam algorithm. The training process takes 30 epochs with the learning rate of 10 −4 . The batch size is 48.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We conduct an ablation study for analyzing different components of Decouple-SSAD. The baseline denotes the network with an individual main stream to perform both local-  Decouple-SSAD provides more accurate category prediction and more precise boundaries than SSAD and BSN.</p><p>ization and classification, which is identical to the SSAD <ref type="bibr" target="#b15">[16]</ref> but with different settings. We make several modifications and improve mAP@0.5 performance of SSAD from 24.6% to 31.2%. For more details, please refer to our public code 2 . Next, we evaluate the classification and proposal branches separately by incorporating each of them with the main stream. We then evaluate another new framework which refines the localization and classification in only one branch through deconvolution, denoted as refinement branch. Finally, we incorporate both branches with the main stream, which is our proposed Decouple-SSAD. <ref type="table" target="#tab_1">Table 1</ref> summarizes the mAP@0.5 performances of different schemes. As expected, both classification and proposal branches could boost the performances against the baseline, where 1% and 2.2% improvements are obtained respectively. Moreover, combining both branches could further enhance the performance significantly, providing 4.6% enhancement against the baseline. The results indicate the advantage of leveraging deeper semantic information for refining the localization and classification results. It can be observed that adding the refinement branch could also improve the performance by 2.8%. However, such improvement is much lower than the result produced by Decouple-SSAD. Though both methods involve utilization of semantics from deeper layers, they are fundamentally different in the way that the former simply mixes up the localization and classification components, while our Decouple-SSAD decouples them to learn the specialized features useful for optimizing the performance of each component. As indicated by our results, such divideand-conquer mechanism could achieve better performance, which verifies the design of Decouple-SSAD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with State-of-the-Art</head><p>We compare with several state-of-the-art techniques of action detection on THUMOS14 in <ref type="table" target="#tab_3">Table 2</ref>. The results indicate that Decouple-SSAD exhibits better performances than other one-stage methods and is comparable with the state-ofthe-art two-stage approaches. In particular, Decouple-SSAD achieves 35.8% of mAP@0.5, which outperforms the best one-stage competitor SS-TAD by 6.6% and is close to the best two-stage approach BSN (36.9%). In addition, when employing the Kinetics pre-trained feature extractor, we could achieve much better performance. The results again validate our idea of decoupling the localization and classification to learn more specialized features. <ref type="figure" target="#fig_2">Figure 2</ref> showcases the detection results of three methods on a video from THUMOS14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have presented Decoupled Single Shot temporal Action Detection (Decouple-SSAD), which explores the separable localization and classification mechanism in one-stage action detection framework. In particular, we study the problem of learning specialized features useful for optimizing each subtask through two separate branches. Each branch produces a set of action anchor layers by applying deconvolution to the feature maps of the main stream. In addition, high-level semantic information from deeper layers is incorporated to enhance the feature representations. The whole framework is trained in an end-to-end manner. More remarkably, we achieve state-of-the-art performance on the standard THU-MOS14 benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The framework overview of our Decouple-SSAD. The input frame sequence is first encoded to 1d feature sequence through 2d/3d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(∆c , ∆w ) for proposal branch is thus calculated by p ov = (pov + pov_p)/2, ∆c = (∆c + ∆cp)/2, ∆w = (∆w + ∆wp)/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization of predicted action instances by our Decouple-SSAD, SSAD, and BSN on one video from THUMOS14. Each bar represents an action instance. The number in the middle of each bar denotes the confidence score of the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This work is supported by the National Key R&amp;D Program of China under Grant NO.2018YFB0203904, the National Natural Science Foundation of China under Grant NO.U1611261 and the Program for Guangdong Introducing Innovative and Enterpreneurial Teams under Grant NO.2016ZT06D211.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Effectiveness of different components in Decouple-SSAD. Refinement branch indicates performing the localization and classification refinement in only one branch.</figDesc><table><row><cell>Component</cell><cell>Performance</cell></row><row><cell>Main Stream</cell><cell></cell></row><row><cell>Classification Branch</cell><cell></cell></row><row><cell>Proposal Branch</cell><cell></cell></row><row><cell>Refinement Branch</cell><cell></cell></row><row><cell>mAP@0.5 (%)</cell><cell>31.2 32.2 33.4 34.0 35.8</cell></row><row><cell cols="2">4. EXPERIMENTS</cell></row><row><cell cols="2">4.1. Implementation Details</cell></row><row><cell cols="2">Dataset. We conduct experiments on the THUMOS14 [31].</cell></row><row><cell cols="2">The training set known as UCF101 [32] has 13,320 trimmed</cell></row><row><cell cols="2">videos and is often used for pre-training. It has 1,010</cell></row><row><cell cols="2">untrimmed validation videos and 1,574 untrimmed test videos</cell></row><row><cell cols="2">of 20 classes. It is a standard practice for researchers to train</cell></row><row><cell cols="2">on the 200 temporal annotated validation set and evaluate on</cell></row><row><cell cols="2">the 213 public test videos. We report mean Average Precision</cell></row><row><cell cols="2">(mAP) metric with IoU thresholds varied from 0.3 to 0.7.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>mAP comparisons on THUMOS14 with various IoU threshold. The last two rows denote the results of our method pre-trained on UCF101 and Kinetics dataset respectively.</figDesc><table><row><cell>Stage</cell><cell>Method</cell><cell>0.3</cell><cell cols="2">mAP@IoU (%) 0.4 0.5 0.6</cell><cell>0.7</cell></row><row><cell></cell><cell>SCNN [9]</cell><cell cols="3">36.3 28.7 19.0 10.3</cell><cell>5.3</cell></row><row><cell></cell><cell>SST [25]</cell><cell cols="3">41.2 31.5 20.0 10.9</cell><cell>4.7</cell></row><row><cell></cell><cell>CDC [34]</cell><cell cols="3">40.1 29.4 23.3 13.1</cell><cell>7.9</cell></row><row><cell>Two-stage</cell><cell>TURN [26] TCN [29]</cell><cell cols="3">46.3 35.5 24.5 14.1 -33.3 25.6 15.9</cell><cell>6.3 9.0</cell></row><row><cell></cell><cell>R-C3D [13]</cell><cell cols="3">44.8 35.6 28.9 19.1</cell><cell>9.3</cell></row><row><cell></cell><cell>SSN [30]</cell><cell cols="4">51.9 41.0 29.8 19.6 10.7</cell></row><row><cell></cell><cell>CBR [35]</cell><cell cols="3">50.1 41.3 31.0 19.1</cell><cell>9.9</cell></row><row><cell></cell><cell>BSN [15]</cell><cell cols="4">53.5 45.0 36.9 28.4 20.0</cell></row><row><cell></cell><cell cols="3">Yeung et al. [17] 36.0 26.4 17.1</cell><cell>-</cell><cell>-</cell></row><row><cell>One-stage</cell><cell>SSAD [16] SS-TAD [18]</cell><cell cols="3">43.0 35.0 24.6 15.4 45.7 -29.2 -</cell><cell>7.7 9.6</cell></row><row><cell></cell><cell>Ours-UCF101</cell><cell cols="4">49.9 44.4 35.8 24.3 13.6</cell></row><row><cell></cell><cell>Ours-Kinetics</cell><cell cols="4">60.2 54.1 44.2 32.3 19.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/hypjudy/Decouple-SSAD</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep quantization: Encoding convolutional activations with deep generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Action recognition by learning deep multigranular spatio-temporal video representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unified spatio-temporal attention networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jointly localizing and describing events for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning deep spatio-temporal dependence for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">TMM</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">R-c3d: region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM. ACM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sap: Self-adaptive proposal model for temporal action detection based on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
