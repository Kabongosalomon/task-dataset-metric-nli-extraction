<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Fan</surname></persName>
							<email>*chenyou.fan@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
							<email>*heng.huang@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Com</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Digits</surname></persName>
						</author>
						<title level="a" type="main">Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel end-to-end trainable Video Question Answering (VideoQA) framework with three major components: 1) a new heterogeneous memory which can effectively learn global context information from appearance and motion features; 2) a redesigned question memory which helps understand the complex semantics of question and highlights queried subjects; and 3) a new multimodal fusion layer which performs multi-step reasoning by attending to relevant visual and textual hints with selfupdated attention. Our VideoQA model firstly generates the global context-aware visual and textual features respectively by interacting current inputs with memory contents. After that, it makes the attentional fusion of the multimodal visual and textual representations to infer the correct answer. Multiple cycles of reasoning can be made to iteratively refine attention weights of the multimodal data and improve the final representation of the QA pair. Experimental results demonstrate our approach achieves state-of-theart performance on four VideoQA benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video Question Answering (VideoQA) is to learn a model that can infer the correct answer for a given question in human language related to the visual content of a video clip. VideoQA is a challenging computer vision task, as it requires to understand a complex textual question first, and then to figure out the answer that can best associate the semantics to the visual contents in an image sequence.</p><p>Recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> proposed to learn models of encoder-decoder structure to tackle the VideoQA problem. A common practice is to use LSTM-based encoders to encode CNN features of video frames and embeddings of question words into encoded visual sequence and word sequence. Proper reasoning is then performed to produce the correct answer, by associating the relevant visual contents with the question. For example, learning soft weights A: Our model: woman Existing model: man ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: Who drives by a hitchhiking man who is smoking? (answer: woman)</head><p>Video Attention Question Attention <ref type="figure">Figure 1</ref>. VideoQA is a challenging task as it requires the model to associate relevant visual contents in frame sequence with the real subject queried in question sentence. For a complex question such as "Who drives by a hitchhiking man who is smoking?", the model needs to understand that the driver is the queried person and then localize the frames in which the driver is driving in the car. of frames will help attend to events that are queried by the questions, while learning weights of regions in every single frame will help detect details and localize the subjects in the query. The former one aims to find relevant frame-level details by applying temporal attention to encoded image sequence <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>. The latter one aims to find region-level details by spatial attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Jang et al. <ref type="bibr" target="#b9">[10]</ref> applied spatiotemporal attention mechanism on both spatial and temporal dimension of video features. They also proposed to use both appearance (e.g., VGG <ref type="bibr" target="#b21">[22]</ref>) and motion features (e.g., C3D <ref type="bibr" target="#b23">[24]</ref>) to better represent video frames. Their practice is to make early fusion of the two features and feed the concatenated feature to a video encoder. But such straightforward feature integration leads to suboptimal results. Gao et al. <ref type="bibr" target="#b4">[5]</ref> proposed to replace the early fusion with a more sophisticated comemory attention mechanism. They used one type of feature to attend to the other and fused the final representations of these two feature types at the final stage. However, this method doesn't synchronize the attentions detected by appearance and motion features, thus could generate incorrect attentions. Meanwhile, this method will also miss the attention which can be inferred by the combined appearance and motion features, but not individual ones. The principal reason for the existing approaches to fail to identify the correct attention is that they separate feature integration and attention learning steps. To address this challenging problem, we propose a new heterogeneous memory to integrate appearance and motion features and learn spatiotemporal attention simultaneously. In our new memory model, the heterogeneous visual features as multi-input will colearn the attention to improve the video understanding.</p><p>On the other hand, VideoQA becomes very challenging if the question has complex semantics and requires multiple steps of reasoning. Several recent work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32]</ref> tried to augment VideoQA with differently embodied memory networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Xu et al. <ref type="bibr" target="#b26">[27]</ref> proposed to refine the temporal attention over video features word by word with a conventional LSTM question encoder plus an additional LSTM based memory unit to store and update the attention. However, this model is easily trapped into irrelevant local semantics, and cannot understand the question based on the global context. Both Zeng et al. <ref type="bibr" target="#b31">[32]</ref> and Gao et al. <ref type="bibr" target="#b4">[5]</ref> used external memory (memory network <ref type="bibr" target="#b22">[23]</ref> and episodic memory <ref type="bibr" target="#b25">[26]</ref> respectively) to make multiple iterations of inference by interacting the encoded question representation with video features conditioning on current memory contents. However, similar to many other work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref>, the question representation used in these approaches is only a single feature vector encoded by an LSTM (or GRU) which lacks capability to capture complex semantics in questions such as shown in <ref type="figure">Fig. 1</ref>. Thus, it is desired to design a new powerful model for understanding the complex semantics of questions in VideoQA. To tackle this problem, we design novel network architecture to integrate both question encoder and question memory which can augment each other. The question encoder learns meaningful representation of question and the re-designed question memory understands the complex semantics and highlights queried subjects by storing and updating global contexts.</p><p>Moreover, we design a multimodal fusion layer which can attend to visual and question hints simultaneously by aligning relevant visual contents with key question words. After gradually refining the joint attention over video and question representations and fusing them with learned soft modality weights, the multi-step reasoning is achieved to infer the correct answer from the complex semantics.</p><p>Our major contributions can be summarized as follows: 1) we introduce a heterogeneous external memory module with attentional read and write operations such that the motion and appearance features are integrated to co-learn attention; 2) we utilize the interaction of visual and question features with memory contents to learn global contextaware representations; 3) we design a multimodal fusion layer which can effectively combine visual and question features with softly assigned attentional weights and also support multi-step reasoning; and 4) our proposed model outperforms the state-of-the-art methods on four VideoQA benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Question Answering (VQA) is an emerging research area <ref type="bibr">[1-3, 11, 15, 17, 29]</ref> to reason the correct answer of a given question which is related to the visual content of an image. Yang et al. <ref type="bibr" target="#b28">[29]</ref> proposed to encode question words into one feature vector which is used as query vector to attend to relevant image regions with stack attention mechanism. Their method supports multi-step reasoning by repeating the query process while refining the query vector. Anderson et al. <ref type="bibr" target="#b1">[2]</ref> proposed to align questions with relevant object proposals in images generated by Faster R-CNN <ref type="bibr" target="#b19">[20]</ref> and compute the visual feature as a weighted average over all proposals. Xiong et al. <ref type="bibr" target="#b25">[26]</ref> proposed to encode image and question features as facts and attend to relevant facts through attention mechanism to generate a contextual vector. Ma et al. <ref type="bibr" target="#b14">[15]</ref> proposed a co-attention model which can attend to not only relevant image regions but also important question words simultaneously. They also suggested to use external memory <ref type="bibr" target="#b20">[21]</ref> to memorize uncommon QA pairs.</p><p>Video Question Answering (VideoQA) extends VQA to video domain which aims to infer the correct answer given a relevant question of the visual content of a video clip. VideoQA is considered to be a challenging problem as reasoning on video clip usually requires memorizing contextual information in temporal scale. Many models have been proposed to tackle this problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Many work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref> utilized both motion (i.e. C3D <ref type="bibr" target="#b23">[24]</ref>) and appearance (i.e. VGG <ref type="bibr" target="#b21">[22]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>) features to better represent video frames. Similar to the spatial mechanism widely used in VQA methods to find relevant image regions, many VideoQA work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref> applied temporal attention mechanism to attend to most relevant frames of a video clip. Jang <ref type="bibr" target="#b9">[10]</ref> utilized both appearance and motion features as video representations and applied spatial and temporal attention mechanism to attend to both relevant regions of a frame and frames of a video. Xu et al. <ref type="bibr" target="#b26">[27]</ref> proposed to refine the temporal attention over frame features at each question encoding step word by word. Both Zeng et al. <ref type="bibr" target="#b31">[32]</ref> and Gao et al. <ref type="bibr" target="#b4">[5]</ref> proposed to use external memory (Memory Network <ref type="bibr" target="#b22">[23]</ref> and Episodic Memory <ref type="bibr" target="#b25">[26]</ref> respectively) to make multiple iterations of inference by interacting the encoded question feature with video features conditioning on current memory contents. Their memory designs maintain a single hidden state feature of current step and update it through time steps. However, this could hardly establish long-term global context as the hidden state feature is updated at every step. Neither are their models able to synchronize appearance and motion features. Our model differs from existing work such that 1) we design a heterogeneous external memory module with attentional read and write operations that can efficiently combine motion and appearance features together; 2) we allow interaction of visual and question features with memory contents to construct global context-aware features; and 3) we design a multimodal fusion layer which can effectively combine visual and question features with softly assigned attentional weights and also support multi-step reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we illustrate our network architecture for VideoQA. We first introduce the LSTM encoders for video features and question embeddings. Then we elaborate on the design of question memory and heterogeneous video memory. Finally, we demonstrate how our designed multimodal fusion layer can attend to relevant visual and textual hints and combine to form the final answer representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video and text representation</head><p>Video representation. Following previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref>, we sample a fixed number of frames (e.g., 35 for TGIF-QA) for all videos in that dataset. We then apply pretrained ResNet <ref type="bibr" target="#b7">[8]</ref> or VGG <ref type="bibr" target="#b21">[22]</ref> network on video frames to extract video appearance features, and use C3D <ref type="bibr" target="#b23">[24]</ref> network to extract motion features. We denote appearance features as f a = [f a 1 , · · · , f a Nv ], and motion features as</p><formula xml:id="formula_0">f m = [f m 1 , · · · , f m Nv ]</formula><p>, in which N v is number of frames. The dimensions of ResNet, VGG and C3D features are 2048, 4096 and 4096. We use two separate LSTM encoders to process motion and appearance features individually first, and late fuse them in the designed memory module which will be discussed in §3.2. In <ref type="figure" target="#fig_0">Fig. 2</ref>, we highlight the appearance encoder in blue and the motion encoder in orange. The inputs fed into the two encoders are raw CNN motion features f m and appearance features f a , and the outputs are encoded motion and appearance features denoted</p><formula xml:id="formula_1">as o m = [o m 1 , · · · , o m Nv ] and o a = [o a 1 , · · · , o a Nv ]</formula><p>. Question representation. Each VideoQA dataset has a pre-defined vocabulary which is composed of the top K most frequent words in the training set. The vocabulary size K of each dataset is shown in <ref type="table" target="#tab_1">Table 1</ref>. We represent each word as a fixed-length learnable word embedding and initialize with the pre-trained GloVe 300-D <ref type="bibr" target="#b18">[19]</ref> feature. We denote the question embedding as a sequence of word embeddings f q = [f q 1 , · · · , f q Nq ], in which N q is number of words in the question. We use another LSTM encoder to process question embedding f q , as highlighted in red in <ref type="figure" target="#fig_0">Fig. 2</ref>. The outputs are the encoded text features</p><formula xml:id="formula_2">o q = [o q 1 , · · · , o q Nq ] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Heterogeneous video memory</head><p>Both motion and appearance visual features are crucial for recognizing the objects and events associated with the questions. Because these two types of features are heterogeneous, the straightforward combination cannot effectively learn the video content. Thus, we propose a new heterogeneous memory to integrate motion and appearance visual features, learn the joint attention, and enhance the spatialtemporal inference. Different to the standard external memory, our new heterogeneous memory accepts multiple inputs including encoded motion features o m and appearance features o a , and uses multiple write heads to determine the content to write. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates the memory structure, which is composed of memory slots M = [m 1 , · · · , m S ] and three hidden states h m , h a and h v . We use two hidden states h m and h a to determine motion and appearance contents which will be written into memory, and use a separate global hidden state h v to store and output global context-aware feature which integrates motion and appearance information. We denote the number of memory slots as S, and sigmoid function as σ. For simplicity, we combine superscript m and a for identical operations on both motion and appearance features.</p><p>Write operation. Firstly we define the motion and appearance content c m/a t to write to memory at t-th time as non-linear mappings from input and previous hidden state </p><formula xml:id="formula_3">for i = 1 . . . S<label>(2)</label></formula><p>satisfying α m/a t sum to 1. Uniquely, we also need to integrate motion and appearance information and make a unified write operation into current memory. Thus we estimate the weights t ∈ R 3 of motion content α m t , appearance content α a t and current memory content M t-1 given by The memory M can be updated at each time step by</p><formula xml:id="formula_4">et =v e tanh(W he h v t-1 + (Wmec m t + Waec a t ) + be) t,i = exp(et,i) 3 j=1 exp(et,j) for i = 1 . . . 3<label>(3)</label></formula><formula xml:id="formula_5">Mt = t,1α m t c m t + t,2α</formula><p>a t c a t + t,3Mt-1 <ref type="bibr" target="#b3">(4)</ref> in which the write weights α m/a t for memory slots determine how much attention should different slots pay to current inputs, while the modality weights t determine which of motion or appearance feature (or none of them if noninformational) from current inputs should the memory pay more attention to. Through this designed memory-write mechanism, we are able to integrate motion and appearance features to learn joint attention, and memorize different spatio-temporal patterns of this video in a synchronized and global context.</p><p>Read operation. The next step is to perform an attentional read operation from the memory M to update memory hidden states. We define the weights of reading from memory slots as β t ={βt,1, . . . , βt,S} given by</p><formula xml:id="formula_6">bt =v b tanh(W hb h v t-1 + (W mb c m t + W ab c a t ) + b b ) βt,i = exp(bt,i) S j=1 exp(bt,j) for i = 1 . . . S<label>(5)</label></formula><p>The content r t read from memory is the weighted sum of each memory slot r t = S i=1 β t,i ·m i in which both motion and appearance information has been integrated.</p><p>Hidden states update. </p><formula xml:id="formula_7">+ W m/a rh r t + b m/a h ) h v t = σ(W v hh h v t-1 + W v rh r t + b v h )<label>(7)</label></formula><p>The global memory hidden state at all time steps h v 1:Nv will be taken as our final video features. In next section, we will discuss how to generate global question features. In Section 3.4, we will introduce how to interact video and question features for answer inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">External question memory</head><p>The existing deep learning based VideoQA methods often misunderstand the complex questions because they understand the questions based on local word information. For example, for question "Who drives by a hitchhiking man who is smoking?", traditional methods are easily trapped by the local words and fail to generate the right attention to the queried person (the driver or the smoker). To address this challenging problem, we introduce the question memory to learn context-aware text knowledge. The question memory can store the sequential text information, learn relevance between words, and understand the question from the global point of view.</p><p>We redesign the memory networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> to persistently store previous inputs and enable interaction between current inputs and memory contents. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the memory module is composed of memory slots M = [m 1 , m 2 , · · · , m S ] and memory hidden state h q . Unlike the heterogeneous memory discussed previously, one hidden state h q is necessary for the question memory. The inputs to the question memory are the encoded texts o q .</p><p>Write operation. We first define the content to write to the memory at t-th time step as c q t which is given by</p><formula xml:id="formula_8">c q t = σ(Woco q t + W hc h q t-1 + bc)<label>(8)</label></formula><p>as a non-linear mapping from current input o q t and previous hidden state h q t-1 to content vector c q t . Then we define the weights of writing to all memory slots αt={αt,1...αt,i...αt,S} such that</p><formula xml:id="formula_9">a t = v a tanh(W ca c q t + W ha h q t-1 + b a ) α t,i = exp(a t,i ) S j=1 exp(a t,j ) for i = 1 . . . S<label>(9)</label></formula><p>satisfying α t sum to 1. Then each memory slot m i is updated by m i = α t,i c t + (1 − α t,i )m i for i = 1 . . . S. Read operation. The next step is to perform attentional read operation from the memory slots M. We define the normalized attention weights β t ={β t,1 ...β t,i ...β t,S } of reading from memory slots such that</p><formula xml:id="formula_10">b t = v b tanh(W cb c q t + W hb h q t-1 + b b ) β t,i = exp(b t,i ) S j=1 exp(b t,j ) for i = 1 . . . S<label>(10)</label></formula><p>The content r t read from memory is the weighted sum of each memory slot content r t = S i=1 β t,i · m i . Hidden state update. The final step of t-th iteration is to update the hidden state h q t as</p><formula xml:id="formula_11">h q t = σ(W oh o q t + W rh rt + W hh h q t-1 + b h )<label>(11)</label></formula><p>We take the memory hidden state of all time steps h q 1:Nq as the global context-aware question features which will be used for inference in Section 3.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multimodal fusion and reasoning</head><p>In this section, we design a dedicated multimodal fusion and reasoning module for VideoQA, which can attend to multiple modalities such as visual and textual features, then make multi-step reasoning with refined attention for each modality. Our design is inspired by Hori et al. <ref type="bibr" target="#b8">[9]</ref> which proposed to generate video captions by combining different types of features such as video and audio. are taken as the input features. The core part is an LSTM controller with its hidden state denoted as s. During each iteration of reasoning, the controller attends to different parts of the video features and question features with temporal attention mechanism, and combines the attended features with learned modality weights φ t , and finally updates its own hidden state s t . Temporal attention. At t-th iteration of reasoning, we first generate two content vectors c v t and c q t by attending to different parts of visual features h v t and question features h q t . The temporal attention weights γ v 1:Nv and γ q 1:Nq are computed by</p><formula xml:id="formula_12">g v/q = v v/q g tanh(W v/q g s t-1 + V v/q g h v/q + b v/q g ) γ v/q i = exp(g v/q i ) N v/q j=1 exp(g v/q j ) for i = 1 . . . N v/q<label>(12)</label></formula><p>and shown by the dashed lines in <ref type="figure" target="#fig_4">Fig. 5</ref>. Then the attended content vectors c v/q t</p><formula xml:id="formula_13">and the transformed d v/q t are c v/q t = N v/q i=1 γ v/q i h v/q i , d v/q t = ReLU(W v/q d c v/q t +b v/q d ) (13)</formula><p>Multimodal fusion. The multimodal attention weights φ t = {φ v t , φ q t } are obtained by interacting the previous hid-den state s t-1 with the transformed content vectors</p><formula xml:id="formula_14">d v/q t p v/q t = v p tanh(W v/q p s t-1 + V v/q p d v/q t + b v/q p ) φ v/q t = exp(p v/q t ) exp(p v</formula><p>Multi-step reasoning. To complete t-th iteration of reasoning, the hidden state s t of LSTM controller is updated by s t = LSTM(x t , s t-1 ). This reasoning process is iterated for L times and we set L = 3. The optimal choice for L is discussed in §4.4. The hidden state s L at last iteration is the final representation of the distilled knowledge. We also apply the standard temporal attention on encoded video features o m and o a as in ST-VQA <ref type="bibr" target="#b9">[10]</ref>, and concatenate with s L to form the final answer representation s A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Answer generation</head><p>We now discuss how to generate the correct answers from answer features s A .</p><p>Multiple-choice task is to choose one correct answer out of K candidates. We concatenate the question with each candidate answer individually, and forward each QA pair to obtain the final answer feature {s A } K i=1 , on top of which we use a linear layer to provide scores for all candidate answers s = {s p , s n 1 , · · · , s n K−1 } in which s p is the correct answer's score and the rest are K − 1 incorrect ones. During training, we minimize the summed pairwise hinge loss <ref type="bibr" target="#b9">[10]</ref> between the positive answer and each negative answer defined as</p><formula xml:id="formula_15">Lmc = K−1 i=1 max(0, m − (s p − s n i ))<label>(16)</label></formula><p>and train the entire network end-to-end. The intuition of L mc is that the score of the true QA pair should be larger than any negative pair by a margin m. During testing, we choose the answer of highest score as the prediction. In <ref type="table" target="#tab_1">Table 1</ref>, we list the number of choices K for each dataset.</p><p>Open-ended task is to choose one correct word as the answer from a pre-defined answer set of size C. We apply a linear layer and softmax function upon s A to provide probabilities for all candidate answers such that p = softmax(W p s L + b p ) in which p ∈ R C . The training error is measured by cross-entropy loss such that Lopen = − C c=1 1{y = c} log(pc) <ref type="bibr" target="#b16">(17)</ref> in which y is the ground truth label. By minimizing L open we can train the entire network end-to-end. In testing phase, the predicted answer is provided by c * = arg max c (p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Implementation details</head><p>We implemented our neural networks in PyTorch <ref type="bibr" target="#b17">[18]</ref> and updated network parameters by Adam solver <ref type="bibr" target="#b12">[13]</ref> with batch size 32 and fixed learning rate 10 −3 . The video and question encoders are two-layer LSTMs with hidden size 512. The dimension D of the memory slot and hidden state is 256. We set the video and question memory sizes to 30 and 20 respectively, which are roughly equal to the maximum length of the videos and questions. We have released our code for boosting further research 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Discussions</head><p>We evaluate our model on four benchmark VideoQA datasets and compare with the state-of-the-art techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset descriptions</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we show the statistics of the four VideoQA benchmark datasets and the experimental settings from their original paper including feature types, vocabulary size, sampled video length, number of videos, size of QA splits, answer set size for open-ended questions, and number of options for multiple-choice questions.</p><p>TGIF-QA <ref type="bibr" target="#b9">[10]</ref> contains 165K QA pairs associated with 72K GIF images based on the TGIF dataset <ref type="bibr" target="#b13">[14]</ref>. TGIF-QA includes four types of questions: 1) counting the number of occurrences of a given action; 2) recognizing a repeated action given its count; 3) identifying the action happened before or after a given action, and 4) answering image-based questions. MSVD-QA and MSRVTT-QA were proposed by Xu et al. <ref type="bibr" target="#b26">[27]</ref> based on MSVD <ref type="bibr" target="#b3">[4]</ref> and MSVTT <ref type="bibr" target="#b27">[28]</ref> video sets respectively. Five different question types exist in both datasets, including what, who, how, when and where.</p><p>The questions are open-ended with pre-defined answer sets of size 1000. YouTube2Text-QA <ref type="bibr" target="#b29">[30]</ref> collected three types of questions (what, who and other) from the YouTube2Text <ref type="bibr" target="#b6">[7]</ref> video description corpus. The video source is also MSVD <ref type="bibr" target="#b3">[4]</ref>. Both open-ended and multiplechoice tasks exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Result analysis</head><p>TGIF-QA result. <ref type="table" target="#tab_2">Table 2</ref> summarizes the experiment results of all four tasks (Count,Action,Trans.,FrameQA) on TGIF-QA dataset. We compare with state-of-the-art methods ST-VQA <ref type="bibr" target="#b9">[10]</ref> and Co-Mem <ref type="bibr" target="#b4">[5]</ref> and list the reported accuracy in the original paper. For repetition counting task (column 1), our method achieves the lowest average L 2 loss compared with ST-VQA and Co-Mem (4.02 v.s. 4.28 and 4.10). For Action and Trans. tasks (column 2,3), our method significantly outperforms the other two by increasing accuracy from prior best 0.682 and 0.743 to 0.739 and 0.778 respectively. For FrameQA task (column 4), our method also   MSVD-QA result. <ref type="table" target="#tab_3">Table 3</ref> summarizes the experiment results on MSVD-QA. It's worth mentioning that there is high class imbalance in both training and test sets, as more than 95% questions are what and who while less than 5% are how, when and where. We list the numbers of their test instances in the table for reference. We compare our model with the ST-VQA <ref type="bibr" target="#b9">[10]</ref>, Co-Mem <ref type="bibr" target="#b4">[5]</ref> and current state-ofthe-art AMU <ref type="bibr" target="#b26">[27]</ref> on MSVD-QA. We show the reported accuracy of AMU in <ref type="bibr" target="#b26">[27]</ref>, while we accommodate the source code of ST-VQA and implement Co-Mem from scratch to obtain their numbers. Our method outperforms all the others on both what and who tasks, and achieves best overall accuracy of 0.337 which is 5.3% better than prior best (0.320). Even though our method slightly underperforms on the How, When and Where questions, the difference are minimal (40,2 and 3) regarding the absolute number of instances due to class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Question type</head><p>What Who How When Where All   YouTube2Text-QA result. In <ref type="table" target="#tab_5">Table 5</ref>, we compare our methods with the state-of-the-art r-ANL <ref type="bibr" target="#b29">[30]</ref> on YouTube2Text-QA dataset. It's worth mentioning that r-ANL utilized frame-level attributes as additional supervision to augment learning while our method does not. For multiple-choice questions, our method significantly outperforms r-ANL on all three types of questions (What, Who, Other) and achieves a better overall accuracy (0.808 v.s. 0.520</p><p>). For open-ended questions, our method outperforms r-ANL on what queries and slightly underperforms on the other two types. Still, our method achieves a better overall accuracy (0.301 v.s. 0.262). We also report the perclass accuracy to make direct comparison with <ref type="bibr" target="#b29">[30]</ref>, and our method is better than r-ANL in this evaluation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attention visualization and analysis</head><p>In <ref type="figure" target="#fig_7">Figs. 1 and 6</ref>, we demonstrate three QA examples with highlighted key frames and words which are recognized by our designed attention mechanism. For visualization purpose, we extract the visual and textual attention weights from our model (Eq. 12) and plot them with bar charts. Darker color stands for larger weights, showing that the corresponding frame or word is relatively important. <ref type="figure">Fig. 1</ref> shows the effectiveness of understanding complex question with our proposed question memory. This question intends to query the female driver though it uses another relative clause to describe the man. Our model focuses on the correct frames in which the female driver is driving in the car and also focuses on the words which describe the woman but not the man. In contrast, ST-VQA <ref type="bibr" target="#b9">[10]</ref>  identify the queried person as its simple temporal attention is not able to gather semantic information in the context of a long sentence. In <ref type="figure" target="#fig_7">Fig. 6(a)</ref>, we provide an example showing that our video memory is learning the most salient frames for the given question while ignoring others. In the first half of the video, it's difficult to know whether the vegetable is onion or potato, due to the lighting condition and camera view. However, our model smartly pays attention to frames in which the onion is cut into pieces by combining both question words "a man cut" and the motion features, and thus determines the correct object type by onion pieces (but not potato slices) from appearance hint. <ref type="figure" target="#fig_7">Fig. 6(b)</ref> shows a typical example illustrating that jointly learning motion and appearance features as our heterogeneous memory design is superior to attending to them separately such as Co-Mem <ref type="bibr" target="#b4">[5]</ref>. In this video, a woman is doing yoga in a gym, and there is a barbell rack at the background. Our method successfully associated the woman with the action of exercising, while Co-Mem <ref type="bibr" target="#b4">[5]</ref> incorrectly pays attention to the barbell and fails to utilize motion information as they separately learn motion and appearance attentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>We perform two ablation studies to investigate the effectiveness of each component of our model. We first study how many iterations of reasoning is sufficient in the designed multimodal fusion layer. After that, we make a comparison of variants of our model to evaluate the contribution of each component. Reasoning iterations. To understand how many iterations of reasoning are sufficient for our VideoQA tasks, we test different numbers and report their accuracy. The validation accuracy on MSVD-QA dataset increases from 0.298 to 0.306 when the number of reasoning iteration L increases from 1 to 3, and seems to saturate at L = 5 (0.307), while drops to 0.304 at L = 7. To balance performance and speed, we choose L = 3 for our experiments throughout the paper. <ref type="bibr">Dataset</ref> EF LF E-M V-M Q-M V+Q MSVD 0.313 0.315 0.318 0.320 0.315 0.337 MSRVTT 0.309 0.312 0.319 0.325 0.321 0.330 <ref type="table">Table 6</ref>. Ablation study of different architectures.</p><p>Different architectures. To understand the effectiveness of our designed memory module, we compare several variants of our models and evaluate on MSVD-QA and MSRVTT-QA, as shown in <ref type="table">Table 6</ref>. Early Fusion (EF) is indeed ST-VQA <ref type="bibr" target="#b9">[10]</ref> which concatenates raw video appearance and motion features at an early stage, before feeding into the LSTM encoder. In <ref type="table">Table 6</ref>, we observe consistent trend that using memory networks (e.g., E-M,V-M,V+Q) to align and integrate multimodal visual features is generally better than simply concatenating them (e.g., EF,LF). In addition, our designed visual memory (V-M) has shown its strengths over episodic memory (E-M) and other memory types (Table 3-5). Furthermore, using both visual memory and question memory (V+Q) increases the performance by 2-7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel end-to-end deep learning framework for VideoQA, with designing new external memory modules to better capture global contexts in video frames, complex semantics in questions, and their interactions. A new multimodal fusion layer was designed to fuse visual and textual modalities and perform multi-step reasoning with gradually refined attention. In empirical studies, we visualized the attentions generated by our model to verify its capability of understanding complex questions and attending to salient visual hints. Experimental results on four benchmark VideoQA datasets show that our new approach consistently outperforms state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Our proposed VideoQA pipeline with highlighted visual memory, question memory, and multimodal fusion layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Our designed heterogeneous visual memory which contains memory slots M , read and write heads α, β, and three hidden states h m , h a and h v .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>we define α m/a t ={α m/a t,1 , . . . , α m/a t,S } as the write weights of c m/a t to each of S memory slot given by a m/a t =v a tanh(W m/a ca c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Our re-designed question memory with memory slots M , read and write heads α, β, and hidden states h q .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Multimodal fusion layer. An LSTM controller with hidden state st attends to relevant visual and question features, and combines them to update current state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>demonstrates our designed module. The hidden states of video memory h v 1:Nv and question memory h q 1:Nq</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>ST</head><label></label><figDesc>0.436 0.824 0.760 0.286 0.330</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of multimodal attentions learned by our model on two QA exemplars. Highly attended frames and words are highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Late Fusion (LF) model uses two separate LSTM encoders to encode video appearance and motion features and then fuses them by concatenation. Episodic Memory (E-M) [26] is a simplified memory network embodiment and we use it as the visual memory to compare against our design. Visual Memory (V-M) model uses our designed heterogeneous visual memory (M v in Fig. 2) to fuse appearance and motion features and generate global context-aware video features. Question Memory (Q-M) model uses our redesigned question memory only (M q in Fig. 2) to better capture complex question semantics. Finally, Visual and Question Memory (V+Q M) is our full model which has both visual and question memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The final step is to update all three hidden states h a , h m and h v</figDesc><table><row><cell>h m/a t</cell><cell>= σ(W hh h m/a m/a t-1 + W oh o m/a</cell><cell>m/a t</cell><cell>(6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Dataset Feature Vocab size Video len Video num Question num Ans size MC numTable 1 .</head><label>1</label><figDesc>Dataset statistics of four VideoQA benchmark datasets. The columns from left to right indicate dataset name, feature types, vocabulary size, sampled video length, number of videos, size of QA splits, answer set size (Ans size) for open-ended questions, and number of options for multiple-choice questions (MC num).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell cols="2">TGIF-QA [10]</cell><cell cols="2">ResNet+C3D</cell><cell>8,000</cell><cell>35</cell><cell>71,741</cell><cell cols="3">125,473 13,941 25,751</cell><cell>1746</cell><cell>5</cell></row><row><cell cols="2">MSVD-QA [27]</cell><cell cols="2">VGG+C3D</cell><cell>4,000</cell><cell>20</cell><cell>1,970</cell><cell>30,933</cell><cell cols="2">6,415 13,157</cell><cell>1000</cell><cell>NA</cell></row><row><cell cols="2">MSRVTT-QA [27]</cell><cell cols="2">VGG+C3D</cell><cell>8,000</cell><cell>20</cell><cell>10,000</cell><cell cols="3">158,581 12,278 72,821</cell><cell>1000</cell><cell>NA</cell></row><row><cell cols="4">Youtube2Text-QA [30] ResNet+C3D</cell><cell>6,500</cell><cell>40</cell><cell>1,970</cell><cell>88,350</cell><cell>6,489</cell><cell>4,590</cell><cell>1000</cell><cell>4</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Question type</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Count (loss)</cell><cell>Action</cell><cell>Trans.</cell><cell>FrameQA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ST-VQA [10]</cell><cell>4.28</cell><cell>0.608</cell><cell>0.671</cell><cell>0.493</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Co-Mem [5]</cell><cell>4.10</cell><cell>0.682</cell><cell>0.743</cell><cell>0.515</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>4.02</cell><cell>0.739</cell><cell>0.778</cell><cell>0.538</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Experiment results on TGIF-QA dataset.</figDesc><table><row><cell cols="7">achieves the best accuracy of 0.538 among all three meth-</cell></row><row><cell cols="5">ods, outperforming the Co-Mem by 4.7%.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Question type and # instances</cell></row><row><cell>Method</cell><cell>What</cell><cell>Who</cell><cell>How</cell><cell cols="2">When Where</cell><cell>All</cell></row><row><cell></cell><cell>8419</cell><cell>4552</cell><cell>370</cell><cell>58</cell><cell>28</cell><cell>13427</cell></row><row><cell cols="5">ST-VQA [10] 0.181 0.500 0.838 0.724</cell><cell>0.286</cell><cell>0.313</cell></row><row><cell>Co-Mem [5]</cell><cell cols="4">0.196 0.487 0.816 0.741</cell><cell>0.317</cell><cell>0.317</cell></row><row><cell>AMU [27]</cell><cell cols="4">0.206 0.475 0.835 0.724</cell><cell>0.536</cell><cell>0.320</cell></row><row><cell>Ours</cell><cell cols="4">0.224 0.501 0.730 0.707</cell><cell>0.429</cell><cell>0.337</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Experiment results on MSVD-QA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Experiment results on MSRVTT-QA dataset.</figDesc><table><row><cell>MSRVTT-QA result. In Table 4, we compare our model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Experiment results on YouTube2Text-QA dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ) + exp(p q t )(14)The fused knowledge x t is computed by the sum of d v/q t with multimodal attention weights φ v/q such thatxt = φ v t d v t + φ q t d q t(15)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/fanchenyou/HME-VideoQA</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niveda</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng-Yok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bret</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiko</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TGIF-QA: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Textbook question answering for multimodal machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tgif: A new dataset and benchmark on animated gif description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anton van den Hengel, and Ian Reid. Visual question answering with memory-augmented networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A taxonomy for neural memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Principe</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1805.00327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video question answering via attributeaugmented attention network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Leveraging video descriptions to learn video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-Hao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
