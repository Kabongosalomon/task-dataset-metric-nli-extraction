<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modulating Image Restoration with Continual Levels via Adaptive Feature Modification Layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modulating Image Restoration with Continual Levels via Adaptive Feature Modification Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In image restoration tasks, like denoising and superresolution, continual modulation of restoration levels is of great importance for real-world applications, but has failed most of existing deep learning based image restoration methods. Learning from discrete and fixed restoration levels, deep models cannot be easily generalized to data of continuous and unseen levels. This topic is rarely touched in literature, due to the difficulty of modulating well-trained models with certain hyper-parameters. We make a step forward by proposing a unified CNN framework that consists of few additional parameters than a single-level model yet could handle arbitrary restoration levels between a start and an end level. The additional module, namely AdaFM layer, performs channel-wise feature modification, and can adapt a model to another restoration level with high accuracy. By simply tweaking an interpolation coefficient, the intermediate model -AdaFM-Net could generate smooth and continuous restoration effects without artifacts. Extensive experiments on three image restoration tasks demonstrate the effectiveness of both model training and modulation testing. Besides, we carefully investigate the properties of AdaFM layers, providing a detailed guidance on the usage of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning methods have achieved great success in image restoration tasks, such as denoising, super-resolution, compression artifacts reduction, etc <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22]</ref>. However, there still exists a large gap of restoration performance between research environment mand real-world applications. In this work, we focus on two main issues that prevent CNN based restoration methods from wide usages.</p><p>First, the degradation levels of real-world images are * The first two authors are co-first authors. <ref type="bibr">(</ref> Is it possible to achieve a compromise ? <ref type="bibr">Figure 1</ref>. Applying q10 or q80 DeJPEG model on images (LIVE1 <ref type="bibr" target="#b14">[15]</ref>) with degradation q30 tends to produce either over-sharpening (left) or over-smoothed (right) images.</p><p>generally continuous, such as JPEG quality q27 and q34.</p><p>On the other hand, the deep restoration models are usually trained with discrete and fix levels (e.g., q20, q30). Applying models with mismatched restoration levels tends to produce either over-sharpening or over-smoothed images, as shown in <ref type="figure">Figure 1</ref>  <ref type="bibr" target="#b0">1</ref> . A straightforward solution is to train a sufficiently large model to handle all degradation levels. However, regardless of the computational burden, this general model is not optimal for each individual level. When we want to slightly adjust the output effects, we have to retrain a new model by refining the model structure, parameters or (and) loss functions, which is a tedious procedure with unpredictable results. Second, in industrial and commercial scenarios (e.g., human-interactive softwares), it is often necessary to consecutively modulate the restoration strength/effect to meet different requirements. For example, the users always ex- pect a tool bar to flexibly adjust the restoration level, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. However, current deep models are trained on fixed degradation levels, and contain no hyperparameters for users to change the final results.</p><p>To fill in the gaps, our goal is to achieve arbitrary-level image restoration and continual model modulation in a unified CNN framework. More formally, the task is to deal with images of degradation levels between a "start" level and an "end" level in a user controllable manner. To facilitate practical usages, we should avoid building a very large model or model zoo, and prevent another training stage at test time. In other words, the solution should contain a small amount of additional parameters and allow continual tuning of parameters in testing.</p><p>This task is non-trivial and rarely studied in literature. Perhaps the most relevant topic to modifying the network outputs is arbitrary style transfer. Specifically, we can treat different levels of degradation as different kinds of styles. A representative approach is the Conditional Instance Normalization (IN) <ref type="bibr" target="#b5">[6]</ref>, which allows users to mix up different styles by tuning IN parameters. Nevertheless, image restoration has higher and finer request on the output image quality. Directly applying Conditional IN in image restoration could produce obvious and large-scale artifacts in the output image (see <ref type="figure" target="#fig_4">Figure 6</ref>). Another similar concept is domain adaptation, which generally appears in high-level vision problems (e.g., image classification and object detection). It adapts/transfers the model trained on the source domain to the target domain. However, domain adaptation cannot easily generalize to unseen data, thus is not appropriate to address our problem.</p><p>In this work, we present a simple yet effective approach that for the first time enables consecutive modulation of the restoration strength with little computation cost. This approach stems from the observation that filters among networks of different restoration levels are similar at patterns while varying on scales and variances. Furthermore, the model outputs could change continuously by modulating the statistics of features/filters. The proposed framework is built upon a novel Adaptive Feature Modification (AdaFM) layer that modifies the middle-layer features with depthwise convolution filters. In practice, we first train a standard restoration CNN for the start level, and then insert AdaFM layers and optimize it to the end level. After the training stage, we fix the CNN parameters, and interpolate the filters of AdaFM layers according to testing restoration level. By tuning a controlling coefficient (ranging from 0 to 1), we can interactively and consecutively manipulate the restoration results/effects. Note that we only need to train the CNN and AdaFM layers once, and no further training is required in the test time.</p><p>To ensure the output quality, we demonstrate that the model with AdaFM layers achieves comparable performance to the single-level image restoration network in both start and end level. Then, we show that the modulatednetwork outputs are noise-free with consecutive restoration effects (see <ref type="figure" target="#fig_0">Figure 2</ref>). Besides, we also examine the properties of AdaFM layers -complexity, range and direction, providing a detailed instruction on the usage of the proposed method. Notably, the added AdaFM layers contribute to less than 4% parameters of the CNN model yet achieves excellent modulation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The proposed Adaptive Feature Modification (AdaFM) layers are inspired by the recent normalization methods in deep CNNs, thus we give a brief review of these works. Normalization has been demonstrated effective in facilitating training very deep neural networks. The most representative method is batch normalization (BN) <ref type="bibr" target="#b7">[8]</ref> that is proposed to address the problem of Internal Covariate Shift in the training process. In particular, BN layer normalizes the output of each neuron using the mean and variance of each batch calculated during the feed-forward process. Later on, Dmitry Ulyanov et al. <ref type="bibr" target="#b16">[17]</ref> achieved significant improvement in style transfer by replacing all the BN layers with their proposed instance normalization (IN) layers. The core idea is to normalize the features based on the statistics across the spatial dimensions of each sample instead of each batch. Recently, several alternative normalization methods have been proposed, such as instance weight nor-malization <ref type="bibr" target="#b13">[14]</ref>, layer normalization <ref type="bibr" target="#b1">[2]</ref>, group normalization (GN) <ref type="bibr" target="#b19">[20]</ref> and etc. The spatial feature transformation (SFT) layer proposed by Wang et al. <ref type="bibr" target="#b17">[18]</ref> further extends the normalization operation to a more general spatial-variant transformation. Specifically, they apply a feature spatialwise transformation on the feature maps according to the semantic segmentation priors. This approach indeed helps generate more realistic textures compared with those popular GAN-based methods. We will compare the proposed AdaFM layer with BN and SFT layers in Section 3.3.</p><p>Furthermore, recent works show that BN and IN have the ability to adapt the model to a different domain with little computation cost. Specifically, Li et al. <ref type="bibr" target="#b10">[11]</ref> propose AdaBN (Adaptive Batch Normalization) to alleviate domain shifts, and show that AdaBN is effective for domain adaptation task by re-computing the statistics of all BN layers across the network. Huang et al. <ref type="bibr" target="#b6">[7]</ref> show that instance normalization (IN) can perform as style normalization by aligning the mean and variance of content features with those style features. In such way, they realize arbitrary style transfer at test time. Moreover, Dumoulin et al. <ref type="bibr" target="#b5">[6]</ref> extended IN to enable multiple style transfer by learning different sets of parameters in normalization layers while the convolution parameters are shared. Our method is different from these works in that 1) the proposed AdaFM layer is independent of either batch or instance samples, 2) the filter size and position of AdaFM layers are flexible, indicating that AdaFM is beyond a normalization operation, 3) the interpolation property of AdaFM layers could achieve continual modulation of restoration levels, which has not been revealed before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation.</head><p>The problem of consecutive modulation of restoration levels can be formulated as follows. Suppose we have a "start" restoration level -L a and an "end" restoration level -L b , the objective is to construct a deep network to handle images with arbitrary degradation level L c (L a ≤ L c ≤ L b ). Our solution pipeline consists of two stages -model training and modulation testing. In model training, we train a basic model and an adaptive model that could deal with level L a and L b , respectively. While in modulation testing, we propose a new network that can realize arbitrary restoration effects between level L a and L b by modulating certain hyper-parameters. In the following sections, we first show two important observations that inspire our method. Then we propose the AdaFM layer and compare it with BN <ref type="bibr" target="#b7">[8]</ref> and SFT <ref type="bibr" target="#b17">[18]</ref>. At last, we describe how to use AdaFM layers in model training and modulation testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Observation</head><p>Observation 1. We find that the learned filters of restoration models trained with different restoration levels are pretty similar at visual patterns, but their weights have different statistics (e.g., mean and variance). An example is shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the filter f a of level L a is like a 2-D Gaussian filter, then the corresponding filter f b finetuned from level L a to level L b will also look like a Gaussian filter but with different mean and variance. We use the Gaussian Denoising problem for illustration. The start level is noise level σ = 15, and the end level is σ = 50. We adopt a simple and standard CNN structure ARCNN <ref type="bibr" target="#b3">[4]</ref> to do the experiments. We first learn the model with noise level σ = 15 and obtain ARCNN-15, then finetune the network on σ = 50 to obtain ARCNN-50. The first layer filters of these two models are visualized in <ref type="figure" target="#fig_1">Figure  3</ref>. In the first glance, these filters look similar with only slight differences. Their mean cosine distance between the corresponding filters is 0.12, indicating that they are very close to each other. To further reveal their relationship, we use a filter to bridge the corresponding filters. Specifically, each filter f 15 in ARCNN-15 is convoluted with another filter g to approximate the corresponding filter f 50 in ARCNN-50. According to the commutative law, we have (g * f 15 ) * x = g * (f 15 * x), where * is convolution. Thus for each feature map x, the parameters of g are optimized with</p><formula xml:id="formula_0">min g ||f 50 * x − g * (f 15 * x)|| 2 .<label>(1)</label></formula><p>The above operation is equivalent to adding a depth-wise convolution layer after each layer of ARCNN-15, and finetuning the added parameters on the σ = 50 problem. When g is of size 1 × 1, it is equal to a scaling and shift operation, changing the mean and variance of the original filter. We use the PSNR gap between their network outputs to show the fitting error. From <ref type="table">Table 2</ref>, we can see that the value of fitting error decreases when the filter size of g increases. The gap is already very small at 1 × 1, which demonstrates our primal assumption. The 5 × 5 filters are also visualized in <ref type="figure" target="#fig_1">Figure 3</ref>, where one can see the differences between f 15 and f 50 . Similar experiments for super resolution and compression artifacts reduction are presented in the supplementary file.</p><p>Observation 2. We find that the network output could  </p><formula xml:id="formula_1">f mid = f 15 + λ(g − I) * f 15 , 0 ≤ λ ≤ 1,<label>(2)</label></formula><p>where λ is an interpolation coefficient. When we modulate λ gradually from 0 to 1, f mid will also change continuously from f 15 to g * f <ref type="bibr" target="#b14">15</ref> . After putting f mid back to the network, we find that the network output will also change continuously in visualization, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Detailed analysis can be found in Section 3.5 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Feature Modification</head><p>Inspired by the above observations, we propose a continual modulation method by introducing an Adaptive Feature Modification layer and the corresponding modulating strategy. The overall framework is depicted in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>Our aim is to add another layer to manipulate the statistics of the filters, so that they could be adapted to another restoration level. As indicated in Observation 1, we can add a depth-wise convolution layer (or a group convolution layer with the group number equal to the number of feature maps) after each convolution layer and before the activation function (e.g., ReLU). We name the added layer as the Adaptive Feature Modification layer, which is formulated as</p><formula xml:id="formula_2">AdaF M (x i ) = g i * x i + b i , 0 &lt; i ≤ N,<label>(3)</label></formula><p>where x i is the input feature map and N is the number of feature maps. g i and b i are the corresponding filter and bias, respectively. It is worth noting that g i depends on the degradation level of input images. To further understand its behaviour, we compare the proposed layer with batch normalization (BN) <ref type="bibr" target="#b7">[8]</ref> and spatial feature transformation (SFT) <ref type="bibr" target="#b17">[18]</ref> layers.</p><p>Comparison with BN layer. When we set the filter size of g i to 1 × 1, the feature modification reduces to a normalization operation. Note that BN <ref type="bibr" target="#b7">[8]</ref> is also put directly after the convolution layer. We compare it with BN as</p><formula xml:id="formula_3">AdaF M (x i ) = g i x i + b i , BN (x i ) = γ( x i − µ σ ) + β,<label>(4)</label></formula><p>where µ, β are the mean and standard deviation of an input batch, γ, β are affine parameters. The 1 × 1 AdaFM filter performs similar to BN without using the batch information. As a special case, we can also use BN to do feature modification and finetune γ, β as g i , b i . Experiments show that using BN achieves almost the same results as the 1 × 1 AdaFM filter.</p><p>Comparison with SFT layer. When the filter size of g is as large as the feature map, it will perform spatial feature transform as SFT layer <ref type="bibr" target="#b17">[18]</ref>. The formulations are shown as <ref type="bibr" target="#b4">(5)</ref> where γ, β are affine parameters. AdaFM and SFT layer share the same function, but different on the parameters. Specifically, γ, β are calculated from another sub-network based on an additional prior, while g i , b i are directly learned with the network.</p><formula xml:id="formula_4">AdaF M (x i ) = g i x i + b i , SF T (x i ) = γ x i + β,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Training</head><p>In this subsection, we discuss how to utilize the proposed AdaFM layer for model training. The entire model, namely AdaFM-Net, consists of a basic network and the AdaFM layers. First, we train the basic network N a bas , which can be any standard CNN model, for the start restoration level L a . Then we insert AdaFM layers to N a bas and form the AdaFM-Net N ada . By fixing the parameters of N a bas , we optimize the parameters of AdaFM layers on the end level L b . Experiments demonstrate that by only finetuning the AdaFM layers, the model N b ada could achieve comparable performance with a basic model N b bas trained from scratch on level L b . As the AdaFM-Net is optimized from L a to L b , we name this process as adaptation, and use adaptation accuracy to denote its performance. Specifically, we can use the PSNR distance between PSNR of N b ada and N b bas as the measurement of adaptation accuracy. There are three factors that affect the adaptation accuracy -filter size, direction, and range.</p><p>(1) For filter size, a larger filter size or more parameters will lead to better adaptation accuracy. We try filter size from 1 × 1 to 7 × 7. From convergence curves shown in <ref type="figure" target="#fig_3">Figure 5</ref>, we find that 3 × 3 performs much better than 1 × 1 while 7 × 7 is only comparable to 5 × 5. Further increasing the filter size could not continuously improve the performance. (2) For direction, different restoration levels have different degrees of difficulty for the same network. Then should we modulate the model from an easy level to a hard level or the opposite direction? Experimentally, we find that from easy to hard is a better choice (see Section 4.2). (3) For range, the smaller of the range/gap |L b − L a |, the better the adaptation accuracy. For example, in super resolution problem, transferring the filters from ×2 to ×3 is easier than from ×2 to ×4. In Section 4, we conduct numerous experiments to choose the best range for super-resolution, denoising and compression artifacts reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Modulation testing</head><p>After the training process, we discuss how to modulate the AdaFM layers according to degradation level at test time. As the features remain the same after convolution with an identity filter, we initialize AdaFM layers with identity filters I and zero biases, which is regarded as the start point of AdaFM layers. Based on Observation 2, we can linearly interpolate the parameters of AdaFM layers as</p><formula xml:id="formula_5">g * i = I + λ(g i − I), b * i = λb i , 0 &lt; i ≤ N,<label>(6)</label></formula><p>where g * i , b * i are the filter and bias of the interpolated AdaFM layers, λ(0 ≤ λ ≤ 1) is the interpolation coefficient determined by the degradation level of input image. After adding the interpolated AdaFM layers back to the basic network N a bas , we can get the AdaFM-Net N c ada for a middle level L c (L a ≤ L c ≤ L b ). The effects of changing the coefficient λ from 0 to 1 are shown in <ref type="figure" target="#fig_0">Figure 2</ref>, 6, where the output effects change continuously along with λ.</p><p>Interestingly, we find that the interpolated network could fairly deal with any restoration level L c between level L a and L b by adjusting the coefficient λ, which behaves like a strength controller in traditional methods. Experimentally, we find that the relationship between the coefficient λ and restoration level L c can be formulated/approximated as a polynomial function:</p><formula xml:id="formula_6">λ = f (L c ) = M j=0 w j L j c ,<label>(7)</label></formula><p>where M is the order and {w j } M 0 are coefficients. To fit this polynomial function, we need to determine at least M points {L i c , λ i } M i=0 . Specially, the start point is {L 0 c = L a , λ 0 = 0} and the end point is {λ M = 1, L M c = L b }. Furthermore, we require a test set with degraded images and ground truth to measure the adaptation accuracy. For a middle level L i c , we use the test images of level L i c as inputs. By adjusting the coefficient λ, the AdaFM-Net could generate a series of outputs. We select the λ that achieves the highest PSNR (evaluated on the test set) as the best coefficient, recorded as λ i for L i c . It is worth noticing that the modulation process and curve fitting require no additional training.</p><p>Extensive experiments show that the fitting curve varies a lot with ranges and problems. Take compression artifacts reduction as an example. If the range is small, such as JPEG quality from q80 to q50, then the fitting function is linear (order M = 1) as shown in <ref type="figure" target="#fig_5">Figure 7</ref>. On the other hand, if the range is large, such as from q80 to q10, then we have to use a curve (order M = 3) for approximation. Similar trend is observed for denoising and super resolution (see details in Section 4.3 and the supplementary file).</p><p>As an alternative choice, we can also use the piece-wise linear function for approximation. Actually, when the range is small enough, the relationship between λ and L c is almost linear. We can train a set of AdaFM-Nets on middle levels</p><formula xml:id="formula_7">{L i c }. For a given level L c (L i c &lt; L c &lt; L i+1 c ), we can use the coefficient λ = (L c −L i c )/(L i+1 c −L i c )</formula><p>to interpolate the AdaFM-Nets between L i c and L i+1 c . This strategy needs to train and store more AdaFM-Nets on middle levels, but the adaptation accuracy is comparably higher due to the small range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Set-up</head><p>Training settings. We use the DIV2K <ref type="bibr" target="#b0">[1]</ref> dataset for all the image restoration tasks. The training data is augmented by horizontal flipping and 90-degree rotations. Following SRResNet <ref type="bibr" target="#b9">[10]</ref>, the mini-batch size is set to 16 and the HR patch size is 96 × 96. The L1 loss <ref type="bibr" target="#b18">[19]</ref> is adopted as the loss function. For model training, the initial learning rate is set to 1 × 10 −4 and then decayed by a factor of 10 after 5 × 10 5 iterations. We adopt the Adam <ref type="bibr" target="#b8">[9]</ref> optimizer with β 1 = 0.9, β 2 = 0.999. All models are built on the PyTorch framework and trained with NVIDIA 1080Ti GPUs.</p><p>The structure of basic model. Based on the widely used SRResNet and DnCNN <ref type="bibr" target="#b21">[22]</ref>, the basic model N bas adopts a general CNN structure that consists of a pair of downsampling (convolution with stride 2) and up-sampling (pixelshuffle <ref type="bibr" target="#b15">[16]</ref> with upscaling factor 2) layers, 16 residual blocks, and several convolution layers. Specifically, the filter number is 64 and the filter size is 3 × 3 for all convolution layers. The residual block contains two convolution layers and a ReLU activation layer. The middle features are processed in a low-resolution (1/4 of the input size) space, while the output size remains the same as the input size. For super-resolution, we can upsample the LR image to the HR image size as SRCNN <ref type="bibr" target="#b4">[5]</ref>. As shown in <ref type="table" target="#tab_2">Table 1</ref>  basic model achieves better PSNR results than SRResNet, DnCNN and ARCNN on super-resolution, denoising and compression artifacts reduction, respectively. As stated in Section 3.4 and 3.5, the basic model is also trained on different levels (as the baseline) to evaluate the performance of AdaFM-Nets. The position of AdaFM layers. As indicated in Section 3.3, we can insert the AdaFM layers after all convolution layers or just in the residual blocks (the same as BN and IN). Moreover, an alternative choice is to add AdaFM layers after all activation layers. To evaluate the above three approaches, we conduct experiments for super resolution task ×3 → ×4 with filter size 5 × 5. From the experimental results, we observe that adding AdaFM layers after activation is inferior to that before activation (32.00 dB, 31.84 dB evaluated on Set5 <ref type="bibr" target="#b2">[3]</ref>). The results of inserting AdaFM layers after all convolution layers and in the residual blocks make little difference (32.01 dB, 32.00 dB evaluated on Set5). To save computation, we insert AdaFM layers just in residual blocks before activation for all experiments.</p><p>Complexity analysis. We calculate the parameters of the basic model and AdaFM layers. Following previous works, we exclude the number of biases that perform add operation in network. The total parameters in basic model include the parameters of 16 residual blocks, 4 convolution layers and a pixelshuffle layer. As we insert the AdaFM layers in residual blocks, the number of AdaFM layers is equal to the number of convolution layers in residual blocks. Thus there are 16×2×64 = 2048 filters in AdaFM layers. When the filter size is 1 × 1, 3 × 3, 5 × 5, the number of parameters is 2048, 18432, 51200, respectively, accounting for 0.15% 1.31% 3.65% of the total parameters in the basic model. Note that these numbers are even smaller than the parameter number of a single residual block (2×64×64×9 = 73728). Nevertheless, as AdaFM-Net is comparably larger than the  <ref type="table">Table 3</ref>. Comparisons with AdaBN <ref type="bibr" target="#b10">[11]</ref> and conditional IN <ref type="bibr" target="#b5">[6]</ref> basic model, we still need to verify whether it significantly improves the model capacity. In super resolution ×4, we train an AdaFM-Net with AdaFM layers of a large filter size 5 × 5 from scratch. The PSNR value on DIV2K (30.39 dB) is almost the same as that of the basic model (30.37 dB), indicating that the performance is not influenced by AdaFM layers. We can safely use the basic model as baseline to test the AdaFM-Nets. In another perspective, this also demonstrates the effectiveness of the proposed strategy, which adapts the model to different restoration levels with little additional computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of Model Training</head><p>In this section, we evaluate our proposed method on three image restoration tasks, super resolution, denoising, and compression artifacts reduction (JPEG Deblocking or DeJPEG). The basic settings are shown below.</p><p>For super-resolution, we train our models in RGB channels and calculate the PSNR in y-channel on two widely used benchmark datasets -Set5 <ref type="bibr" target="#b2">[3]</ref> and the test set of DIV2K <ref type="bibr" target="#b0">[1]</ref>. We evaluate our methods on upscaling factors ×2, ×3, ×4, ×5, ×6. All other settings remain the same as SRCNN <ref type="bibr" target="#b4">[5]</ref>. In denoising, we use Gaussian noise and consider 5 noise levels, i.e., σ = 15, 25, 35, 50, 75. Following DnCNN <ref type="bibr" target="#b21">[22]</ref>, the models are trained with RGB channels and evaluated in RGB channels on CSBD68 <ref type="bibr" target="#b12">[13]</ref> dataset. For DeJPEG, we use the JPEG quality q = 80, 60, 40, 20, 10 in MATLAB JPEG encoder. Similar as ARCNN <ref type="bibr" target="#b3">[4]</ref>, our models are trained and tested in y channel only. LIVE1 <ref type="bibr" target="#b14">[15]</ref> dataset is used for evaluation.</p><p>Filter Size. First, we need to determine the filter size of AdaFM layers for different problems. We denote the adaptation from the start level L a to the end level L b as L a → L b . The basic model is trained on L a , and AdaFM-Net is tested on L b . For the super resolution task ×3 → ×4, we compare the performance of AdaFM-Net with various filter sizes -1×1, 3×3, 5×5 and 7×7. The convergence curves on Set5 are plotted in <ref type="figure" target="#fig_3">Figure 5</ref> , and the quantitative results are presented in <ref type="table">Table 2</ref>. In general, larger filters can achieve better performance. Notably, the PSNR gap between 1 × 1 and 3 × 3 is larger than 0.4 dB. However, this trend does not always hold when the filter size is expanded to 7×7. Therefore, we use the filter size 5×5 to conduct the following experiments for the super resolution tasks. Similar as in super resolution, we compare the performance with different filter sizes (1×1, 3×3, 5×5 and 7×7) for denoising task σ15 → σ75 and DeJPEG task q80 → q10. Results shown in <ref type="table">Table 2</ref> indicate that in both two tasks, filter size 1×1 can already achieve excellent performance. The PSNR gap between 1 × 1 and 7 × 7 is less than 0.1 dB. Considering the computation cost, we use filter size 1 × 1 for all denoising and DeJPEG experiments.</p><p>Direction. The second step is to find the best adaptation direction. Before experiments, it is essential to clarify the way of measurement. For task L a → L b , the baseline is the basic model trained on L b with performance P bas , and the AdaFM-Net is finetuned on L b with performance P ada . Then the PSNR distance |P bas −P ada | is used to evaluate the adaptation accuracy of AdaFM-Net. In experience, 0.3 dB is regarded as a significant PSNR gap in image restoration. In other words, if the distance |P bas −P ada | exceeds 0.3 dB, then the adaptation is NOT well-suited for applications.</p><p>We conduct three pairs of experiments -super resolution task ×3 → ×4 and ×4 → ×3, denoising task σ15 → σ75 and σ75 → σ15, DeJPEG task q80 → q10 and q10 → q80. Results are shown in <ref type="table" target="#tab_4">Table 4</ref>, 5. In all three problems, the tasks with direction from easy to hard (i.e., ×3 → ×4, σ15 → σ75, q80 → q10) achieve better adaptation results. Here, easy and hard refer to the difficulty of restoring the input images. For example, in DeJPEG, the PSNR distance of q80 → q10 is 0.2 dB, which is much lower than that of the inverse direction q10 → q80 -1.04 dB.</p><p>Range. In this subsection, we investigate the influence of the adaptation range. Generally, by fixing the start level L a , we change the end level L b and test the adaptation accuracy.</p><p>Different from previous sections, we start discussion with denoising and DeJPEG, where the trend of range is more obvious. In denoising, we start with σ15 and change the end level from σ25 to σ75. In DeJPEG, we start with q80 and change the end level from q60 to q10. The adaptation results are shown in <ref type="table">Table 5</ref>. It is observed that better adaptation accuracy is obtained with a smaller range. In addition, the proposed AdaFM can easily handle very large range in either denoising or DeJPEG.</p><p>For super resolution, we find it hard to adapt the model across even 2 upscaling factors. For example, in <ref type="table" target="#tab_4">Table 4</ref>, the PSNR distance for the task ×2 → ×4 exceeds 0.3 dB for all three test sets, indicating that we should not further enlarge the range to 3. When fixing the range to be 1 and 2 upscaling factors, we change both the start and end level to see the change of results. From <ref type="table" target="#tab_4">Table 4</ref>, we can conclude that the adaptation is easier (lower PSNR distance) with a harder start level (e.g., ×4 → ×5 is better than ×3 → ×4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparison with AdaBN and Conditional IN</head><p>We compare with state-of-the-art methods on super resolution task ×3 → ×4, denoising task σ15 → σ75 and DeJPEG task q80 → q10. To compare with AdaBN <ref type="bibr" target="#b10">[11]</ref>, we train a network with batch normalization after all convolutional layers in the residual blocks, and then change all the statistics in BN layers during testing. We also use conditional IN <ref type="bibr" target="#b6">[7]</ref> to handle different levels of restoration. The results are shown in <ref type="table">Table 3</ref>. It can be obviously observed that neither of the two methods can obtain reasonable super resolution, denoising and DeJPEG results. Therefore, they are not suitable for image restoration tasks. Qualitative comparisons are also shown in <ref type="figure" target="#fig_4">Figure 6</ref>, where we observe clear artifacts on their output images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Modulation Testing</head><p>In modulation testing, continuously manipulating the interpolation coefficient λ could gradually change the output effect. If the input image is fixed, then the output image will become sharper or smoother with the increase of λ, as shown in <ref type="figure" target="#fig_4">Figure 6</ref>. On the other hand, we can choose different λ to deal with different kinds of degraded images. As presented in Section 3.5, the coefficient λ can be formulated as a polynomial function of restoration level L c λ = M j=0 w j L j c . In this subsection, we investigate the curving fitting with different ranges in DeJPEG problem. Similar investigations on super resolution and denoising problems can be found in supplementary file.</p><p>We first investigate the DeJPEG task q80 → q10. We select 6 middle levels -L c = q70, 60, 50, 40, 30, 20 -between q80 and q10. Then for a given level L c , we use the test images of L c as inputs, and adjust λ to obtain different outputs of AdaFM-Net. After calculating the PSNR values on LIVE1 test set, we select the λ that achieves the best PSNR as the best coefficient. For example, see the blue line in <ref type="figure" target="#fig_5">Figure 7</ref>, the best coefficient for level q60 and q30 are 0.14, 0.40, respectively. After we have obtained all middle points, we fit the curve by a cubic function:λ = 1.51 − 6.24 × 10 −2 L c + 1.01 × 10 −3 L 2 c − 5.91 × 10 −6 L 3 c . Then for arbitrary levels between q80 and q10, we can use this function to predict its corresponding interpolation coefficient. If we test a smaller range, such as q80 → q50, then a simple straight line could fairly connect all middle points (see the orange line in <ref type="figure" target="#fig_5">Figure 7</ref>). In other words, the polynomial function is linear. This property holds for smaller ranges such as q80 → q60.</p><p>To verify whether the interpolated image is of high quality, we use the PSNR distance on LIVE1 test set as the evaluation metric. Specifically, the basic model trained on level L c is used as the baseline, and the PSNR distance is calculated between the PSNR of AdaFM-Net and that of a well-trained baseline model. The smaller of the PSNR distance the better of the adaptation/modulation accuracy. <ref type="figure" target="#fig_5">Figure 7</ref> illustrates the PSNR distances in two DeJPEG tasks. It is observed that all PSNR distances are below 0.2 dB, indicating that the output quality is good enough for practical usages. Further, the PNSR distances of the smallrange task q80 → q50 is much lower than the large-range task q80 → q10. Thus modulation across smaller ranges achieves better performance. For higher request of modulation quality, we can decompose a large range to several small ranges, and train AdaFM-Nets for each sub-task. We can balance the performance and computation burden according to different applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a method that allows continual modulation of restoration levels in a single CNN for versatile and flexible image restoration. The core idea of our method is to handle images with arbitrary degradation levels with a single model, which consists of a basic model and a modulation layer -AdaFM layer. We further propose the learning and modulating strategies of the AdaFM layers. In test time, the model can be adapted to any restoration level by directly adjusting the AdaFM layers without an additional training stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>We can modulate the tool bar to obtain continual restoration effect in DeJPEG, Super Resolution and Denoising.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Filter visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The left part presents the basic model and the AdaFM-Net. The right part shows how AdaFM works in the adaptation process and the modulation testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The performances of adaptation with different filter sizes of AdaFM layers in super resolution on Set5 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Left: Artifacts on the output images produced by AdaBN and conditional instance normalization. Right: Modulation testing in Denoising (CBSD68), DeJEPG (LIVE1) and Super Resolution (Set14<ref type="bibr" target="#b20">[21]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Top: the curve fitting with different ranges in DeJPEG problem; Bottom: the value of PSNR distance is annotated above each bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>e-mail: jw.he@siat.ac.cn; chao.dong@siat.ac.cn).</figDesc><table><row><cell>q30 DeJPEG</cell><cell></cell></row><row><cell>q80</cell><cell>q10</cell></row><row><cell>model</cell><cell>model</cell></row><row><cell>over-sharpening</cell><cell>over-smoothed</cell></row></table><note>† Corresponding author (e-mail: yu.qiao@siat.ac.cn).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>change continuously by modulating the statistics of features/filters. As the filter g is gradually updated by gradient descent, what if we control the updating process by interpolating the intermediate results? Specifically, we can obtain the intermediate filter f mid by the following function:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparisons with the state-of-the-art methods in PSNR.</figDesc><table><row><cell>, the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>PSNR(dB) 1×1 3×3 5×5 7×7 baseline SR Set5 31.42 31.88 32.00 32.03 32.13 DIV2K100 29.89 30.20 30.28 30.30 30.37 DeJPEG LIVE1 29.35 29.39 29.41 29.42 29.55 Denoising CBSD68 26.35 26.38 26.39 26.40 26.49 Table 2. The PSNR results of adaptation with different kernel sizes of AdaFM layers in three tasks.</figDesc><table><row><cell cols="4">PSNR(dB) AdaBN Conditional IN AdaFM-Net</cell></row><row><cell>Set5 ×3</cell><cell>34.04</cell><cell>33.53</cell><cell>34.34</cell></row><row><cell>×4</cell><cell>28.70</cell><cell>31.30</cell><cell>32.00</cell></row><row><cell>LIVE1 q80</cell><cell>38.29</cell><cell>36.99</cell><cell>38.81</cell></row><row><cell>q10</cell><cell>27.61</cell><cell>28.89</cell><cell>29.35</cell></row><row><cell>CBSD68 σ15</cell><cell>33.83</cell><cell>31.33</cell><cell>34.10</cell></row><row><cell>σ75</cell><cell>19.68</cell><cell>24.15</cell><cell>26.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Adaptation results. The PSNR distances within 0.2 dB are shown in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Adaptation in Super Resolution</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">range1</cell><cell></cell><cell cols="2">range2</cell><cell></cell><cell>direction</cell></row><row><cell></cell><cell cols="10">×2 → ×3 ×3 → ×4 ×4 → ×5 ×5 → ×6 ×2 ⇒ ×4 ×3 ⇒ ×5 ×4 ⇒ ×6 ×3 ← ×4 ×2 ⇐ ×4</cell></row><row><cell cols="2">Set5</cell><cell cols="2">34.34</cell><cell>32.13</cell><cell>30.26</cell><cell>28.74</cell><cell>32.13</cell><cell>30.26</cell><cell>28.74</cell><cell>34.34</cell><cell>37.84</cell></row><row><cell cols="2">AdaFM-Net</cell><cell cols="2">33.98</cell><cell>32.00</cell><cell>30.16</cell><cell>28.73</cell><cell>31.66</cell><cell>29.98</cell><cell>28.61</cell><cell>34.11</cell><cell>37.11</cell></row><row><cell cols="2">PSNR distance</cell><cell></cell><cell>0.36</cell><cell>0.13</cell><cell>0.10</cell><cell>0.01</cell><cell>0.47</cell><cell>0.28</cell><cell>0.13</cell><cell>0.23</cell><cell>0.73</cell></row><row><cell cols="2">DIV2K100</cell><cell cols="2">32.35</cell><cell>30.37</cell><cell>29.04</cell><cell>28.10</cell><cell>30.37</cell><cell>29.04</cell><cell>28.10</cell><cell>32.35</cell><cell>36.00</cell></row><row><cell cols="2">AdaFM-Net</cell><cell cols="2">32.07</cell><cell>30.28</cell><cell>29.02</cell><cell>28.09</cell><cell>30.01</cell><cell>28,88</cell><cell>28.00</cell><cell>32.14</cell><cell>35.13</cell></row><row><cell cols="2">PSNR distance</cell><cell></cell><cell>0.28</cell><cell>0.09</cell><cell>0.02</cell><cell>0.01</cell><cell>0.36</cell><cell>0.16</cell><cell>0.10</cell><cell>0.21</cell><cell>0.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">range</cell><cell></cell><cell>direction</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">DeJPEG 80→60 80→40 80→20 80→10</cell><cell>80←10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LIVE1</cell><cell cols="2">36.00</cell><cell>34.34</cell><cell>31.93</cell><cell>29.55</cell><cell>38.81</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">AdaFM-Net 35.98</cell><cell>34.29</cell><cell>31.81</cell><cell>29.35</cell><cell>37.77</cell><cell></cell><cell></cell><cell></cell></row><row><cell>distance</cell><cell>0.02</cell><cell></cell><cell>0.05</cell><cell>0.12</cell><cell>0.20</cell><cell>1.04</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Denoising 15→25 15→35 15→50 15→75</cell><cell>15←75</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CBSD68 31.44</cell><cell>29.82</cell><cell>28.20</cell><cell>26.49</cell><cell>34.10</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">AdaFM-Net 31.43</cell><cell>29.78</cell><cell>28.13</cell><cell>26.35</cell><cell>33.42</cell><cell></cell><cell></cell><cell></cell></row><row><cell>distance</cell><cell>0.01</cell><cell></cell><cell>0.04</cell><cell>0.07</cell><cell>0.14</cell><cell>0.68</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Table 5. Adaptation results of DeJPEG and Denoising. The PSNR</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">distances within 0.2 dB are shown in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<idno>abs/1603.04779</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR) workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="63" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
