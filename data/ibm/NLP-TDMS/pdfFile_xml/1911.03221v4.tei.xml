<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dreem Open Datasets: Multi-Scored Sleep Datasets to compare Human and Automated sleep staging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Guillot</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Sauvet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">H</forename><surname>During</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Thorey</surname></persName>
						</author>
						<title level="a" type="main">Dreem Open Datasets: Multi-Scored Sleep Datasets to compare Human and Automated sleep staging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Automated Sleep Stage classification</term>
					<term>deep learn- ing</term>
					<term>PSG</term>
					<term>EEG</term>
					<term>open datasets</term>
					<term>inter-rater agreement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sleep stage classification constitutes an important element of sleep disorder diagnosis. It relies on the visual inspection of polysomnography records by trained sleep technologists. Automated approaches have been designed to alleviate this resource-intensive task. However, such approaches are usually compared to a single human scorer annotation despite an interrater agreement of about 85 % only. The present study introduces two publicly-available datasets, DOD-H including 25 healthy volunteers and DOD-O including 55 patients suffering from obstructive sleep apnea (OSA). Both datasets have been scored by 5 sleep technologists from different sleep centers. We developed a framework to compare automated approaches to a consensus of multiple human scorers. Using this framework, we benchmarked and compared the main literature approaches to a new deep learning method, SimpleSleepNet, which reach state-of-the-art performances while being more lightweight. We demonstrated that many methods can reach human-level performance on both datasets. SimpleSleepNet achieved an F1 of 89.9 % vs 86.8 % on average for human scorers on DOD-H, and an F1 of 88.3 % vs 84.8 % on DOD-O. Our study highlights that stateof-the-art automated sleep staging outperforms human scorers performance for healthy volunteers and patients suffering from OSA. Considerations could be made to use automated approaches in the clinical setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S LEEP has a crucial impact in human health. Sleep disorders are a common public health issue. For instance, in the US, studies have shown that millions of people are affected <ref type="bibr" target="#b0">[1]</ref>. Polysomnography (PSG) is the gold standard for the diagnosis of highly prevalent sleep disorders such as obstructive sleep apnea (OSA). It consists of recording various biophysiological signals such as electroencephalogram (EEG), electrooculogram (EOG), electromyogram (EMG), and can include breathing and cardiac signals. Sleep stage classification consists of the visual inspection and classification of 30seconds epochs of PSG by sleep technologist. The output of this process is the hypnogram, the diagram of sleep stages throughout the night. It is a systematic and valuable preliminary step in performing a diagnosis. Sleep stages are labeled by sleep technologist following the American Association of Sleep Medicine (AASM) rules <ref type="bibr" target="#b1">[2]</ref>. These rules set out 5 stages, based on the various waveforms observed on each A. Guillot and V. <ref type="bibr">Thorey</ref>  signal of the PSG: wake, rapid eye movement (REM), non-REM sleep stage 1 (N1), 2 (N2) and 3 (N3). It typically takes a sleep technologist 30 minutes to an hour to perform sleep staging on a whole record, i.e. about one thousand 30-second epochs, making it time-consuming and expensive. Another important aspect of sleep staging is the relatively low interrater agreement. Indeed, by definition, the AASM rules act as guidelines but do not fully characterize the natural variability that a PSG signal can measure. Hence, a study conducted on the AASM Inter-scorer Reliability dataset shows an average inter-rater agreement of 82.6% using sleep stages from more than 2,500 experimented sleep scorers <ref type="bibr" target="#b2">[3]</ref>. Agreement varies between sleep stages with in decreasing order: 90.5 % for REM, 85.2 % for N2, 84.1 % for Wake and only 67.4 % for N3 and 63.0 % on N1. Importantly, this agreement also varies depending on patient, sleep disorders and across sleep centers <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b2">[3]</ref>.</p><p>Algorithmic approaches have been developed to automatize the process. They are composed of two steps: feature extraction from raw signals and then classification into sleep stages. Among the automated sleep staging methods, we distinguish two main categories: the expert approaches and the deep learning approaches. An expert approach relies on hand-crafted feature extraction followed by a learnt classifier. On the other hand, a deep learning approach learns both the features and the classifier from example epochs.</p><p>Numerous studies have focused on expert approaches to classify sleep stages. Spectral and temporal features are computed on raw EEG signals <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref> or on multimodal PSG signals <ref type="bibr" target="#b6">[7]</ref>. A classifier, like a random forest or a multilayer perceptron, is then trained on top of these features to estimate the current sleep stage. Most recent approaches take into account successive sleep epochs and feed their features to a recurrent neural network (RNN) to model the time dynamics of sleep <ref type="bibr" target="#b7">[8]</ref>.</p><p>Following the general trend in machine learning, deep learning has also brought new feature extraction methods for automated sleep staging. In <ref type="bibr" target="#b8">[9]</ref> a convolutional neural network (CNN) extracts relevant features from a single channel raw EEG signal. <ref type="bibr" target="#b9">[10]</ref> strongly improves the previous approaches by dividing the CNN into two branches to extract features at different scales. A RNN is added after the CNN to model the dependency between contiguous sleep epochs. <ref type="bibr" target="#b10">[11]</ref> proposed a lighter CNN which can deal efficiently with multimodal data while having fewer parameters than previous methods. <ref type="bibr" target="#b11">[12]</ref> [13] <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b14">[15]</ref> [16] <ref type="bibr" target="#b16">[17]</ref> have all reported state-of-theart performances on various sleep staging datasets with CNN. These models (excluding <ref type="bibr" target="#b10">[11]</ref>) have millions of parameters which increases computational cost and the risk of overfitting while lowering data efficiency. Most of these models are applied on a single signal from the PSG which may limit the accuracy of the estimated sleep stages. <ref type="bibr" target="#b17">[18]</ref> [19] <ref type="bibr" target="#b19">[20]</ref> introduce a different approach, the raw PSG signals of a sleep epoch are transformed into a short term Fourier transform and processed either by a 1D CNN or by a RNN followed by an attention layer <ref type="bibr" target="#b20">[21]</ref>. To model temporal dependencies <ref type="bibr" target="#b17">[18]</ref> feeds the succession of encoded sleep epochs into a second RNN. State-of-the-art performance are reached on the publicly available MASS dataset <ref type="bibr" target="#b21">[22]</ref>.</p><p>Most automated approaches are trained and evaluated on a single manual sleep scoring making it difficult to evaluate how they actually perform considering the low inter-rater agreement. One notable exception, <ref type="bibr" target="#b16">[17]</ref> deals with the issue of interrater variability using annotations from 6 sleep technologists on a subset of training records. However the multiple sleep staging annotations are not currently publicly available. Another challenge in the evaluation and comparison of automated approaches is that no shared dataset has made a consensus for benchmarking different approaches when it has been shown that performance can greatly vary across datasets <ref type="bibr" target="#b22">[23]</ref>. In this study we introduce two publicly available datasets; DOD-H (Dreem Open Dataset -Healthy) and DOD-O (Dreem Open Dataset -Obstructive). DOD-H is built from recordings from 25 healthy adult volunteers. DOD-O is built from recordings from 55 patients suffering from obstructive sleep apnea (OSA). Both datasets were scored by 5 experienced sleep technologists across 3 different sleep centers. Using these datasets we propose a methodology inspired from <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b23">[24]</ref> to evaluate a sleep stage algorithm against multiple sleep technologists, in order to simulate a real-life setting. This evaluation framework is available at http://github.com/Dreem-Organization/dreemlearning-evaluation together with the scores from the various sleep technologists and the PSG data for both DOD-O and DOD-H. Using this framework we benchmark and compare several approaches from the literature <ref type="bibr" target="#b9">[10]</ref> [18] <ref type="bibr" target="#b10">[11]</ref> [25] <ref type="bibr" target="#b8">[9]</ref>. We also introduce and benchmark a new deep learning method, SimpleSleepNet, inspired by SeqSleepNet <ref type="bibr" target="#b17">[18]</ref>, DeepSleepNet <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref>. First, we compare the performance of human scorers and recent literature models (including SimpleSleep-Net) on DOD-H and DOD-O. Then, SimpleSleepNet is used to study the impact on sleep staging performance of the following factors: temporal context, dataset size, number of input signals, size and complexity of the model. The benchmark code is publicly available at https://github.com/Dreem-Organization/dreem-learning-open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATERIALS AND METHODS</head><p>A. Datasets 1) Dataset 1: healthy patients: Dataset 1 was collected at the French Armed Forces Biomedical Research Institute's (IRBA) Fatigue and Vigilance Unit (Bretigny-Sur-Orge, France) from 25 volunteers. Volunteers were recruited without regard to gender or ethnicity from the local community via flyers. Volunteers were healthy sleepers without sleep complaints between the ages of 18 and 65, their PSG results confirmed the absence of a sleep disorder. More details and exclusion criteria can be found in <ref type="bibr" target="#b23">[24]</ref>. Demographics are summarized in <ref type="table" target="#tab_2">Table I</ref>. All participants received financial compensation commensurate with the burden of study participation. The study was approved by the Committees of Protection of Persons (CPP), declared to the French National Agency for Medicines and Health Products Safety, and carried out in compliance with the French Data Protection Act and International Conference on Harmonization (ICH) standards and the principles of the Declaration of Helsinki of 1964 as revised in 2013. The data used for this study is composed of 12 EEG derivations (C3/M2, F4/M1, F3/F4, F3/M2, F4/O2, F3/O1, FP1/F3, FP1/M2, FP1/O1, FP2/F4, FP2/M1, FP2/O2), 1 EMG signal, left and right EOG signals and 1 electrocardiogram (ECG) sampled at 250 Hz recorded with a Siesta PSG devices (Compumedics). Each record was scored independently by 5 experienced sleep technologists from 3 different sleep centers following the current AASM Scoring Manual and Recommendations (Version 2.5, Released April 2, 2018). This is based off original 2007 AASM Scoring Manual <ref type="bibr" target="#b1">[2]</ref>. All scorers are registered Polysomnography Technologists, with over 5 years of clinical / scoring experience.</p><p>2) Dataset 2: patients with OSA: The dataset 2 was collected at the Stanford Sleep Medicine Center and consists of PSG recordings from 55 patients (Clinical trial number NCT03657329). Patients were included in the study based on clinical suspicion for sleep-related breathing disorder. Individuals with a diagnosed sleep disorder different from OSA were excluded from this study. Exclusion criteria can be found in <ref type="bibr" target="#b25">[26]</ref>. Demographics are given in <ref type="table" target="#tab_2">Table I</ref>. All trial participants gave their informed written consent prior to participation. They received compensation for their participation. The data used for this study is composed of 8 EEG derivations (C3/M2, C4/M1, F3/F4, F3/M2, F4/O2, F3/O1, O1/M2, O2/M1), 1 EMG derivation, left and right EOG signals and 1 electrocardiogram (ECG) sampled at 250 Hz recorded with a Somno HD PSG devices (Somnomedics). Each record was scored independently by 5 experienced sleep technologists from 3 different sleep centers following the current AASM Scoring Manual and Recommendations (Version 2.5, Released April 2, 2018). This is based off original 2007 AASM Scoring Manual <ref type="bibr" target="#b1">[2]</ref>. All scorers are registered Polysomnography Technologists, with over 5 years of clinical / scoring experience.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DOD-H</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation in the context of multi-scoring</head><p>The process of evaluating the performance of a human scorer, or an automated approach, against a consensus of multiple human scorers is inspired from <ref type="bibr" target="#b16">[17]</ref> and has been presented in our previous work <ref type="bibr" target="#b23">[24]</ref>. The goal is to use reduce the known inter scorer variability for sleep stage classification by using a majority vote from the sleep experts. In this section, we highlight the main aspects and differences.</p><p>Soft-Agreement: When taking a majority vote between sleep experts, ties can occur. In case of ties, we choose to use the label of the most reliable scorer. The most reliable scorer will be the one that is the most in agreement with all the other scorers on a record. To find this scorer, we defined Soft-Agreement in <ref type="bibr" target="#b23">[24]</ref> as follows. Notations : Let y j ∈ 4 T be the sleep staging associated to scorer j taking values in {0, 1, 2, 3, 4} standing respectively for Wake, N1, N2, N3 and REM with size T epochs. Let N be the number of scorers. Letŷ j ∈ {0, 1} 5×T be the one hot encoding of y j .</p><p>To evaluate a sleep staging of one record against multiple sleep staging methods, we introduced in [24] a Soft-Agreement metric defined as:</p><formula xml:id="formula_0">Soft-Agreement j = 1 T T t=0ẑ j [y j ]</formula><p>with:ẑ</p><formula xml:id="formula_1">j [t] = N i=1 i =jŷ i [t] max N i=1 i =jŷ i [t]</formula><p>∀t This metric measures how close the sleep staging of interest is from all the other scorers sleep staging. It values 1 if the sleep staging of interest is always in agreement with the majority vote (or one of the majority votes in case of ties).</p><p>Other metrics: To merge multiple sleep stagings into a single consensus sleep staging, we simply take the majority vote on each 30second epoch. When a tie occurs on a specific epoch, we take the sleep stage scored on the sleep staging with the highest Soft-Agreement on the record. This differs from our previous work <ref type="bibr" target="#b23">[24]</ref> where we used the scorer with the highest softagreement over all the records of the dataset, hence inducing a dependency to the dataset. We also compute a weight between 0 and 1 for each epoch based on how many scorers voted for the consensus sleep staging epoch. These weights are used to balance the importance of each epoch in the computation of each of the following metrics.</p><p>To measure agreement between two sleep stagings on a specific record, we measure F1 -score = 2 * P r * Re P r+Re with P r = T P T P +F P and Re = T P T P +F N , and TP, FP, and FN are the number of true positives, false positives, and false negatives, respectively. The score is computed per-class, one class against the others, and averaged taking the proportion of each class into account. We also provide Accuracy, as the ratio of correct answers and Cohen's Kappa, κ = pj −pe 1−pe where p j is the relative observed agreement and p e is the hypothetical probability of chance agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SimpleSleepNet</head><p>SimpleSleepNet is a new automated sleep staging model based on recent advances in the field. The initial stage in SimpleSleepNet is inspired by <ref type="bibr" target="#b17">[18]</ref>. Where it differs from the latter is in its use of a channel-wise dropout. Moreover, it replaces the filter bank with a linear layer, recombines the EEG derivations using a linear layer (an approach inspired by <ref type="bibr" target="#b10">[11]</ref>), and omits the norm from the GRU. The second stage, which models epoch dependencies, is inspired by <ref type="bibr" target="#b9">[10]</ref> but differs from it in that it uses positional embedding. SimpleSleepNet uses fewer parameters than the other models by reducing the size of the hidden layers. In this section, a comprehensive description of each module of SimpleSleepNet is presented. <ref type="figure" target="#fig_1">Figure 1</ref> summarizes the overall architecture of the network.</p><p>1) Spectrogram: The short-term Fourier transform (STFT) is computed on the preprocessed signals of each of the epochs. Preprocessing is defined in section III-B. Each epoch is in R C,30·f s where C denotes the number of channels and f s the signal frequency. During training, signals are randomly set to zero before computing the STFT with a probability p kill to reduce overfitting.</p><p>Similarly to <ref type="bibr" target="#b17">[18]</ref>, the STFT is computed over 256 points of signal every one second with a Hanning window. The logpower of the STFT is taken and clipped between -20 and 20. Each epoch is thus represented by a time-frequency picture S ∈ R C,T,N where C is the number of signals, T = 28 the number of time-steps and N = 129 the number of frequency bins. The clipped STFT is 0-mean 1-variance normalized signal-wise independently of the timestep. Mean and variance are computed over all the training records.</p><p>2) Signals and frequencies reduction: First the N frequency bins are linearly reduced into n ≤ N filters, and the C input signals are linearly reduced into c ≤ C signals. Their weights matrices are respectively in R n,N and R c,C and their bias in R n and R c . The linear projections are applied respectively along the frequencies and signals axis to project the initial spectrogram from R C,T,N into R c,T,n The two projections are applied independently. Dropout is then applied with a probability p 1 .</p><p>3) GRU with attention: The recombined signals are reshaped into R T,c.n and fed to a bidirectional Gated Recurrent Unit (GRU) <ref type="bibr" target="#b26">[27]</ref> with m 1 hidden units to build a representation in R T,2.m1 . Dropout is applied after the GRU with the same probability p 1 . Then, the output of the GRU is fed into an attention layer. The attention layer is implemented as presented in <ref type="bibr" target="#b20">[21]</ref> with context size m ctx . The attention layer reweights and sums the GRU hidden states along the time axis to build a vector representation of the current sleep epoch in  = cos( t.π l ) for l in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">60,</ref><ref type="bibr">90,</ref><ref type="bibr">120,</ref><ref type="bibr">150</ref>]. The con-</p><formula xml:id="formula_2">catenation [i epoch t , i cycle t,30 , . . . , i cycle t,150</formula><p>] ∈ R 6 is then projected, using a linear layer with weights and bias in R 6,6 and R 6 , to build i t . Then, i t is concatenated with the output of the attention layer to compute the current epoch representation a t ∈ R 2.m1+6 5) Sequence encoder and classifier: Given a temporal context k and a central epoch t, the epochs a t−k , ..., a t+k are fed to a two layers bidirectional GRU with skip-connections (SkipGRU) and m 2 hidden units. The SkipGRU is similar to the sequence encoder of DeepSleepNet <ref type="bibr" target="#b9">[10]</ref> with additional intermediary skip connections. Given its input size 2.m 1 + 6, the SkipGRU has a weights matrix W skip ∈ R m2,2.m1+6 and a bias vector b skip ∈ R m2 and follows:</p><formula xml:id="formula_3">h t = 1 2 [GRU (a t , h t−1 ) + W skip a t + b skip ]</formula><p>The bidirectional SkipGRU is built by concatenating the outputs of a forward and of a backward SkipGRU. Dropout is applied on h t with a probability p 2 . We denote h t−k , . . . , h t+k ∈ R 2·k2 its outputs. This sequence is fed to a final softmax classification layer which outputs the sleep stages probabilitiesπ</p><formula xml:id="formula_4">(t) −k , . . . ,π (t) k ∈ R 5</formula><p>6) Loss function: Since SimpleSleepNet outputs several sleep stages estimates instead of a single one, the loss has to be modified accordingly (similarly to <ref type="bibr" target="#b17">[18]</ref>). Let S = [s t−k , . . . , s t+k ] be the input sequence of the spectrograms from 2k + 1 sleep epochs. For the epoch t, the loss is defined as</p><formula xml:id="formula_5">L(S, y) = − 1 2k+1 k i=−kŷ t+i · log(π (t) t+i (S))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation</head><p>At evaluation time, the multiple available predictions for an epoch are aggregated following <ref type="bibr" target="#b17">[18]</ref>: given an epoch t and a temporal context k, the aggregated sleep stage probabilities is the geometric</p><formula xml:id="formula_6">meanπ (t) = exp 1 2k+1 i=k i=−k log(π (t+i) t ) and the predicted sleep stage used for evaluation isỹ (t) = argmax j∈[[0,5]]π (t) j III. EXPERIMENTS A. Baselines</formula><p>To benchmark the current state-of-the-art in automated sleep staging on both DOD-O and DOD-H, we selected recent approaches from the literature reporting good performances on publicly available datasets. These approaches were reimplemented in Pytorch <ref type="bibr" target="#b28">[29]</ref>, for reproducibility the code is publicly available in the following repository: https://github.com/Dreem-Organization/dreem-learningopen. The presented approach SimpleSleepNet is also included in the benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark setup</head><p>Soft-Agreement was computed for all scorers on all records. Following II-B we used these values to build a consensus hypnogram for every record. The human scorers are individually evaluated against the consensus hypnograms built from the four others. The automated approaches are trained and evaluated with the consensus hypnograms built from the four overall best scorers in terms of overall best Soft-Agreement. On DOD-H the 5 human scorers had an overall Soft-Agreement of respectively 0.87, 0.91, 0.92, 0.84 and 0.92 so scorers 1, 2, 3 and 5 are selected. On DOD-O, the 5 human scorers had an overall Soft-Agreement of 0.88, 0.87, 0.88, 0.88 and 0.91 respectively, so scorers 1, 2, 4 and 5 are selected. In practice, ties occurred on average for 7.3 % of the epochs in DOD-H and 9.9 % of the epochs in DOD-O. The same preprocessing is used for all the models, a band-pass filter is applied between [0.4, 18]Hz to remove residual PSG noise, then, the signals are linearly resampled at f s = 100Hz to reduce the training computational cost. Each signal is then clipped and divided by 500 to remove extreme values. Predictions on each epoch are computed using a temporal context of past and future epochs (see section II-D). To ensure having points for the very first and last epochs of the record, a zero-padding with at least the same length as the temporal context is added at the start and end of each record. The models are trained using back propagation with the Adam optimizer and a learning rate of 0.001, momentum parameters β 1 = 0.9 and β 2 = 0.999 and a batch size of 32. All the models are trained for a maximum of 100 epochs with early stopping. The training was stopped when validation accuracy stopped improving for more than 15 epochs. The model with the best validation accuracy is used to evaluate the model. The temporal context is set to 21 for all the models, increased temporal context has been shown to improve performances in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b18">[19]</ref>. Hence, using the same temporal context ensures the benchmark fairness. Furthermore for each model from the literature, several set of hyper-parameters were evaluated on DOD-O and DOD-H, the best run is reported for these models. On DOD-H the models were evaluated in a leave-one-out way: 18 records are used for training, 6 are kept for validation and 1 is kept to test the model. On DOD-O the models were evaluated in a 10-folds validation way: 37 records are used for training, 12 are used for validation and 6 records to evaluate the model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Benchmark on DODO and DODH</head><p>The overall, best and worst performances of the five scorers are reported in <ref type="table" target="#tab_2">Table III</ref>   <ref type="figure">Figure 2</ref> shows the scorers confusion matrices on both dataset, most of the errors involve N1 being mistaken for WAKE or N2 and N3 being mistaken for N2.</p><p>The performances of the automated approaches are also given in <ref type="table" target="#tab_2">Table III</ref>. SimpleSleepNet shows the best performance on both datasets for the considered metrics when compared to both humans and other approaches. On DOD-H, SimpleSleep-Net is better than the best scorer and shows a lower SD with an F1 of 89.9 ± 4.1 %. On DOD-O, it also performs better but with a slightly higher SD than the best scorer with an F1 of 88.3 ± 9.0 %. With the exception of <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b8">[9]</ref>, every model performs better with a much lower variability on DOD-H than on DOD-O. Except <ref type="bibr" target="#b8">[9]</ref>, most models have F1 scores which are on par with the scorers' average and above the worst scorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. SimpleSleepNet ablation study</head><p>To assess the importance of each of the modules of the architecture of SimpleSleepNet, ablated models were trained on both datasets. While technically not being an ablation of the model itself, the influence of the preprocessing step is assessed in No filtering where the filtering is removed. In No channel dropout the channel dropout is removed (p kill = 0). Then, we evaluate the effects of the blocks of the epoch encoder.</p><p>In No frequency reductions the linear frequency reduction is removed, in Filter bank it is replaced by a filter-bank <ref type="bibr" target="#b17">[18]</ref>, in No channel recombination the linear channel recombination is removed and in No attention the attention layer is replaced by an average-pooling layer. The architecture of the sequence encoder is analyzed by removing the positional embedding in No positional embedding, by using a single layer in the GRU encoder in Single GRU layer, and by removing the skipconnection in No skip connection.</p><p>The results are shown in <ref type="table" target="#tab_2">Table IV</ref>. Removing the frequencies reduction layer or the channel dropout are the most impacting ablations on both datasets. Other ablations do not significantly impact the performance on DOD-H. However, on DOD-O, the filtering and the filter bank greatly impact the performance. Other ablations also demonstrate the slight improvement provided by each layer on DOD-O. Overall, the full model presents the best ranking on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Influence of the experimental setup 1) Model size:</head><p>To assess the influence of the model size on performances, two variants of SimpleSleepNet are evaluated SimpleSleepNet-Small and SimpleSleepNet-Large. SimpleSleepNet-Small (resp. SimpleSleepNet-Large) has hidden units of size m 1 = m 2 = 12 (resp. m 1 = m 2 = 50) in both GRU and the attention layer context size is set to   Increasing the model size increases SimpleSleepNet performances both on DOD-O and DOD-H as shown in <ref type="table" target="#tab_8">Table V</ref>. On DOD-H F1 increases by 0.5 % for the large model and is reduced by 0.6 % for the small model. On DOD-O, F1 is increased by 0.7 % with the large model and reduced by 1.1 % when using the small model. On both datasets, using larger models reduces variance significantly.</p><p>2) Performances on a single EEG derivation: We assess the performance of SimpleSleepNet on a the F4-O2 derivation on both datasets in <ref type="table" target="#tab_2">Table VI</ref>. Performances are significantly   3) Size of the training set: Labelling records is a costly and long process, hence having data efficient models is crucial. To assess the data efficiency of SimpleSleepNet, the model was trained with training set of increasing size k (1 to 19 for the DOD-H dataset, and 1 to 40 for the DOD-O dataset). For a given training repetition, the split is built in the following way for DOD-H (resp. DOD-O), first 3 (resp. 5) records are randomly sampled for the validation set and 3 (resp. 5) records are sampled for the test set. Out of the 19 (resp. 45) remaining records, the training set of size k is built with the first k records. This experiment is repeated 20 times. The mean F1 and the 95 % confidence interval on the test set are computed over the 20 experiments are presented <ref type="figure" target="#fig_3">Figure 3</ref>. Human level performances are reached on both datasets with less than 20 records, DOD-O has a steeper learning than DOD-H. On DOD-H the F1 reaches a plateau where incremental gains are low with 12 records, while 25 records are required to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Direct transfer learning</head><p>In a real-life, clinical setting, one may wish to train a staging model of a source dataset and to use it on another unlabelled dataset. To assess the transferability of SimpleSleepNet, we train and validate it on DOD-H (resp. DOD-O) and test it on DOD-O (resp. DOD-H). The experiment is repeated 20 times, for each repetition, 70 % of the records from the source dataset are randomly selected for training and the remaining 30 % for validation. All the records of the target dataset are used to test the model performance.  G. Benchmark on external dataset 1) MASS SS3 <ref type="bibr" target="#b21">[22]</ref>: The MASS SS3 cohort is composed of 62 nights from healthy subjects, done with a full PSG montage (20 scalp EEG,2 EOG, 3 EMG and 1 ECG) and manually scored by a sleep expert according to the AASM standard. The models were trained on the C4-O1, F4-EOG Left, F8-Cz, on the average of the two EOGs and on the average of EMG-Chin1 and EMG-Chin2 which are available for all records and frequently used by the models evaluated on MASS. We used the same preprocessing and training parameters as in the previous section III-B. The models are evaluated in a 31-folds validation way (as in <ref type="bibr" target="#b9">[10]</ref>).</p><p>2) Sleep EDF <ref type="bibr" target="#b30">[31]</ref>: The Sleep EDF database contains 197 nights from 106 subjects, amongst these nights, 153 are from 82 subjects without any sleep-related medications (SC study) and 44 are from subjects with trouble falling asleep (ST study). 22 of the 44 nights are done after a Temazepam intake. We consider two splits, S-EDF-20 with the subjects 0 to 19 from the SC study and S-EDF-Extended will all the subjects from the database. Similarly to <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b31">[32]</ref>, we only considered the epochs in-between 30 minutes before the first non-wake epoch epoch and 30 minutes after the last non-wake epoch. The models are trained and evaluated using a 20-folds CV on S-EDF-20 and 10-folds CV on S-EDF-Extended. Records from a subject are in the same fold. The models are trained on the FPZ-Cz, Pz-Oz and the EOG derivation without further processing.</p><p>3) Results: The results are presented <ref type="table" target="#tab_2">Table VIII</ref>. Our implementation of the literature models reaches equal or improved performance when compared to the original publications. This improvement can be explained by three different reasons. First, we used more derivations than in the original papers. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b9">[10]</ref> used a single derivation and <ref type="bibr" target="#b17">[18]</ref> threederivations. Secondly, the prediction from a single epoch is the average of the prediction over the temporal context (as in <ref type="bibr" target="#b17">[18]</ref>, see II-D). Finally, our preprocessing is more aggressive than in the original paper. These differences concern only input and output data, not the models themselves. This ensure that all the models are compared in the same conditions of input, preprocessing and prediction. SimpleSleepNet achieve the best performance on Sleep EDF. On MASS, DeepSleepNet shows the best Macro-F1 score closely followed by SimpleSeepNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>DOD-H and DOD-O multiple scoring highlight the previously described and relatively high inter-rater variability regarding sleep staging. This confirms the need for automated sleep staging approaches to train and compare with a consensus of human scorers instead of a single human scorer for a more realistic evaluation of performance. The Soft-Agreement and the methodology presented allow to handle multiple scorers and especially situations when a tie between scorers occurs. Another solution could be using yet more scorers to reduce ties occurrence and improve the fairness of the built consensus.</p><p>Due to an increased sleep fragmentation, manual sleep staging is more difficult on patients with OSA than healthy subjects. This is also true for most automated approaches. Indeed, the accuracy is lower and presents higher variance on DOD-O than on DOD-H. There are also more ties on  DOD-O than DOD-H. This is in agreement with <ref type="bibr" target="#b16">[17]</ref> where models accuracy drops by 9 % on narcoleptic subjects vs healthy subjects and with <ref type="bibr" target="#b32">[33]</ref> where the scorers reliability was much higher on healthy subjects than on those with OSA. Besides, the training requires more recordings to reach human performance on DOD-O than on DOD-H. All those elements suggest that the inter-subjects variability is higher within DOD-O than within DOD-H. Yet, interestingly, transfer learning from DOD-O to DOD-H is much more effective than the other way around. This implies that data acquired from patients suffering from OSA contains information related to healthy sleep as well as information specific to OSA. This also shows that although SimpleSleepNet reaches a better F1 on DOD-H than on DOD-O, the model trained on DOD-O is much better in its generalization capacity than the one trained on DOD-H. These analyses could be extended to datasets with other sleep-related issues to see how much they impact the performance of human and automated sleep staging. This also suggests that a dataset containing high inter-subject variability, for instance with a mix of both abnormal and normal sleep, would probably lead to better models in terms of their ability to generalize. This is also highlighted in <ref type="bibr" target="#b16">[17]</ref>.</p><p>The transfer learning experiment also highlights a practical limitation regarding the usability of such automatic method outside of the scope of the same population of patients or/and device than the one on which it has been trained on. This is also discussed in <ref type="bibr" target="#b22">[23]</ref>. In practice, this limits the use of such automatic sleep staging method in a clinical setup. Training models on a cohort of several patients with a mix of both abnormal and normal sleep recorded on different PSG devices and scored by different scorers would greatly improve the generalization of sleep staging models. However, the use of different devices implies dealing with possibly different modalities and missing signals, which is a problem that has to our knowledge not been tackled yet.</p><p>SimpleSleepNet, DeepSleepNet and SeqSleepNet outperform the average human scorer on both DOD-O and DOD-H. Most other automated approaches perform with an accuracy close to human scorers. The confusion matrix also shows similar pattern of mistakes between humans and SimpleSleepNet. Given a few annotated records, automated sleep staging could reach similar performances to human scorers in a clinical setting if the data are acquired with a consistent PSG montage and patient typology. This is often the case in a typical sleep clinic setting. That being said, an interesting direction of research would be to create a model able to adapt to various PSG montage without fine-tuning or weight modifications.</p><p>On external datasets, SimpleSleepNet, DeepSleepNet, and SeqSleepNet also show the best performances. However, these datasets were scored by a single expert. Inter-rater variability prevents us from drawing strong conclusions regarding the absolute performance of the various models on these datasets. Specifically, the models could be overfitting on human expert scoring.</p><p>We observe that most benchmarked methods using datadriven feature extraction perform better than the expert feature extraction approach. This is especially true on DOD-O and SleepEDF-Extended which present a higher level of variability, suggesting a better ability for such deep learning models to capture relevant information in complex data like abnormal sleep.</p><p>SimpleSleepNet outperforms the best human scorer and all other sleep staging models on DOD-O and DOD-H. It is also among the best-ranked models on external datasets. It uses significantly fewer parameters than other approaches. The presented ablation study shows that the various building blocks of SimpleSleepNet allow reaching the best performance on DOD-O. SimpleSleepNet reaches close-to-human performance with only a few (∼10) recordings, suggesting that sleep stage classification is a relatively simple problem in terms of data quantity needed to reach satisfactory performance. The temporal context and number of signals also seem to play a minor role in improving performance.</p><p>The results provided in this study are available with both data and code for reproducibility. It should be noted that the benchmarked automated approaches were all reimplemented. The performances of our implementation were validated on the MASS SS3 and SleepEDF datasets with performance similar or above the original implementations. Furthermore, to ensure the fairness of the benchmark, every method was tuned to provide good results on the datasets of this study. All reported results are from a single run, rerunning the experiments might result in slightly different results due to randomness and variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we introduced two open multi-scored sleep staging datasets with 25 from healthy subjects and 55 nights patients suffering from OSA. We proposed a methodology for evaluation against multiple human scorers. We showed the relevance of a multi-scored sleep dataset to assess how automated sleep staging performs in a clinical setting. We demonstrated that recent automated sleep staging performances are often onpar with the average human scorer, and that the best automated sleep staging are better than the best human scorer. We also introduced a new efficient sleep staging model, SimpleSleepNet, which outperforms previous state-of-the-art models and human scorers on both datasets and on two frequently benchmarked datasets. Better understanding and quantification of the performance of such automated approaches could be a step toward a broader use of these approaches in sleep clinics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>R 2.m1 4) Positional embeddings: Positional embeddings have recently been used in Transformer architectures [28] to model time dependency. Here, positional embedding is used to include global context in the sequential modelling layer. The positional embedding of an epoch is composed of the scaled index epoch i epoch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>SimpleSleepNet overview diagram: h t−1 , h t−1 represent the hidden state from the previous epoch of the sequence and h t+1 , h t+1 the hidden state from the next epoch of the sequence. at is the embedding of the current epoch. the beginning of the night. Then i epoch t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 )</head><label>1</label><figDesc>Mixed neural network (Expert approach)<ref type="bibr" target="#b7">[8]</ref>: The Mixed neural network (MNN) computes aggregated features (average, median, maximum, minimum, standard deviation, entropy) on the raw signal. The aggregation is performed on the complete epoch and on sliding windows of 5 seconds with 3.5 seconds of overlap. Similarly, time-frequency features are computed using the Fourier transform over windows of 5 seconds with 3.5 seconds of overlap and on the complete epoch. The amplitude of the Fourier transform is summed over frequency bands of interest for sleep, general statistics are computed for each epoch and for each band and are used as additional features. The computed features are fed to a twolayer, fully-connected neural network (FCNN) with dropout and then to a bidirectional LSTM followed by a classification layer. The features are computed on the F4-M1 derivation on DOD-H dataset and on F4-O2 on DOD-O.2) Tsinalis et al.<ref type="bibr" target="#b8">[9]</ref>: Tsinalis et al.<ref type="bibr" target="#b8">[9]</ref> introduced the first CNN for sleep staging. The model takes 630 seconds of raw signals (which is equivalent to 21 sleep epochs) centered on the current epoch. The signal is fed to two successive convolution + pooling layers with Relu activations. The features are then flattened and fed to a two-layer FCNN followed by the classification layer. The network estimates the sleep stage of the central epoch. The parameters are those provided in the original paper. However, for a fair comparison with the other models, the net is trained on all the PSG signals instead of the single channel without any other architectural change.3) Chambon et al.<ref type="bibr" target="#b10">[11]</ref>: Chambon et al.<ref type="bibr" target="#b10">[11]</ref> built a convolutional model to handle multivariate and multi-modal signals. The model uses 630 seconds (21 sleep epochs) of signals as its input, the model classifies the central epoch. First a convolution of size 1 is applied, the convolution does not take into account the time and is only applied over the signals. This convolution models the dependencies between the different signals to learn virtual signals which are good representations of the original signals. Then a succession of two Convolution and Pooling layer blocks is applied on each virtual signal independently. Processing each signal independently reduces the overall complexity and increases the inference and training speed. The output of the CNN is flattened before being fed to a final classification layer. The parameters are the one used in the original paper, the net is trained on all the PSG signals. 4) DeepSleepNet<ref type="bibr" target="#b9">[10]</ref>: DeepSleepNet improves<ref type="bibr" target="#b8">[9]</ref> with a hierarchical model, first, each epoch is encoded, then the succession of the epochs is processed by a recurrent network to model temporal dependencies. Instead of having only one convolutional layer, each sleep epoch is encoded by two distinct convolutional networks with different filters and pooling sizes. The first network has smaller filter sizes and is focused on temporal information while the second network has a larger filter size and focuses on frequency information. The output of both networks are concatenated to build the representation of the epoch. To deal with the stage transition, a succession of 2 bidirectional LSTM with a skip-connection processes the sequence of encoded sleep epochs. The model is trained on all the signals. 5) SeqSleepNet<ref type="bibr" target="#b17">[18]</ref>: SeqSleepNet takes the spectrogram of the signal as the input, the number of Fourier bins is reduced with a learned frequency filter-bank which projects the original bins on a smaller frequency space. The reduced STFT is then fed to a bidirectional LSTM with recurrent batch-normalization<ref type="bibr" target="#b29">[30]</ref> followed by an attention layer. The attention layer reduces the temporal dimension and encodes the 30-second sleep epoch into a single vector. The encoded representations of consecutive sleep epochs are then fed to a bidirectional GRU, the output of the GRU is used by the classification layer to output the final sleep stage estimate. 6) SimpleSleepNet: The Fourier bins are projected on n = 30 filters and the original number of channels is kept (c = C).The dropouts probabilities p kill , p 1 , p 2 are set to 0.5. m 1 = m 2 = 25 hidden units are used in both the epoch encoder and the sequence encoder. The attention context size m ctx is also set to 25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Evolution of the F1 w.r.t the training set size on DOD-O (right) and DOD-H (left) dataset. lower compared with a model trained on the full montage, the single channel model F1 score is 3.9 % points lower on DOD-O and 3.3 % points lower on DOD-H. The model F1-score with single channel is still on par with the scorers average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Evolution of the F1 w.r.t the temporal context on DOD-H (right) and DOD-O (left) dataset. reach a plateau on DOD-O . The average scorer performance is reached with 7 records (resp. 15). In addition to the increased F1, the standard deviation of the test F1 strongly decreases with the number of training records. 4) Temporal context: We study the impact of the temporal context on the performance by training SimpleSleepNet on DOD-H and DOD-O with varying sizes of temporal context. The size of the temporal context is incrementally increased from 1 (no temporal context apart from the current epoch) to 21 (ten epochs before and after the current epoch). Results are presented Figure 4. Even with a single epoch, performances are decent on both dataset with a F1 of 85.5 ± 6.5 % on DOD-H and 83.0 ± 11.6 % on DOD-O. The F1 sensibly increases when the temporal context is increased from 1 to 7, then it plateaus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>are with the Algorithm Team, Dreem, Paris E. H. During is with the Center for Sleep Sciences and Medicine, Stanford University, Stanford, California, USA</figDesc><table /><note>F. Sauvet is with the French Armed Forces Biomedical Research Institute (IRBA), Fatigue and Vigilance Unit, Bretigny sur Orge, France; EA 7330 VIFASOM, Paris Descartes University, Paris, France</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>Demographics for DOD-H and DOD-O. More information can be found here [24] for DOD-H and [26] for DOD-O. All values are average across all subjects.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The number of parameters of each model and training time for one epoch on a Titan-X on DOD-O are given for referenceTable II.</figDesc><table><row><cell>Model</cell><cell cols="2"># parameters training duration (sec.)</cell></row><row><cell>SimpleSleepNet-Small</cell><cell>3.9 × 10 4</cell><cell>123</cell></row><row><cell>SimpleSleepNet</cell><cell>9.3 × 10 4</cell><cell>126</cell></row><row><cell>Chambon et al. [11]</cell><cell>1.2 × 10 5</cell><cell>367</cell></row><row><cell>Mixed NN [8]</cell><cell>2.0 × 10 5</cell><cell>9.98</cell></row><row><cell>SimpleSleepNet-Large</cell><cell>2.5 × 10 5</cell><cell>129</cell></row><row><cell>SeqSleepNet [18]</cell><cell>4.1 × 10 5</cell><cell>75.8</cell></row><row><cell>DeepSleepNet [10]</cell><cell>1.7 × 10 7</cell><cell>95.2</cell></row><row><cell>Tsinalis et al. (CNN) [9]</cell><cell>1.7 × 10 8</cell><cell>268</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Number of parameters and train time per epoch on a Titan X on the DOD-H dataset. 18 records are used for training and 6 for validation.</figDesc><table /><note>The order of magnitude and the ranking is the same on DOD-O.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III :</head><label>III</label><figDesc>Performance metrics of each of the baseline models. Average, best and worst human scorers performance are also given. The best (resp. worse) scorer is the scorer with the highest (lowest) F1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc></figDesc><table><row><cell>Performance metrics of SimpleSleepNet variants with smaller</cell></row><row><cell>(SimpleSleepNet-Small) and larger (SimpleSleepNet-Large) layer size than</cell></row><row><cell>the original models.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Performance metrics are compared when SimpleSleepNet is trained on the F4-02 derivation only vs when it is trained on all PSG channels. The Scorers (avg.) fromTable IIIis given for reference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VII :</head><label>VII</label><figDesc>Performance metrics of SimpleSleepNet when trained on DOD-O (resp. DOD-H) and evaluated on the other dataset DOD-H (resp. DOD-O). The models are trained and validated on 20 random splits of the source dataset and evaluated against the target dataset. The results of the experiment are shown in Table VII. When SimpleSleepNet is trained on DOD-O and evaluated on DOD-H, the F1 drops from 89.9 % to 84.8 % compared to a model trained from scratch on DOD-H. The standard deviation of the performance metrics almost doubles. The performance drop is bigger when the model is trained on DOD-H and evaluated on DOD-O, the F1 drops from 88.3 % to 62.6 %.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE VIII :</head><label>VIII</label><figDesc>Macro-F1 of the baseline models on MASS and Sleep EDF. For consistency with the literature we report the epoch-wise Macro F1. Moreover, since the computation are done epoch-wise, we cannot report subject variability as in the other tables. (*) are reported on the complete MASS dataset.<ref type="bibr" target="#b0">(1)</ref> is trained on F4-EOG and (2) on Fpz-Cz to limit the number of parameters.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t and of five cyclic indexes i cycle t,lwhere t is the number of sleep epochs since</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank the Fatigue and Vigilance team including Drogou C., Erblang M., Dorey R., Quiquempoix M., Gomez-Merino D. and Rabat A. for their help in the clinical trial at the Institut de Recherche Biologique des Armées (IRBA). We would like to thanks Dr. Michael E. Ballard, Hugo Jourde, Polina Davidenko, Sarah deLanda and Stephanie Lettieri for their help in realizing the clinical trial at the Stanford Sleep Medicine Center. We also would like to thank the Dreem team, including Mason Harris for his help gathering both datasets together with the sleep technologists' scorings, and Pierrick Arnal, Théo Moutakanni, Clémence Pinaud and Olivier Tranzer for their help with the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>m ctx = 12 (resp. m ctx = 50). SimpleSleepNet-Small has approximately three times less parameters and SimpleSleepNet-Large three times more parameters than SimpleSleepNet as show in <ref type="table">Table II</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">National Institutes of Health Sleep Disorders Research Plan</title>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Bethesda, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Center on Sleep Disorders Research and others ; National Institutes of Health</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The AASM manual for the scoring of sleep and associated events : rules, terminology, and technical specifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Iber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ancoli-Israel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Chesson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Quan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>American Academy of Sleep Medicine</publisher>
			<pubPlace>Westchester, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The American Academy of Sleep Medicine inter-scorer reliability program: Respiratory events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Hout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Sleep Medicine</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Danker-Hopfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interrater reliability for sleep scoring according to the Rechtschaffen &amp; Kales and the new AASM standard</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classification Automatique des Stades du Sommeil par Réseaux de Neurones Artificiels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kerkeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alexandre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Bedoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dogui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Troisième Workshop &quot;Applications Médicales de l&apos;Informatique : Nouvelles Approches</title>
		<meeting><address><addrLine>Monastir/Tunisie</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic analysis of single-channel sleep eeg: Validation in healthy individuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berthomier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sleep</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1587" to="95" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning machines and sleeping brains: Automatic sleep stage classification using decision-tree multi-class support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lajnef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mixed Neural Network Approach for Temporal Sleep Stage Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="324" to="333" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<title level="m">Automatic Sleep Stage Scoring with Single-Channel EEG Using Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeepSleepNet: A model for automatic sleep stage scoring based on raw single-channel EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Deep Learning Architecture for Temporal Sleep Stage Classification Using Multivariate and Multimodal Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Galtier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Arnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wainrib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutional neural network for sleep stage scoring from raw singlechannel EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mirek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vercueil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Payen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="107" to="114" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic Sleep Stage Classification based on Convolutional Neural Network and Fine-grained Segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for interpretable analysis of EEG sleep stage scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilamala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>MLSP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Expert-level sleep scoring with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1643" to="1650" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fernández-Varela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Athanasakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hernández-Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Moret-Bonillo</surname></persName>
		</author>
		<title level="m">Sleep Staging with Deep Learning: A convolutional model</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural network analysis of sleep stages enables efficient diagnosis of narcolepsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Stephansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SeqSleepNet: End-to-End Hierarchical Recurrent Neural Network for Sequence-to-Sequence Automatic Sleep Staging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">De</forename><surname>Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="410" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic Sleep Stage Classification Using Single-Channel EEG : Learning Sequential Features with Attention-Based Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Ch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1452" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Oliver</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">De</forename><surname>Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DNN Filter Bank Improves 1-Max Pooling CNN for Single-Channel EEG Automatic Sleep Stage Classification</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Montreal archive of sleep studies: An open-access resource for instrument benchmarking and exploratory research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Sleep Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain adaptation with optimal transport improves EEG sleep stage classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Galtier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Workshop on Pattern Recognition in Neuroimaging, PRNI 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The dreem headband as an alternative to polysomnography for eeg signal acquisition and sleep staging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename></persName>
		</author>
		<ptr target="https://www.biorxiv.org/content/early/2019/06/10/662734" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Sleep Stage Scoring Using Time-Frequency Analysis and Stacked Sparse Autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">AI vs Humans for the diagnosis of sleep apnea</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Thorey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Arnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>During</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09936</idno>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Recurrent batch normalization</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Zwinderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A C</forename><surname>Kamphuisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J L</forename><surname>Oberye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1185" to="1194" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-time: A fully convolutional network for time series segmentation applied to sleep staging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perslev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J R</forename><surname>Jennum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4415" to="4426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inter-scorer reliability between sleep centers can teach us what to improve in the scoring rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Penzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fietze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical sleep medicine : JCSM : official publication of the American Academy of Sleep Medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="89" to="91" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
