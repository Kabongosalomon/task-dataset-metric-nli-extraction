<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LONG-TAILED RECOGNITION BY ROUTING DIVERSE DISTRIBUTION-AWARE EXPERTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
							<email>xdwang@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley / ICSI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lian</surname></persName>
							<email>longlian@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley / ICSI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
							<email>zhongqi.miao@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley / ICSI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>ziwei.liu@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
							<email>stellayu@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley / ICSI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LONG-TAILED RECOGNITION BY ROUTING DIVERSE DISTRIBUTION-AWARE EXPERTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural data are often long-tail distributed over semantic classes. Existing recognition methods tend to focus on gaining performance on tail classes, often at the expense of losing performance on head classes and with increased classifier variance. The low tail performance manifests itself in large inter-class confusion and high classifier variance. We aim to reduce both the bias and the variance of a long-tailed classifier by RoutIng Diverse Experts (RIDE), consisting of three components: 1) a shared architecture for multiple classifiers (experts); 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; and 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts. With on-par computational complexity, RIDE significantly outperforms the state-of-the-art methods by 5% to 7% on all the benchmarks including CIFAR100-LT, ImageNet-LT and iNaturalist 2018. RIDE is also a universal framework that can be applied to different backbone networks and integrated into various long-tailed algorithms and training mechanisms for consistent performance gains. Our code is publicly available at: https://github.com/frank-xwang/RIDE-LongTailRecognition. arXiv:2010.01809v2 [cs.CV] 7 Apr 2021 Published as a conference paper at ICLR 2021 Under review as a conference paper at ICLR 2021 All Many-shot Med-shot Few-shot acc bias var acc bias var acc bias var acc bias var CE 31.6 0.60 0.47 57.3 0.28 0.35 28.2 0.61 0.51 6.3 0.94 0.57 ⌧ -norm 35.8 0.52 0.49 55.9 0.28 0.37 33.2 0.53 0.52 16.1 0.78 0.60 cRT 36.4 0.50 0.50 51.3 0.32 0.41 38.6 0.44 0.50 17.0 0.76 0.61 LDAM 34.4 0.53 0.51 55.1 0.28 0.38 31.9 0.53 0.54 13.9 0.81 0.63 RIDE + LDAM 40.5 0.50 0.42 60.5 0.28 0.30 38.7 0.50 0.44 20.1 0.74 0.52</p><p>ABSTRACT Natural data are often long-tail distributed over semantic classes. Existing recognition methods tend to focus on tail performance gain, often at the expense of head performance loss from increased classifier variance. The low tail performance manifests itself in large between-class confusion and high classifier variance. We aim to reduce both the bias and the variance of a long-tailed classifier by RoutIng Diverse Experts (RIDE). It has three components: 1) a shared architecture for multiple classifiers (experts); 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; and 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts. With on-par computational complexity, RIDE significantly outperforms the state-of-the-art methods by 5% to 7% on all the benchmarks including CIFAR100-LT, ImageNet-LT and iNaturalist. RIDE is also a universal framework that can be applied to different backbone networks and integrated into various re-balancing or re-weighting methods for consistent performance gains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Real-world recognition often has a long-tail distribution: A few classes contain many instances, whereas most classes contain only a few instances. For extreme imbalanced classification, learning to discriminate among them is challenging, as the classifier training objective on the few tail instances can be easily overwhelmed by that for many head instances, whereas for the multitude of tail-class few-shot recognition tasks.</p><p>Long-tailed recognition is usually handled either by class re-balancing/re-weighting strategies which give more importance to tail instances <ref type="bibr" target="#b0">(Cao et al., 2019;</ref><ref type="bibr" target="#b10">Kang et al., 2020;</ref><ref type="bibr" target="#b15">Liu et al., 2019)</ref> or by grouping methods, where long-tailed data are separated into groups by their class frequencies and models focusing on each individual group are combined to form a multi-expert framework <ref type="bibr" target="#b28">(Zhou et al., 2020;</ref><ref type="bibr" target="#b25">Xiang et al., 2020)</ref>. However, all these methods generally gain on tail classes at the cost of performance loss on head classes.</p><p>Here, we quantitatively analyze the long-tail classifier's performance change in terms of bias and variance analysis with respect to fluctuations in the training set: We randomly sample CIFAR100 <ref type="bibr" target="#b12">(Krizhevsky, 2009)</ref> according to a long-tailed distribution a few times, train a model each time, and then estimate the per-class bias and variance of the classifiers.</p><p>The prediction error of model h on instance x with output Y varies with the training data D. The expected variance with respect to D has a well-known bias-variance decomposition:</p><p>Error(x; h) = E[(h(x; D) − Y ) 2 ] = Bias(h) 2 + Variance(h) + irreducible error.</p><p>(1)</p><p>The model bias measures the accuracy of the prediction with respect to the true value, the variance of the method measures the stability of the prediction, and the irreducible error measures the precision of the prediction and is irrelevant to the model h. This same concept can be extended to classification using 0-1 loss between the predicted class and the ground-truth label <ref type="bibr" target="#b4">(Domingos, 2000)</ref>. Below is a quick summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The natural data we encounter in practice often has a long tail distribution: A few classes contain many instances, while most classes contain only a few instances. Learning discrimination among them is challenging, as the few tail instances can be easily overwhelmed by many head instances.</p><p>Long-tailed recognition is usually handled either by class re-balancing/re-weighting strategies giving more importance to tail instances <ref type="bibr" target="#b0">(Cao et al., 2019;</ref><ref type="bibr" target="#b10">Kang et al., 2020;</ref><ref type="bibr" target="#b15">Liu et al., 2019)</ref>, or by multiexpert methods, where long-tailed data are separated into parts by their frequencies and models focusing on individual parts are combined <ref type="bibr" target="#b28">(Zhou et al., 2020;</ref><ref type="bibr" target="#b25">Xiang &amp; Ding, 2020)</ref>. However, all these methods generally gain on tail classes at the cost of performance loss on head classes.</p><p>The state-of-the-art (SOTA) methods on iNaturalist <ref type="bibr" target="#b21">(Van Horn et al., 2018)</ref> are cRT and ⌧ -norm <ref type="bibr" target="#b10">(Kang et al., 2020)</ref> and BBN <ref type="bibr" target="#b28">(Zhou et al., 2020)</ref>. The former belongs to the re-balancing type with a two-stage optimization for learning a good representation and classifier, whereas the latter belongs to the multi-expert type with two experts focusing on head and tail classes.</p><p>We analyze the performance of a long-tail classifier in terms of bias and variance with respect to fluctuations in the training set: We randomly sample CIFAR100 <ref type="bibr" target="#b12">(Krizhevsky, 2009)</ref> according to a long-tailed distribution a few times, train a model each time, and then estimate the per-class bias and variance of the classifier. 1 (a) Comparisons of the mean accuracy, per-class bias and variance of baseline methods and our RIDE method. Better (worse) metrics than the distribution-unaware cross entropy (CE) reference are marked in green (red).</p><p>(b) Histograms of the largest softmax probability of the other classes (the hardest negative) per instance. <ref type="figure">Figure 1</ref>: The proposed method RIDE gains accuracies over SOTA by reducing both model bias and variance. a) These metrics are evaluated over 20 independently trained models, each on a random sampled set of CIFAR100 with an imbalance ratio of 100 and 300 samples for class 0. Compared to the standard cross-entropy (CE) classifier, existing SOTA methods almost always increase the variance and some reduce the tail bias at the cost of increasing the head bias. b) Compared with the many-shot classes, LDAM is more likely to confuse the tail classes with the hardest negative class, with an average score of 0.59. RIDE with LDAM can greatly reduce the confusion with the nearest negative class of each instance, especially for samples from the few-shot categories.</p><p>Let D be a set of training sets, x be a sample in the dataset. A loss function L(t; y) measures the cost of predicting y ∈ Y when the true value is t. Assume the optimal prediction y * for x is the prediction that minimizes E t [L(t; y * )], where the subscript t denotes that the expectation is taken with respect to all possible values of t, weighted by their probabilities given x. The main prediction y m for a loss function L over the training set D (which is treated as a random variable) is defined as y m = argmin y E D [L(y, y )]. That is, the main prediction is the one that "differs least" from all the predictions in Y according to L.</p><p>The bias is the loss incurred by the main prediction y m relative to the optimal prediction y * , and the variance is the average loss incurred by predictions y relative to the main prediction y m . We then evaluate bias and variance with zero one loss according to the evaluation metric in <ref type="bibr" target="#b4">(Domingos, 2000)</ref> for classification, i.e. bias = L zero-one (y * , y m ), variance = E D [L zero-one (y m , y)].</p><p>Fig. 1a uses the standard cross-entropy (CE) classifier as a performance reference and compares three well-known long-tail methods, cRT and τ -norm <ref type="bibr" target="#b10">(Kang et al., 2020)</ref> and LDAM <ref type="bibr" target="#b0">(Cao et al., 2019)</ref>, with our proposed method. The former adopts a two-stage optimization, first representation learning and then classification learning, whereas the latter is trained end-to-end with a marginal loss. On the basis of the preliminary experiments, following observations can be obtained:</p><p>1. On the mean accuracy: All the long-tail methods increase the overall, medium-shot, and fewshot accuracies, but these previous methods all decrease the many-shot accuracy. Our method increases accuracies on all splits. 2. On the model bias: All the long-tail methods reduce the overall, medium-shot, and few-shot bias.</p><p>The reduction tends to be greater for the tail classes. Our method decreases bias more than other methods on the tail classes. 3. On the model variance: All the current long-tail methods increase the overall, many-shot, medium-shot, and few-shot variance, except cRT has a slight reduction for medium-shot. Our method reduces variances throughout the class spectrum.</p><p>That is, current long-tail methods sacrifice the head performance to improve the tail performance, and their models also become more unstable with respect to the training data variation. Our method is the only one that improves accuracies and reduces bias/variance throughout the class spectrum. Figure 2: RIDE applies a two-stage optimization process. a) We first jointly optimize multiple diverse experts with distribution-aware diversity loss. b) An expert assignment module that could dynamically assign "ambiguous" samples to extra experts is trained in stage two. At test time, we combine the predictions of assigned experts to form a robust prediction. Since tail classes are inclined to be confused with other classes, by adding the expert assignment module, the data imbalance ratio for later experts can be automatically reduced without any distribution-aware loss, which allows focusing less on confident head classes and more on tail classes. c) RIDE outperforms SOTA methods (i.e. LFME <ref type="bibr" target="#b25">(Xiang et al., 2020)</ref> for CIFAR100-LT, LWS <ref type="bibr" target="#b10">(Kang et al., 2020)</ref> for ImageNet-LT and BBN <ref type="bibr" target="#b28">(Zhou et al., 2020)</ref> for iNaturalist) on all experimented benchmarks. <ref type="figure">Fig. 1b</ref> provides further insight into the model bias by examining the largest softmax probability in the other classes: The smaller this value is, the less the confusion, and the lower the bias. It shows that the poor tail performance manifests itself at increasingly larger confusion and bias, and our method significantly reduces the confusion and bias for the tail classes.</p><p>We propose a novel multi-expert approach, called RoutIng Diverse Experts (RIDE), not only to reduce the model variance for all the classes, but also to reduce the model bias for the tail classes, both of which existing long-tail methods all fail to accomplish.</p><p>RIDE adopts multiple experts for model variance reduction and employs an additional distributionaware diversity loss L D-Diversity for reducing the model bias. With a dynamic expert routing module, RIDE assigns another trained and distinctive expert for a second (or third, ...) opinion when it is called for. Both the shared architecture for experts and the routing module effectively reduce the computational complexity of our multi-expert model to a level even lower than a baseline model with the same backbone.</p><p>RIDE delivers 5%∼7% higher accuracies than the current SOTA methods on CIFAR100-LT, ImageNet-LT <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> and iNaturalist <ref type="bibr" target="#b21">(Van Horn et al., 2018)</ref>. RIDE is also a universal framework that can be applied to different backbone networks for improving existing long-tail algorithms such as focal loss <ref type="bibr" target="#b13">(Lin et al., 2017)</ref>, LDAM <ref type="bibr" target="#b0">(Cao et al., 2019)</ref>, τ -norm <ref type="bibr" target="#b10">(Kang et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Re-balancing/re-weighting. The classical way to achieve balance for long-tailed datasets is to control each class's sample frequencies based on class numbers such as the under-and over-sampling method <ref type="bibr" target="#b6">(He &amp; Garcia, 2009</ref>). Data augmentation is another common way to achieve sample balance. For example, , <ref type="bibr" target="#b1">Chu et al. (2020)</ref>, and <ref type="bibr" target="#b11">Kim et al. (2020)</ref> generate augmented samples in the feature space to supplement tail class samples. Besides sample-wise balancing, re-weighting the loss functions can also introduce learning balance by either putting larger weights on more challenging and sparse classes <ref type="bibr" target="#b13">(Lin et al., 2017;</ref><ref type="bibr" target="#b2">Cui et al., 2019;</ref><ref type="bibr" target="#b0">Cao et al., 2019;</ref><ref type="bibr" target="#b23">Wu et al., 2020)</ref> or randomly ignoring gradients from head classes <ref type="bibr" target="#b18">(Tan et al., 2020)</ref>. However, both sample-wise and loss-wise balancing conceptually increase the learning focus on tail classes, which increases the sensitivity to small fluctuations of tail classes and greatly increases the variance as in <ref type="figure">Fig. 1</ref>.</p><p>Knowledge transfer. Another major direction to learning balance is transferring knowledge from head to tail classes <ref type="bibr" target="#b15">(Liu et al., 2019;</ref><ref type="bibr" target="#b29">Zhu &amp; Yang, 2020;</ref><ref type="bibr" target="#b10">Kang et al., 2020;</ref><ref type="bibr" target="#b9">Jamal et al., 2020;</ref><ref type="bibr" target="#b22">Wang et al., 2017)</ref>. For example, the representative approaches in this direction, OLTR <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> and inflated memory <ref type="bibr" target="#b29">(Zhu &amp; Yang, 2020)</ref>, harness memory banks to store and transfer mid-and high-level features from head classes to enhance tail classes' feature generalization. However, this line of work usually do not have effective controls over the transferring process, and therefore would sacrifice the performance of head classes.</p><p>Ensemble and grouping. A more related direction of balanced learning to this project is ensemble and grouping, in which samples are usually separated into different groups based on the position of the class from head to tail and individual models with focus on each group are ensembled at the end to form a multi-expert framework. BBN <ref type="bibr" target="#b28">(Zhou et al., 2020</ref>) has a two-branch architecture with an adaptive fusion procedure at the end, where one branch focuses on the head classes (by directly learning from an imbalanced dataset), and the other focuses on the tail classes (by balancing the dataset with sampling techniques). LFME <ref type="bibr" target="#b25">(Xiang et al., 2020</ref>) takes a step further by distilling a unified model from multiple teacher models; each focuses on classifying a relatively balanced group of the dataset (e.g., many-shot classes, medium-shot classes, and few-shot classes). BBN and LFME still suffer from performance loss in head classes because each expert do not have a balanced access to the whole dataset, which damages the overall generalizability, especially head classes. In our approach, we apply a diverse distribution-aware individual loss with instance-balanced sampling to each routed expert, so that the information between experts are shared to boost generalizability.</p><p>RIDE has several appealing properties over traditional ensemble methods: 1) RIDE requires much fewer parameters and computational cost, RIDE's model complexity can even be smaller than the baseline model with higher accuracy. 2) By adding the expert assignment module, not all experts are activated for all samples. The dynamic collaboration between experts enables RIDE to be more efficient. 3) All these experts are jointly optimized, which reduces the cost for running back-propagation and optimization steps. 4) We share partial convolutional layers of these experts that are considered relatively task-agnostic and reduce the channel dimensions of non-shared layers. Since the capability of each expert is lower, each expert is less easier to be over-fitted on the tail classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RIDE: ROUTING DIVERSE DISTRIBUTION-AWARE EXPERTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OVERALL FRAMEWORK</head><p>The multi-expert framework consists of two components as in <ref type="figure">Fig. 2a</ref>: f θ , shared part across experts, and Ψ = {ψ θ1 , ..., ψ θn }, which is expert-independent and evaluated with the output of f θ .</p><p>Although our method is applicable to multiple backbone networks, which is also indicated in our experiments, we take ResNet as an example to illustrate our design for its commonness. We exploit a property of convolutional neural networks in the design of the multi-expert framework: Since early CNN layers typically contain filters that generally process only task-agnostic information of an image, as indicated by the practice of freezing early layers in transfer learning, we share the first two stages of ResNet across all experts, i.e. f θ . Later stages, including the fully connected layer, are independent between experts, i.e. ψ θi , where i ∈ [1, n]. In addition, in order to further cut down the overall model complexity, the number of filters in ψ θi is reduced by 1/4. Then, to obtain logits of each experts, we first get the intermediate representation from f θ , and then pass it into each ψ θi independently to get logits. All these n experts are co-trained on long-tailed data with the proposed distribution aware diversity loss L D-Diversify and a classification loss L Classify , such as cross-entropy and LDAM.</p><p>Expert assignment module is added to dynamically assign additional cascaded experts to hard samples. Since a weak expert is sufficient to provide a high-quality representation for easy samples, not all samples require extra knowledge from additional experts. The expert assignment module is trained with a separate stage on the top of frozen feature extractor as in <ref type="figure">Fig. 2b</ref>.</p><p>Making joint decisions with geometric mean. During testing time, assume m, where m ≤ n, is the number of experts assigned to the tested sample, we handle the outputs of these models by taking arithmetic mean on logits of m experts, which produces logits with the same ranking as taking the geometric mean on the probabilities, followed by a softmax layer to calculate the probability of the jth class:</p><formula xml:id="formula_0">p j = softmax( 1 m m i=1 ψ θi (f θ (x))</formula><p>). In our setting, this approach performs better than taking the arithmetic mean on the probabilities, i.e. p j = 1 m m i=1 softmax(ψ θi (f θ ( x))).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DIVERSE DISTRIBUTION-AWARE INDIVIDUAL LOSS</head><p>Previous multi-branch methods such as BBN apply a classification loss (e.g. cross-entropy) on aggregated output of models. While this is intuitive due to same formula in training and evaluation, in our empirical experiments, we found that applying this method is not sufficient for the best performance. We hypothesize that taking the loss over the geometric mean in Section 3.1 leads to correlated experts, which prevents the experts to have different knowledge. In order to make the experts model decoupled, we present a loss function with two specifically-designed components.</p><p>Individual loss: decoupling the training of multiple experts. We define the loss function above as "collaborative loss", with expression:</p><formula xml:id="formula_1">L collaborative = 1 N N i=1 L( 1 n n j=1 ψ θj (f θ ( x i )), y i ),</formula><p>where N is the mini-batch size, L is the classification loss. This loss is shown to behave well in several recently proposed multi-expert models. However, in our multi-experts setting, this method only achieves accuracy comparable to a single-expert model with roughly the same number of parameters, which indicates that the collaborative loss does not train experts to have diverse knowledge to complement other experts in decision making, i.e. the collaborative loss trains correlated experts. This means the best performance of multi-experts overall is not shown.</p><p>To reduce the effect of correlated experts, we propose a new loss function, "individual loss", which applies the loss in the middle of the module as soon as each expert's logits are computed. Since the loss calculation process does not depend on post-aggregation output, it results in almost no interexpert relationship expect the shared parameters. The formal expression of it, in training time, is:</p><formula xml:id="formula_2">L individual = 1 nN N i=1 n j=1 L i (ψ θj (f θ ( x i )), y i ).</formula><p>The test time formulation is the same as the one mentioned in Section 3.1. Although it requires the model to have more than one output, which might add a little implementation difficulty, this loss function achieves a better accuracy and contributes a large amount of improvements in most of our experiments.</p><p>A diversified distribution-aware loss is proposed to penalize the inter-expert correlation further since we discover from experiments that although the individual loss, as well as the random initialization, already leads to branches that are diverse, adding more diversity with a external loss benefits an additional small amount most of the time. Since a complex model needs to be fitted on a few tail class samples, it will capture the noise along with the underlying pattern in data and thus we encourage diversity in tail classes to alleviate the influence of noise, we reference the idea of temperature in contrastive loss <ref type="bibr" target="#b5">(Hadsell et al., 2006;</ref><ref type="bibr" target="#b24">Wu et al., 2018)</ref> to enable the diversity loss to be distribution aware, formulated as:</p><formula xml:id="formula_3">L i D-Diversify = − λ n − 1 n j =i D KL (φ i ( x, T ), φ j ( x, T )) (2) where φ i ( x, T ) = softmax(ψ θi (f θ ( x))/ T ), i is the expert index, D KL (·, ·) is the KL-Divergence (D KL (P, Q) = x∈X P (x) log P (x) Q(x) )</formula><p>, λ is the balancing factor between diversity and classification loss between ground truth label y and model predictions, T is temperatures of each sample and division is performed as element-wise division, and X is the probability space of P (x) and Q(x).</p><p>The optimization objective of L i D-Diversify is to encourage the diversity between expert i and other experts. Choosing a small temperature would assign large negative gradients (i.e. smaller overall gradients) to samples with "divergent" predictions and very small negative gradients (i.e. larger overall gradients) to samples with "homogeneous" predictions. L i D-Diversify could also be used as a regularization term. The temperature T i for samples in class i can be calculated by:</p><formula xml:id="formula_4">T i = ηψ i + η(1 − max(Ψ)); Ψ = {ψ 1 , ..., ψ C } = {γ · C · n i C k=1 n k + (1 − γ)} C i=1<label>(3)</label></formula><p>where C is the number of classes, γ is the balancing factor, n i is the number of samples of class i, and η is the base temperature. This way, we give lower temperature to tail classes, which generates higher probability for the tail classes in distributions that we apply KL Divergence on, encouraging more diversity in tail classes. Similar to the practice of deferred reweight <ref type="bibr" target="#b0">(Cao et al., 2019)</ref>, we enable temperatures only after the network has been trained for several epochs to allow the feature extractor to learn stabilized features.</p><p>The total loss presented above can be formulated as:</p><formula xml:id="formula_5">L Total = 1 nN N i=1 n j=1 L j Classify (φ j ( x), y) − λ n − 1 n j =k D KL (φ j ( x, T ), φ k ( x, T ))<label>(4)</label></formula><p>where j and k are the expert indices, L j Classify (., .) can be LDAM loss, focal loss, etc., depending on the training mechanisms we choose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ROUTING DIVERSIFIED EXPERTS</head><p>An expert assignment module that dynamically routes cascaded experts is proposed to further reduce the overall computational complexity. The expert assignment decision is based on the information from the logits and the input feature. The input feature is normalized to stabilize training process and is transformed by a fully-connected layer to reduce dimension and aggregate information. We only take the top z scores of logits, since other logits are generally too small to provide much information about the "ambiguity" of the sample. z is commonly set to a number ranging from 10 to 50. The module has negligible computation cost and negligible additional parameters.</p><p>Let l i and v i be the top z logits and the normalized feature of expert i, respectively. The binary output of expert assignment module is:</p><formula xml:id="formula_6">y ea = W 2 ( l i ⊕ σ(W 1 v i )), where ⊕ denotes concatenation, σ(·) denotes ReLU function, W 1 ∈ R d ×d , W 2 ∈ R 1×(d +z)</formula><p>, and d is the dimension of input feature. d is set to 16, as a too large d may cause overfitting and unstable optimization.</p><p>The expert assignment module is optimized with the routing loss, a weighted variant of binary cross entropy loss:</p><formula xml:id="formula_7">L Routing = −ω p y log( 1 1 + e −yea ) − ω n (1 − y) log(1 − 1 1 + e −yea )<label>(5)</label></formula><p>where the ground truth y is constructed as: if the current expert does not predict the sample correctly but one of the next experts gives correct prediction, the ground truth is set to 1 (considered as a positive sample), otherwise it is 0. ω p and ω n are the re-weighting factor (set to 100 for all positive samples and 1 for all negative samples). The expert assignment module is trained on a separate stage, with all other parameters frozen. Each expert, except the last one, has an independent expert assignment module with shared W 2 . Therefore, the number of expert assignment modules is n − 1.</p><p>Since we use a binary cross entropy loss with logits input in training, the output of this module is the logits that decide whether we need to activate more experts, and the predicted probability can be obtained by appending a sigmoid function at test time. If the predicted probability is greater or equal to 0.5, which means that our predictor gives larger probability to having additional experts, we decide to proceed to the next expert. In case that one needs to manually adjust the proportion of samples passing through each expert, i.e. the "easiness for each expert assignment module to be satisfied", one could adjust the positive weight of the binary cross entropy loss, ω p , at training time.</p><p>As observed in experiments, using a fixed positive weight (ω p = 100) across all our experiments leads to a good trade-off of our EA module in accuracy and computational complexity.</p><p>Self-distillation step is optional but recommended if further improvements (about 0.4%∼0.8% for most experiments) are desired. Unlike previous methods with fixed number of experts (e.g. BBN, LFME), the total number of experts can be arbitrarily adjusted to balance accuracy and computation cost. This enables self-distillation from a model with more (6 in our setting) experts to the same model with fewer experts to obtain further improvements. We choose knowledge distillation <ref type="bibr" target="#b8">(Hinton et al., 2015)</ref> by default. The implementation details and comparison on various distillation algorithms such as CRD <ref type="bibr" target="#b19">(Tian et al., 2019)</ref> are investigated in Appendix Section A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS AND IMPLEMENTATIONS</head><p>We conduct experiments on three major long-tailed recognition benchmarks and different backbone networks to prove the effectiveness and universality of RIDE:  <ref type="bibr" target="#b28">(Zhou et al., 2020)</ref> and LFME <ref type="bibr" target="#b25">(Xiang et al., 2020)</ref>, which also contain multiple experts (or branches), RIDE (2 experts) outperforms them by a large margin with fewer GFlops. The relative computation cost (averaged on testing set) with respect to the baseline model and absolute improvements against SOTA (colored in green) are reported. † denotes our reproduced results with released code. ‡ denotes results copied from <ref type="bibr" target="#b0">(Cao et al., 2019)</ref>.   <ref type="table" target="#tab_1">Table 1</ref> shows that RIDE outperforms state-of-the-art methods by a large margin on CIFAR100-LT. The average computational cost is even about 10% less than baseline models when we only apply 2 cascaded experts as in BBN. Compared with LFME <ref type="bibr" target="#b25">(Xiang et al., 2020)</ref> and BBN <ref type="bibr" target="#b28">(Zhou et al., 2020)</ref>, which also apply multiple experts, RIDE significantly surpasses both by more than 5.3% and 6.5%, respectively. Since LDAM is end-to-end optimized, we choose it as the default training method for simplicity, unless otherwise noticed.   <ref type="figure">Figure 3</ref>: Extend RIDE to various long-tailed recognition methods. Consistent improvements can be observed on CIFAR100-LT, which illustrates that the proposed method can be applied to various training mechanisms, either methods that are end-to-end (e.g. LDAM) or require another stage of process (e.g. cRT and τnorm). By using RIDE, cross-entropy loss (without any re-balancing strategies) can even outperforms current SOTA method LFME. Although higher accuracy can be obtained using distillation, we did not apply it here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MFlops</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR100-LT.</head><p>Integrating RIDE with various methods. <ref type="figure">Fig. 3</ref> indicates that our method will likely be able to benefit from the future advancements to the loss function and training process. Consistent improvements can be observed. Cosine classifier is a baseline that we constructed in which we normalize the weights of the classifier and apply resample in the classifier retrain stage, similar to cRT. Since two-stage methods require an additional training stage, we use LDAM as our default choice. ImageNet-LT. We further evaluate RIDE on ImageNet-LT with various backbones as in <ref type="table" target="#tab_4">Table 2</ref>. Compared with current SOTA methods, LWS and cRT, RIDE achieves new state-of-the-art results and outperforms SOTA by more than 7.7% with ResNet-50. ResNeXt-50 is based on group convolution <ref type="bibr" target="#b26">(Xie et al., 2017)</ref>, which divides all filters into several groups and aggregates information from multiple groups. ResNeXt-50 generally performs better than ResNet-50 on multiple tasks. Using ResNeXt-50, we can see a performance improvement of 6.9%. iNaturalist. iNaturalist 2018 is a naturally imbalanced fine grained dataset with 8,142 categories. <ref type="table" target="#tab_5">Table 3</ref> shows that RIDE outperforms current SOTA by 6.3%. Surprisingly, RIDE obtains very similar results in many-shots, medium-shots and few-shots, which is a very ideal result for the long tailed recognition task. Current SOTA method BBN also uses multiple experts. Compared with BBN, which significantly decreases the performance on many-shot classes by about 23%, RIDE increases the accuracy on few-shot without sacrificing the performance on many-shot categories.</p><p>Contribution of each component of RIDE. RIDE is jointly trained with L D-Diversify and L Classify , we use LDAM as L Classify by default. <ref type="table">Table 4</ref> shows that the architectural change from the original ResNet-32 to the RIDE variant with 2 ∼ 4 experts contributes 2.7% ∼ 4.3% accuracy improvements. The change from applying our classification loss naively to model output (collaborative loss) to applying to each expert as an individual loss brings 1.5% improvements. Adding diversity further improves about 0.9%. The computation cost is greatly reduced by adding the expert assignment module. Knowledge distillation from RIDE with 6 experts obtains another 0.6% increase. All these coherent components of RIDE enable it to get 7.1% increase compared with baseline LDAM.</p><p>Influence of expert number. <ref type="figure">Fig. 4</ref> shows the influence of expert number for each split. Compared with many-shot, which only obtains 3.8% relative improvements by using 8 experts, the accuracy of <ref type="table">Table 4</ref>: Ablation studies on the effectiveness of each component on CIFAR100-LT. LDAM <ref type="bibr" target="#b0">(Cao et al., 2019)</ref> is used as our classification loss. The first 3 RIDE models only have architectural change without changes in training method. The performance without L Individual checked indicates directly applying classification loss onto the final model output, which is the mean expert logits. This is referred to as collaborative loss above. In contrast, if L Individual if checked, we apply individual loss to each individual expert. The difference between collaborative loss and individual loss is described above.   <ref type="figure">Figure 4</ref>: # experts vs. top-1 accuracy for each split (All, Many/Medium/Few) of CIFAR100-LT. Compared with the many-shot split, which is 3.8% relatively improved by adding more experts, the few-shot split can get more benefits, that is, a relative improvement of 16.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Many</head><p>Medium Few # expert = 3 # expert = 4 <ref type="figure">Figure 5</ref>: The proportion of the number of experts allocated to each split of CIFAR100-LT. For RIDE with 3 or 4 experts, more than half of many-shot instances only require one expert. On the contrary, more than 76% samples of few-shot classes require opinions from additional experts.</p><p>few-shot classes is relative increased by more than 16%, which indicates that few-shot classes can enjoy more benefits from using more experts. No distillation is applied in this comparison.</p><p>Number of experts allocated to each split. <ref type="figure">Fig. 5</ref> shows that: Most instances in the many-shot classes are assigned only 1 expert, whereas those in few-shot classes are often assigned more experts. The low confidence in low-shot instances requires the model to seek a second (or a third, ...) opinion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SUMMARY</head><p>We propose a novel multi-expert approach for long-tailed recognition. It trains partially shared diverse distribution-aware experts and routes an instance to additional experts when necessary, with computational complexity comparable to a single expert. RIDE significantly outperforms SOTA methods by a large margin on all the benchmarks including CIFAR100-LT, ImageNet-LT, and iNaturalist. It can also be applied to various backbones and methods with consistent performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 DATASETS AND IMPLEMENTATIONS</head><p>We conduct experiments on three major long-tailed recognition benchmarks and different backbone networks to prove the effectiveness and universality of RIDE:</p><p>1.CIFAR100-LT <ref type="bibr" target="#b12">(Krizhevsky, 2009;</ref><ref type="bibr" target="#b0">Cao et al., 2019)</ref>: The original version of CIFAR-100 contains 50,000 images on training set and 10,000 images on validation set with 100 categories. The long-tailed version of CIFAR-100 follows an exponential decay in sample sizes across different categories. We conduct experiment on CIFAR100-LT with an imbalance factor of 100, i.e. the ratio between the most frequent class and the least frequent class.</p><p>To make fair comparison with previous works, we follow the training recipe of <ref type="bibr" target="#b0">(Cao et al., 2019)</ref> on CIFAR100-LT. We train the ResNet-32 <ref type="bibr" target="#b7">(He et al., 2016)</ref> backbone network by SGD optimizer with a momentum of 0.9. CIFAR100-LT is trained for 200 epochs with standard data augmentations <ref type="bibr" target="#b7">(He et al., 2016)</ref> and a batch size of 128 on one RTX 2080Ti GPU. The learning rate is initialized as 0.1 and decayed by 0.01 at epoch 120 and 160 respectively. 2.ImageNet-LT <ref type="bibr" target="#b3">(Deng et al., 2009;</ref><ref type="bibr" target="#b15">Liu et al., 2019)</ref>: ImageNet-LT is constructed by sampling a subset of ImageNet-2012 following the Pareto distribution with the power value α = 6 <ref type="bibr" target="#b15">(Liu et al., 2019)</ref>. ImageNet-LT consists of 115.8k images from 1,000 categories, with the largest and smallest categories containing 1,280 and 5 images, respectively. Multiple backbone networks are experimented on ImageNet-LT, including ResNet-10, ResNet-50 and ResNeXt-50 <ref type="bibr" target="#b26">(Xie et al., 2017)</ref>. All backbone networks are trained with a batch size of 256 on 8 RTX 2080Ti GPUs for 100 epochs using SGD with an initial learning rate of 0.1 decayed by 0.1 at 60 epochs and 80 epochs. We utilize standard data augmentations as in <ref type="bibr" target="#b7">(He et al., 2016)</ref>. 3.iNaturalist <ref type="bibr" target="#b21">(Van Horn et al., 2018)</ref>: The iNaturalist-2018 dataset is an imbalanced datasets with 437,513 training images from 8,142 classes with a balanced test set of 24,426 images. We use ResNet-50 as the backbone network and apply the same training recipe as ImageNet-LT, except that we use a batch size of 512.   <ref type="bibr">(Zhou et al., 2020) (left)</ref> and ImageNet-LT's current state-of-the-art method cRT <ref type="bibr">(Kang et al., 2020) (right)</ref>. RIDE improves the performance of few-and medium-shots categories without sacrificing the accuracy on many-shots, and outperforms BBN on many-shots by a large margin (more than 20% absolute increase).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ADDITIONAL EXPERIMENTS</head><p>Ablation study on distillation methods. Self-distillation step is optional but recommended if further improvements (0.4%∼0.8% for most experiments) are desired. We apply distillation from a more powerful model with more experts into a model with fewer experts. A simple way to transfer knowledge is knowledge distillation (KD) <ref type="bibr" target="#b8">(Hinton et al., 2015)</ref>, which applies KD loss (L KD = T 2 D KL ( l teacher /T, l expert i /T )) to match the distribution of logits of a teacher and a student. We found that for teacher model with more experts using smaller distillation loss factor gives better performance. We hypothesize that since we distill from the same teachers, giving large distillation factor prevents the branches from becoming as diversified as it is able to. We also explored other distillation methods, such as CRD <ref type="bibr" target="#b19">(Tian et al., 2019)</ref>, PKT <ref type="bibr" target="#b17">(Passalis &amp; Tefas, 2018)</ref>, and SP (Tung <ref type="table">Table 5</ref>: Comparison of different distillation methods. We transfer from a model based on ResNet-32 with 6 experts to a model of the same type, except with fewer experts. We use CIFAR100-LT for the following comparison. No expert assignment module is used in the following experiments. Following the procedure for CRD <ref type="bibr" target="#b19">(Tian et al., 2019)</ref>, we also apply KD when we transfer from a teacher to students with other distillation methods.  <ref type="bibr">, 2019)</ref>, and compared the differences in <ref type="table">Table 5</ref>. Although adding other methods along with KD may boost performance, the difference is small. Therefore, we opt for simplicity and use KD only unless otherwise noticed.</p><p>Detailed results for ImageNet-LT experiments. We list details of our ResNet-50 experiments in ImageNet-LT on <ref type="table">Table 7</ref>. With 2 experts, we are able to achieve about 7% gain in accuracy with computational cost about 10% less than baseline. In contrast to previous methods that sacrifice many-shot accuracy to get few-shot accuracy, we improve on all three splits on ImageNet-LT. From 3 experts to 4 experts, we keep the same many-shot accuracy while increasing the few-shot accuracy, indicating that we are using the additional computational power to improve on the hardest part of the data rather than uniformly applying to all samples.</p><p>We also list our ResNet-10 and ResNeXt-50 experiments on <ref type="table">Table 6</ref> and 8, respectively, to compare against other works evaluated on these backbones. Our method also achieves lower computational cost and higher performance when compared to other methods.</p><p>As illustrated in <ref type="figure" target="#fig_4">Fig. 6</ref>, our approach provides a comprehensive treatment to all the many-shot, medium-shot and few-shot classes, achieving substantial improvements to current state-of-the-art on all aspects. Compared with cRT which reduces the performance on the many-shot classes, RIDE can achieves significantly better performance on the few-shot classes without impairing the manyshot classes. Similar observations can be obtained in the comparison with the state-of-the-art method BBN <ref type="bibr" target="#b28">(Zhou et al., 2020)</ref> on iNaturalist.</p><p>Comparison with ensemble method. Since our method requires the joint decision from several experts, which raw ensembles also do, we also compare against ensembles of LDAM in <ref type="figure">Fig.7</ref> on CIFAR100-LT. In the figure, even our method with 4 experts has less computational cost than the minimum computational cost for the ensemble of 2 LDAM models. This indicates that our model is much more efficient and powerful in terms of computational cost and accuracy than ensemble on long-tailed datasets.</p><p>t-SNE visualization. We also provide the t-SNE visualization of embedding space on CIFAR100-LT as in <ref type="figure" target="#fig_6">Fig. 8</ref>. Compared with the baseline method LDAM, the feature embedding of RIDE is more compact for both the head and tail classes and better separated from the neighboring classes. This greatly reduces the difficulty for the classifier to distinguish the tail category.  <ref type="figure">Figure 7</ref>: Comparison between our method and multiple LDAM models ensembled together. In the figure, ensembles of LDAM start from 1 ensemble (original LDAM) to 7 ensembles, and RIDE starts from 2 experts to 4 experts. Our method achieves higher accuracy with substantially less computational cost compared to ensemble method. What if we apply RIDE to balanced datasets? We also conducted experiments on CIFAR100 to check if our method can achieve similar performance gains on balanced datasets. However, we only obtained an improvement of about 1%, which is much smaller than the improvements observed on the CIFAR100-LT. Compared with balanced datasets, long-tailed datasets can get more benefits from RIDE. <ref type="table">Table 6</ref>: Top-1 accuracy comparison with state-of-the-art methods on ImageNet-LT <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> with ResNet-10. Performance on Many-shot (&gt;100), Medum-shot (≤100 &amp; &gt;20) and Fewshot (≤20) are also provided. Results marked with † are copied from <ref type="bibr" target="#b15">(Liu et al., 2019)</ref>. Results with ‡ are from <ref type="bibr" target="#b25">(Xiang et al., 2020)</ref>.  <ref type="table">Table 7</ref>: Top-1 accuracy comparison with state-of-the-art methods on ImageNet-LT <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> with ResNet-50. Performance on Many-shot (&gt;100), Medum-shot (≤100 &amp; &gt;20) and Fewshot (≤20) are also provided. Results marked with † are copied from <ref type="bibr" target="#b10">(Kang et al., 2020)</ref>.  <ref type="table">Table 8</ref>: Top-1 accuracy comparison with state-of-the-art methods on ImageNet-LT <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> with ResNeXt-50. Performance on Many-shot (&gt;100), Medum-shot (≤100 &amp; &gt;20) and Few-shot (≤20) are also provided. Results marked with † are copied from <ref type="bibr" target="#b10">(Kang et al., 2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GFlops</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GFlops</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GFlops</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>CIFAR100-LT<ref type="bibr" target="#b0">(Cao et al., 2019)</ref>: The long-tailed version of CIFAR100 follows an exponential decay in sample sizes across different classes. We conduct experiments on CIFAR100-LT with an imbalance factor of 100, with the ResNet-32<ref type="bibr" target="#b7">(He et al., 2016)</ref> as a backbone network.ImageNet-LT(Liu et al., 2019): RIDE with ResNet-50 and ResNeXt-50 (Xie et al., 2017) are experimented on ImageNet-LT. More details and experiments on other backbones are listed in appendix. iNaturalist (Van Horn et al., 2018): We use ResNet-50 as the backbone network and apply the same training recipe as ImageNet-LT except with batch size 512 as Kang et al. (2020) does.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The absolute accuracy difference of RIDE (blue) over iNaturalist's current state-of-theart method BBN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8</head><label>8</label><figDesc>: t-SNE visualization of LDAM's and our model's embedding space of CIFAR100-LT. The feature embedding of RIDE is more compact for both head and tail classes and better separated. This behavior greatly reduces the difficulty for the classifier to distinguish the tail category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Top-1 accuracy comparison with state-of-the-arts on CIFAR100-LT with an imbalance ratio of 100. Compared with BBN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Top-1 accuracy comparison with state-of-the-art methods on ImageNet-LT<ref type="bibr" target="#b15">(Liu et al., 2019)</ref> with ResNet-50 and ResNeXt-50. RIDE achieves consistent performance improvements on various backbones. Results marked with † are copied from<ref type="bibr" target="#b10">(Kang et al., 2020)</ref>. We compare GFlops against the baseline model. Detailed results on each split are listed in appendix materials.</figDesc><table><row><cell>Methods</cell><cell cols="2">ResNet-50 GFlops Acc. (%)</cell><cell cols="2">ResNeXt-50 GFlops Acc. (%)</cell></row><row><cell>Cross Entropy (CE)  † OLTR  † (Liu et al., 2019) NCM (Kang et al., 2020) τ -norm (Kang et al., 2020) cRT (Kang et al., 2020) LWS (Kang et al., 2020) RIDE (2 experts) RIDE (3 experts) RIDE (4 experts)</cell><cell>4.11 (1.0x) -4.11 (1.0x) 4.11 (1.0x) 4.11 (1.0x) 4.11 (1.0x) 3.71 (0.9x) 4.36 (1.1x) 5.15 (1.3x)</cell><cell>41.6 -44.3 46.7 47.3 47.7 54.4 (+6.7) 54.9 (+7.2) 55.4 (+7.7)</cell><cell>4.26 (1.0x) -4.26 (1.0x) 4.26 (1.0x) 4.26 (1.0x) 4.26 (1.0x) 3.92 (0.9x) 4.69 (1.1x) 5.19 (1.2x)</cell><cell>44.4 46.3 47.3 49.4 49.6 49.9 55.9 (+6.0) 56.4 (+6.5) 56.8 (+6.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art methods on iNaturalist<ref type="bibr" target="#b21">(Van Horn et al., 2018)</ref>. RIDE outperforms current SOTA BBN, which also contains multiple "experts", by a large margin on manyshot classes. Results marked with † are from BBN<ref type="bibr" target="#b28">(Zhou et al., 2020)</ref> and Decouple<ref type="bibr" target="#b10">(Kang et al., 2020)</ref>. BBN's results are from the released checkpoint. Relative improvements to SOTA result of each split (colored with gray) are also listed, with the largest boost from few-shot classes.</figDesc><table><row><cell>Methods CE  † CB-Focal  † OLTR LDAM + DRW  † cRT τ -norm LWS BBN RIDE (2 experts) RIDE (3 experts) RIDE (4 experts)</cell><cell>GFlops 4.14 (1.0x) 4.14 (1.0x) 4.14 (1.0x) 4.14 (1.0x) 4.14 (1.0x) 4.14 (1.0x) 4.14 (1.0x) 4.36 (1.1x) 3.67 (0.9x) 4.17 (1.0x) 4.51 (1.1x)</cell><cell>All 61.7 61.1 63.9 64.6 65.2 65.6 65.9 66.3 71.4 (+5.1) 72.2 (+5.9) 72.6 (+6.3)</cell><cell>Many 72.2 -59.0 -69.0 65.6 65.0 49.4 70.2 (+1.2) 70.2 (+1.2) 70.9 (+1.9)</cell><cell>Medium 63.0 -64.1 -66.0 65.3 66.3 70.8 71.3 (+0.5) 72.2 (+1.4) 72.4 (+1.6)</cell><cell>Few 57.2 -64.9 -63.2 65.9 65.5 65.3 71.7 (+5.8) 72.7 (+6.8) 73.1 (+7.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Knowledge distillation step is optional if further improvements are desired. Various knowledge distillation techniques are compared in the appendix.</figDesc><table><row><cell>Methods LDAM + DRW RIDE</cell><cell>#expert L Individual L D-Diversify EA distill GFlops Acc. (%) 1 42.0 2 1.1x 44.7 (+2.7) 3 1.5x 46.1 (+4.1) 4 1.8x 46.3 (+4.3) 4 1.8x 47.8 (+5.8) 4 1.8x 48.7 (+6.7) 4 1.8x 49.3 (+7.3) 4 1.3x 49.1 (+7.1)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported, in part, by Berkeley Deep Drive, US Government fund through Etegent Technologies on Low-Shot Detection and Semi-supervised Detection, and NTU NAP and A*STAR via Industry Alignment Fund: Industry Collaboration Projects Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1567" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03673</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified bias-variance decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th International Conference on Machine Learning</title>
		<meeting>17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7610" to="7619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1567" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">M2m: Imbalanced classification via major-tominor translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13896" to="13905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep representation learning on long-tailed data: A learnable embedding augmentation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuchu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Largescale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations with probabilistic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distribution-balanced loss for multi-label classification in long-tailed datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="162" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="247" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BBN: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inflated episodic memory with region self-attention for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4344" to="4353" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
