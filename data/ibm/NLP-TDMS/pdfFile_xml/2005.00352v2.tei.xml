<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
							<email>louismartin@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
							<email>angelafan@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">LORIA</orgName>
								<address>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>De La Clergerie</surname></persName>
							<email>eric.de_la_clergerie@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
							<email>abordes@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
							<email>benoit.sagot@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Progress in sentence simplification has been hindered by a lack of labeled parallel simplification data, particularly in languages other than English. We introduce MUSS, a Multilingual Unsupervised Sentence Simplification system that does not require labeled simplification data. MUSS uses a novel approach to sentence simplification that trains strong models using sentence-level paraphrase data instead of proper simplification data. These models leverage unsupervised pretraining and controllable generation mechanisms to flexibly adjust attributes such as length and lexical complexity at inference time. We further present a method to mine such paraphrase data in any language from Common Crawl using semantic sentence embeddings, thus removing the need for labeled data. We evaluate our approach on English, French, and Spanish simplification benchmarks and closely match or outperform the previous best supervised results, despite not using any labeled simplification data. We push the state of the art further by incorporating labeled simplification data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence simplification is the task of making a sentence easier to read and understand by reducing its lexical and syntactic complexity, while retaining most of its original meaning. Simplification has a variety of important societal applications, for example increasing accessibility for those with cognitive disabilities such as aphasia <ref type="bibr">(Carroll et al., 1998)</ref>, dyslexia <ref type="bibr" target="#b23">(Rello et al., 2013)</ref>, and autism <ref type="bibr">(Evans et al., 2014)</ref>, or for non-native speakers <ref type="bibr" target="#b18">(Paetzold and Specia, 2016)</ref>. Research has mostly focused on English simplification, where source texts and associated simplified texts exist and can be automatically aligned, such as English Wikipedia and Simple English Wikipedia <ref type="bibr" target="#b39">(Zhang and Lapata, 2017)</ref>. However, such data is limited in terms of size and domain, and difficult to find in other languages. Additionally, simplifying a sentence can be achieved in multiple ways, and depend on the target audience. Simplification guidelines are not uniquely defined, outlined by the stark differences in English simplification benchmarks <ref type="bibr">(Alva-Manchego et al., 2020a)</ref>. This highlights the need for more general models that can adjust to different simplification contexts and scenarios.</p><p>In this paper, we propose to train controllable models using sentence-level paraphrase data only, i.e. parallel sentences that have the same meaning but phrased differently. In order to generate simplifications and not paraphrases, we use ACCESS <ref type="bibr">(Martin et al., 2020)</ref> to control attributes such as length, lexical and syntactic complexity. Paraphrase data is more readily available, and opens the door to training flexible models that can adjust to more varied simplification scenarios. We propose to gather such paraphrase data in any language by mining sentences from Common Crawl using semantic sentence embeddings. We show that simplification models trained on mined paraphrase data perform as well as models trained on existing large paraphrase corpora (cf. Appendix D). Moreover, paraphrases are more straightforward to mine than simplifications, and we show that they lead to models with better performance than equivalent models trained on mined simplifications (cf. Section 5.4).</p><p>Our resulting Multilingual Unsupervised Sentence Simplification method, MUSS, is unsupervised because it can be trained without relying on labeled simplification data, 1 even though we mine using supervised sentence embeddings. <ref type="bibr">2</ref> We additionally incorporate unsupervised pretraining <ref type="bibr" target="#b12">(Liu et al., , 2020</ref> and apply MUSS on English, <ref type="bibr">1</ref> We use the term labeled simplifications to refer to parallel datasets where texts were manually simplified by humans.</p><p>2 Previous works have also used the term unsupervised simplification to describe works that do not use any labeled parallel simplification data while leveraging supervised components such as constituency parsers and knowledge bases <ref type="bibr" target="#b8">(Kumar et al., 2020)</ref>, external synonymy lexicons <ref type="bibr" target="#b29">(Surya et al., 2019)</ref>, and databases of simplified synonyms <ref type="bibr" target="#b41">(Zhao et al., 2020)</ref>. We shall come back to these works in Section 2.</p><p>French, and Spanish to closely match or outperform the supervised state of the art in all languages. MUSS further improves the state of the art on all English datasets by incorporating additional labeled simplification data.</p><p>To sum up, our contributions are as follows:</p><p>• We introduce a novel approach to training simplification models with paraphrase data only and propose a mining procedure to create large paraphrase corpora for any language.</p><p>• Our approach obtains strong performance. Without any labeled simplification data, we match or outperform the supervised state of the art in English, French and Spanish. We further improve the English state of the art by incorporating labeled simplification data.</p><p>• We release MUSS pretrained models, paraphrase data, and code for mining and training 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Data-driven methods have been predominant in English sentence simplification in recent years <ref type="bibr">(Alva-Manchego et al., 2020b)</ref>, requiring large supervised training corpora of complex-simple aligned sentences <ref type="bibr" target="#b36">(Wubben et al., 2012;</ref><ref type="bibr" target="#b38">Xu et al., 2016;</ref><ref type="bibr" target="#b39">Zhang and Lapata, 2017;</ref><ref type="bibr" target="#b40">Zhao et al., 2018;</ref><ref type="bibr">Martin et al., 2020)</ref>. Methods have relied on English and Simple English Wikipedia with automatic sentence alignment from similar articles <ref type="bibr" target="#b42">(Zhu et al., 2010;</ref><ref type="bibr">Coster and Kauchak, 2011;</ref><ref type="bibr" target="#b35">Woodsend and Lapata, 2011;</ref><ref type="bibr" target="#b3">Kauchak, 2013;</ref><ref type="bibr" target="#b39">Zhang and Lapata, 2017)</ref>. Higher quality datasets have been proposed such as the Newsela corpus <ref type="bibr" target="#b37">(Xu et al., 2015)</ref>, but they are rare and come with restrictive licenses that hinder reproducibility and widespread usage. Simplification in other languages has been explored in Brazilian Portuguese <ref type="bibr" target="#b0">(Aluísio et al., 2008)</ref>, Spanish , <ref type="bibr">Italian (Brunato et al., 2015;</ref><ref type="bibr" target="#b31">Tonelli et al., 2016)</ref>, Japanese <ref type="bibr">(Goto et al., 2015;</ref><ref type="bibr">Kajiwara and Komachi, 2018;</ref><ref type="bibr" target="#b2">Katsuta and</ref><ref type="bibr" target="#b2">Yamamoto, 2019), and</ref><ref type="bibr">French (Gala et al., 2020)</ref>, but the lack of a large labeled parallel corpora has slowed research down. In this work, we show that a method trained on automatically mined corpora can reach state-ofthe-art results in each language.</p><p>When labeled parallel simplification data is unavailable, systems rely on unsupervised simplification techniques, often inspired from machine translation. The prevailing approach is to split a monolingual corpora into disjoint sets of complex and simple sentences using readability metrics. Then simplification models can be trained by using automatic sentence alignments <ref type="bibr">(Kajiwara and</ref><ref type="bibr">Komachi, 2016, 2018)</ref>, auto-encoders <ref type="bibr" target="#b29">(Surya et al., 2019;</ref><ref type="bibr" target="#b41">Zhao et al., 2020)</ref>, unsupervised statistical machine translation <ref type="bibr" target="#b2">(Katsuta and</ref><ref type="bibr">Yamamoto, 2019), or back-translation (Aprosio et al., 2019)</ref>. Other unsupervised simplification approaches iteratively edit the sentence until a certain simplicity criterion is reached <ref type="bibr" target="#b8">(Kumar et al., 2020)</ref>. The performance of unsupervised methods are often below their supervised counterparts. MUSS bridges the gap with supervised method and removes the need for deciding in advance how complex and simple sentences should be separated, but instead trains directly on paraphrases mined from the raw corpora.</p><p>Previous work on parallel dataset mining have been used mostly in machine translation using document retrieval <ref type="bibr" target="#b16">(Munteanu and Marcu, 2005)</ref>, language models <ref type="bibr" target="#b6">(Koehn et al., 2018</ref><ref type="bibr" target="#b5">(Koehn et al., , 2019</ref>, and embedding space alignment (Artetxe and <ref type="bibr">Schwenk, 2019b)</ref> to create large corpora <ref type="bibr" target="#b30">(Tiedemann, 2012;</ref><ref type="bibr" target="#b25">Schwenk et al., 2019)</ref>. We focus on paraphrasing for sentence simplifications, which presents new challenges. Unlike machine translation, where the same sentence should be identified in two languages, we develop a method to identify varied paraphrases of sentences, that have a wider array of surface forms, including different lengths, multiple sentences, different vocabulary usage, and removal of content from more complex sentences.</p><p>Previous unsupervised paraphrasing research has aligned sentences from various parallel corpora <ref type="bibr">(Barzilay and Lee, 2003)</ref> with multiple objective functions . Bilingual pivoting relied on MT datasets to create large databases of word-level paraphrases <ref type="bibr" target="#b21">(Pavlick et al., 2015)</ref>, lexical simplifications <ref type="bibr" target="#b7">Kriz et al., 2018)</ref>, or sentence-level paraphrase corpora <ref type="bibr" target="#b34">(Wieting and Gimpel, 2018</ref>). This has not been applied to multiple languages or to sentence-level simplification. Additionally, we use raw monolingual data to create our paraphrase corpora instead of relying on parallel MT datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we describe MUSS, our approach to mining paraphrase data and training controllable simplification models on paraphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mining Paraphrases in Many Languages</head><p>Sequence Extraction Simplification consists of multiple rewriting operations, some of which span multiple sentences (e.g. sentence splitting or fusion). To allow such operations to be represented in our paraphrase data, we extract chunks of texts composed of multiple sentences, we refer to these small pieces of text by sequences. We extract such sequences by first tokenizing a document into individual sentences {s 1 , s 2 , . . . , s n } using the NLTK sentence tokenizer <ref type="bibr">(Bird and Loper, 2004)</ref>. We then extract sequences of adjacent sentences with maximum length of 300 characters: e.g.</p><formula xml:id="formula_0">{[s 1 ], [s 1 , s 2 ], [s 1 , . . . , s k ], [s 2 ], [s 2 , s 3 ], ...}.</formula><p>We can thus align two sequences that contain a different number of sentences, and represent sentence splitting or sentence fusion operations.</p><p>These sequences are further filtered to remove noisy sequences with more than 10% punctuation characters and sequences with low language model probability according to a 3-gram Kneser-Ney language model trained with kenlm (Heafield, 2011) on Wikipedia.</p><p>We extract these sequences from CCNet , an extraction of Common Crawl (an open source snapshot of the web) that has been split into different languages using fasttext language identification <ref type="bibr">(Joulin et al., 2017)</ref> and various language modeling filtering techniques to identify high quality, clean sentences. For English and French, we extract 1 billion sequences from CCNet. For Spanish we extract 650 millions sequences, the maximum for this language in CCNet after filtering out noisy text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creating a Sequence Index Using Embeddings</head><p>To automatically mine our paraphrase corpora, we first compute n-dimensional embeddings for each extracted sequence using LASER (Artetxe and <ref type="bibr">Schwenk, 2019b)</ref>. LASER provides joint multilingual sentence embeddings in 93 languages that have been successfully applied to the task of bilingual bitext mining <ref type="bibr" target="#b25">(Schwenk et al., 2019)</ref>. In this work, we show that LASER can also be used to mine monolingual paraphrase datasets.</p><p>Mining Paraphrases After computing the embeddings for each language, we index them for fast nearest neighbor search using faiss.</p><p>Each of these sequences is then used as a query q i against the billion-scale index that returns a set of top-8 nearest neighbor sequences according to the semantic LASER embedding space using L2 distance, resulting in a set of candidate paraphrases are {c i,1 , . . . , c i,8 }.</p><p>We then use an upper bound on L2 distance and a margin criterion following (Artetxe and <ref type="bibr">Schwenk, 2019a)</ref> to filter out sequences with low similarity. We refer the reader to Appendix Section A.1 for technical details.</p><p>The resulting nearest neighbors constitute a set of aligned paraphrases of the query sequence: {(q i , c i,1 ), . . . , (q i , c i,j )}. We finally apply poor alignment filters. We remove sequences that are almost identical with character-level Levenshtein distance ≤ 20%, when they are contained in one another, or when they were extracted from two overlapping sliding windows of the same original document.</p><p>We report statistics of the mined corpora in English, French and Spanish in <ref type="table">Table 1</ref>, and qualitative examples of the resulting mined paraphrases in Appendix <ref type="table" target="#tab_11">Table 6</ref>. Models trained on the resulting mined paraphrases obtain similar performance than models trained on existing paraphrase datasets (cf. Appendix Section D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simplifying with ACCESS</head><p>In this section we describe how we adapt ACCESS <ref type="bibr">(Martin et al., 2020)</ref> to train controllable models on mined paraphrases, instead of labeled parallel simplifications.</p><p>ACCESS is a method to make any seq2seq model controllable by conditioning on simplification-specific control tokens. We apply it on our seq2seq pretrained transformer models based on the BART <ref type="bibr" target="#b10">(Lewis et al., 2019)</ref> architecture (see next subsection).</p><p>Training with Control Tokens At training time, the model is provided with control tokens that give oracle information on the target sequence, such as the amount of compression of the target sequence relative to the source sequence (length control). For example, when the target sequence is 80% of the length of the original sequence, we provide the &lt;NumChars_80%&gt; control token. At inference time we can then control the generation by selecting a given target control value. We adapt the original Levenshtein similarity control to only consider replace operations but otherwise use the same controls as <ref type="bibr">Martin et al. (2020)</ref>. The controls used are therefore character length ratio, replace-only Levenshtein similarity, aggregated word frequency ratio, and dependency tree depth ratio. For instance we will prepend to every source in the training set the following 4 control tokens with samplespecific values, so that the model learns to rely on them: &lt;NumChars_XX%&gt; &lt;LevSim_YY%&gt; &lt;WordFreq_ZZ%&gt; &lt;DepTreeDepth_TT%&gt;. We refer the reader to the original paper <ref type="bibr">(Martin et al., 2020)</ref> and Appendix A.2 for details on ACCESS and how those control tokens are computed.</p><p>Selecting Control Values at Inference Once the model has been trained with oracle controls, we can adjust the control tokens to obtain the desired type of simplifications. Indeed, sentence simplification often depends on the context and target audience <ref type="bibr">(Martin et al., 2020)</ref>. For instance shorter sentences are more adapted to people with cognitive disabilities, while using more frequent words are useful to second language learners. It is therefore important that supervised and unsupervised simplification systems can be adapted to different conditions: <ref type="bibr" target="#b8">(Kumar et al., 2020)</ref> do so by choosing a set of operation-specific weights of their unsupervised simplification model for each evaluation dataset, <ref type="bibr" target="#b29">(Surya et al., 2019)</ref> select different models using SARI on each validation set. Similarly, we set the 4 control hyper-parameters of ACCESS using SARI on each validation set and keep them fixed for all samples in the test set. 4 . These 4 control hyper-parameters are intuitive and easy to interpret: when no validation set is available, they can also be set using prior knowledge on the task and 4 Details in Appendix A.2 still lead to solid performance (cf. Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Leveraging Unsupervised Pretraining</head><p>We combine our controllable models with unsupervised pretraining to further extend our approach to text simplification. For English, we finetune the pretrained generative model BART <ref type="bibr" target="#b10">(Lewis et al., 2019)</ref> on our newly created training corpora.</p><p>BART is a pretrained sequence-to-sequence model that can be seen as a generalization of other recent pretrained models such as <ref type="bibr">BERT (Devlin et al., 2018)</ref>. For non-English, we use its multilingual generalization MBART <ref type="bibr" target="#b12">(Liu et al., 2020)</ref>, which was pretrained on 25 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setting</head><p>We assess the performance of our approach on three languages: English, French, and Spanish. We detail our experimental procedure for mining and training in Appendix Section A. In all our experiments, we report scores on the test sets averaged over 5 random seeds with 95% confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>In addition to comparisons with previous works, we implement and hereafter describe multiple baselines to assess the performance of our models, especially for French and Spanish where no previous simplification systems are available.</p><p>Identity The entire original sequence is kept unchanged and used as the simplification.</p><p>Truncation The original sequence is truncated to the first 80% words. It is a strong baseline according to standard simplification metrics.</p><p>Pivot We use machine translation to provide a baseline for languages for which no simplification corpus is available. The source non-English sentence is translated to English, simplified with our best supervised English simplification system, and then translated back into the source language. For French and Spanish translation, we use CCMATRIX <ref type="bibr" target="#b25">(Schwenk et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Systems</head><p>BTRLTS <ref type="bibr" target="#b41">(Zhao et al., 2020)</ref> 33.95 7.59 33.09 8.39 37.22 3.80 UNTS <ref type="bibr" target="#b29">(Surya et al., 2019)</ref> 35.19 7.60 36.29 7.60 --RM+EX+LS+RO <ref type="bibr" target="#b8">(Kumar et al., 2020)</ref> 36  <ref type="bibr" target="#b39">(Zhang and Lapata, 2017)</ref> 36.59 7.66 36.97 7.66 38.00 4.90 DMASS-DCSS <ref type="bibr" target="#b40">(Zhao et al., 2018)</ref> 38.67 7.73 39.92 7.73 --ACCESS <ref type="bibr">(Martin et al., 2020)</ref> 40  Gold Reference We report gold reference scores for ASSET and TurkCorpus as multiple references are available. We compute scores in a leave-oneout scenario where each reference is evaluated against all others. The scores are then averaged over all references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We evaluate with the standard metrics SARI and FKGL. We report BLEU <ref type="bibr" target="#b19">(Papineni et al., 2002)</ref> only in Appendix <ref type="table" target="#tab_15">Table 10</ref> due its dubious suitability for sentence simplification <ref type="bibr" target="#b28">(Sulem et al., 2018)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SARI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Data</head><p>For all languages we use the mined data described in <ref type="table">Table 1</ref> as training data. We show that training with additional labeled simplification data leads to even better performance for English. We use the labeled datasets WikiLarge <ref type="bibr" target="#b39">(Zhang and Lapata, 2017)</ref> and Newsela <ref type="bibr" target="#b37">(Xu et al., 2015)</ref>. WikiLarge is composed of 296k simplification pairs automatically aligned from English Wikipedia and Simple English Wikipedia. Newsela is a collection of news articles with professional simplifications, aligned into 94k simplification pairs by <ref type="bibr" target="#b39">Zhang and Lapata (2017</ref> Spanish For Spanish we use the Spanish part of Newsela <ref type="bibr" target="#b37">(Xu et al., 2015)</ref>. We use the alignments from (Aprosio et al., 2019), composed of 2794 validation and 2795 test sentence pairs. Even though sentences were aligned using the CATS simplification alignment tool <ref type="bibr">(Štajner et al., 2018)</ref>, some alignment errors remain and automatic scores should be taken with a pinch of salt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">English Simplification</head><p>We compare models trained on our mined corpus (see <ref type="table">Table 1</ref>) with models trained on labeled simplification datasets (WikiLarge and Newsela). We also compare to other state-of-the-art supervised models: Dress-LS <ref type="bibr" target="#b39">(Zhang and Lapata, 2017)</ref>, DMASS-DCSS <ref type="bibr" target="#b40">(Zhao et al., 2018)</ref>, <ref type="bibr">EditNTS (Dong et al., 2019)</ref>, ACCESS <ref type="bibr">(Martin et al., 2020)</ref>; and the unsupervised models: UNTS <ref type="bibr" target="#b29">(Surya et al., 2019)</ref>, BTRLTS <ref type="bibr" target="#b41">(Zhao et al., 2020)</ref>, and RM+EX+LS+RO <ref type="bibr" target="#b8">(Kumar et al., 2020)</ref>. Results are shown in <ref type="table" target="#tab_4">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUSS Unsupervised Results</head><p>On the ASSET benchmark, with no labeled simplification data, MUSS obtains a +5.98 SARI improvement with respect to previous unsupervised methods, and a +2.52 SARI improvement over the state-of-theart supervised methods. For the TurkCorpus and Newsela datasets, the unsupervised MUSS approach achieves strong results, either outperforming or closely matching both unsupervised and supervised previous works. When incorporating labeled data from Wiki-Large and Newsela, MUSS obtains state-of-the-art results on all datasets. Using labeled data along with mined data does not always help compared to training only with labeled data, especially with the Newsela training set. Newsela is already a high quality dataset focused on the specific domain of news articles. It might not benefit from additional lesser quality mined data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of Simplifications</head><p>Various examples from our unsupervised system are shown in <ref type="table">Table 4</ref>. Examining the simplifications, we see reduced sentence length, sentence splitting, and simpler vocabulary usage. For example, the words in the town's western outskirts is changed into near the town and aerial nests is simplified into nests in the air. We also witnessed errors related factual consistency and especially with respect with named entity hallucination or disappearance which would be an interesting area of improvement for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">French and Spanish Simplification</head><p>Our unsupervised approach to simplification can be applied to any language. Similar to English, we first create a corpus of paraphrases composed of 1.4 million sequence pairs in French and 1.0 million sequence pairs in Spanish (cf. <ref type="table">Table 1)</ref>. To incorporate multilingual pretraining, we replace the monolingual BART with MBART, which was trained on 25 languages.</p><p>We report the performance of models trained on the mined corpus in <ref type="table" target="#tab_6">Table 3</ref>. Unlike English, where labeled parallel training data has been created using Simple English Wikipedia, no such datasets exist for French or Spanish. Similarly, no other simplification systems are available in these languages. We thus compare to several baselines, namely the identity, truncation and the strong pivot baseline.</p><p>Results MUSS outperforms our strongest baseline by +8.25 SARI for French, while matching the pivot baseline performance for Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>History Landsberg prison, which is in the town's western outskirts, was completed in 1910. Simplified The Landsberg prison, which is near the town, was built in 1910.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>The name "hornet" is used for this and related species primarily because of their habit of making aerial nests (similar to the true hornets) rather than subterranean nests. Simplified The name "hornet" is used for this and related species because they make nests in the air (like the true hornets) rather than in the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Nocturnes is an orchestral composition in three movements by the French composer Claude Debussy. Simplified Nocturnes is a piece of music for orchestra by the French composer Claude Debussy. <ref type="table">Table 4</ref>: Examples of Generated Simplifications. We show simplifications generated by our best unsupervised model: MUSS trained on mined data only. Bold highlights differences between original and simplified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2Seq</head><p>BART+ACCESS <ref type="formula">20</ref>  Besides using state-of-the-art machine translation models, the pivot baseline relies on a strong backbone simplification model that has two advantages compared to the French and Spanish simplification model. First the simplification model of the pivot baseline was trained on labeled simplification data from WikiLarge, which obtains +1.5 SARI in English compared to training only on mined data. Second it uses the stronger monolingual BART model instead of <ref type="bibr">MBART.</ref> In Appendix <ref type="table" target="#tab_15">Table 10</ref>, we can see that MBART has a small loss in performance of 1.54 SARI compared to its monolingual counterpart BART, due to the fact that it handles 25 languages instead of one. Further improvements could be achieved by using monolingual BART models trained for French or Spanish, possibly outperforming the pivot baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Human Evaluation</head><p>To further validate the quality of our models, we conduct a human evaluation in all languages according to adequacy, fluency, and simplicity and report the results in <ref type="table">Table 5</ref>.</p><p>Human Ratings Collection For human evaluation, we recruit volunteer native speakers for each language (5 in English, 2 in French, and 2 in Spanish). We evaluate three linguistic aspects on a 5 point Likert scale (0-4): adequacy (is the meaning preserved?), fluency (is the simplification fluent?) and simplicity (is the simplification actually simpler?). For each system and each language, 50 simplifications are annotated and each simplification is rated once only by a single annotator. The simplifications are taken from ASSET (English), ALECTOR (French), and Newsela (Spanish).</p><p>Discussion <ref type="table">Table 5</ref> displays the average ratings along with 95% confidence intervals. Human judgments confirm that our unsupervised and supervised MUSS models are more fluent and produce simpler outputs than previous state-of-the-art <ref type="bibr" target="#b14">(Martin et al., 2019)</ref>. They are deemed as fluent and simpler than the human simplifications from AS-SET test set, which indicates our model is able to reach a high level of simplicity thanks to the control mechanism. In French and Spanish, our unsupervised model performs better or similar in all aspects than the strong supervised pivot baseline which has been trained on labeled English simplifications. In Spanish the gold reference surprisingly obtains poor human ratings, which we found to be caused by errors in the automatic alignments of source sentences with simplified sentences of the same article, as previously highlighted in Subsection 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>French</head><p>Spanish Adequacy Fluency Simplicity Adequacy Fluency Simplicity Adequacy Fluency Simplicity ACCESS <ref type="bibr">(Martin et al., 2020)</ref> 3.10±0.32 3.46±0.28 1.40±0.29  <ref type="table">Table 5</ref>: Human Evaluation We display human ratings of adequacy, fluency and simplicity for previous work ACCESS, pivot baseline, reference human simplifications, our best unsupervised systems (BART+ACCESS for English, MBART+ACCESS for other languages), and our best supervised model for English. Scores are averaged over 50 ratings per system with 95% confidence intervals. †Low ratings of the gold reference in Spanish Newsela is due to automatic alignment errors.</p><formula xml:id="formula_1">- - - - - - Pivot baseline - - - 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablations</head><p>Mining Simplifications vs. Paraphrases In this work, we mined paraphrases to train simplification models. This has the advantage of making fewer assumptions earlier on, by keeping the mining and models as general as possible, so that they are able to adapt to more simplification scenarios.</p><p>We also compared to directly mining simplifications using simplification heuristics to make sure that the target side is simpler than the source, following previous work (Kajiwara and Komachi, 2016; <ref type="bibr" target="#b29">Surya et al., 2019)</ref>. To mine a simplification dataset, we followed the same paraphrase mining procedure of querying 1 billion sequences on an index of 1 billion sequences. Out of the resulting paraphrases, we kept only pairs that either contained sentence splits, reduced sequence length, or simpler vocabulary (similar to how previous work enforce an FKGL difference). We removed the paraphrase constraint that enforced sentences to be different enough. We tuned these heuristics to optimize SARI on the validation set. The resulting dataset has 2.7 million simplification pairs. In <ref type="figure" target="#fig_0">Figure 1a</ref>, we show that seq2seq models trained on mined paraphrases achieve better performance. A similar trend exists with BART and ACCESS, thus confirming that mining paraphrases can obtain better performance than mining simplifications.</p><p>How Much Mined Data Do You Need? We investigate the importance of a scalable mining approach that can create million-sized training corpora for sentence simplification. In <ref type="figure" target="#fig_0">Figure 1b</ref>, we analyze the performance of training our best model on English on different amounts of mined data. By increasing the number of mined pairs, SARI drastically improves, indicating that efficient mining at scale is critical to performance. Unlike humancreated training sets, unsupervised mining allows for large datasets in multiple languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvements from Pretraining and Control</head><p>We compare the respective influence of pretraining BART and controllable generation ACCESS in <ref type="figure" target="#fig_0">Figure 1c</ref>. While both BART and ACCESS bring improvement over standard sequence-to-sequence, they work best in combination. Unlike previous approaches to text simplification, we use pretraining to train our simplification systems. We find that the main qualitative improvement from pretraining is increased fluency and meaning preservation. For example, in Appendix <ref type="table">Table 9</ref>, the model trained only with ACCESS substituted culturally akin with culturally much like, but when using BART, it is simplified to the more fluent closely related. While models trained on mined data see several million sentences, pretraining methods are typically trained on billions. Combining pretraining with controllable simplification enhances simplification performance by flexibly adjusting the type of simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a sentence simplification approach that does not rely on labeled parallel simplification data thanks to controllable generation, pretraining and large-scale mining of paraphrases from the web. This approach is language-agnostic and matches or outperforms previous state-of-the-art results, even from supervised systems that use labeled simplification data, on three languages: English, French, and Spanish. In future work, we plan to investigate how to scale this approach to more languages and types of simplification, and to apply this method to paraphrase generation. Another interesting direction for future work would to examine and improve factual consistency, especially related to named entity hallucination or disappearance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental details</head><p>In this section we describe specific details of our experimental procedure. <ref type="figure">Figure 2</ref> is a overall reminder of our method presented in the main paper. <ref type="figure">Figure 2</ref>: Sentence Simplification Models for Any Language without Simplification Data. Sentences from the web are used to create a large scale index that allows mining millions of paraphrases. Subsequently, we finetune pretrained models augmented with controllable mechanisms on the paraphrase corpora to achieve sentence simplification models in any language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Mining Details</head><p>Sequence Extraction We only consider documents from the HEAD split in CCNet-this represents the third of the data with the best perplexity using a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase Mining</head><p>We compute LASER embeddings of dimension 1024 and reduce dimensionality with a 512 PCA followed by random rotation. We further compress them using 8 bit scalar quantization. The compressed embeddings are then stored in a faiss inverted file index with 32,768 cells (nprobe=16). These embeddings are used to mine pairs of paraphrases. We return the top-8 nearest neighbors, and keep those with L2 distance lower than 0.05 and relative distance compared to other top-8 nearest neighbors lower than 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrases Filtering</head><p>The resulting paraphrases are filtered to remove almost identical paraphrases by enforcing a case-insensitive character-level Levenshtein distance <ref type="bibr" target="#b9">(Levenshtein, 1966)</ref> greater or equal to 20%. We remove paraphrases that come from the same document to avoid aligning sequences that overlapped each other in the text. We also remove paraphrases where one of the sequence is contained in the other. We further filter out any sequence that is present in our evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>Seq2Seq training We implement our models with fairseq <ref type="bibr" target="#b17">(Ott et al., 2019)</ref>. All our models are Transformers <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> based on the BART Large architecture (388M parameters), keeping the optimization procedure and hyper-parameters fixed to those used in the original implementation <ref type="bibr">(Lewis et al., 2019) 7</ref> . We either randomly initialize weights for the standard sequence-to-sequence experiments or initialize with pretrained BART for the BART experiments. When initializing the weights randomly, we use a learning rate of 3.10 −4 versus the original 3.10 −5 when finetuning BART. For a given seed, the model is trained on 8 Nvidia V100 GPUs during approximately 10 hours.</p><p>Controllable Generation For controllable generation, we use the open-source ACCESS implementation <ref type="bibr" target="#b13">(Martin et al., 2018)</ref>. We use the same control parameters as the original paper, namely length, Levenshtein similarity, lexical complexity, and syntactic complexity. <ref type="bibr">8</ref> As mentioned in Section "Simplifying with ACCESS", we select the 4 ACCESS hyperparameters using SARI on the validation set. We use zero-order optimization with the NEVERGRAD library <ref type="bibr" target="#b22">(Rapin and Teytaud, 2018)</ref>. We use the One-PlusOne optimizer with a budget of 64 evaluations (approximately 1 hour of optimization on a single GPU). The hyper-parameters are contained in the [0.2, 1.5] interval.</p><p>The 4 hyper-parameter values are then kept fixed for all sentences in the associated test set.</p><p>Translation Model for Pivot Baseline For the pivot baseline we train models on ccMatrix <ref type="bibr" target="#b25">(Schwenk et al., 2019)</ref>. Our models use the Transformer architecture with 240 million parameters with <ref type="bibr">LayerDrop (Fan et al., 2019)</ref>. We train for 36 hours on 8 GPUs following the suggested parameters in <ref type="bibr" target="#b17">Ott et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Reference Baseline</head><p>To avoid creating a discrepancy in terms of number of references be-7 All hyper-parameters and training commands for fairseq can be found here: https://github.com/ pytorch/fairseq/blob/master/examples/ bart/README.summarization.md tween the gold reference scores, where we leave one reference out, and when we evaluate the models with all references, we compensate by duplicating one of the other references at random so that the total number of references is unchanged.</p><p>A.3 Evaluation Details SARI score computation We use the latest version of SARI implemented in EASSE <ref type="bibr">(Alva-Manchego et al., 2019)</ref> which fixes bugs and inconsistencies from the traditional implementation of SARI. As a consequence, we also recompute scores from previous systems that we compare to. We do so by using the system predictions provided by the respective authors, and available in EASSE.</p><p>ALECTOR Sentence-level Alignment The ALECTOR corpus comes as source documents and their manual simplifications but not sentence-level alignment is provided. Luckily, most of these documents were simplified line by line, each line consisting of a few sentences. For each source document, we therefore align each line, provided it is not too long (less than 6 sentences), with the most appropriate line in the simplified document, using the LASER embedding space. The resulting alignments are split into validation and test by randomly sampling the documents for the validation (450 sentence pairs) and rest for test (416 sentence pairs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Links to Datasets</head><p>The datasets we used are available at the following addresses:</p><p>• CCNet: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Characteristics of the mined data</head><p>We show in <ref type="figure" target="#fig_1">Figure 3</ref> the distribution of different surface features of our mined data versus those of WikiLarge. Some examples of mined paraphrases are shown in <ref type="table" target="#tab_11">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Set ACCESS Control Parameters Without Parallel Data</head><p>In our experiments we adjusted our model to the different dataset conditions by selecting our ACCESS control tokens with SARI on each validation set. When no such parallel validation set exists, we show that strong performance can still be obtained by using prior knowledge for the given downstream application. This can be done by setting all 4 ACCESS control hyper-parameters to an intuitive guess of the desired compression ratio.</p><p>To illustrate this for the considered evaluation datasets, we first independently sample 50 source sentences and 50 random unaligned simple sentences from each validation set. These two groups of non-parallel sentences are used to approximate the character-level compression ratio between complex and simplified sentences. We do so by dividing the average length of the simplified sentences by the average length of the 50 source sentences. We finally use this approximated compression ratio as the value of all 4 ACCESS hyper-parameters. In practice, we obtain the following approximations: ASSET = 0.8, TurkCorpus = 0.95, and Newsela = 0.4 (rounded to 0.05). Results in <ref type="table" target="#tab_12">Table 7</ref> show that the resulting model performs very close to when we adjust the ACCESS hyper-parameters using SARI on the complete validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Comparing to Existing Paraphrase Datasets</head><p>We compare using our mined paraphrase data with existing large-scale paraphrase datasets in Table 8. We use PARANMT <ref type="bibr" target="#b34">(Wieting and Gimpel, 2018)</ref>, a large paraphrase dataset created using back-translation on an existing labeled parallel machine translation dataset. We use the same 5 million top-scoring sentences that the authors used to train their sentence embeddings. Training MUSS on the mined data or on PARANMT obtains similar results for text simplification, confirming that mining  <ref type="bibr">(Martin et al., 2020)</ref>. Replace-only Levenshtein similarity only considers replace operations in the traditional Levenshtein similarity and assigns 0 weights to insertions and deletions.</p><p>Query For insulation, it uses foam-injected polyurethane which helps ensure the quality of the ice produced by the machine. It comes with an easy to clean air filter. Mined It has polyurethane for insulation which is foam-injected. This helps to maintain the quality of the ice it produces. The unit has an easy to clean air filter.</p><p>Query Here are some useful tips and tricks to identify and manage your stress. Mined Here are some tips and remedies you can follow to manage and control your anxiety.</p><p>Query As cancer cells break apart, their contents are released into the blood. Mined When brain cells die, their contents are partially spilled back into the blood in the form of debris.</p><p>Query The trail is ideal for taking a short hike with small children or a longer, more rugged overnight trip. Mined It is the ideal location for a short stroll, a nature walk or a longer walk.</p><p>Query Thank you for joining us, and please check out the site.</p><p>Mined Thank you for calling us. Please check the website.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Influence of BART on Fluency</head><p>In <ref type="table">Table 9</ref>, we present some selected samples that highlight the improved fluency of simplifications when using BART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Scores</head><p>BLEU We report additional BLEU scores for completeness. These results are displayed along with SARI and FKGL for English. These BLEU scores should be carefully interpreted. They have been found to correlate poorly with human judgments of simplicity <ref type="bibr" target="#b28">(Sulem et al., 2018)</ref>. Furthermore, the identity baseline achieves very high BLEU scores on some datasets (e.g. 92.81 on AS-SET or 99.36 on TurkCorpus), which underlines the weaknesses of this metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation Scores</head><p>We report English validation scores to foster reproducibility in <ref type="table" target="#tab_16">Table 11</ref>.</p><p>Seq2Seq Models on Mined Data When training a Transformer sequence-to-sequence model (Seq2Seq) on WikiLarge compared to the mined corpus, models trained on the mined data perform better. It is surprising that a model trained solely on Original They are culturally akin to the coastal peoples of Papua New Guinea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCESS</head><p>They're culturally much like the Papua New Guinea coastal peoples. BART+ACCESS They are closely related to coastal people of Papua New Guinea</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Orton and his wife welcomed Alanna Marie Orton on July 12, 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCESS</head><p>Orton and his wife had been called Alanna Marie Orton on July 12. BART+ACCESS Orton and his wife gave birth to Alanna Marie Orton on July 12, 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>He settled in London, devoting himself chiefly to practical teaching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCESS</head><p>He set up in London and made himself mainly for teaching. BART+ACCESS He settled in London and devoted himself to teaching. <ref type="table">Table 9</ref>: Influence of BART on Simplifications. We display some examples of generations that illustrate how BART improves the fluency and meaning preservation of generated simplifications. paraphrases achieves such good results on simplification benchmarks. Previous works have shown that simplification models suffer from not making enough modifications to the source sentence and found that forcing models to rewrite the input was beneficial <ref type="bibr" target="#b36">(Wubben et al., 2012;</ref><ref type="bibr">Martin et al., 2020)</ref>. This is confirmed when investigating the F1 deletion component of SARI which is 20 points higher for the model trained on paraphrases.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Ablations We display averaged SARI scores on the English ASSET test set with 95% confidence intervals (5 runs). (a) Models trained on mined simplifications or mined paraphrases, (b) MUSS trained on varying amounts of mined data, (c) Models trained with or without BART and/or ACCESS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Density of several text features in WikiLarge and our mined data. The WordRank ratio is a measure of lexical complexity reduction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Unsupervised and Supervised Sentence Simplification for English. We display SARI and FKGL on ASSET, TurkCorpus and Newsela test sets for English. Supervised models are trained on WikiLarge for the first two test sets, and Newsela for the last. Best SARI scores within confidence intervals are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Sentence simplification is commonly evaluated with SARI<ref type="bibr" target="#b38">(Xu et al., 2016)</ref>, which compares model-generated simplifications with the source sequence and gold references. It averages F1 scores for addition, keep, and deletion operations. We compute SARI with the EASSE simplification evaluation suite(Alva-Manchego et al., 2019). 5</figDesc><table><row><cell></cell><cell>French</cell><cell>Spanish</cell></row><row><cell>Baselines</cell><cell>SARI ↑</cell><cell>SARI ↑</cell></row><row><cell>Identity</cell><cell>26.16</cell><cell>16.99</cell></row><row><cell>Truncate</cell><cell>33.44</cell><cell>27.34</cell></row><row><cell>Pivot</cell><cell>33.48±0.37</cell><cell>36.19±0.34</cell></row><row><cell>MUSS †</cell><cell>41.73±0.67</cell><cell>35.67±0.46</cell></row><row><cell>FKGL We report readability scores using the</cell><cell></cell><cell></cell></row><row><cell>Flesch-Kincaid Grade Level (FKGL) (Kincaid</cell><cell></cell><cell></cell></row><row><cell>et al., 1975), a linear combination of sentence</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Unsupervised Sentence Simplification in French and Spanish. We display SARI scores in French (ALECTOR) and Spanish (Newsela). Best SARI scores within confidence intervals are in bold.†MBART+ACCESS model. lengths and word lengths. FKGL was designed to be used on English texts only, we do not report it on French and Spanish.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>). 64.4 Evaluation DataEnglish We evaluate our English models on AS-SET (Alva-Manchego et al., 2020a), TurkCorpus<ref type="bibr" target="#b38">(Xu et al., 2016)</ref> and Newsela<ref type="bibr" target="#b37">(Xu et al., 2015)</ref>.</figDesc><table><row><cell>TurkCorpus and ASSET were created using the</cell></row><row><cell>same 2000 valid and 359 test source sentences.</cell></row><row><cell>TurkCorpus contains 8 reference simplifications</cell></row><row><cell>per source sentence and ASSET contains 10 ref-</cell></row><row><cell>erences per source. ASSET is a generalization of</cell></row><row><cell>TurkCorpus with a more varied set of rewriting op-</cell></row><row><cell>erations, and considered simpler by human judges</cell></row><row><cell>(Alva-Manchego et al., 2020a). For Newsela, we</cell></row><row><cell>evaluate on the split from (Zhang and Lapata,</cell></row><row><cell>2017), which includes 1129 validation and 1077</cell></row><row><cell>test sentence pairs.</cell></row><row><cell>French For French, we use the ALECTOR</cell></row><row><cell>dataset (Gala et al., 2020) for evaluation. ALEC-</cell></row><row><cell>TOR is a collection of literary (tales, stories) and</cell></row><row><cell>scientific (documentary) texts along with their man-</cell></row><row><cell>ual document-level simplified versions. These doc-</cell></row><row><cell>uments were extracted from material available to</cell></row><row><cell>French primary school pupils. We split the dataset</cell></row><row><cell>in 450 validation and 416 test sentence pairs (see</cell></row><row><cell>Appendix A.3 for details).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>78±0.40 2.10±0.47 1.16±0.31 2.02±0.28 3.48±0.22 2.20±0.29 Gold Reference 3.44±0.28 3.78±0.17 1.80±0.28 3.46±0.25 3.92±0.10 1.66±0.31 2.18 †±0.43 3.38±0.24 1.26 †±0.36 MUSS (unsup.) 3.20±0.28 3.84±0.14 1.88±0.33 2.88±0.34 3.50±0.32 1.22±0.25 2.26±0.29 3.48±0.25 2.56±0.29 MUSS (sup.) 3.12±0.34 3.90±0.14 2.22±0.36</figDesc><table><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Workshopon Integrating Artificial Intelligence and Assistive Technology, pages 7-10.William Coster and DavidKauchak. 2011. Learning to simplify sentences using wikipedia. In Proceedings of the workshop on monolingual text-to-text generation, pages 1-9.</figDesc><table><row><cell>Fernando Alva-Manchego, Louis Martin, Antoine Bor-des, Carolina Scarton, Benoît Sagot, and Lucia Spe-cia. 2020a. ASSET: A dataset for tuning and eval-uation of sentence simplification models with multi-</cell><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under-standing. arXiv preprint arXiv:1810.04805.</cell></row><row><cell>ple rewriting transformations. In Procedings of ACL 2020, pages 4668-4679.</cell><cell>Yue Dong, Zichao Li, Mehdi Rezagholizadeh, and Jackie Chi Kit Cheung. 2019. Editnts: An neural</cell></row><row><cell>Fernando Alva-Manchego, Louis Martin, Carolina Scarton, and Lucia Specia. 2019. EASSE: Eas-ier automatic sentence simplification evaluation. In</cell><cell>programmer-interpreter model for sentence simplifi-cation through explicit editing. In Proceedings of ACL2019, pages 3393-3402.</cell></row><row><cell>EMNLP-IJCNLP: System Demonstrations.</cell><cell>Richard Evans, Constantin Orasan, and Iustin Dor-</cell></row><row><cell>Fernando Alva-Manchego, Carolina Scarton, and Lu-cia Specia. 2020b. Data-Driven Sentence Simplifi-cation: Survey and Benchmark. Computational Lin-guistics, pages 1-87.</cell><cell>nescu. 2014. An evaluation of syntactic simplifi-cation rules for people with autism. In Proceed-ings of the 3rd Workshop on Predicting and Improv-ing Text Readability for Target Reader Populations, pages 131-140.</cell></row><row><cell>Alessio Palmero Aprosio, Sara Tonelli, Marco Turchi, Matteo Negri, and Mattia A Di Gangi. 2019. Neural text simplification in low-resource conditions using weak supervision. In Proceedings of the Workshop</cell><cell>Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing transformer depth on demand with struc-tured dropout. arXiv preprint arXiv:1909.11556.</cell></row><row><cell>on Methods for Optimizing and Evaluating Neural Language Generation, pages 37-44.</cell><cell>Núria Gala, Anaïs Tack, Ludivine Javourey-Drevet, Thomas François, and Johannes C Ziegler. 2020.</cell></row><row><cell>Mikel Artetxe and Holger Schwenk. 2019a. Margin-based parallel corpus mining with multilingual sen-tence embeddings. In Proceedings of the 57th An-</cell><cell>Alector: A Parallel Corpus of Simplified French Texts with Alignments of Misreadings by Poor and Dyslexic Readers. In Proceedings of LREC 2020.</cell></row><row><cell>nual Meeting of the Association for Computational Linguistics, pages 3197-3203, Florence, Italy. Asso-ciation for Computational Linguistics.</cell><cell>Isao Goto, Hideki Tanaka, and Tadashi Kumano. 2015. Japanese news simplification: Task design, data set construction, and analysis of simplified text. Pro-</cell></row><row><cell>Mikel Artetxe and Holger Schwenk. 2019b. Mas-</cell><cell>ceedings of MT Summit XV.</cell></row><row><cell>sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Transac-tions of the Association for Computational Linguis-tics, 7:597-610.</cell><cell>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the sixth workshop on statistical machine translation, pages 187-197.</cell></row><row><cell>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In Proceedings of NAACL-HLT 2003, pages 16-23.</cell><cell>Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. 2020. Neural crf model for sen-tence alignment in text simplification. In Proceed-ings of the 58th Annual Meeting of the Association</cell></row><row><cell>Steven Bird and Edward Loper. 2004. NLTK: The nat-</cell><cell>for Computational Linguistics.</cell></row><row><cell>ural language toolkit. In Proceedings of the ACL In-</cell><cell></cell></row><row><cell>teractive Poster and Demonstration Sessions, pages</cell><cell>Armand Joulin, Edouard Grave, Piotr Bojanowski, and</cell></row><row><cell>214-217, Barcelona, Spain.</cell><cell>Tomas Mikolov. 2017. Bag of tricks for efficient text</cell></row><row><cell></cell><cell>classification. In Proceedings of EACL 2017, pages</cell></row><row><cell>Dominique Brunato, Felice Dell'Orletta, Giulia Ven-</cell><cell>427-431.</cell></row><row><cell>turi, and Simonetta Montemagni. 2015. Design and</cell><cell></cell></row><row><cell>annotation of the first Italian corpus for text simpli-</cell><cell>Tomoyuki Kajiwara and M Komachi. 2018. Text sim-</cell></row><row><cell>fication. In Proceedings of The 9th Linguistic Anno-</cell><cell>plification without simplified corpora. The Journal</cell></row><row><cell>tation Workshop, pages 31-41.</cell><cell>of Natural Language Processing, 25:223-249.</cell></row><row><cell></cell><cell>Tomoyuki Kajiwara and Mamoru Komachi. 2016.</cell></row><row><cell></cell><cell>Building a monolingual parallel corpus for text sim-</cell></row><row><cell></cell><cell>plification using sentence similarity based on align-</cell></row><row><cell></cell><cell>ment between word embeddings. In Proceedings</cell></row></table><note>John Carroll, Guido Minnen, Yvonne Canning, Siob- han Devlin, and John Tait. 1998. Practical simpli- fication of English newspaper text to assist aphasic readers. In Proceedings of the AAAI-98</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>ALECTOR: This dataset has to be requested from the authors(Gala et al., 2020).</figDesc><table><row><cell cols="2">https://github.com/facebookresearch/</cell></row><row><cell>cc_net.</cell><cell></cell></row><row><cell>• WikiLarge:</cell><cell></cell></row><row><cell cols="2">https://github.com/XingxingZhang/</cell></row><row><cell>dress.</cell><cell></cell></row><row><cell>• ASSET:</cell><cell></cell></row><row><cell cols="2">https://github.com/facebookresearch/</cell></row><row><cell cols="2">asset or https://github.com/feralvam/</cell></row><row><cell>easse.</cell><cell></cell></row><row><cell>• TurkCorpus:</cell><cell></cell></row><row><cell cols="2">https://github.com/cocoxu/</cell></row><row><cell>simplification/</cell><cell>or https://github.</cell></row><row><cell cols="2">com/feralvam/easse.</cell></row><row><cell cols="2">• Newsela: This dataset has to be requested at</cell></row><row><cell cols="2">https://newsela.com/data.</cell></row></table><note>•</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Examples of Mined Paraphrases. Paraphrases, although sometimes not preserving the entire meaning, display various rewriting operations, such as lexical substitution, compression or sentence splitting. SARI on valid 42.65±0.23 40.85±0.15 38.09±0.59 Approx. value 42.49±0.34 39.57±0.40 36.16±0.35</figDesc><table><row><cell></cell><cell>ASSET</cell><cell>TurkCor.</cell><cell>Newsela</cell></row><row><cell>Method</cell><cell>SARI ↑</cell><cell>SARI ↑</cell><cell>SARI ↑</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Set ACCESS Controls Wo. Parallel Data Setting ACCESS parameters of MUSS +MINED model either using SARI on the validation set or using only 50 unaligned sentence pairs from the validation set. All ACCESS parameters are set to the same approximated value: ASSET = 0.8, TurkCorpus = 0.95, and Newsela = 0.4).MINED42.65±0.23 40.85±0.15 38.09±0.59 PARANMT 42.50±0.33 40.50±0.16 39.11±0.88</figDesc><table><row><cell cols="4">paraphrase data is a viable alternative to using exist-</cell></row><row><cell cols="4">ing paraphrase datasets relying on labeled parallel</cell></row><row><cell cols="3">machine translation corpora.</cell><cell></cell></row><row><cell></cell><cell>ASSET</cell><cell>TurkCor.</cell><cell>Newsela</cell></row><row><cell>Data</cell><cell>SARI ↑</cell><cell>SARI ↑</cell><cell>SARI ↑</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Mined Data vs. ParaNMTWe compare SARI scores of MUSS trained either on our mined data or on PARANMT<ref type="bibr" target="#b34">(Wieting and Gimpel, 2018)</ref> on the test sets of ASSET, TurkCorpus and Newsela.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>87±0.36 68.95±1.33 6.49±0.15 40.04±0.30 73.56±1.18 8.77±0.08 ---71±1.55 88.56±1.06 8.62±0.34 35.79±0.89 90.24±2.52 8.63±0.34 22.23±1.99 21.75±0.45 8.00±0.26 MUSS WikiLarge 43.63±0.71 76.28±4.30 6.25±0.42 42.62±0.27 78.28±3.95 6.98±0.95 40.00±0.63 14.42±6.85 3.51±0.53 MUSS WikiLarge + MINED 44.15±0.56 72.98±4.27 6.05±0.51 42.53±0.36 78.17±2.20 7.60±1.06 39.50±0.42 15.52±0.99 3.19±0.49 MUSS Newsela 42.91±0.58 71.40±6.38 6.91±0.42 41.53±0.36 74.29±4.67 7.39±0.42 42.59±1.00 18.61±4.49 2.74±0.98 MUSS Newsela + MINED 41.36±0.48 78.35±2.83 6.96±0.26 40.01±0.51 83.77±1.00 8.26±0.36 41.17±0.95 16.87±4.55 2.70±1.00 03±0.63 61.76±2.19 9.41±0.07 38.06±0.47 63.70±2.43 9.43±0.07 30.36±0.71 12.98±0.32 8.85±0.13 MUSS (MBART) MINED 41.11±0.70 77.22±2.12 7.18±0.21 39.40±0.54 77.05±3.02 8.65±0.40 34.76±0.96 19.06±1.15 5.44±0.25 MUSS (BART) MINED 42.65±0.23 66.23±4.31 8.23±0.62 40.85±0.15 63.76±4.26 8.79±0.30 38.09±0.59 14.91±1.39 5.12±0.47</figDesc><table><row><cell></cell><cell>Data</cell><cell></cell><cell>ASSET</cell><cell></cell><cell></cell><cell>TurkCorpus</cell><cell></cell><cell></cell><cell>Newsela</cell><cell></cell></row><row><cell cols="2">Baselines and Gold Reference</cell><cell>SARI ↑</cell><cell>BLEU ↑</cell><cell>FKGL ↓</cell><cell>SARI ↑</cell><cell>BLEU ↑</cell><cell>FKGL ↓</cell><cell>SARI ↑</cell><cell>BLEU ↑</cell><cell>FKGL ↓</cell></row><row><cell>Identity Baseline</cell><cell>-</cell><cell>20.73</cell><cell>92.81</cell><cell>10.02</cell><cell>26.29</cell><cell>99.36</cell><cell>10.02</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Truncate Baseline -</cell><cell>29.85</cell><cell>84.94</cell><cell>7.91</cell><cell>33.10</cell><cell>88.82</cell><cell>7.91</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Reference 44.Supervised Systems (This Work) -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Seq2Seq 32.Unsupervised Systems (This Work) WikiLarge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Seq2Seq</cell><cell>MINED</cell><cell>38.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Detailed English Results. We display SARI, BLEU, and FKGL on ASSET, TurkCorpus and Newsela English evaluation datasets (test sets). 87±1.90 90.21±1.14 8.31±0.34 35.87±1.09 91.06±2.24 8.31±0.34 20.89±4.08 20.97±0.53 8.27±0.46 MUSS WikiLarge 45.58±0.28 78.85±4.44 5.61±0.31 43.26±0.42 78.39±3.08 6.73±0.38 39.66±1.80 14.82±7.17 4.64±1.85 MUSS WikiLarge + MINED 45.50±0.69 73.16±4.41 5.83±0.51 43.17±0.19 77.52±3.01 7.19±1.02 40.50±0.56 16.30±0.97 3.57±0.60 MUSS Newsela 43.91±0.10 70.06±10.05 6.47±0.29 41.94±0.21 74.03±7.51 6.99±0.53 42.36±1.32 19.18±6.03 3.20±1.01 MUSS Newsela + MINED 42.48±0.41 77.86±3.13 6.41±0.13 40.77±0.52 83.04±1.16 7.68±0.30 41.68±1.60 17.23±5.28 2.97±0.91 Unsupervised Systems (This Work) Seq2Seq MINED 38.88±0.22 61.80±0.94 8.63±0.13 37.51±0.10 62.04±0.91 8.64±0.13 30.35±0.23 13.04±0.45 8.87±0.12 MUSS (MBART) MINED 41.68±0.72 77.11±2.02 6.56±0.21 39.60±0.44 75.64±2.85 8.04±0.40 34.59±0.59 18.19±1.26 5.76±0.22 MUSS (BART) MINED 43.01±0.23 67.65±4.32 7.75±0.53 40.61±0.18 63.56±4.30 8.28±0.18 38.07±0.22 14.43±0.97 5.40±0.41</figDesc><table><row><cell></cell><cell>Data</cell><cell></cell><cell>ASSET</cell><cell></cell><cell></cell><cell>TurkCorpus</cell><cell></cell><cell></cell><cell>Newsela</cell><cell></cell></row><row><cell cols="2">Baselines and Gold Reference</cell><cell>SARI ↑</cell><cell>BLEU ↑</cell><cell>FKGL ↓</cell><cell>SARI ↑</cell><cell>BLEU ↑</cell><cell>FKGL ↓</cell><cell>SARI ↑</cell><cell>BLEU ↑</cell><cell>FKGL ↓</cell></row><row><cell>Identity Baseline</cell><cell>-</cell><cell>22.53</cell><cell>94.44</cell><cell>9.49</cell><cell>26.96</cell><cell>99.27</cell><cell>9.49</cell><cell>12.00</cell><cell>20.69</cell><cell>8.77</cell></row><row><cell cols="2">Truncate Baseline -</cell><cell>29.95</cell><cell>86.67</cell><cell>7.39</cell><cell>32.90</cell><cell>89.10</cell><cell>7.40</cell><cell>24.64</cell><cell>18.97</cell><cell>6.90</cell></row><row><cell>Reference</cell><cell>-</cell><cell cols="2">45.22±0.94 72.67±2.83</cell><cell cols="5">6.13±0.56 40.66±0.11 77.21±0.45 8.31±0.04 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Supervised Systems (This Work)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Seq2Seq</cell><cell>WikiLarge</cell><cell>33.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>English Results on Validation Sets. We display SARI, BLEU, and FKGL on ASSET, TurkCorpus and Newsela English datasets (validation sets).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/ muss</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use the latest version of SARI implemented in EASSE (Alva-Manchego et al., 2019) which fixes bugs and inconsistencies from the traditional implementation. We thus recompute scores from previous systems that we compare to, by using the system predictions provided by the respective authors available in EASSE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We experimented with other alignments (wiki-auto and newsela-auto (Jiang et al., 2020)) but with lower performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We modify the Levenshtein similarity parameter to only consider replace operations, by assigning a 0 weight to insertions and deletions. This change helps decorrelate the Levenshtein similarity control token from the length control token and produced better results in preliminary experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards Brazilian Portuguese automatic text simplification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Sandra M Aluísio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><forename type="middle">G</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renata Pm</forename><surname>Maziero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fortes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM symposium on Document engineering</title>
		<meeting>the eighth ACM symposium on Document engineering</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1147" to="1158" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving text simplification by corpus expansion with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Katsuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhide</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Asian Language Processing (IALP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="216" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving text simplification language modeling using unsimplified text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2013</title>
		<meeting>ACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1537" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert P Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Findings of the WMT 2019 shared task on parallel corpus filtering for lowresource conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="54" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">L</forename><surname>Forcada</surname></persName>
		</author>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="726" to="739" />
		</imprint>
	</monogr>
	<note>Findings of the WMT 2018 shared task on parallel corpus filtering</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simplification using paraphrases and context-based lexical substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reno</forename><surname>Kriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2018</title>
		<meeting>NAACL 2018</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative edit-based unsupervised sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Golab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7918" to="7928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised paraphrasing by simulated annealing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03588</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08210</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reference-less quality estimation of text simplification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Clergerie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Automatic Text Adaptation</title>
		<meeting>the 1st Workshop on Automatic Text Adaptation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Éric Villemonte de la Clergerie, Djamé Seddah, and Benoît Sagot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CamemBERT: a Tasty French Language Model</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Éric de la Clergerie, and Antoine Bordes. 2020. Controllable sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2020</title>
		<meeting>LREC 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving machine translation performance by exploiting non-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Dragos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="504" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2019 (Demonstrations)</title>
		<meeting>NAACL 2019 (Demonstrations)<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised lexical simplification for non-native speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gustavo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2002</title>
		<meeting>ACL 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple ppdb: A paraphrase database for simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="143" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ppdb 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-CoLing</title>
		<meeting>ACL-CoLing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Nevergrad -A gradientfree optimization platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<ptr target="https://GitHub.com/FacebookResearch/Nevergrad" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simplify or help?: text simplification strategies for people with dyslexia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luz</forename><surname>Rello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility</title>
		<meeting>the 10th International Cross-Disciplinary Conference on Web Accessibility</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making it simplext: Implementation and evaluation of a text simplification system for spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luz</forename><surname>Rello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biljana</forename><surname>Drndarevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04944</idno>
		<title level="m">Ccmatrix: Mining billions of high-quality parallel sentences on the web</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic text simplification for Spanish: Comparative evaluation of various simplification strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference recent advances in natural language processing</title>
		<meeting>the international conference recent advances in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CATS: A tool for customized alignment of text simplification corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic structural evaluation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elior</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2018</title>
		<meeting>the NAACL-HLT 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="685" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised neural text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<meeting>ACL 2018</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2058" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parallel Data, Tools and Interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2012</title>
		<meeting>LREC 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SIMPITIKI: a Simplification corpus for Italian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Palmero Aprosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Saltori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Italian Conference on Computational Linguistics</title>
		<meeting>the Third Italian Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">CCnet: Extracting high quality monolingual datasets from web crawl data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<meeting>ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to simplify sentences with quasi-synchronous grammar and integer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2011</title>
		<meeting>EMNLP 2011<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2012</title>
		<meeting>ACL 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Problems in current text simplification research: New data can help. Transactions of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="283" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="584" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Integrating transformer and paraphrase rules for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Saptono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bambang</forename><surname>Parmanto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2018</title>
		<meeting>EMNLP 2018<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3164" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semi-supervised text simplification with back-translation and asymmetric denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A monolingual tree-based translation model for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1353" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
