<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autofocus Layer for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Ancha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Nanavati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Nori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Autofocus Layer for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the autofocus convolutional layer for semantic segmentation with the objective of enhancing the capabilities of neural networks for multi-scale processing. Autofocus layers adaptively change the size of the effective receptive field based on the processed context to generate more powerful features. This is achieved by parallelising multiple convolutional layers with different dilation rates, combined by an attention mechanism that learns to focus on the optimal scales driven by context. By sharing the weights of the parallel convolutions we make the network scale-invariant, with only a modest increase in the number of parameters. The proposed autofocus layer can be easily integrated into existing networks to improve a model's representational power. We evaluate our models on the challenging tasks of multi-organ segmentation in pelvic CT and brain tumor segmentation in MRI and achieve very promising performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a fundamental problem in medical image analysis. Automatic segmentation systems can improve clinical pipelines, facilitating quantitative assessment of pathology, treatment planning and monitoring of disease progression. They can also facilitate large-scale research studies, by extracting measurements from magnetic resonance images (MRI) or computational tomography (CT) scans of large populations in an efficient and reproducible manner.</p><p>For high performance, segmentation algorithms are required to use multiscale context <ref type="bibr" target="#b5">[6]</ref>, while still aiming for pixel-level accuracy. Multi-scale processing provides detailed cues, such as texture information of a structure, combined with contextual information, such as a structure's surroundings, which can facilitate decisions that are ambiguous when based only on local context. Note that such a mechanism is also part of the human visual system, via foveal and peripheral vision.</p><p>A large volume of research has sought algorithms for effective multi-scale processing. An overview of traditional approaches can be found in <ref type="bibr" target="#b5">[6]</ref>. Contemporary segmentation systems are often powered by convolutional neural networks (CNNs). The various network architectures proposed to effectively capture image context can be broadly grouped into three categories. The first type creates an image pyramid at multiple scales. The image is down-sampled and processed at different resolutions. Farabet et al. trained the same filters to perform on all such versions of an image to achieve scale invariance <ref type="bibr" target="#b4">[5]</ref>. In contrast, DeepMedic <ref type="bibr" target="#b8">[9]</ref> proposed learning dedicated pathways for several scales, to enable 3D CNNs to extract more patterns from a larger context in a computationally efficient manner. The second type uses an encoder that gradually down-samples to capture more context, followed by a decoder that learns to upsample the segmentations, combining multi-scale context using skip connections <ref type="bibr" target="#b10">[11]</ref>. Later extensions include U-net <ref type="bibr" target="#b14">[15]</ref>, which used a larger decoder to learn upsampling features instead of segmentations as in <ref type="bibr" target="#b10">[11]</ref>. Learning to upsample with a decoder, however, increases model complexity and computational requirements, when downsampling may not be even necessary. Finally, driven by this idea, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> proposed dilated convolutions to process greater context without ever downsampling the feature maps. Taking it further, DeepLab <ref type="bibr" target="#b2">[3]</ref> introduced the module Atrous Spatial Pyramid Pooling (Aspp), where dilated convolutions with varying rates are applied in parallel to capture multi-scale information. The activations from all scales are naively fused via summation or concatenation.</p><p>We propose the autofocus layer, a novel module that enhances the multi-scale processing of CNNs by learning to select the 'appropriate' scale for identifying different objects in an image. Our work on autofocus shares similarities with Aspp in that we also use parallel dilated convolutional filters to capture both local and more global context. The crucial difference is that instead of naively aggregating features from all scales, the autofocus layer adaptively chooses the optimal scale to focus on in a data-driven, learned manner. In particular, our autofocus module uses an attention mechanism <ref type="bibr" target="#b0">[1]</ref> to indicate the importance of each scale when processing different locations of an image <ref type="figure" target="#fig_0">(Fig. 1</ref>). The computed attention maps, one per scale, serve as filters for the patterns extracted at that scale. Autofocus also enhances interpretability of a network as the attention maps reveal how it locally 'zooms in or out' to segment different context. Compared to the use of attention in <ref type="bibr" target="#b3">[4]</ref>, our solution is modular and independent of architecture.</p><p>We extensively evaluate and compare our method with strong baselines on two tasks: multi-organ segmentation in pelvic CT and brain tumor segmentation in MRI. We show that thanks to its adaptive nature, the autofocus layer copes well with biological variability in the two tasks, improving performance of a wellestablished model. Despite its simplicity, our system is competitive with more elaborate pipelines, demonstrating the potential of the autofocus mechanism. Additionally, autofocus can be easily incorporated into existing architectures by replacing a standard convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dilated convolution</head><p>As they are fundamental to our work, we first present the basics of dilated convolutions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> while introducing notation. The standard 3D dilated convolutional layer at depth l with dilation rate r can be represented as a mapping   can be derived recursively as follows:</p><formula xml:id="formula_0">Conv r l : F l−1 → F r l , where F l−1 ∈ R W ×H ×D ×C and F r l ∈ R W</formula><formula xml:id="formula_1">φ {x,y,z} l = φ {x,y,z} l−1 + r l (θ {x,y,z} l − 1)η {x,y,z} l ,<label>(1)</label></formula><p>Here η {x,y,z} l ∈ N 3 denotes the stride of the receptive field at layer l, which is a product of the strides of kernels in preceding layers. It can be observed from Eqn (1) that greater context can be captured by increasing dilation r l but in less detail as the input signal is probed more sparsely. Thus greater r l leads to a 'zoom out' behavior. Usually, the dilation rate r is a hyperparameter that is manually set and fixed for each layer. Standard convolution is a special case when r = 1. Below we describe the autofocus mechanism that adaptively chooses the optimal dilation rate for different areas of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Autofocus convolutional layer</head><p>Unambiguously classifying different objects in an image is likely to require different combinations of local and global information. For example, large structures may be better segmented by processing a large receptive field φ l at the expense of fine details, while small objects may require focusing on high resolution local information. Consequently, architectures that statically define multi-scale processing may be suboptimal. Our adaptive solution, the autofocus module, is summarized in <ref type="figure" target="#fig_0">Fig. 1</ref> and formalized in the following.</p><p>Given activations of the previous layer F l−1 , we capture multi-scale information by processing it in parallel via K convolutional layers with different dilation rates r k . They produce K tensors F r k l ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), each set to have same number of channels C. They detect patterns at K different scales which we merge in a data-driven manner by introducing a soft attention mechanism <ref type="bibr" target="#b0">[1]</ref>.</p><p>Within the module we construct a small attention network ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>) that processes F l−1 . In this work it consists of two convolutional layers. The first, Conv l,1 , applies 3×3×3 kernels, produces half the number of channels than those in F l−1 (empirically chosen) and is followed by a ReLU activation function f . The second, Conv l,2 , applies 1×1×1 filters and produces a tensor with K channels, one per scale. It is followed by an element-wise softmax σ that normalizes the K activations for each voxel to add up to one. Let this normalized output be</p><formula xml:id="formula_2">Λ l = [Λ 1 l , Λ 2 l , · · · , Λ K l ] ∈ R W ×H×D×K . Formally: Λ l = σ(Conv l,2 (f (Conv l,1 (F l−1 ))))<label>(2)</label></formula><p>In the above, Λ k l ∈ R W ×H×D is an attention map that corresponds to the k-th scale. For any specific spatial location (voxel), the corresponding K values from the K attention maps Λ k l can be interpreted as how much focus to put on each scale. Thus the final output of the autofocus layer is computed by fusing the outputs from the parallel dillated convolutions as follows:</p><formula xml:id="formula_3">F l = K k=1 Λ k l · F r k l<label>(3)</label></formula><p>where · is an element-wise multiplication. Note that the attention weights Λ k l are shared across all channels of tensor F r k l for scale k. Since the attention maps are predicted by a fully convolutional network, different attention is predicted for each voxel, driven by the image context for the optimal choice of scale ( <ref type="figure" target="#fig_0">Fig. 1(c)</ref>).</p><p>The increase in representational power offered by each autofocus layer naturally comes with computational requirements as the module is based in parallelism of K dilated convolutional layers. Therefore an appropriate balance should be sought, which we investigate in Sec. 3 with very promising results.</p><p>Scale invariance: The size of some anatomical structures such as bones and organs may vary, while the overall appearance is rather similar. For others, size may correlate with appearance. For instance, the texture of large developed tumors differs from early-stage small tumors. This suggests that scale invariance could be leveraged to regularize learning but must be done appropriately. We make the parallel filters in an autofocus layer share parameters. This makes the number of trainable parameters independent of K, with only the attention module adding parameters over a standard convolution. As a result, each parallel filter seeks patterns with similar appearance but of different sizes. Hence, the network is adaptively scale-invariant -the attention mechanism chooses the scale in a data-driven manner, unlike Farabet et al. <ref type="bibr" target="#b4">[5]</ref>, whose network learns shared filters between different scales but naively concatenates all their responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Autofocus Neural Networks</head><p>The proposed autofocus layer can be integrated into existing architectures to improve their multi-scale processing capabilities by replacing standard or dilated convolutions. To demonstrate this, we chose DeepMedic (Dm) <ref type="bibr" target="#b8">[9]</ref> with residual connections <ref type="bibr" target="#b7">[8]</ref> as a starting point. Dm uses different pathways with high and low resolution inputs for multi-scale processing. Instead, we keep only its highresolution pathway and seek to empower it with our method. First, we enhance it with standard dilated convolutions with rate 2 in its last 6 hidden layers to enlarge its receptive field, arriving at the Basic model that serves as another baseline. We now define a family of AFNets by converting the last n hidden layers of Basic to autofocus layers-denoted as "Afn-n", where n ∈ {1, . . . , 6}. <ref type="figure" target="#fig_2">Fig. 2</ref> shows AFNet-4. The proposed AFNets are trained end-to-end.</p><formula xml:id="formula_4">1 " 3 " 3 " 3 " 3 " 3 " 3 " 3 " 3 " % ' " ( ) * + , 30×73 " 30×71 " 40×67 " 40×63 " 40×59 " 40×55 " 50×51 " 50×47 " C×47 " 75 "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>We extensively evaluate AFNets on the tasks of multi-organ and brain tumor segmentation. Specifically, on both tasks we perform: (1) a study where we successively add autofocus to more layers of the Basic network to explore its impact, and (2) comparison of AFNets with baselines. Finally, (3) we evaluate on the public benchmark BRATS'15 and show that our method competes with state-of-the-art pipelines regardless its simplicity, showing its potential. Baselines: We compare AFNets with the previously defined Basic model to show the contribution of autofocus layer over standard dilated convolutions. Similarly, we compare with DeepMedic <ref type="bibr" target="#b8">[9]</ref>, denoted as Dm, to compare our adaptive multi-scale processing with the static multi-scale pathways. Finally, we place an Aspp module <ref type="bibr" target="#b2">[3]</ref> on top of Basic, comparison of which against Afn-1 shows contribution of the attention mechanism. Aspp-c and Aspp-s represent fusion of Aspp activations via concatenation and summation respectively. Source codes and pretrained models in PyTorch framework are online available at: https://github.com/yaq007/Autofocus-Layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ADD and UW datasets of pelvic CT scans Material:</head><p>We use two databases of pelvic CT scans, collected from patients diagnosed with prostate cancer in different clinical centers. The first, referred to as Add, contains 86 scans with varying number of 512x512 slices and 3mm inter-slice spacing. Uw consists of 34 scans of 512x512 slices with 1mm interslice spacing. Expert oncologists manually delineated in all images the following structures: prostate gland, seminal vesicles (SV), bladder, rectum, left femur and right femur. Each scan is normalized so that its intensities have zero mean and unit variance. We also re-sample Uw to the spacing of Add. To produce a stringent test of the models' generalization, we train them for this multi-class problem using the Add data and then evaluate them on Uw data.</p><p>Configuration details: Basic, Aspp and Afn models were trained with the ADAM optimizer for 300 epochs to minimize the soft dice loss <ref type="bibr" target="#b12">[13]</ref>. Each batch consists of 7 segments of size 75 3 . The learning rate starts at 0.001 and is reduced to 0.0001 after 200 epochs. We use dilation rates 2, 6, 10 and 14 (K = 4) for both Aspp and the autofocus modules. It takes around 20 hours to train an AFNet with 2 NVIDIA TITAN X GPUs. Performance of DeepMedic was obtained by training the public software <ref type="bibr" target="#b8">[9]</ref> with default parameters, but without augmentation and by sampling each class equally, similar to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Brain tumor segmentation data (BRATS 2015)</head><p>Material: The training database of BRATS'15 <ref type="bibr" target="#b11">[12]</ref> consists of multi-modal MR scans of 274 cases, along with corresponding annotations of the tumors. We normalize each scan so that intensities belonging to the brain have zero mean and unit variance. For our ablation study, we train all models on the same 193 subjects and evaluate their performance on 54 subjects. The subsets were chosen randomly, including both high and low grade gliomas. Results on the remaining 23 cases aren't reported as they were used for configuration during development. Following standard protocol, we report performance for segmenting the whole tumor, core and enhancing tumor. Finally, to compare with other methods, we train AFNet-6 on all 274 images, segment the 110 test cases of BRATS'15 (no annotations publicly available) and submit predictions for online evaluation.</p><p>Configuration details: Settings are similar to Kamnitsas et al. <ref type="bibr" target="#b8">[9]</ref> for a fair comparison. For each method in <ref type="table">Table 2</ref> we report the average of three runs with different seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Ablation study: Results from the ablation study on the cervical CT database and the BRATS database are summarized in <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref>  We observe the following: (a) Building Afn-1 by converting the last layer of Basic to autofocus improves performance, while (b) the gains surpass those by the popular Aspp for most classes of the tasks. It is important to note that Aspp adds multiple parallel convolutional layers without sharing weights between them. This incurs a large increase in the number of parameters, and is therefore partly the reason for improvements of Aspp over Basic (see <ref type="table">Table 3</ref>). (c) Converting more layers of the Basic baseline to autofocus layers tends to improve performance. An exception is Afn-4 vs. Afn-5/6 on the Uw dataset. We speculate that this is due to randomness in training and suboptimal optimization. (d) Empowering the high-resolution pathway of DeepMedic with adaptive autofocus quickly outperforms the gains from the static second pathway on pelvic scan and brain tumor segmentation except for the enhancing tumor. We speculate that gains are more profound in the former task due to the greater variation in the size of structures, where the adaptive nature of autofocus shines. Finally we note that by sharing weights across scales, AFNets have small number of trainable parameters, shown in <ref type="table">Table 3</ref>, which could enable rapid learning from little data, which is however left for future work. On the downside, the multiple scales on each autofocus layer increase memory and computation requirements.</p><p>Comparison with state-of-the-art on BRATS'15: Performance on test data of BRATS'15 obtained via the online evaluation platform is shown on Table 4, along with other top published methods. Afn-6 compares favorably to the semi-automatic methods that topped the BRATS'15 challenge <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, as well as DeepMedic with the second static lower-resolution pathway. Note that in <ref type="bibr" target="#b13">[14]</ref> high and low grade gliomas were separated by visual inspection and then passed to an appropriately specialized CNN, giving them an advantage over other methods. Our model is only surpassed by the pipelines of <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b6">[7]</ref>, who both used ensembles of CNNs with deep supervision and more aggressive data augmentation. The promising performance obtained by our simple method indicates the potential of the autofocus layer, which can be adopted in more elaborate systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed an autofocus convolutional layer for segmentation of biomedical images. An autofocus layer can adapt the network's receptive field at different spatial locations in a data-driven manner. Our extensive evaluation of AFNets shows that they cope well with biological variability in different tasks and generalize well on both MR and CT images. We have shown that the autofocus convolutional layer can be integrated into existing network architectures to substantially increase their representational power with only a small increase in model parameters. In addition, the interpretability of autofocus layers can leverage understanding of deep learning systems. Investigating the potential of autofocus modules in regression problems would be interesting future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An autofocus convolutional layer with the number of candidate dilation rates K = 4. (a) The attention model. (b) A weighted summation of activations from parallel dilated convolutions. (c) An example of attention maps for a small (r 1 ) and larger (r 2 ) dilation rate. The first row is the input and the segmentation result of Afn-6 (described in Sec. 2.3). The second row shows how the module 'zooms out' for more context when processing large or ambiguous structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>×H×D×C are input and output tensors with C channels (feature maps) of size (W × H × D). For neurons in F r l , the size φ {x,y,z} l ∈ N 3 of their receptive field on the input image can be controlled via r. For dilated convolution layers with kernel size θ {x,y,z} l ∈ N 3 , φ {x,y,z} l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The AFNet-4 model. Layers 1-2 are standard convolutions and 3-4 are dilated with rate 2. Layers 4-8 are autofocus layers, denoted with red. All layers except the classification layer use 3 3 kernels. Yellow rectangles represent ReLU layers. Residual connections are used. Number and size of feature maps shown as (number × size).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Basic Aspp-s Aspp-c Dm Afn-1 Afn-2 Afn-3 Afn-4 Afn-5 Afn-6 Prostate 50.94 55.83 58.67 69.66 63.36 75.43 76.66 75.30 73.81 76.15 Bladder 72.83 81.53 80.43 93.54 88.76 92.38 93.56 94.49 94.28 93.32 Rectum 64.39 67.30 67.04 70.74 71.46 76.20 78.45 79.80 78.96 78.82 SV 53.97 50.11 59.37 56.75 61.68 65.03 65.12 64.83 60.87 63.24 LFemur 91.60 94.13 93.81 94.68 93.24 95.18 93.52 94.59 93.42 95.16 RFemur 91.65 94.34 92.78 94.63 91.93 94.38 91.61 94.60 94.16 95.75 Mean Dice 70.90 74.98 75.35 78.89 78.41 83.10 83.15 83.94 82.58 83.74 Performance on multi-organ segmentation problem of baseline models and AFN on Uw database, after being trained on Add. Absolute dice scores are shown. Ablation study on BRATS'15 training database via cross-validation on 54 random held-out cases. Dice scores shown in format mean(standard deviation).</figDesc><table><row><cell></cell><cell cols="2">Basic Aspp-s Aspp-c Dm Afn-1 Afn-2 Afn-3 Afn-4 Afn-5 Afn-6</cell></row><row><cell cols="2">Whole</cell><cell>87.90 87.83 87.90 88.93 88.03 88.63 88.42 88.88 89.19 89.30 (8.57) (8.36) (8.08) (7.05) (8.29) (8.02) (9.28) (8.31) (7.87) (8.00)</cell></row><row><cell>Core</cell><cell cols="2">72.61 74.08 73.58 71.42 73.93 73.43 73.79 73.91 74.32 74.11 (26.39) (24.16) (24.57) (27.48) (25.04) (24.90) (26.00) (25.49) (24.80) (24.19)</cell></row><row><cell>Enh</cell><cell cols="2">74.37 73.06 73.47 76.08 73.17 73.94 74.21 74.08 74.48 75.62 (25.23) (26.93) (25.36) (25.12) (26.98) (25.83) (27.58) (25.70) (26.56) (25.02)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>respectively. Number of trainable parameters in convolutional kernels of different models. Dice scores achieved by state-of-the-art methods on BRATS'15 test database. used CNN ensembles and more extensive augmentation.</figDesc><table><row><cell>Models</cell><cell>Basic</cell><cell cols="2">Aspp-s</cell><cell>Aspp-c</cell><cell>Dm</cell><cell>Afn-1</cell><cell>Afn-6</cell></row><row><cell>Params</cell><cell>315,725</cell><cell cols="2">967,330</cell><cell>478,435</cell><cell>662,555</cell><cell>349,904</cell><cell>450,209</cell></row><row><cell cols="8">Models Afn-6 peres1  † *  [14] bakas1  † [2] kamnk1/Dm [9] kayab1  *  [10] isenf1  *  [7]</cell></row><row><cell cols="2">Whole 84%</cell><cell>83%</cell><cell></cell><cell>81%</cell><cell>84%</cell><cell>85%</cell><cell>85%</cell></row><row><cell>Core</cell><cell>69%</cell><cell>72%</cell><cell></cell><cell>63%</cell><cell>67%</cell><cell>72%</cell><cell>74%</cell></row><row><cell>Enh</cell><cell>63%</cell><cell>60%</cell><cell></cell><cell>58%</cell><cell>63%</cell><cell>61%</cell><cell>64%</cell></row><row><cell cols="3">Cases 110/110 53/110</cell><cell cols="2">53/110</cell><cell>110/110</cell><cell>110/110</cell><cell>110/110</cell></row></table><note>† are semi-automatic.*</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Glistrboost: combining multimodal mri segmentation, registration, and biophysical tumor growth modeling with gradient boosting machines for glioma segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI BraTS Challenge</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention to scale: Scaleaware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context based object categorization: A critical survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation and radiomics survival prediction: Contribution to the brats 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kickingereder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI BraTS Challenge</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepmedic for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI BraTS Challenge</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F J</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cnn-based segmentation of medical imaging data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kayalibay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03056</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using convolutional neural networks in mri images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
