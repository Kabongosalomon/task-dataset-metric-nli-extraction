<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multigrid Method for Efficiently Training Video Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-10">10 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Multigrid Method for Efficiently Training Video Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-10">10 Jun 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training competitive deep video models is an order of magnitude slower than training their counterpart image models. Slow training causes long research cycles, which hinders progress in video understanding research. Following standard practice for training image models, video model training has used a fixed mini-batch shape: a specific number of clips, frames, and spatial size. However, what is the optimal shape? High resolution models perform well, but train slowly. Low resolution models train faster, but are less accurate. Inspired by multigrid methods in numerical optimization, we propose to use variable mini-batch shapes with different spatial-temporal resolutions that are varied according to a schedule. The different shapes arise from resampling the training data on multiple sampling grids. Training is accelerated by scaling up the mini-batch size and learning rate when shrinking the other dimensions. We empirically demonstrate a general and robust grid schedule that yields a significant out-of-the-box training speedup without a loss in accuracy for different models (I3D, nonlocal, SlowFast), datasets (Kinetics,  Charades), and training settings (with and without pretraining, 128  GPUs or 1 GPU). As an illustrative example, the proposed multigrid method trains a ResNet-50 SlowFast network 4.5× faster (wall-clock time, same hardware) while also improving accuracy (+0.8% absolute) on Kinetics-400 compared to baseline training. Code is available online. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Training deep networks (CNNs <ref type="bibr" target="#b26">[27]</ref>) on video is more computationally intensive than training 2D CNN image models, potentially by an order of magnitude. Long training time slows progress in video understanding research, hinders scaling out to real-world data sources, and consumes significant amounts of energy and hardware. Is this slow training unavoidable, or might there be video-specific optimization strategies that can accelerate training?</p><p>3D CNN video models are trained using mini-batch optimization methods (e.g., SGD) that process one mini-batch <ref type="bibr" target="#b0">1</ref>   <ref type="figure">Figure 1</ref>. Training time vs. top-1 accuracy on Kinetics-400 with a ResNet-50 SlowFast network. Each point corresponds to a model trained for a specific number of epochs. Multigrid training, the method developed in this paper, obtains a significantly better tradeoff than baseline training. For example, under default settings, multigrid training is 4.5× faster while achieving higher (+0.8% absolute) top-1 accuracy. All methods here, and throughout the paper, use the same hardware and software implementation.</p><p>per iteration. The mini-batch shape B×T ×H×W 2 (minibatch size × number of frames × height × width) is typically constant throughout training. A variety of considerations go into selecting this input shape, but a common heuristic is to make the T ×H×W dimensions large in order to improve accuracy, e.g., as observed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>This heuristic is only one possible choice, however, and in general there are trade-offs. For example, one may use a smaller number of frames and/or spatial size while simultaneously increasing the mini-batch size B. With such an exchange, it is possible to process the same number of epochs (passes over the dataset) with lower wall-clock time because each iteration processes more examples. The resulting trade-off is faster training with lower accuracy.</p><p>The central idea of this paper is to avoid this trade-offi.e., to have faster training without losing accuracy-by making the mini-batch shape variable during training. By viewing the input video clips in a mini-batch as raw video signals that are sampled on a sampling grid (to be defined), we can draw a connection to multigrid methods for numeri-cal analysis <ref type="bibr" target="#b0">[1]</ref>. These methods exploit coarse-to-fine grids to accelerate optimization. Intuitively, if we use large minibatches with relatively small time and space dimensions (a 'coarse grid') early in training and small mini-batches with large time and space dimensions (a 'fine grid') later, then SGD may be able to scan through the data more quickly on average while finally solving for a high accuracy model, akin to how coarse grids enable solving problems on finer grids more rapidly in multigrid numerical solvers <ref type="bibr" target="#b0">[1]</ref>.</p><p>Multigrid training is possible because video models are compatible with input data of variable space and time dimensions due to weight sharing operations (e.g., convolutions). In addition, CNNs are effective at learning patterns at multiple scales, e.g., as observed when training with data augmentation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>. We observe similar multi-scale robustness and generalization with multigrid training.</p><p>Our proposed multigrid training method is simple and effective. It is easy to implement and typically only requires small changes to a data loader. Empirically, it works with default learning rate schedules and hyper-parameters already in use. No tuning is required. Moreover, multigrid training works robustly out-of-the-box for different models (I3D <ref type="bibr" target="#b2">[3]</ref>, non-local <ref type="bibr" target="#b46">[47]</ref>, SlowFast <ref type="bibr" target="#b8">[9]</ref>), datasets (Kinetics-400 <ref type="bibr" target="#b22">[23]</ref>, Something-Something V2 <ref type="bibr" target="#b13">[14]</ref>, and Charades <ref type="bibr" target="#b35">[36]</ref>), initializations (random and pre-trained), and hardware scales (e.g., 128 GPUs or 1 GPU). We observe a consistent speedup and performance gain in all cases without tuning. As an example, we train a SlowFast network ∼4.5× faster in wall-clock time on the large-scale Kinetics dataset ( <ref type="figure">Fig. 1</ref>) while also reaching a higher accuracy (+0.8% absolute). We hope these benefits provided by multigrid training will make research on video understanding more accessible, scalable, and economical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D CNN video models extend 2D CNNs to model both spatial and temporal patterns. They are currently the state of the art for video understanding <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>. These methods are computationally expensive, both for training and inference <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b48">49]</ref>. Some recent studies propose lighter weight models that use efficient temporal modules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref> and/or exploit temporal redundancy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b50">51]</ref>. In this paper, we show that the training time of state-of-the-art efficient models <ref type="bibr" target="#b8">[9]</ref> can still be reduced significantly.</p><p>Efficient training can also be advanced through, e.g., optimization methods (e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>), pre-training <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, distributed training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b49">50]</ref>, or advances in hardware <ref type="bibr" target="#b21">[22]</ref> and software design <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. In this paper, we propose a complementary direction that exploits variable mini-batch shapes for fast training. Related to our method, Wang et al. <ref type="bibr" target="#b46">[47]</ref> and Feichtenhofer et al. <ref type="bibr" target="#b9">[10]</ref> initialize larger models with smaller, fully-trained ones. These methods can potentially speed up training as well, and (as can be seen later) are a special case of multigrid training.</p><p>Multi-scale training in segmentation <ref type="bibr" target="#b15">[16]</ref> and classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38]</ref> uses multiple image crop sizes. However, the mini-batch shape remains fixed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref>. Multigrid training on the other hand uses variable mini-batch shapes. He et al. <ref type="bibr" target="#b16">[17]</ref> change the input shapes, but fix the mini-batch size. These methods shows that training with variable scales can be beneficial. Multigrid training enjoys the same property.</p><p>Multigrid methods were originally proposed for numerical boundary value problems, and later developed into an entire field in computational mathematics <ref type="bibr" target="#b0">[1]</ref>. They typically involve iterating through cycles of coarse and fine problems, and exploit the fact that a coarse problem can be solved efficiently to speed up the overall problem solving. He and Xu <ref type="bibr" target="#b14">[15]</ref> connect multigrid methods to deep networks through identifying the correspondence between steps in traditional multigrid methods and operators in a convolutional neural network. In this paper, we take inspiration from multigrid concepts from a more abstract view to accelerate video model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multigrid Training for Video Models</head><p>To develop our multigrid training method we will consider a reference video model (e.g., C3D <ref type="bibr" target="#b42">[43]</ref>, I3D <ref type="bibr" target="#b2">[3]</ref>) that is trained by a baseline mini-batch optimizer (e.g., SGD) that operates on mini-batches of shape B×T ×H×W (mini-batch size × number of frames × height × width) for some number of epochs (e.g., 100). The spatial-temporal shape, T ×H×W , arises from resampling source videos in the training dataset according to a sampling grid that is specified by a temporal span, a spatial span, a temporal stride, and a spatial stride (defined in §3.1). These concepts intuitively correspond to a grid's duration/area (span) and sampling rate (stride). The baseline optimizer holds the mini-batch shape constant across all training iterations.</p><p>Proposed Multigrid Method. Inspired by multigrid methods in numerical analysis, which solve optimization problems on alternating coarse and fine grids, the core observation in this paper is that the underlying sampling grid that is used to train video models need not be constant during training. In fact, we will show in experiments that by varying the sampling grid and the mini-batch size during training it is possible to reduce training complexity substantially (in terms of total FLOPs and wall-clock time) while achieving similar accuracy in comparison with the baseline.</p><p>The fundamental concept that enables multigrid training is the balance between computation allocated to processing more examples per mini-batch vs. the computation allocated to processing larger time and space dimensions. To control this balance, we will consider temporal and spatial shapes t×w×h that are formed by resampling source videos with a new sampling grid that has its own spans and strides. When changing the input shape we use a scaled mini-batch size b satisfying the relation b·t·h·w = B·T ·H·W , or</p><formula xml:id="formula_0">b = B T t H h W w ,<label>(1)</label></formula><p>which yields computation (in FLOPs) that is roughly equal to the computation of the aforementioned baseline minibatch for typical 3D CNNs. <ref type="bibr" target="#b2">3</ref> Our multigrid method uses a set of sampling grids and a grid schedule that determines which grid to use in each training iteration. If training is run for a similar number of epochs regardless of the choice of grids, 4 then by making b&gt;B on average the entire training process can use fewer total FLOPs and have a lower wall-clock time.</p><p>We will experimentally investigate two questions: (i) is there a set of grids with a grid schedule that can lead to faster training without a loss in accuracy? and, (ii) if so, does it robustly generalize to new models and datasets without modification? In the following we will develop the core multigrid training concepts in detail ( §3.1), provide an implementation (i.e., a set of grids and a grid schedule) that work well in practice ( §3.2), and then explore ablation and generalization experiments ( §4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multigrid Training Concepts</head><p>Sampling Grids. Each video in a dataset is a discrete signal that was sampled from an underlying continuous signal generated by the physical world. The video has some number of frames and pixels per frame, which are related to the physical world by the temporal and spatial resolution of the recording device (which depends on a number of camera properties). When using one of these source videos in a training mini-batch, a sampling grid is used to resample it.</p><p>A sampling grid in one dimension (space or time) is defined by two quantities: a span and a stride. Their units are defined w.r.t. the source video being resampled. <ref type="bibr" target="#b4">5</ref> For the time dimension, the units are frames while for the spatial dimensions the units are pixels. The span is the support size of the grid and defines the duration or area that the grid covers. The stride is the spacing between sampling points. Dividing <ref type="bibr" target="#b2">3</ref> In practice, the computation is not exactly equal, because of rounding (e.g., w can be W √ 2</p><p>), padding, and, e.g., fully connected or non-local layers. We ignore these subtleties and only use approximate FLOPs as a rough design principle. All speedups are measured by wall-clock time. <ref type="bibr" target="#b3">4</ref> In practice, a similar number of epochs (e.g., within a factor of 2) are typically used for a given dataset, even for very different models. <ref type="bibr" target="#b4">5</ref> Between two videos these units may have different physical meanings if the videos were captured by cameras with different properties (e.g., a 24 frame span from a 24 FPS video vs. a 24 frame span from a 30 FPS video). These properties may be unknown and therefore we define grid units with respect to source videos, not the physical world. the span by the stride gives the number of points in the grid, which determines the shape of the input data. Note that different grids can yield the same data shape, which implies that the mini-batch size will only change (Equation <ref type="formula" target="#formula_0">(1)</ref>) if a change in the sampling grid also changes the data shape.</p><p>We note that spatial sampling grids already appear in the baseline optimizer if it uses multi-scale spatial data augmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref>. Under our multigrid perspective, multi-scale spatial data augmentation changes the spatial spans and strides of the resampling grid proportionally so that the resulting mini-batch always has the same H×W spatial shape. In contrast, we will change spans and strides by different factors, which results in a different spatial shape h×w for each grid (and likewise for the time dimension).</p><p>Grid Scheduling. We use mini-batch optimizers, which have as their most basic scheduling unit a single mini-batch iteration in which one model update is performed. The training schedule consists of some number of mini-batch iterations and is often expressed in terms of epochs. For example, training may consist of 100 or 200 epochs worth of iterations. Within this overall training schedule it is common to let the learning rate vary, such as annealing it according to a schedule defined in terms of iterations or epochs.</p><p>Scheduling other training properties is also possible. Central to our multigrid method is the idea of scheduling the sampling grids that are used throughout training. When changing grids, the mini-batch size is always scaled according to Equation (1) so that mini-batch FLOPs are held roughly constant. Grid scheduling is highly flexible, admitting a large design space from simply cycling through a sequence of pre-defined grids to using randomized grids. In §3.2 we will present a randomized, hierarchical schedule that works well in practice.</p><p>Multigrid Properties. Multigrid training relies on two properties of the data and model. First, resampling the data on different grids requires a suitable operator. For video, this operator can be a reconstruction filter applied to the source discrete signal followed by computing the values at the points specified by the grid (e.g., bilinear interpolation). Second, the model must be compatible with inputs that are resampled on different grids, and therefore might have different shapes during training. Models that are composed of functions that use weight sharing across the dimensions that are resampled, e.g., 2D and 3D convolutions, recurrent functions, and self-attention, are compatible and cover most of the commonly used architectures; fully-connected layers, unless their inputs are pooled to a fixed size, are not compatible. <ref type="bibr" target="#b5">6</ref> We will focus on models that use 2D and 3D con- volutions, as well as self-attention operations in the form of non-local blocks <ref type="bibr" target="#b46">[47]</ref>; all models end with global average pooling and a single full-connected layer as the classifier, as is common practice.</p><formula xml:id="formula_1">1× 2× 4× 8× 16× (T , H, W ) • • • Iterations Mini-batch size (a) Baseline 1× 2× 4× 8× 16× • • • • • • • • • • • • • • • • • • ( T 4 , H √ 2 , W √ 2 ) ( T 2 , H √ 2 , W √ 2 ) ( T 2 ,H,W ) (T ,H,W ) Iterations (b) Long cycles 1× 2× 4× 8× 16× • • • (T , H 2 , W 2 ) (T , H √ 2 , W √ 2 ) (T ,H,W ) Iterations (c) Short cycles 1× 2× 4× 8× 16× • • • • • • • • • • • • • • • • • • (T , H 2 , W 2 ) (T , H √ 2 , W √ 2 ) (T ,H,W ) Iterations (d) Long + short cycles</formula><p>Training and Testing Distributions. The focus of this work is on multigrid methods for training and therefore we use a standard inference method that uses a single shape for the testing data. This choice, however, may introduce a mismatch between the data distribution used to train the model and the data distribution used at test time. To close this gap, training may be finished with some number of 'fine-tuning' iterations that use grids more closely aligned with the testing distribution, e.g., see <ref type="bibr" target="#b41">[42]</ref>. We find that this fine-tuning gives a small, but consistent improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>Multigrid training involves a choice of sampling grids and a grid schedule, which leads to a rich design space. We use a hierarchical schedule that involves alternating between mini-batch shapes at two different frequencies: a long cycle that moves through a set of base shapes, generated by a variety of grids, staying on each shape for several epochs, and a short cycle that moves through a set of shapes that are 'nearby' the current base shape, staying on each one for a single iteration. This hierarchical grid schedule is described in more detail shortly and illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>The remainder of this subsection provides details for this design, which we have found to work well in practice. After presenting these details, we will explore what design decisions are important in ablation experiments.</p><p>Optimizer. We use SGD with momentum and a stepwise learning rate decay schedule since these are common choices in practice <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref>. Using other learning rate schedules and optimizers is also possible. Specific schedules are given in each experimental section.</p><p>Long Cycle. We use sampling grids that result in an ordered sequence of S=4 base mini-batch shapes of nondecreasing size along each dimension:</p><formula xml:id="formula_2">8B× T 4 × H √ 2 × W √ 2 , 4B× T 2 × H √ 2 × W √ 2 , 2B× T 2 ×H×W</formula><p>, and B×T ×H×W . These four shapes cover an intuitive range and work well in practice. The long cycle is synchronized with the stepwise learning rate decay schedule: a full cycle over the S shapes occurs exactly once for each learning rate stage. We train on each shape for the same number of iterations.</p><p>We use a simple randomized strategy to generate a minibatch with the target input shape for each training iteration. For each video to be used in the mini-batch, we select a random span from a specified range and set the stride such that the desired shape is produced when sampling on the resulting grid. For the spatial dimensions, this strategy amounts to resizing a random crop to the desired shape using bilinear interpolation (similar to random cropping used in image classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>). For the temporal dimension, this strategy amounts to selecting a random temporal crop and subsampling its frames. The sampling range for spans is specified in each experimental section. Short Cycle. The short cycle rapidly moves through a variety of spatial shapes, changing at each iteration. By default, we use the following 3-shape short cycle. For iteration i, let m = i (mod 3); if m=0, then we set the spatial shape to</p><formula xml:id="formula_3">H 2 × W 2 ; if m=1, we use H √ 2 × W √ 2</formula><p>; otherwise, the current base spatial shape from the long cycle is used.</p><p>The short cycle can be applied on its own or in conjunction with the long cycle. The mini-batch size is again scaled using Equation <ref type="bibr" target="#b0">(1)</ref>. The same randomized grid strategy is applied to sample data for the target mini-batch shape.</p><p>Learning Rate Scaling. When the mini-batch size changes due to the long cycle, we apply the linear scaling rule <ref type="bibr" target="#b12">[13]</ref> to adjust the learning rate by the mini-batch size scaling factor (thus either 8×, 4×, 2×, or 1×). We found that this adjust-ment is harmful if applied to mini-batch size changes due to the short cycle and therefore we only adjust the learning rate when the long cycle base shape changes.</p><p>Fine-tuning Phase. If the baseline optimizer uses L learning rate (LR) stages, then we apply the long and short cycles in the first L−1 LR stages. We use the corresponding L-th stage for fine-tuning to help match the training and testing distributions, similar to <ref type="bibr" target="#b41">[42]</ref>. In the first half of the finetuning iterations we use the L−1-st learning rate and in the second half we use the final (L-th) learning rate. While fine-tuning we use the short cycle (as data augmentation), but not the long cycle.</p><p>Batch Normalization. The behavior of Batch Normalization (BN) <ref type="bibr" target="#b19">[20]</ref> depends on mini-batch statistics. In traditional trainers, the constant mini-batch size is also a hyperparameter that impacts BN behaviors (e.g., the noisiness of the statistics). As our multigrid method uses variable minibatch sizes, it is desirable to decouple its impact on BN from that of training speedup. The following heuristic works well in practice: we compute BN statistics with a standardized sub-mini-batch of size 8; when the short cycle increases the overall mini-batch size by 2× or 4×, we likewise increase the BN sub-mini-batch size to 16 and 32, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on Kinetics</head><p>We conduct ablation studies on the Kinetics-400 dataset <ref type="bibr" target="#b22">[23]</ref>, which is used in prior research and requires classifying each video into one of 400 categories. It contains ∼240k training videos and ∼20k validation videos on which we report results. Performance is measured by top-1 and top-5 accuracy.</p><p>Baseline Model and Training. We use a ResNet-50 (R50) SlowFast network <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> with a 32-frame fast pathway, speed ratio α=4, and channel ratio β=1/8 as our default model. Input frames are sampled at a temporal stride of 2.</p><p>Our baseline training recipe follows Feichtenhofer et al. <ref type="bibr" target="#b8">[9]</ref>. We run synchronous SGD for 112k iterations on 128 GPUs with a mini-batch size of 4 clips per GPU (∼239 epochs) with initial learning rate of 0.8. (We perform single GPU experiments in §5.) The learning rate is decreased by 10× at iterations 44k, 72k, and 92k. <ref type="bibr" target="#b6">7</ref> We use a weight decay of 10 −4 , momentum of 0.9, and a linear learning rate warm-up <ref type="bibr" target="#b12">[13]</ref> from 0.002 over 16k iterations. Input clips are random 224×224 spatial crops from clips that are randomly resized such that the shorter side ∈ [256, 340] pixels.</p><p>At test time, we sample 10 clips per video with uniform temporal spacing and combine the predictions with average pooling following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47]</ref>. We use 224×224 center crop testing by default <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b47">48]</ref> and present results with other settings in the Appendix.</p><p>We select these training and inference procedures based on validation accuracy using the baseline training method. We adopt the exact same recipe for multigrid training experiments, aside from multigrid specific changes. This choice may put multigrid training at a disadvantage, but it reflects the realistic scenario in which one wants to apply multigrid training to accelerate an already known training schedule without further tuning.</p><p>Evaluation. Speedup factors are wall-clock GPU training time on P100 GPUs with CUDA 9.2 and cuDNN 7.6.3. For fair comparison, the same hardware and software implementation is used for all methods. We note that multigrid training exploits larger mini-batches, which increases data loading throughput requirements. Training may become IO bound if the data loader is not optimized appropriately or if remote data access is used. With sufficient local disk and an optimized data loader, training is typically not IO bound.</p><p>Multigrid Training Details. To sample data with spatial shape h×w that is smaller than H×W , we change the default random short-side interval to [256 h H , 340], noting that w=h in our experiments. For the temporal dimension, we take t (t&lt;T ) frames with random stride in [2, 2 T t ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>We compare multigrid training to baseline training in <ref type="figure">Fig. 3</ref>. In addition to the default baseline, one could speed up training by using a smaller spatial-temporal shape with a larger mini-batch size and learning rate, so we also compare to this baseline variant. For each method, we experiment with training schedules that range from 0.25× to 3× the number of baseline epochs (∼239) to study the trade-off between training time and accuracy. Overall, multigrid training always achieves a better trade-off than baseline training. For example, multigrid training with both the long and short cycles can iterate through 1.5× more epochs than baseline method, while only requiring 1/3.4× the number of iterations, 1/4.5× training time, and achieving higher accuracy (75.6% → 76.4%). The wall-clock speedup is greater than the iteration reduction factor, as a larger mini-batch with smaller space/time dimensions is more parallelism-friendly on modern GPUs. Both the long and short cycles improve the trade-off and using both together performs the best.</p><p>In <ref type="figure">Fig. 3</ref> we also observe that baseline training suffers a decline in accuracy when training for ≥1.5× epochs. With either long and/or short cycles, a decrease in accuracy is not observed for schedules up to 2.0× epochs, indicating that variable grids can help prevent overfitting.</p><p>In the following we use multigrid training with long and short cycles and 1.5× more epochs than the baseline as our default since it obtains a good trade-off.   <ref type="figure">Figure 3</ref>. Multigrid vs. baseline training. Each point corresponds to one model trained with a specific schedule choice. Annotations denote training epochs relative to the baseline 1.0× schedule. For example, '1.5×' denotes training for 1.5× more epochs than the default '1.0×' baseline schedule (112k iterations or ∼239 epochs). We see that all variants of multigrid training achieve a better trade-off than baseline training, which uses a constant mini-batch shape. Also note that multigrid training can iterate through the same number of epochs more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>Long Cycle Design. By default, we use S=4 long cycle shapes with a 1.5× epoch schedule. In <ref type="table">Table 1a</ref>, we explore using fewer shapes, where we take the last S ′ &lt;S shapes, for S ′ ∈{1, 2, 3}. The short cycle is used in these experiments, and the S ′ =1 setting is equivalent to using the short cycle only. We run all variants for the same number of training iterations (to roughly preserve total training FLOPs), noting that methods which use fewer shapes will process fewer epochs due to having smaller mini-batches on average compared to the S=4 design.</p><p>We see that using each additional shape improves accuracy and saturates at S=4 (default). The improvement in accuracy is possibly due to the more examples seen by the model given the same amount of iterations. Compared with S=1 (i.e., short cycle only), our default choice improves the top-1 accuracy by absolute 2.4% (74.0% → 76.4%), while being slightly faster (4.0× → 4.5×). All results use the finetuning phase, which we find is beneficial to varying degrees in different settings. With the default schedule, it leads to 0.4% absolute gain (76.0% → 76.4%; not shown in table). Short Cycle Design. Adding each input shape to the short cycle leads to a clear accuracy improvement, <ref type="table" target="#tab_3">Table 1b</ref>. Our default short cycle design (3-shape) improves over 1-shape (i.e., no short cycle / long-cycle only) by absolute 1.9% (74.5% → 76.4%) in top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generalization to Different Training Settings</head><p>Next we study how multigrid training generalizes to different training settings that are common in practice.</p><p>Pre-training. In our main results, we train models from random initialization. We see in <ref type="table">Table 2a</ref> that with Im-ageNet <ref type="bibr" target="#b34">[35]</ref> pre-training, our multigrid method obtains a similar speedup and performance gain. (We will present more results on ImageNet-pre-trained models in §4.4.)</p><p>Temporal Shape. Next we show generalization of multigrid training for models of different temporal shapes T . We compare models that use 16-frame, 32-frame (default), and 64-frame input clips. <ref type="bibr" target="#b7">8</ref> In all cases <ref type="table">(Table 2b)</ref>, multigrid training achieves a consistent accuracy gain and speedup. The 64-frame model enjoys the largest performance gain (75.9% → 77.6%) and the best speedup (5.5×).</p><p>Spatial Shape. We also demonstrate generalization of our method for models of different spatial shapes H×W . We increase the baseline shape from 224×224 (default) to 320×320 and study the impact. Inference for the 320×320 model is analogous to the 224×224 case; we resize shorter side to 352 pixels and test on center 320×320 crops. In <ref type="table">Table 2c</ref>, we see that multigrid training leads to an even larger performance gain (75.1% → 76.8%) and a more significant speedup (6.5×) in the 320×320 case. Also note with the baseline method 320×320 does not work better than 224×224, possibly due to overfitting, similar to what is reported in Tan et al. <ref type="bibr" target="#b40">[41]</ref>. On the other hand, with multigrid training, spatial scaling brings improvement, possibly due to the data augmentation brought by multigrid training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generalization to Different Models</head><p>So far we have focused on state-of-the-art SlowFast network <ref type="bibr" target="#b8">[9]</ref> for analysis. We next demonstrate generalization of multigrid training to different networks by presenting results using a standard R50-I3D model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> and its extension with non-local blocks (I3D-NL) <ref type="bibr" target="#b46">[47]</ref>.  <ref type="table">Table 1</ref>. Ablation Study. We perform ablations on Kinetics-400 using an R50-SlowFast network. We analyze the impact of the long cycle <ref type="table">(Table 1a</ref>) and short cycle (  <ref type="table">Table 2</ref>. Generalization Analysis. We study how multigrid training generalizes to models both with and without ImageNet pre-training <ref type="table">(Table 2a</ref>) and models of different temporal <ref type="table">(Table 2b</ref>) and spatial <ref type="table">(Table 2c)</ref> shapes. All experiments use R50-SlowFast with results on Kinetics-400. We use the default setting for multigrid training (1.5× more epochs, corresponding to 3.4× fewer iterations than baseline) in all settings. We observe that the default choice brings consistent speedup and performance gain in all cases.</p><p>Implementation Details. Both models are ImageNet-pretrained with 3D convolutions inflated from 2D convolutions following common practice <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b46">47]</ref>. Each input clip consists of 16 frames, sampled at a stride of 4. I3D-NL additionally contains 5 (dot product) non-local blocks <ref type="bibr" target="#b46">[47]</ref> in res 3 and res 4 stages. The exact model specification is given in the Appendix. The baseline recipe trains for 100k iterations using 128 GPUs, with a mini-batch size of 2 clips per GPU (∼106 epochs) and a learning rate of 0.04, which is decreased by a factor of 10 at iteration 37.5k and 75k. We do not use learning rate warm-up <ref type="bibr" target="#b12">[13]</ref> following prior work <ref type="bibr" target="#b46">[47]</ref>. Other training details are analogous to SlowFast training. We note again that this training recipe is selected to be the best for the baseline training method and we apply multigrid training on top without further tuning.</p><p>Evaluation. We summarize the results in <ref type="table">Table 3</ref>. For both I3D and I3D-NL, multigrid training with the default schedule (1.5× epoch) obtains similar or better accuracy, while being up to 3.9× faster. We also experiment with a shorter baseline schedule ('baseline <ref type="bibr">1 3.3</ref>  <ref type="table">' in table)</ref>, which trains for the same number of iterations as the multigrid training. The shorter baseline schedule obtains a lower accuracy (3.7% and 3.2% absolute top-1 lower than multigrid). We also see that I3D-NL has a lower speedup than I3D. This is in part due to the less optimized NL operator than convolution, consuming a large portion of the training time. We ob-  <ref type="table">Table 3</ref>. Kinetics-400 accuracy with I3D and I3D-NL. While developed on SlowFast <ref type="bibr" target="#b8">[9]</ref>, multigrid training provides a consistent speedup and performance gain with I3D <ref type="bibr" target="#b2">[3]</ref> and I3D-NL <ref type="bibr" target="#b46">[47]</ref>.</p><p>serve consistent improvements with larger backbone models (R101); see the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Case Study: 1-GPU Training on Kinetics</head><p>Our experiments thus far use a large number of GPUs (128) in parallel. However, a more common training recipe may use far fewer GPUs (e.g., 1 to 8) and given that one of our goals is to make video research more accessible by reducing computational requirements it is important to explore the application of our multigrid method in the few-GPU regime, without any tuning.</p><p>As a case study, we use a single GPU to train an I3D model on Kinetics-400 using the quick training recipe from the public repository 9 of Wang et al. <ref type="bibr" target="#b46">[47]</ref>. We apply multigrid training on top without further tuning. trains for 1200k iterations (after adjusting with the linear scaling rule <ref type="bibr" target="#b12">[13]</ref>) on one GPU with 8 clips (∼40 epochs). The learning rate is 0.00125, which is decreased by a factor of 10 at iteration 600k and 1000k. Dropout and random scaling are disabled to accelerate convergence given the short schedule. Each input clip consists of 8 frames, sampled at a stride of 8, when using the baseline optimizer.</p><p>Other training details are the same as the I3D experiments. <ref type="table" target="#tab_5">Table 4</ref> shows that multigrid training generalizes well out-of-the-box to a few-GPU, short-schedule setting. With multigrid training we are able to achieve 72.5% (73.1% with 30-crop testing <ref type="bibr" target="#b46">[47]</ref>) top-1 accuracy in 2 days using only 1 GPU, while the baseline method would need nearly 1 week. (When using a small model, we observe a smaller wall-clock speedup of ∼3.3× compared to a larger model, which typically yields a ∼4.5× speedup). We hope the reduced training time with multigrid training will make video understanding research more accessible and economical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments on Something-Something V2</head><p>We next evaluate multigrid training on the Something-Something V2 dataset <ref type="bibr" target="#b13">[14]</ref>, which contains 169k training, and 25k validation videos. Each video shows an interaction with everyday objects. The task is classification with 174 action classes. Performance is evaluated by top-1 and top-5 accuracy. This task is known to require more 'temporal modeling' to solve than Kinetics <ref type="bibr" target="#b48">[49]</ref>. Implementation Details. We use an R50-SlowFast model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> with 64-frame fast pathway with speed ratio α=4 and channel ratio β=1/8. The model is pre-trained on Kinetics-400 following prior work <ref type="bibr" target="#b28">[29]</ref>. The baseline training recipe trains for 230k iterations on 8 GPUs, with a mini-batch size of 2 clips per GPU and a learning rate of 0.03, which is decreased by a factor of 10 at iteration 150k and 190k. Other training details are analogous to Kinetics experiments; see the Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>. Similar to what we observe on Kinetics, multigrid training obtains a better trade-off than baseline training on Something-Something V2 (   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments on Charades</head><p>We finally evaluate our method on the Charades dataset <ref type="bibr" target="#b35">[36]</ref>, which is relatively small, consisting of only 9,848 videos in 157 action classes. The task is to predict all actions in a video. Performance is measured by mAP. Implementation Details. We use the same R50-SlowFast model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>, with the same Kinetics pre-training as the Something-Something experiments. Training details are available in the Appendix.</p><p>Results. Overall we observe consistent results compared with Kinetics and Something-Something V2 ( <ref type="table" target="#tab_8">Table 6</ref>). The default multigrid training is 5.7× faster, while achieving slightly better mAP. Overall, we see that even for the smaller Charades dataset, with strong large-scale pretraining, multigrid training is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We propose a multigrid method for fast training of video models. Our method varies the sampling grid and the minibatch size during training, and can process the same number of epochs using a small fraction of the computation of the baseline trainer. With a single out-of-the-box setting, it works on multiple datasets and models, and consistently brings a ∼3-6× speedup with comparable or higher accuracy. It works across a spectrum of hardware settings from 128 GPU distributed training to single GPU training. We hope the reduced training time will make video understanding research more accessible, scalable, and economical. As expected, R101-SlowFast outperforms R50-SlowFast and we observe a consistent speedup and accuracy gain over the baseline with multigrid training.</p><p>Long Cycle Design. By default we use multiple long cycles that are synchronized with the stepwise learning rate (LR) schedule (i.e., one long cycle period per LR stage). We compare our default design ('multi-cycle') with an alternative that uses only a single long cycle period ('singlecycle') throughout all of training. Note that the single-cycle design does not use a fine-tuning phase as it is unclear how to incorporate it into this design. We observe that our default, multi-cycle design works better. In the multi-cycle design, the later shapes, which are closer to the final testing distribution, are used with each LR. We conjecture that exposing the model to these shapes with the larger (earlier) LRs is important for generalizing to the testing distribution. In contrast, the single-cycle design only uses the later shapes with relatively low LRs.</p><p>Cosine Learning Rate Schedule. We develop multigrid training assuming a stepwise LR schedule. Next we experiment with a cosine LR schedule. We experiment with both the multi-cycle and the single-cycle design for long cycles. No further modifications are applied to multigrid training. <ref type="bibr">LR</ref>  We observe that multigrid training on a cosine schedule obtains a slightly lower accuracy than the default stepwise schedule. The lower accuracy is possibly due to the relatively smaller learning rates used in larger (later) shapes as the LR is monotonically decreasing in a cosine schedule. However, it still obtains a consistent speedup and a comparable accuracy to baseline, suggesting robustness of the multigrid strategy. The two long-cycle designs obtain a similar accuracy.</p><p>Testing Settings Next we present results with additional test-time settings that are common in the literature. Here we use the 64-frame R50-SlowFast due to its high accuracy.</p><p>Our multigrid method trains this model 5.5× faster than the baseline. As expected, using 3-crop (left-center-right) testing improves accuracy for both baseline and multigrid training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Supplementary Implementation Details</head><p>The I3D and I3D-NL architectures used in generalization analysis are shown below (assuming 16×224×224 inputs): 'I3D-NL' additionally uses non-local operators <ref type="bibr" target="#b46">[47]</ref> after blocks 1 and 3 of res 3 , and blocks 1, 3, and 5 of res 4 .</p><p>Something-Something V2 Training. We use a linear warm-up <ref type="bibr" target="#b12">[13]</ref> for 2k iterations from 0.0001 and a weight decay of 10 −6 . As Something-Something V2 requires distinguishing between directions, we disable random flipping during training. Following <ref type="bibr" target="#b28">[29]</ref>, we use segment-based input frame sampling, i.e., we split each video into segments, and from each of them, sample one frame to form a clip.</p><p>Charades Training. The baseline method trains for 28k iterations with a learning rate of 0.0375, which is decreased by a factor of 10 at iteration 20k and 24k.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>A general and robust grid schedule ( §3.2). We contrast multigrid training with standard baseline training. (a) Baseline training methods typically use a fixed mini-batch shape throughout training. (b) Multigrid long cycles loop over inputs from small shapes (with large mini-batch sizes) to large shapes (with small mini-batch sizes), staying on each shape for several epochs. (c) Multigrid short cycles rapidly move through a variety of spatial shapes, changing at each iteration. (d) Multigrid long + short cycles (our default setting) combines long and short cycles, and moves through shapes at two frequencies simultaneously. Dark green points in (b), (c), and (d) correspond to one full period of a long cycle, a full short cycle, and a long+short cycle, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>github.com/facebookresearch/SlowFast/blob/master/projects/multigrid</figDesc><table><row><cell>Validation top-1 accuracy</cell><cell>65 70 75 80</cell><cell cols="3">Multigrid 4.5× faster, 76.4%</cell><cell></cell><cell cols="2">Baseline 75.6% Multigrid training (ours) Baseline training</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Wall-clock training time in hours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1b</head><label>1b</label><figDesc>) designs. All variants of multigrid training use the same number of training iterations as our default 1.5× epoch schedule; this roughly preserves the total training FLOPs. We report wall-clock speedup relative to the baseline trained for 1.0× epochs.</figDesc><table><row><cell></cell><cell cols="2">pre-train? speedup top-1</cell><cell>top-5</cell><cell>T</cell><cell></cell><cell cols="2">speedup top-1</cell><cell>top-5</cell><cell>H×W</cell><cell></cell><cell cols="2">speedup top-1</cell><cell>top-5</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>75.6</cell><cell>91.9</cell><cell>16</cell><cell>Baseline</cell><cell>-</cell><cell>74.8</cell><cell>91.4</cell><cell>224</cell><cell>Baseline</cell><cell>-</cell><cell>75.6</cell><cell>91.9</cell></row><row><cell>Multigrid</cell><cell>4.5×</cell><cell>76.4</cell><cell>92.4</cell><cell>16</cell><cell cols="2">Multigrid 4.0×</cell><cell>75.2</cell><cell>91.9</cell><cell>224</cell><cell cols="2">Multigrid 4.5×</cell><cell>76.4</cell><cell>92.4</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>75.4</cell><cell>91.9</cell><cell>32</cell><cell>Baseline</cell><cell>-</cell><cell>75.6</cell><cell>91.9</cell><cell>320</cell><cell>Baseline</cell><cell>-</cell><cell>75.1</cell><cell>91.8</cell></row><row><cell>Multigrid</cell><cell>4.5×</cell><cell>76.0</cell><cell>92.4</cell><cell>32</cell><cell cols="2">Multigrid 4.5×</cell><cell>76.4</cell><cell>92.4</cell><cell>320</cell><cell cols="2">Multigrid 6.5×</cell><cell>76.8</cell><cell>92.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell>Baseline</cell><cell>-</cell><cell>75.9</cell><cell>92.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell cols="2">Multigrid 5.5×</cell><cell>77.6</cell><cell>93.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a) Pre-training</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Temporal shape T</cell><cell></cell><cell></cell><cell cols="3">(c) Spatial shape H×W</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Case study: 1-GPU training on Kinetics-400. Multigrid training reduces the training time from nearly 1 week to 2 days on a single GPU. We hope the reduced training time will make video understanding research more accessible and economical.</figDesc><table><row><cell>This schedule</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 )</head><label>5</label><figDesc>. With the default 1.5×epoch training, multigrid training is 5.6× faster while obtaining a slightly higher accuracy. Multigrid training behaves consistently for the 'spatial heavy' Kinetics dataset and the 'temporal heavy' Something-Something V2 dataset.</figDesc><table><row><cell></cell><cell>speedup</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>Baseline</cell><cell>-</cell><cell cols="2">60.9±0.31 87.2±0.13</cell></row><row><cell>Baseline 1 5.2</cell><cell>5.2×</cell><cell cols="2">54.6±0.13 83.0±0.14</cell></row><row><cell>Multigrid 1.0×</cell><cell>8.3×</cell><cell cols="2">60.0±0.31 86.8±0.05</cell></row><row><cell>Baseline 1 3.4</cell><cell>3.4×</cell><cell cols="2">57.3±0.13 84.7±0.15</cell></row><row><cell>Multigrid 1.5× (default)</cell><cell>5.6×</cell><cell cols="2">61.2±0.18 87.4±0.12</cell></row><row><cell>Baseline 1 2.6</cell><cell>2.6×</cell><cell cols="2">58.7±0.06 85.8±0.15</cell></row><row><cell>Multigrid 2.0×</cell><cell>4.2×</cell><cell cols="2">61.7±0.20 87.8±0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Results on Something-Something V2. Multigrid training achieves a better trade-off than baseline training. Results are the mean and standard deviation over 5 runs.</figDesc><table><row><cell></cell><cell>speedup</cell><cell>mAP (%)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>38.0±0.18</cell></row><row><cell>Baseline 1 5.3</cell><cell>5.3×</cell><cell>27.5±0.15</cell></row><row><cell>Multigrid 1.0×</cell><cell>8.6×</cell><cell>36.8±0.31</cell></row><row><cell>Baseline 1 3.5</cell><cell>3.5×</cell><cell>31.5±0.26</cell></row><row><cell>Multigrid 1.5× (default)</cell><cell>5.7×</cell><cell>38.2±0.06</cell></row><row><cell>Baseline 1 2.6</cell><cell>2.6×</cell><cell>33.6±0.13</cell></row><row><cell>Multigrid 2.0×</cell><cell>4.3×</cell><cell>37.4±0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Results on Charades. Multigrid training shows consistent speedups compared with the other datasets. Results are the mean and standard deviation over 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>A. Appendix A.1. Supplementary Experiments R101-SlowFast Results. We demonstrate generalization of multigrid training to deeper backbones by extending our default R50-SlowFast network to R101-SlowFast. All other designs and training procedures remain unchanged.</figDesc><table><row><cell>backbone</cell><cell></cell><cell cols="2">speedup top-1</cell><cell>top-5</cell></row><row><cell cols="2">R50 (default) Baseline</cell><cell>-</cell><cell>75.6</cell><cell>91.9</cell></row><row><cell cols="3">R50 (default) Multigrid 4.5×</cell><cell>76.4</cell><cell>92.4</cell></row><row><cell>R101</cell><cell>Baseline</cell><cell>-</cell><cell>76.5</cell><cell>92.4</cell></row><row><cell>R101</cell><cell cols="2">Multigrid 4.4×</cell><cell>77.0</cell><cell>92.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We omit the channel dimension (3 for RGB) for clarity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">If an appropriate operator exists for 'resampling' model parameters so that they are compatible with new input shapes, then these parameters may still be usable with multigrid training. This concept can be combined with weight sharing, e.g., by dilating or resizing model filters to mirror the data sampling grid, though preliminary experiments did not improve results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use the stepwise learning rate schedule rather than the cosine schedule used in Feichtenhofer et al.<ref type="bibr" target="#b8">[9]</ref> because it is still more common. Results with a cosine schedule are available in the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">α=2 in the 16-frame model to avoid a degenerated slow pathway.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/facebookresearch/video-nonlocal-net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We would like to thank Haoqi Fan for helping with the code release and Ishan Nigam, Santhosh K. Ramakrishnan, and Xingyi Zhou for helpful comments on an earlier draft. This material is partially supported by the National Science Foundation under Grant No. IIS-1845485 and the Facebook Fellowship to C-Y Wu.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wf Briggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">F</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccormick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>2nd Edition. SIAM</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Massively parallel video networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Mazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In OSDI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Dan C Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1102.0183</idno>
		<title level="m">High-performance neural networks for visual object classification</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Largescale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Carl Doersch, and Andrew Zisserman. Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Something Something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MgNet: A unified framework of multigrid and convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Mathematics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">STM: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SCSampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Sungjoon Son, Gyutae Park, and Nojun Kwak</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grouped spatial-temporal aggregation for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition with spatial-temporal discriminative filter banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evolving space-time neural architectures for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural rejuvenation: Improving deep network training by enhancing computational resource utilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3D residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patrice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Don&apos;t decay the learning rate, increase the batch size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ImageNet training in minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
