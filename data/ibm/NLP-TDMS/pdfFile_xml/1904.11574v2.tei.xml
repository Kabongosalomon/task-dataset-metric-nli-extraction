<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TVQA+: Spatio-Temporal Grounding for Video Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
							<email>jielei@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
							<email>licheng@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
							<email>tlberg@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TVQA+: Spatio-Temporal Grounding for Video Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations. 1 1 Dataset and code are publicly available: http: //tvqa.cs.unc.edu, https://github.com/ jayleicn/TVQAplus</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We have witnessed great progress in recent years on image-based visual question answering (QA) tasks <ref type="bibr" target="#b1">(Antol et al., 2015;</ref><ref type="bibr" target="#b47">Yu et al., 2015;</ref><ref type="bibr" target="#b54">Zhu et al., 2016b)</ref>. One key to this success has been spatial attention <ref type="bibr" target="#b0">(Anderson et al., 2018;</ref><ref type="bibr" target="#b37">Shih et al., 2016;</ref><ref type="bibr" target="#b26">Lu et al., 2016)</ref>, where neural models learn to attend to relevant regions for predicting the correct answer. Compared to image-based QA, there has been less progress on the performance of video-based QA tasks. One possible reason is that attention techniques are hard to generalize to the temporal nature of videos. Moreover, due to the high cost of annotation, most existing video QA datasets only contain QA pairs, without providing labels for the key clips or regions needed to answer the question. Inspired by previous work on grounded image and video captioning <ref type="bibr" target="#b27">(Lu et al., 2018;</ref><ref type="bibr" target="#b52">Zhou et al., 2019)</ref>, we propose methods that explicitly localize video clips as well as spatial regions for answering videobased questions. Such methods are useful in many scenarios, such as natural language guided spatiotemporal localization, and adding explainability to video question answering, which is potentially useful for decision making and model debugging. To enable this line of research, we also collect new joint spatio-temporal annotations for an existing video QA dataset.</p><p>In the past few years, several video QA datasets have been proposed, e.g., MovieFIB <ref type="bibr" target="#b28">(Maharaj et al., 2017)</ref>, MovieQA <ref type="bibr" target="#b39">(Tapaswi et al., 2016)</ref>, TGIF-QA <ref type="bibr" target="#b15">(Jang et al., 2017)</ref>, PororoQA , MarioQA <ref type="bibr" target="#b30">(Mun et al., 2017)</ref>, and TVQA <ref type="bibr" target="#b21">(Lei et al., 2018)</ref>. TVQA is one of the largest video QA datasets, providing a large video QA dataset built on top of 6 famous TV series. Be-cause TVQA was collected on television shows, it is built on natural video content with rich dynamics and complex social interactions, where questionanswer pairs are written by people observing both videos and their accompanying dialogues, encouraging the questions to require both vision and language understanding to answer. Movie <ref type="bibr" target="#b39">(Tapaswi et al., 2016;</ref><ref type="bibr" target="#b28">Maharaj et al., 2017)</ref> and television show <ref type="bibr" target="#b21">(Lei et al., 2018)</ref> videos come with the limitation of being scripted and edited, but they are still more realistic than cartoon/animation  and game <ref type="bibr" target="#b30">(Mun et al., 2017)</ref> videos, and they also come with richer, real-world-inspired inter-human interactions and span across diverse domains (e.g., medical, crime, sitcom, etc.), making them a useful testbed to study complex video understanding by machine learning models.</p><p>One key property of TVQA is that it provides temporal annotations denoting which parts of a video clip are necessary for answering a proposed question. However, none of the existing video QA datasets (including TVQA) provide spatial annotation for the answers. Actually, grounding spatial regions correctly could be as important as grounding temporal moments for answering a given question. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>, to answer the question of "What is Sheldon holding when he is talking to Howard about the sword?", we need to localize the moment when "he is talking to Howard about the sword?", as well as look at the region of "What is Sheldon holding".</p><p>Hence, in this paper, we first augment a subset of the TVQA dataset with grounded bounding boxes, resulting in a spatio-temporally grounded video QA dataset, TVQA+. It consists of 29.4K multiplechoice questions grounded in both the temporal and the spatial domains. To collect spatial groundings, we start by identifying a set of visual concept words, i.e., objects and people, mentioned in the question or correct answer. Next, we associate the referenced concepts with object regions in individual frames, if there are any, by annotating bounding boxes for each referred concept (see examples in <ref type="figure" target="#fig_0">Fig. 1</ref>). Our TVQA+ dataset has a total of 310.8K bounding boxes linked with referred objects and people, spanning across 2.5K categories (more details in Sec. 3).</p><p>With such richly annotated data, we then propose the task of spatio-temporal video question answering, which requires intelligent systems to localize relevant moments, detect referred objects and people, and answer questions. We further design several metrics to evaluate the performance of the proposed task, including QA accuracy, object grounding precision, temporal localization accuracy, and a joint temporal localization and QA accuracy. To address spatio-temporal video question answering, we propose a novel end-to-end trainable model, Spatio-Temporal Answerer with Grounded Evidence (STAGE), which effectively combines moment localization, object grounding, and question answering in a unified framework. We find that the QA performance benefits from both temporal moment and spatial region supervision. Additionally, we provide visualization of temporal and spatial localization, which is helpful for understanding what our model has learned. Comprehensive ablation studies demonstrate how each of our annotations and model components helps to improve the performance of the tasks.</p><p>To summarize, our contributions are:</p><p>• We collect TVQA+, a large-scale spatiotemporal video question answering dataset, which augments the original TVQA dataset with frame-level bounding box annotations. To our knowledge, this is the first dataset that combines moment localization, object grounding, and question answering.</p><p>• We design a novel video question answering framework, Spatio-Temporal Answerer with Grounded Evidence (STAGE), to jointly localize moments, ground objects, and answer questions. By performing all three sub-tasks together, our model achieves significant performance gains over the baselines, as well as presents insightful, interpretable visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Question Answering In recent years, multiple question answering datasets and tasks have been proposed to facilitate research towards this goal, in both vision and language communities, in the form of visual question answering <ref type="bibr" target="#b1">(Antol et al., 2015;</ref><ref type="bibr" target="#b47">Yu et al., 2015;</ref><ref type="bibr" target="#b15">Jang et al., 2017)</ref> and textual question answering <ref type="bibr" target="#b33">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b42">Weston et al., 2016)</ref>, respectively. Video question answering <ref type="bibr" target="#b21">(Lei et al., 2018;</ref><ref type="bibr" target="#b39">Tapaswi et al., 2016;</ref> with naturally occurring subtitles are particularly interesting, as it combines both visual and textual information for question answering. Different from Dataset Origin Task #Clips/#QAs #Boxes Temporal (#Sentences) Annotation</p><p>MovieFIB <ref type="bibr" target="#b28">(Maharaj et al., 2017)</ref> Movie QA 118.5K/349K -MovieQA <ref type="bibr" target="#b39">(Tapaswi et al., 2016)</ref> Movie QA 6.8K/6.5K -TGIF-QA <ref type="bibr" target="#b15">(Jang et al., 2017)</ref> Tumblr QA 71.7K/165.2K -PororoQA  Cartoon QA 16.1K/8.9K -DiDeMo <ref type="bibr" target="#b13">(Hendricks et al., 2017)</ref> Flickr TL 10.5K/40.5K -Charades-STA <ref type="bibr" target="#b8">(Gao et al., 2017)</ref> Home TL -/19.5K -TVQA <ref type="bibr" target="#b21">(Lei et al., 2018)</ref> TV Show QA/TL 21.8K/152.5K -ANet-Entities <ref type="bibr" target="#b52">(Zhou et al., 2019)</ref> Youtube existing video QA tasks, where a system is only required to predict an answer, we propose a novel task that additionally grounds the answer in both spatial and temporal domains.</p><p>Language-Guided Retrieval Grounding language in images/videos is an interesting problem that requires jointly understanding both text and visual modalities. Earlier works <ref type="bibr" target="#b16">(Kazemzadeh et al., 2014;</ref><ref type="bibr" target="#b15">Yu et al., 2017</ref><ref type="bibr" target="#b46">Yu et al., , 2018b</ref><ref type="bibr" target="#b35">Rohrbach et al., 2016)</ref> focused on identifying the referred object in an image. Recently, there has been a growing interest in moment retrieval tasks <ref type="bibr" target="#b13">(Hendricks et al., 2017</ref><ref type="bibr" target="#b12">(Hendricks et al., , 2018</ref><ref type="bibr" target="#b8">Gao et al., 2017)</ref>, where the goal is to localize a short clip from a long video via a natural language query. Our work integrates the goals of both tasks, requiring a system to ground the referred moments and objects simultaneously.</p><p>Temporal and Spatial Attention Attention has shown great success on many vision and language tasks, such as image captioning <ref type="bibr" target="#b0">(Anderson et al., 2018;</ref><ref type="bibr" target="#b43">Xu et al., 2015)</ref>, visual question answering <ref type="bibr" target="#b0">(Anderson et al., 2018;</ref><ref type="bibr" target="#b40">Trott et al., 2018)</ref>, language grounding <ref type="bibr" target="#b46">(Yu et al., 2018b)</ref>, etc. However, sometimes the attention learned by the model itself may not agree with human expectations <ref type="bibr" target="#b24">(Liu et al., 2016;</ref><ref type="bibr" target="#b4">Das et al., 2016)</ref>. Recent works on grounded image captioning and video captioning <ref type="bibr" target="#b27">(Lu et al., 2018;</ref><ref type="bibr" target="#b52">Zhou et al., 2019)</ref> show better performance can be achieved by explicitly supervising the attention. In this work, we use annotated frame-wise bounding box annotations to supervise both temporal and spatial attention. Experimental results demonstrate the effectiveness of supervising both domains in video QA.  , joined via a link word, ("before", "when", "after"), to a localization part that temporally locates when the question occurs ("he spilled the milk"). Models should answer questions using both visual information from the video, as well as language information from the naturally associated dialog (subtitles). Since the video clips on which the questions were collected are usually much longer than the context needed for answering the questions, the TVQA dataset also provides a temporal timestamp annotation indicating the minimum span (context) needed to answer each question. While the TVQA dataset provides a novel question format and temporal annotations, it lacks spatial grounding information, i.e., bounding boxes of the concepts (objects and people) mentioned in the QA pair. We hypothesize that object annotations could provide an additional useful training signal for models to learn a deeper understanding of visual information. Therefore, to complement the original TVQA dataset, we collect frame-wise bounding boxes for visual concepts mentioned in the questions and correct answers. Since the full TVQA dataset is very large, we start by collecting bounding box annotations for QA pairs associated with The Big Bang Theory. This subset contains 29,383 QA pairs from 4,198 clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>Identify Visual Concepts To annotate the visual concepts in video frames, the first step is to identify them in the QA pairs. We use the Stanford CoreNLP part-of-speech tagger  to extract all nouns in the questions and correct answers. This gives us a total of 152,722 words from a vocabulary of 9,690 words. We manually label the non-visual nouns (e.g., "plan", "time", etc.) in the top 600 nouns, removing 165 frequent non-visual nouns from the vocabulary.</p><p>Bounding Box Annotation For the selected The Big Bang Theory videos from TVQA, we first ask Amazon Mechanical Turk workers to adjust the start and end timestamps to refine the temporal annotation, as we found the original temporal annotation were not ideally tight. We then sample one frame every two seconds from each span for spatial annotation. For each frame, we collect the bounding boxes for the visual concepts in each QA pair. We also experimented with semi-automated annotation for people with face detection  and recognition model <ref type="bibr" target="#b25">(Liu et al., 2017)</ref>, but they do not work well mainly due to many partial occlusion of faces (e.g., side faces) in the frames. During annotation, we provide the original videos (with subtitles) to help the workers understand the context for the given QA pair. More annotation details (including quality check) are presented in the appendix.  <ref type="table" target="#tab_2">Table 2</ref>. Note that we follow the same data splits as the original TVQA dataset, supporting future research on both TVQA and TVQA+. <ref type="table">Table 1</ref> compares TVQA+ dataset with other videolanguage datasets. TVQA+ is unique as it supports three tasks: question answering, temporal localization, and spatial localization. It is also of reasonable size compared to the grounded video captioning dataset ANet-Entities <ref type="bibr" target="#b52">(Zhou et al., 2019)</ref>. On average, we obtain 2.09 boxes per image and 10.58 boxes per question. The annotated boxes cover 2,527 categories. We show the number of boxes (in log scale) for each of the top 60 categories in <ref type="figure" target="#fig_1">Fig. 2</ref>. The distribution has a long tail, e.g., the number of boxes for the most frequent category "sheldon" is around 2 orders of magnitude larger than the 60th category "glasses". We also show the distribution of bounding box area over image area ratio in <ref type="figure" target="#fig_2">Fig. 3</ref> (left). The majority of boxes are fairly small compared to the image, which makes object grounding challenging. <ref type="figure" target="#fig_2">Fig. 3</ref>      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Analysis</head><formula xml:id="formula_0">N j W M l O U t a g U U n U D o p n g C W s B B 8 G 6 q W I k D g T r B K O b a b 3 z w J T m M m n C O G V e T A Y J j z g l Y C z f P m 7 i P v C Y a X z n y x 8 M f b v i V J 2 Z 8 D K 4 B V R Q o Y Z v f / Z D S b O Y J U A F 0 b r n O i l 4 O V H A q W C T c j / T L C V 0 R A a s Z z A h Z o 2 X z 8 6 f 4 D P j h D i S y r w E 8 M z 9 P Z G T W O t x H J j O m M B Q L 9 a m 5 n + 1 X g b R t Z f z J M 2 A J X S + K M o E B o m n W e C Q K 0 Z B j A 0 Q q r i 5 F d M h U Y S C S a x s Q n A X v 7 w M 7 V r V v a i 6 9 5 e V e q 2 I o 4 R O 0 C k 6 R y 6 6 Q n V 0 i x q o h S j K 0 R N 6 Q a / W o / V s v V n v 8 9 Y V q 5 g 5 Q n 9 k f X w D 5 m e U w A = = &lt; / l a t e x i t &gt; T ⇥ Ls ⇥ d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o B J l / c W U f p H e s s m n S 3 Q r e S k a E t k = " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + f o H Q 9 + u O F V n J r w M b g E V V K j h 2 5 / 9 M K F Z z C R Q Q b T u u U 4 K X k 4 U c C r Y p N z P N E s J H Z E B 6 x m U x K z x 8 t n 5 E 3 x m n B B H i T J P A p 6 5 v y d y E m s 9 j g P T G R M Y 6 s X a 1 P y v 1 s s g u v Z y L t M M m K T z R V E m M C R 4 m g U O u W I U x N g A o Y q b W z E d E k U o m M T K J g R 3 8 c v L 0 K 5 V 3 Y u q e 3 9 Z q d e K O E r o B J 2 i c + S i K 1 R H t 6 i B W o i i H D 2 h F / R q P V r P 1 p v 1 P m 9 d s Y q Z I / R H 1 s c 3 6 X 2 U w g = = &lt; / l a t e x i t &gt; Lh ⇥ d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y r 6 d m S x k 2 n d l g 8 w Y L d I 1 S 6 M y l V E = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j w 4 s F D B f s B T S i b z a Z d u t m E 3 Y l Q S v + G F w + K e P X P e P P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F 2 Z S G H T d b 2 d t f W N z a 7 u 0 U 9 7 d 2 z 8 4 r B w d t 0 2 a a 8 Z b L J W p 7 o b U c C k U b 6 F A y b u Z 5 j Q J J e + E o 9 u Z 3 3 n i 2 o h U P e I 4 4 0 F C B 0 r E g l G 0 k n / f H x I f R c I N i f q V q l t z 5 y C r x C t I F Q o 0 + 5 U v P 0 p Z n n C F T F J j e p 6 b Y T C h G g W T f F r 2 c 8 M z y k Z 0 w H u W K m r X B J P 5 z V N y b p W I x K m 2 p Z D M 1 d 8 T E 5 o Y M 0 5 C 2 5 l Q H J p l b y b + 5 / V y j G + C i V B Z j l y x x a I 4 l w R T M g u A R E J z h n J s C W V a 2 F s J G 1 J N G d q Y y j Y E b / n l V d K u 1 7 z L m v d w V W 3 U i z h K c A p n c A E e X E M D 7 q A J L W C Q w T O 8 w p u T O y / O u / O x a F 1 z i p k T + A P n 8 w c 6 X 5 E a &lt; / l a t e x i t &gt; T ⇥ Lh ⇥ d &lt; l a</formula><formula xml:id="formula_1">= " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = &lt; / l a t e x i t &gt; T ⇥ Lh ⇥ d &lt; l a</formula><formula xml:id="formula_2">= " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = &lt; / l a t e x i t &gt; T ⇥ Lh ⇥ 3d &lt; l a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>Our proposed method, Spatio-Temporal Answerer with Grounded Evidence (STAGE), is a unified framework for moment localization, object grounding and video QA. First, STAGE encodes the video and text (subtitle, QA) via frame-wise regional visual representations and neural language representations, respectively. The encoded video and text representations are then contextualized using a Convolutional Encoder. Second, STAGE computes attention scores from each QA word to object regions and subtitle words. Leveraging the attention scores, STAGE is able to generate QA-aware representations, as well as automatically detecting the referred objects/people. The attended QA-aware video and subtitle representation are then fused together to obtain a joint frame-wise representation. Third, taking the frame-wise representation as input, STAGE learns to predict QA relevant temporal spans, then combines the global and local (span localized) video information to answer the questions.</p><p>In the following, we describe STAGE in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Formulation</head><p>In our tasks, the inputs are: (1) a question with 5 candidate answers;</p><p>(2) a 60-second long video; (3) a set of subtitle sentences. Our goal is to predict the answer and ground it both spatially and temporally. Given the question, q, and the answers, {a k } 5 k=1 , we first formulate them as 5 hypotheses (QA-pair) h k = [q, a k ] and predict their correctness scores based on the video and subtitle context <ref type="bibr" target="#b31">(Onishi et al., 2016)</ref>. We denote the ground-truth (GT) answer index as y ans and thus the GT hypothesis as h y ans . We then extract video frames {v t } T t=1 at 0.5 FPS (T is the number of frames for each video). Subtitle sentences are then temporally aligned with the video frames. Specifically, for each frame v t , we pair it with two neighboring sentences based on the subtitle timestamps. We choose two neighbors since this keeps most of the sentences at our current frame rate, and also avoids severe misalignment between the frames and the sentences. The set of aligned subtitle sentences are denoted as {s t } T t=1 . We denote the number of words in each hypothesis and subtitle as L h , L s , respectively. We use N o to denote the number of object regions in a frame, and d = 128 as the hidden size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">STAGE Architecture</head><p>Input Embedding Layer For each frame v t , we use Faster R-CNN (Ren et al., 2015) pre-trained on Visual Genome <ref type="bibr" target="#b20">(Krishna et al., 2017)</ref> to detect objects and extract their regional representation as our visual features <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>. We keep the top-20 object proposals and use PCA to reduce the feature dimension from 2048 to 300, to save GPU memory and computation. We denote o t,r ∈ R 300 as the r-th object embedding in the t-th frame. To encode the text input, we use BERT (Devlin et al., 2019), a transformer-based language model <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> that achieves state-ofthe-art performance on various NLP tasks. Specifically, we first fine-tune the BERT-base model using the masked language model and next sentence pre-diction objectives on the subtitles and QA pairs from TVQA+ train set. Then, we fix its parameters and use it to extract 768D word-level embeddings from the second-to-last layer for the subtitles and each hypothesis. Both embeddings are projected into a 128D space using a linear layer with ReLU.</p><p>Convolutional Encoder Inspired by the recent trend of replacing recurrent networks with CNNs <ref type="bibr" target="#b5">(Dauphin et al., 2016;</ref><ref type="bibr">Yu et al., 2018a)</ref> and Transformers <ref type="bibr" target="#b41">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b6">Devlin et al., 2019)</ref> for sequence modeling, we use positional encoding (PE), CNNs, and layer normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref> to build our basic encoding block. As shown in the bottom-right corner of <ref type="figure" target="#fig_6">Fig. 4</ref>, it is comprised of a PE layer and multiple convolutional layers, each with a residual connection <ref type="bibr" target="#b11">(He et al., 2016)</ref> and layer normalization. We use Layernorm(ReLU(Conv(x)) + x) to denote a single Conv unit and stack N conv of such units as the convolutional encoder. x is the input after PE, Conv is a depthwise separable convolution (Chollet, 2017). We use two convolutional encoders at two different levels of STAGE, one with kernel size 7 to encode the raw inputs, and another with kernel size 5 to encode the fused video-text representation. For both encoders, we set N conv = 2.</p><p>QA-Guided Attention For each hypothesis h k = [q, a k ], we compute its attention scores w.r.t. the object embeddings in each frame and the words in each subtitle sentence, respectively. Given the encoded hypothesis H k ∈ R L h ×d for the hypothesis h k with L h words, and encoded visual feature V t ∈ R No×d for the frame v t with N o objects, we compute their matching scores M k,t ∈ R L h ×No = H k V T t . We then apply softmax at the second dimension of M k,t to get the normalized scoresM k,t . Finally, we compute the QA-aware visual repre-</p><formula xml:id="formula_3">sentation V att k,t ∈ R L h ×d =M k,t V t .</formula><p>Similarly, we compute QA-aware subtitle representation S att k,t . Video-Text Fusion The above two QA-aware representations are then fused together as:</p><formula xml:id="formula_4">F k,t = [S att k,t ; V att k,t ; S att k,t V att k,t ]W F + b F ,</formula><p>where denotes hadamard product, W F ∈ R 3d×d and b F ∈ R d are trainable weights and bias, F k,t ∈ R L h ×d is the fused video-text representation. After collecting F att k,t from all time steps, we get F att k ∈ R T ×L h ×d . We then apply another convolutional encoder with a max-pooling layer to obtain the output A k ∈ R T ×d .</p><p>Span Predictor To predict temporal spans, we predict the probability of each position being the start or end of the span. Given the fused input A k ∈ R T ×d , we produce start probabilities p 1 k ∈ R T and end probabilities p 2 k ∈ R T using two linear layers with softmax, as shown in the top-right corner of <ref type="figure" target="#fig_6">Fig. 4</ref>. Different from existing works <ref type="bibr">Yu et al., 2018a</ref>) that used the span predictor for text only, we use it for a joint localization of both video and text, which requires properly-aligned joint embeddings.</p><p>Span Proposal and Answer Prediction Given the max-pooled video-text representation A k , we use a linear layer to further encode it. We run maxpool across all the time steps to get a global hypothesis representation G g k ∈ R d . With the start and end probabilities from the span predictor, we generate span proposals using dynamic programming . At training time, we combine the set of proposals with IoU ≥ 0.5 with the GT spans, as well as the GT spans to form the final proposals {st p , ed p } <ref type="figure" target="#fig_0">(Ren et al., 2015)</ref>. At inference time, we take the proposals with the highest confidence scores for each hypothesis. For each proposal, we generate a local representation G l k ∈ R d by maxpooling A k,stp:edp . The local and global representations are concatenated to obtain G k ∈ R 2d . We then forward {G k } 5 k=1 through softmax to get the answer scores p ans ∈ R 5 . Compared with existing works <ref type="bibr" target="#b15">(Jang et al., 2017;</ref><ref type="bibr" target="#b51">Zhao et al., 2017)</ref> that use soft temporal attention, we use more interpretable hard attention, extracting local features (together with global features) for question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Inference</head><p>In this section, we describe the objective functions used in the STAGE framework. Since our spatial and temporal annotations are collected based on the question and GT answer, we only apply the attention loss and span loss on the targets associated with the GT hypothesis (question + GT answer), i.e., M k=y ans ,t , p 1 k=y ans and p 2 k=y ans . For brevity, we omit the subscript k=y ans in the following.</p><p>Spatial Supervision While the attention described in Sec. 4.2 can be learned in a weakly supervised end-to-end manner, we can also train it with supervision from GT boxes. We define a box as positive if it has an IoU ≥ 0.5 with the GT box. Consider the attention scores M t,j ∈ R No from a concept word w j in GT hypothesis h y ans to the set of proposal boxes' representations {o t,r } No r=1 at frame v t . We expect the attention on positive boxes to be higher than the negative ones, and therefore use LSE  loss for the supervision:</p><formula xml:id="formula_5">L t,j = rp∈Ωp,rn∈Ωn log 1 + exp(M t,j,rn − M t,j,rp ) ,</formula><p>where M t,j,rp is the r p -th element of the vector M t,j . Ω p and Ω n denote the set of positive and negative box indices, respectively. LSE loss is a smoothed alternative to the widely used hinge loss, it is easier to optimize than the original hinge loss . During training, we randomly sample two negatives for each positive box. We use L att i to denote the attention loss for the i-th example, which is obtained by summing over all the annotated frames {v t } and concepts {w j } for L att t,j . We define the overall attention</p><formula xml:id="formula_6">loss L att = 1 N N i=1 L att i .</formula><p>At inference time, we choose the boxes with scores higher than 0.2 as the predictions.</p><p>Temporal Supervision Given softmax normalized start and end probabilities p 1 and p 2 , we apply cross-entropy loss:</p><formula xml:id="formula_7">L span = − 1 2N N i=1 log p 1 y 1 i + log p 2 y 2 i ,</formula><p>where y 1 i and y 2 i are the GT start and end indices. Answer Prediction Similarly, given answer probabilities p ans , our answer prediction loss is:</p><formula xml:id="formula_8">L ans = − 1 N N i=1 log p ans y ans i ,</formula><p>where y ans i is the index of the GT answer. Finally, the overall loss is a weighted combination of the three objectives above: L = L ans + w att L att + w span L span , where w att and w span are set as 0.1 and 0.5 based on validation set tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>As introduced, our task is spatio-temporal video question answering, requiring systems to temporally localize relevant moments, spatially detect referred objects and people, and answer questions. In this section, we first define the evaluation metrics, then compare STAGE against several baselines, and finally provide a comprehensive analysis of our model. Additionally, we also evaluate STAGE on the full TVQA dataset. <ref type="bibr">Model</ref> QA Grd. Temp. ASA Acc.</p><p>mAP mIoU ST-VQA <ref type="bibr" target="#b15">(Jang et al., 2017)</ref> 48.28 --two-stream <ref type="bibr" target="#b21">(Lei et al., 2018)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Metrics</head><p>To measure QA performance, we use classification accuracy (QA Acc.). We evaluate span prediction using temporal mean Intersection-over-Union (Temp. mIoU) following previous work <ref type="bibr" target="#b13">(Hendricks et al., 2017)</ref> on language-guided video moment retrieval. Since the span depends on the hypothesis (QA pair), each QA pair provides a predicted span, but we only evaluate the span of the predicted answer. Additionally, we propose Answer-Span joint Accuracy (ASA), that jointly evaluates both answer prediction and span prediction. For this metric, we define a prediction to be correct if the predicted span has an IoU ≥ 0.5 with the GT span, provided that the answer prediction is correct. Finally, to evaluate object grounding performance, we follow the standard metric from the PASCAL VOC challenge <ref type="bibr" target="#b7">(Everingham et al., 2015)</ref> and report the mean Average Precision (Grd. mAP) at IoU threshold 0.5. We only consider the annotated words and frames when calculating the mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Baseline Methods</head><p>We consider the two-stream model <ref type="bibr" target="#b21">(Lei et al., 2018)</ref> as our main baseline. In this model, two streams are used to predict answer scores from subtitles and videos respectively and final answer scores are produced by summing scores from both streams. We retrain the model using the official code 2 on TVQA+ data, with the same feature as STAGE. We also consider ST-VQA <ref type="bibr" target="#b15">(Jang et al., 2017)</ref> model, which is primarily designed for question answering on short videos (GIFs). We also provide STAGE variants that use only video or subtitle to study the effect of using only one of the modalities. <ref type="table" target="#tab_6">Table 3</ref> shows the test results of STAGE and the baselines. STAGE outperforms the baseline model (two-stream) by a large margin in QA Acc., 3 with 9.83% relative gains. Additionally, STAGE also lo-   calizes the relevant moments with temporal mIoU of 32.49% and detects referred objects and people with mAP of 27.34%. However, a large gap is still observed between STAGE and human, showing space for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Analysis</head><p>Backbone Model Given the full STAGE model defined in Sec. 4, we define the backbone model as the ablated version of it, where we remove the span predictor along with the span proposal module, as well as the explicit attention supervision. We further replace the CNN encoders with RNN encoders, and remove the aligned fusion from the backbone model. This baseline model uses RNN to encode input sequences and interacts QA pairs with subtitles and videos separately. The final confidence score is the sum of the confidence scores from the two modalities. In the backbone model, we align subtitles with video frames from the start, fusing their representation conditioned on the input QA pair, as in <ref type="figure" target="#fig_6">Fig. 4</ref>. We believe this aligned fusion is essential for improving QA performance, as the latter part of STAGE has a joint understanding of both video and subtitles. With both changes, our backbone model obtains 68.31% on QA Acc., significantly higher than the baseline's 65.79%. The results are shown in <ref type="table" target="#tab_8">Table 4</ref>.  <ref type="bibr" target="#b18">(Kim et al., 2019b)</ref> 66.38 66.77 multi-task <ref type="bibr" target="#b17">(Kim et al., 2019a)</ref> 66  Temporal and Spatial Supervision In <ref type="table" target="#tab_8">Table 4</ref>, we also show the results when using temporal and spatial supervision. After adding temporal supervision, the model is be able to ground on the temporal axis, which also improves the model's performance on other tasks. Adding spatial supervision gives additional improvements, particularly for Grd. mAP, with 121.92% relative gain.</p><p>Span Proposal and Local Feature In the second-to-last row of <ref type="table" target="#tab_8">Table 4</ref>, we show our full STAGE model, which is augmented with local features G l for question answering. Local features are obtained by max-pooling the span proposal regions, which contain more relevant cues for answering the questions. With G l , we achieve the best performance across all metrics, indicating the benefit of using local features.</p><p>Inference with GT Span The last row of <ref type="table" target="#tab_8">Table 4</ref> shows our model uses GT spans instead of predicted spans at inference time. We observe better QA Acc. with GT spans.</p><p>Accuracy by Question Type In <ref type="table" target="#tab_9">Table 5</ref>, we show a breakdown of QA Acc. by question type. We observe a clear increasing trend on "what", "who", and "where" questions after using the backbone net and adding attention/span modules in each column. Interestingly, for "why" and "how" questions, our full model fails to present overwhelming performance, indicating some reasoning (textual) module to be incorporated as future work.</p><p>Qualitative Examples We show two correct predictions in <ref type="figure" target="#fig_7">Fig. 5</ref>, where <ref type="figure" target="#fig_7">Fig. 5(a)</ref> uses grounded objects to answer the question, and <ref type="figure" target="#fig_7">Fig. 5(b)</ref> uses text. More examples (including failure cases) are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TVQA Results</head><p>We also conduct experiments on the full TVQA dataset <ref type="table" target="#tab_12">(Table 6)</ref>, without relying on the bounding boxes and refined timestamps in TVQA+. Without temporal supervision, STAGE backbone is able to achieve 3.91% relative gain from the best published result (multi-task) on  TVQA test-public set. Adding temporal supervision, performance is improved to 70.23%. For a fair comparison, we also provided STAGE variants using GloVe <ref type="bibr" target="#b32">(Pennington et al., 2014)</ref> instead of BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> as text feature. Using GloVe, STAGE models still achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We collected the TVQA+ dataset and proposed the spatio-temporal video QA task. This task requires systems to jointly localize relevant moments, detect referred objects/people, and answer questions. We further introduced STAGE, an end-to-end trainable framework to jointly perform all three tasks. Comprehensive experiments show that temporal and spatial predictions help improve QA performance, as well as providing explainable results. Though our STAGE achieves state-of-the-art performance, there is still a large gap compared with human performance, leaving space for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Timestamp Annotation</head><p>During our initial analysis, we find the original timestamp annotations from the TVQA <ref type="bibr" target="#b21">(Lei et al., 2018)</ref> dataset to be somewhat loose, i.e., around 8.7% of 150 randomly sampled training questions had a span that was at least 5 seconds longer than what is needed. To have better timestamps, we asked a set of Amazon Mechanical Turk (AMT) workers to refine the original timestamps. Specifically, we take the questions that have a localized span length of more than 10 seconds (41.33% of the questions) for refinement while leaving the rest unchanged. During annotation, we show a question, its correct answer, its associated video (with subtitle), as well as the original timestamp to the AMT workers (illustrated in <ref type="figure" target="#fig_8">Fig. 6</ref>, with instructions omitted). The workers are asked to adjust the start and end timestamps to make the span as small as possible, but need to contain all the information mentioned in the QA pair. We show span length distributions of the original and the refined timestamps from TVQA+ train set in <ref type="figure" target="#fig_9">Fig. 7</ref>. The average span length of the original timestamps is 14.41 secs, while the average for the refined timestamps is 7.2 secs.</p><p>In <ref type="table" target="#tab_15">Table 7</ref> we show STAGE performance on TVQA+ val set using the original timestamps and the refined timestamps. Models with the refined timestamps performs consistently better than the ones with the original timestamps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Bounding Box Annotation</head><p>At each step, we show a question, its correct answer, and the sampled video frames to an AMT   worker. (illustrated in <ref type="figure" target="#fig_10">Fig. 8</ref>). We do not annotate the wrong answers as most of them cannot be grounded in the video. We checked 200 sampled QAs -only 3.13% of the wrong answers could be grounded, while 46% of the correct answers could be grounded. As each QA pair has multiple visual concepts as well as multiple frames, each task shows one pair of a concept word and a sampled frame. For example, in <ref type="figure" target="#fig_10">Fig. 8</ref>, the word "laptop" is highlighted, and workers are instructed to draw a box around it. In our MTurk instructions, we required workers to draw boxes for each instance of a plural word. E.g., for the word "everyone", the worker need to draw a box for each person in the frame. Note, it is possible that the highlighted word will be a non-visual word or a visual word that is not present in the frame being shown. In that case, the workers are allowed to check the box indicating the object is not present. Recent works <ref type="bibr" target="#b49">(Zellers et al., 2019;</ref><ref type="bibr" target="#b10">Gu et al., 2018)</ref> suggest the use of pre-trained detectors for semi-automated annotation. However, since TVQA+ has a wide range of categories (see <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="table">Table 1)</ref>, it is challenging to use off-the-shelf detectors in the annotation process. As face detection and recognition might be easier than recognizing open set objects, we initially also tried using strong face detection  and recognition (Liu et al., 2017) model for character face annotation, but the quality was much poorer than expected. Thus, we decided to invest the required funds to collect boxes manually and ensure their accuracy. After the collection, with the GT labels, we again used the above models to test face retrieval performance for 12 most frequently appeared characters in TVQA+. To allow <ref type="bibr" target="#b25">(Liu et al., 2017)</ref> to work, we manually collected 5 GT faces for each character as our gallery set. At test time, we assign each test face the label of its closest neighbor from the gallery set in the learned embedding space. This method achieves 55.6 F1/74.4 Precision/44.4 Recall. Such performance is not strong enough to support further research. We found the main reason is due to many partial occlusion of faces (e.g., side faces) in TV shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Quality</head><p>To ensure the quality of the collected bounding boxes, we only allow workers from Englishspeaking countries to participate the task. Besides, we set high requirements for workers -they needed to have at least 3000 accepted HITs and 95% accept rate. Qualified workers were well paid. We also kept track of the quality of the data during collection -workers with poor annotations were disqualified to work on our task. After collection, we further conducted an in-house check, 95.5% of 200 sampled QAs are correctly labeled, indicating the high quality of our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Training Details</head><p>We optimize our model using Adam with an initial learning rate of 1e-3, weight decay 3e-7. A mini-  batch contains 16 questions. We train the model for maximum 100 epochs with early stop -if QA Acc. is not improving for consecutive 5 epochs, the training is stopped. CNN hidden size is set to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Vision-Language Pretrained Features</head><p>In addition, we also consider features from LXMERT <ref type="bibr" target="#b38">(Tan and Bansal, 2019)</ref>. This model is pretrained on a large amount of image-text pairs from multiple image captioning <ref type="bibr" target="#b23">(Lin et al., 2014;</ref><ref type="bibr" target="#b20">Krishna et al., 2017)</ref> and image question answering <ref type="bibr" target="#b9">(Goyal et al., 2017;</ref><ref type="bibr" target="#b14">Hudson and Manning, 2019;</ref><ref type="bibr" target="#b53">Zhu et al., 2016a)</ref> datasets. Specifically, we use video frame-question pairs as input to LXMERT, and use the extracted features to replace Faster R-CNN object features and BERT question features. For answers and subtitles, we still use the original BERT features.</p><p>The results are shown in <ref type="table" target="#tab_17">Table 8</ref>. We notice that using LXMERT feature lowers STAGE's performance. This is not surprising, as the domains in which the LXMERT model are pretrained on are very different from TVQA+: (captions/questions+image) vs (subtitles+QAs+videos). Future work includes more investigation into adapting these pre-trained vision-language models for more challenging video+dialogue domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 More Prediction Examples</head><p>We show 6 correct prediction examples from STAGE in <ref type="figure">Fig. 9</ref>. As can be seen from the figure, correct examples usually have correct temporal and spatial localization. In <ref type="figure" target="#fig_0">Fig. 10</ref> we show 6 incorrect examples. Incorrect object localization is one of the most frequent failure reason, while the model is able to localize common objects, it is difficult for it to localize unusual objects ( <ref type="figure" target="#fig_0">Fig. 10(a, d)</ref>), small objects ( <ref type="figure" target="#fig_0">Fig. 10(b)</ref>). Incorrect temporal localization is another most frequent failure reason, e.g., <ref type="figure" target="#fig_0">Fig. 10(c, f)</ref>. There are also cases where the objects being referred are not present in the sampled frame, as in <ref type="figure" target="#fig_0">Fig. 10(e)</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Samples from TVQA+. Questions and correct answers are temporally localized to clips, and spatially localized to frame-level bounding boxes. Colors indicate corresponding box-object pairs. Subtitles are shown in dashed blocks. Wrong answers are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Box distributions for top 60 categories in TVQA+ train set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Box/image area ratios (left) and span length distributions (right) in TVQA+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " 2 H 8 F 5 E d G 9 q Z N O 8 N 7 b 2 5 g a v R W L K I = " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O N K K v Q G b Q i T y a Q d O s m E m R O h h I K v 4 s a F I m 5 9 D n e + j d M 2 g r b + M P D x n 3 M 4 Z / 4 g F V y D 4 3 x Z K 6 t r 6 x u b p a 3 y 9 s 7 u 3 r 5 9 c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " T r H Q j l T s y L A Z y j K P t K 4 T W w k o S r s = " &gt; A A A B / 3 i c b Z D L S s N A F I Z P v N Z 6 i w p u 3 A w W w V V J W k G X B T c u X F T o D d o Q J p N p O 3 Q y C T M T o c Q u f B U 3 L h R x 6 2 u 4 8 2 2 c t h G 0 9 Y e B j / + c w z n z B w l n S j v O l 7 W y u r a + s V n Y K m 7 v 7 O 7 t 2 w e H L R W n k t A m i X k s O w F W l D N B m 5 p p T j u J p D g K O G 0 H o + t p v X 1 P p W K x a O h x Q r 0 I D w T r M 4 K 1 s X z 7 u I F 6 m k V U o V t / + I P V 0 L d L T t m Z C S 2 D m 0 M J c t V 9 + 7 M X x i S N q N C E Y 6 W 6 r p N o L 8 N S M 8 L p p N h L F U 0 w G e E B 7 R o U 2 O z x s t n 9 E 3 R m n B D 1 Y 2 m e 0 G j m / p 7 I c K T U O A p M Z 4 T 1 U C 3 W p u Z / t W 6 q + 1 d e x k S S a i r I f F E / 5 U j H a B o G C p m k R P O x A U w k M 7 c i M s Q S E 2 0 i K 5 o Q 3 M U v L 0 O r U n a r Z f f u o l S r 5 H E U 4 A R O 4 R x c u I Q a 3 E A d m k D g A Z 7 g B V 6 t R + v Z e r P e 5 6 0 r V j 5 z B H 9 k f X w D U g q U 9 A = = &lt; / l a t e x i t &gt; T ⇥ Lh ⇥ d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8 = " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = &lt; / l a t e x i t &gt; T ⇥ d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 o U I a m f a O E F + W 8 i G q u o B 9 y I C g 9 8 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 V + o V t K J v N p l 2 6 2 Y T d i V B K / 4 U X D 4 p 4 9 d 9 4 8 9 + 4 b X P Q 1 g c D j / d m m J k X p F I Y d N 1 v p 7 C x u b W 9 U 9 w t 7 e 0 f H B 6 V j 0 / a J s k 0 4 y 2 W y E R 3 A 2 q 4 F I q 3 U K D k 3 V R z G g e S d 4 L x 3 d z v P H F t R K K a O E m 5 H 9 O h E p F g F K 3 0 2 C R 9 F D E 3 J B y U K 2 7 V X Y C s E y 8 n F c j R G J S / + m H C s p g r Z J I a 0 / P c F P 0 p 1 S i Y 5 L N S P z M 8 p W x M h 7 x n q a J 2 j T 9 d X D w j F 1 Y J S Z R o W w r J Q v 0 9 M a W x M Z M 4 s J 0 x x Z F Z 9 e b i f 1 4 v w + j W n w q V Z s g V W y 6 K M k k w I f P 3 S S g 0 Z y g n l l C m h b 2 V s B H V l K E N q W R D 8 F Z f X i f t W t W 7 q n o P 1 5 V 6 L Y + j C G d w D p f g w Q 3 U 4 R 4 a 0 A I G C p 7 h F d 4 c 4 7 w 4 7 8 7 H s r X g 5 D O n 8 A f O 5 w / H K Z B H &lt; / l a t e x i t &gt; T ⇥ d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 o U I a m f a O E F + W 8 i G q u o B 9 y I C g 9 8 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 V + o V t K J v N p l 2 6 2 Y T d i V B K / 4 U X D 4 p 4 9 d 9 4 8 9 + 4 b X P Q 1 g c D j / d m m J k X p F I Y d N 1 v p 7 C x u b W 9 U 9 w t 7 e 0 f H B 6 V j 0 / a J s k 0 4 y 2 W y E R 3 A 2 q 4 F I q 3 U K D k 3 V R z G g e S d 4 L x 3 d z v P H F t R K K a O E m 5 H 9 O h E p F g F K 3 0 2 C R 9 F D E 3 J B y U K 2 7 V X Y C s E y 8 n F c j R G J S / + m H C s p g r Z J I a 0 / P c F P 0 p 1 S i Y 5 L N S P z M 8 p W x M h 7 x n q a J 2 j T 9 d X D w j F 1 Y J S Z R o W w r J Q v 0 9 M a W x M Z M 4 s J 0 x x Z F Z 9 e b i f 1 4 v w + j W n w q V Z s g V W y 6 K M k k w I f P 3 S S g 0 Z y g n l l C m h b 2 V s B H V l K E N q W R D 8 F Z f X i f t W t W 7 q n o P 1 5 V 6 L Y + j C G d w D p f g w Q 3 U 4 R 4 a 0 A I G C p 7 h F d 4 c 4 7 w 4 7 8 7 H s r X g 5 D O n 8 A f O 5 w / H K Z B H &lt; / l a t e x i t &gt; (ed st + 1) ⇥ d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J i W d o Z j J B p M F Q 4 v 9 + / v 2 2 p 4 + j R M = " &gt; A A A C F 3 i c b Z D J S g N B E I Z 7 X G P c o h 6 9 N A Y h I o a Z K O g x 4 M V j B L N A J o S e n p q k S c 9 C d 4 0 Y h r y F F 1 / F i w d F v O r N t 7 G z H D S x o O H j / 6 u o r t 9 L p N B o 2 9 / W 0 v L K 6 t p 6 b i O / u b W 9 s 1 v Y 2 2 / o O F U c 6 j y W s W p 5 T I M U E d R R o I R W o o C F n o S m N 7 g e + 8 1 7 U F r E 0 R 0 O E + i E r B e J Q H C G R u o W y q 6 E A E v U R X j A D P z R 2 Z Q 0 j k 4 d 6 i r R 6 + O J c U U I m v r 5 b q F o l + 1 J 0 U V w Z l A k s 6 p 1 C 1 + u H / M 0 h A i 5 Z F q 3 H T v B T s Y U C i 5 h l H d T D Q n j A 9 a D t s G I m T 2 d b H L X i B 4 b x a d B r M y L k E 7 U 3 x M Z C 7 U e h p 7 p D B n 2 9 b w 3 F v / z 2 i k G V 5 1 M R E m K E P H p o i C V F G M 6 D o n 6 Q g F H O T T A u B L m r 5 T 3 m W I c T Z T j E J z 5 k x e h U S k 7 5 2 X n 9 q J Y r c z i y J F D c k R K x C G X p E p u S I 3 U C S e P 5 J m 8 k j f r y X q x 3 q 2 P a e u S N Z s 5 I H / K + v w B 8 x i f E w = = &lt; / l a t e x i t &gt; video {vt} T t=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 u r 7 S d / n A E m Z x p j H P k e a A z 1 F U Y g = " &gt; A A A C B X i c b V D J S g N B E O 1 x j X E b 9 a i H w S B 4 C j M q m I s Q 8 O I x Q j b I x K G n U 0 m a 9 C x 0 1 w T D M B 6 8 + C t e P C j i 1 X / w 5 t / Y W Q 6 a + K D g 8 V 4 V V f X 8 W H C F t v 1 t L C 2 v r K 6 t 5 z b y m 1 v b O 7 v m 3 n 5 d R Y l k U G O R i G T T p w o E D 6 G G H A U 0 Y w k 0 8 A U 0 / M H 1 2 G 8 M Q S o e h V U c x d A O a C / k X c 4 o a s k z j 1 y E e 0 y H v A N R 9 u C m Q w / d z E v x y s n u q p 5 Z s I v 2 B N Y i c W a k Q G a o e O a X 2 4 l Y E k C I T F C l W o 4 d Y z u l E j k T k O X d R E F M 2 Y D 2 o K V p S A N Q 7 X T y R W a d a K V j d S O p K 0 R r o v 6 e S G m g 1 C j w d W d A s a / m v b H 4 n 9 d K s F t q p z y M E 4 S Q T R d 1 E 2 F h Z I 0 j s T p c A k M x 0 o Q y y f W t F u t T S R n q 4 P I 6 B G f + 5 U V S P y s 6 5 0 X n 9 q J Q L s 3 i y J F D c k x O i U M u S Z n c k A q p E U Y e y T N 5 J W / G k / F i v B s f 0 9 Y l Y z Z z Q P 7 A + P w B u r W Z T g = = &lt; / l a t e x i t &gt; subtitle {st} T t=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S R h G u S c V 9 o 2 S s j 1 x x R f L k l D b x b 8 = " &gt; A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q E s X B o v g q i Q q 2 I 1 Q c O O y Q l / Q x D C Z T t q h k w c z N 2 I J c e f G X 3 H j Q h G 3 f o I 7 / 8 Z p m o W 2 H h g 4 n H P P z N z j x Z x J M M 1 v r b S 0 v L K 6 V l 6 v b G x u b e / o u 3 s d G S W C 0 D a J e C R 6 H p a U s 5 C 2 g Q G n v V h Q H H i c d r 3 x 1 d T v 3 l E h W R S 2 Y B J T J 8 D D k P m M Y F C S q x / a Q O 8 h l Y m X h 7 M H O 5 U u 2 J m b w q W V 3 b Z c v W r W z B z G I r E K U k U F m q 7 + Z Q 8 i k g Q 0 B M K x l H 3 L j M F J s Q B G 1 P 0 V O 5 E 0 x m S M h 7 S v a I g D K p 0 0 X y Q z j p U y M P x I q B O C k a u / E y k O p J w E n p o M M I z k v D c V / / P 6 C f h 1 J 2 V h n A A N y e w h P + E G R M a 0 F W P A B C X A J 4 p g I p j 6 q 0 F G W G A C q r u K K s G a X 3 m R d E 5 r 1 l n N u j m v N u p F H W V 0 g I 7 Q C b L Q B W q g a 9 R E b U T Q I 3 p G r + h N e 9 J e t H f t Y z Z a 0 o r M P v o D 7 f M H U R y a v g = = &lt; / l a t e x i t &gt; hypothesis hk &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U E b l 0 f 3 N i Y 3 G t 1 8 Y F f t f 3 1 X U J x I = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k q i g j 0 W v H i s Y D + g D W G z n T Z L N x / s T s Q Q K v 4 V L x 4 U 8 e r v 8 O a / c d v m o K 0 P B h 7 v z T A z z 0 8 E V 2 j b 3 0 Z p Z X V t f a O 8 W d n a 3 t n d M / c P 2 i p O J Y M W i 0 U s u z 5 V I H g E L e Q o o J t I o K E v o O O P r 6 d + 5 x 6 k 4 n F 0 h 1 k C b k h H E R 9 y R l F L n n n U R 3 j A P M i S G A N Q X E 0 e A 2 / s m V W 7 Z s 9 g L R O n I F V S o O m Z X / 1 B z N I Q I m S C K t V z 7 A T d n E r k T M C k 0 k 8 V J J S N 6 Q h 6 m k Y 0 B O X m s / M n 1 q l W B t Y w l r o i t G b q 7 4 m c h k p l o a 8 7 Q 4 q B W v S m 4 n 9 e L 8 V h 3 c 1 5 l K Q I E Z s v G q b C w t i a Z m E N u A S G I t O E M s n 1 r R Y L q K Q M d W I V H Y K z + P I y a Z / X n I u a c 3 t Z b d S L O M r k m J y Q M + K Q K 9 I g N 6 R J W o S R n D y T V / J m P B k v x r v x M W 8 t G c X M I f k D 4 / M H r + 2 W j w = = &lt; / l a t e x i t &gt; Overview of the proposed STAGE framework.less than 10 seconds, the largest spans are up to 20 seconds. The average span length is 7.2 seconds, which is short compared to the average length of the full video clips (61.49 seconds).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Example predictions from STAGE. Span predictions are shown on the top, each block represents a frame, the color indicates the model's confidence for the spans. For each QA, we show grounding examples and scores for one frame in GT span. GT boxes are in green. Predicted and GT answers are labeled by Pred and GT, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Timestamp refinement interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Comparison between the original and the refined timestamps in the TVQA+ train set. The refined timestamps are generally tighter than the original ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Bounding box annotation interface. Here, the worker is asked to draw a box around the highlighted word "laptop".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Wrong prediction examples from STAGE. The span predictions are shown on the top of each example, each block represents a frame, the color indicates the model's confidence for the predicted spans. For each QA, we show grounding examples and scores for one frame in GT span, GT boxes are shown in green. Model predicted answers are labeled by Pred, GT answers are labeled by GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Data Statistics for TVQA+ dataset.</figDesc><table /><note>3 Dataset In this section, we describe the TVQA+ Dataset, the first video question answering dataset with both spatial and temporal annotations. TVQA+ is built on the TVQA dataset introduced by Lei et al.. TVQA is a large-scale video QA dataset based on 6 popular TV shows, containing 152.5K multiple choice questions from 21.8K, 60-90 sec- ond long video clips. The questions in the TVQA dataset are compositional, where each question is comprised of two parts, a question part ("where was Sheldon sitting")</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>TVQA+ test set results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of STAGE on TVQA+ val set. Each row adds an extra component to the row above it.</figDesc><table><row><cell>Model</cell><cell cols="2">baseline +CNN</cell><cell>+AF</cell><cell>+TS</cell><cell>+SS</cell><cell>+LF</cell></row><row><cell>what (60.52%)</cell><cell>65.66</cell><cell>66.43</cell><cell cols="4">67.58 70.76 71.25 72.34</cell></row><row><cell>who (10.24%)</cell><cell>65.37</cell><cell>64.08</cell><cell cols="4">64.72 72.17 73.14 74.11</cell></row><row><cell>where (9.68%)</cell><cell>65.41</cell><cell>64.38</cell><cell cols="4">68.49 71.58 71.58 74.32</cell></row><row><cell>why (9.55%)</cell><cell>74.31</cell><cell>78.82</cell><cell cols="4">77.43 79.86 78.12 76.39</cell></row><row><cell>how (9.05%)</cell><cell>60.81</cell><cell>67.03</cell><cell cols="4">69.23 66.30 69.96 67.03</cell></row><row><cell>total (100%)</cell><cell>65.79</cell><cell>67.25</cell><cell cols="4">68.31 71.40 71.99 72.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>QA Acc. by question type on TVQA+ val set.</figDesc><table><row><cell>For brevity, we only show top-5 question types (per-</cell></row><row><cell>centage in brackets). AF=Aligned Fusion, TS=Temp.</cell></row><row><cell>Sup., SS=Spat. Sup., LF=Local Feature. Each column</cell></row><row><cell>adds an extra component to the column before it.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>QA Acc. on the full TVQA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>STAGE performance comparison between the original timestamps and the refined timestamps on TVQA+ val set. Each row adds an extra component to the row above it.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>LXMERT 71.46 21.01 26.31 18.04 STAGE 74.83 27.34 32.49 22.23</figDesc><table><row><cell>Model</cell><cell>QA Acc.</cell><cell>Grd. mAP</cell><cell>Temp. ASA mIoU</cell></row><row><cell>STAGE-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 8 :</head><label>8</label><figDesc>TVQA+ test set results with LXMERT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>00:10.268 → 00:11.848 Lesley: ...no extraneous spittle. 00:13.146 → 00:15.356 Lesley: On the other hand, no arousal. Q: What does Lesley say there was none of when Leonard asked about the kiss? A1: Lesley says there was no arousal. Pred GT A2: Lesley says there was no passion. A3: There was no kiss. A4: Lesley says the kiss lacked a certain fire. A5: Lesley says there was no excitement in the kiss. Correct prediction examples from STAGE. The span predictions are shown on the top of each example, each block represents a frame, the color indicates the model's confidence for the predicted spans. For each QA, we show grounding examples and scores for one frame in GT span, GT boxes are shown in green. Model predicted answers are labeled by Pred, GT answers are labeled by GT.</figDesc><table><row><cell>00:34.309 → 00:37.019 -What's that? -Tea. 00:37.729 → 00:38.899 Sheldon: When people are upset... Q: Where is Leonard sitting before Sheldon brings him the tea ? A1: Sheldon's bed. A2: The armchair. A3: The floor. A4: His bed. A5: The couch. Pred GT (a) 00:00.060 → 00:02.020 -Oh, hey, Leonard. -Good afternoon, Penny. 00:02.187 → 00:04.567 Leonard: So, hi, hey. Uh... Q: Who visited Penny in her house before dinner? A1: No one visited Penny in her house. A2: Howard visited Penny in her house. A3: Raj visited Penny in her house. A4: Sheldon visited Penny in her house. A5: Leonard visited Penny in her house. Pred GT (c) Q: Where was Penny when she called to Leonard? A1: Penny was working at a restaurant. Pred GT A2: Penny was at home. A3: Penny was walking in the street. A4: Penny was at bed. A5: Penny was in the kitchen. 00:41.444 → 00:43.274 Leonard: -What's up? -Yeah, well, I'm at work too. 00:43.446 → 00:46.656 Penny: And you'll never guess who's here infecting my entire station. (e) 00:27,095 → 00:42.315 Leonard: Sheldon? 00:45.072 → 01:08.032 Leonard: Hello? Q: What is Leonard holding when he comes out of the bedroom? A1: Leonard is holding his cell phone. Pred A2: Leonard is holding a baseball bat. A3: Leonard is holding a shovel. A4: Leonard is holding a coat hanger. A5: Leonard is holding a mock lightsaber. GT (a) 00:17.350 → 00:19.640 Howard: Plus Superman and Godzilla. 00:20.020 → 00:21.690 Leonard: No, no, no. Orcs are magic. Q: Who grab a bottle after Leonard talked? A1: Sheldon. A2: Howard. A3: Penny. A4: Raj. GT A5: Leonard. Pred (c) 00:23.443 → 00:27.403 -You gotta take one for the team. -Yeah. Sack up, dude. 00:28,823 → 00:30.403 Leonard: Fine. Q: What was Leonard 's drink when they are talking about taking one for the team? A1: Fanta. A2: bottle of water. Pred A3: Sprite. A4: Gatorade. A5: Coke Cola. GT Figure 9: 00:00.343 → 00:03.763 (b) 00:00.141 → 00:01.391 Raj: I don't believe it. 00:01.559 → 00:02.599 Howard: Neither do I. Q: What is Leonard holding when he is listening to Raj? A1: A notepad. A2: A book. A3: A yellow cup . Pred GT A4: A cell phone. A5: A set of keys. (d) 00:50.790 → 00:52.290 Leonard, it's 2 in the morning. 00:53.918 → 00:59.018 -So? -So it's my turn. Q: Where was Leonard when Sheldon walked into the living room at 2am? A1: On the couch. A2: In the time machine. Pred GT A3: In the kitchen. A4: In his room. A5: He wasn't there. (f) Past Howard: I haven't seen your Oreos! 00:03.972 → 00:07.062 Past Howard: Just take your bath without them! Q: What was Raj doing when Howard was shouting at someone? A1: Raj was playing some music. A2: Raj was seated in the couch. A3: Raj was taking a shower. Pred A4: Raj was not in the room. A5: Raj was eating lots of cookies in his mouth as he watched Howard. GT (b) 00:26.568 → 00:27.818 Leonard: Sounds like a breakthrough. 00:27.986 → 00:30.486 Should I call Science and tell them to hold the cover? Q: What is Leonard wearing when he is talking to Sheldon? A1: A scarf. A2: A hat. A3: A suit. GT A4: A kilt. Pred A5: Jogging pants. (d) 00:03.743 → 00:06.663 Penny: ...something Elton John would drive through the Everglades. 00:12.502 → 00:14.332 Sheldon: It only moves in time. Q: What direction did Sheldon turn to when Penny insulted their time machine ? A1: He looked at his hands. A2: To the left. A3: Up towards the ceiling. Pred A4: He turned to Penny. A5: To the right. GT (e) (f)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jayleicn/TVQA 3 This also holds true when considering mean (standarddeviation) of 5 runs: 74.20 (0.42).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the reviewers for their helpful feedback. This research is supported by NSF Awards #1633295, 1562098, 1405822, DARPA MCS Grant #N66001-19-2-4031, DARPA KAIROS Grant #FA8750-19-2-1004, Google Focused Research Award, and ARO-YIP Award #W911NF-18-1-0336.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Harsh Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Localizing moments in video with temporal language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatiotemporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gaining extra supervision via multi-task learning for multi-modal video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Dong</forename><surname>Yoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>IJCNN</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive attention memory network for movie story question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Dong</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepstory: Video story qa by deep embedded memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving pairwise ranking for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<title level="m">Neural baby talk. In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher Joseph</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Marioqa: Answering questions by watching gameplay videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Who did what: A large-scale person-centered cloze dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Interpretable counting for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1502.05698</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visual madlibs: Fill in the blank description generation and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A joint speaker-listener-reinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video question answering via hierarchical spatio-temporal attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Xiaofei He, and Yueting Zhuang</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
