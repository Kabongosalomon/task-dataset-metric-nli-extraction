<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELF-LABELLING VIA SIMULTANEOUS CLUSTERING AND REPRESENTATION LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SELF-LABELLING VIA SIMULTANEOUS CLUSTERING AND REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard crossentropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Our method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. Code and models are available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning from unlabelled data can dramatically reduce the cost of deploying machine learning algorithms to new applications, thus amplifying their impact in the real world. Self-supervision is an increasingly popular framework for learning without labels. The idea is to define pretext learning tasks that can be constructed from raw data alone, but that still result in neural networks that transfer well to useful applications.</p><p>Much of the research in self-supervision has focused on designing new pretext tasks. However, given supervised data such as ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref>, the standard classification objective of minimizing the cross-entropy loss still results in better or at least as good pre-training than any of such methods (for a given amount of data and for a given model complexity). This suggests that the task of classification is sufficient for pre-training networks, provided that suitable data labels are available. In this paper, we thus focus on the problem of obtaining the labels automatically by designing a self-labelling algorithm.</p><p>Learning a deep neural network together while discovering the data labels can be viewed as simultaneous clustering and representation learning. The latter can be approached by combining cross-entropy minimization with an off-the-shelf clustering algorithm such as K-means. This is precisely the approach adopted by the recent DeepCluster method , which achieves excellent results in unsupervised representation learning. However, combining representation learning, which is a discriminative task, with clustering is not at all trivial. In particular, we show that the combination of cross-entropy minimization and K-means as adopted by DeepCluster cannot be described as the optimization of an overall learning objective; instead, there exist degenerate solutions that the algorithm avoids via particular implementation choices.</p><p>In order to address this technical shortcoming, in this paper, we contribute a new principled formulation for simultaneous clustering and representation learning. The starting point is to minimize a single loss, the cross-entropy loss, for learning the deep network and for estimating the data labels. This is often done in semi-supervised learning and multiple instance learning. However, when applied naively to the unsupervised case, it immediately leads to a degenerate solution where all data points are mapped to the same cluster.</p><p>We solve this issue by adding the constraint that the labels must induce an equipartition of the data, which we show maximizes the information between data indices and labels. We also show that the resulting label assignment problem is the same as optimal transport, and can therefore be solved in polynomial time by linear programming. However, since we want to scale the algorithm to millions of data points and thousands of labels, standard transport solvers are inadequate. Thus, we also propose to use a fast version of the Sinkhorn-Knopp algorithm for finding an approximate solution to the transport problem efficiently at scale, using fast matrix-vector algebra.</p><p>Compared to methods such as DeepCluster, the new formulation is more principled and allows to more easily demonstrate properties of the method such as convergence. Most importantly, via extensive experimentation, we show that our new approach leads to significantly superior results than DeepCluster, achieving the new state of the art for representation learning approaches. In fact, the method's performance surpasses others that use a single type of supervisory signal for self-supervision, and is on par or better than very recent contributions as well <ref type="bibr" target="#b26">Misra &amp; van der Maaten, 2019;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our paper relates to two broad areas of research: (a) self-supervised representation learning, and (b) more specifically, training a deep neural network using pseudo-labels, i.e. the assignment of a label to each image. We discuss closely related works for each.</p><p>Self-supervised learning: A wide variety of methods that do not require manual annotations have been proposed for the self-training of deep convolutional neural networks. These methods use various cues and proxy tasks namely, in-painting , patch context and jigsaw puzzles <ref type="bibr" target="#b27">Mundhenk et al., 2017)</ref>, clustering <ref type="bibr" target="#b2">Bautista et al., 2016)</ref>, noise-as-targets <ref type="bibr" target="#b3">(Bojanowski &amp; Joulin, 2017)</ref>, colorization <ref type="bibr" target="#b25">Larsson et al., 2017)</ref>, generation <ref type="bibr" target="#b35">Ren &amp; Lee, 2018;</ref>, geometry <ref type="bibr" target="#b11">(Dosovitskiy et al., 2016)</ref>, predicting transformations  and counting . Most recently, contrastive methods have shown great performance gains,  by leveraging augmentation and adequate losses. In , predicting rotation  is combined with instance retrieval  and multiple tasks are combined in <ref type="bibr" target="#b7">(Doersch &amp; Zisserman, 2017)</ref>.</p><p>Pseudo-labels for images: In the self-supervised domain, we find a spectrum of methods that either give each data point a unique label <ref type="bibr" target="#b11">Dosovitskiy et al., 2016)</ref> or train on a flexible number of labels with K-means , with mutual information <ref type="bibr" target="#b20">(Ji et al., 2018)</ref> or with noise <ref type="bibr" target="#b3">(Bojanowski &amp; Joulin, 2017)</ref>. In  a large network is trained with a pretext task and a smaller network is trained via knowledge transfer of the clustered data. Finally, <ref type="bibr" target="#b0">(Bach &amp; Harchaoui, 2008;</ref><ref type="bibr" target="#b38">Vo et al., 2019)</ref> use convex relaxations to regularized affine-transformation invariant linear clustering, but can not scale to larger datasets.</p><p>Our contribution is a simple method that combines a novel pseudo-label extraction procedure from raw data alone and the training of a deep neural network using a standard cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We will first derive our self-labelling method, then interpret the method as optimizing labels and targets of a cross-entropy loss and finally analyze similarities and differences with other clusteringbased methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SELF-LABELLING</head><p>Neural network pre-training is often achieved via a supervised data classification task. Formally, consider a deep neural network x = Φ(I) mapping data I (e.g. images) to feature vectors x ∈ R D . The model is trained using a dataset (e.g. ImageNet) of N data points I 1 , . . . , I N with corresponding labels y 1 , . . . , y N ∈ {1, . . . , K}, drawn from a space of K possible labels. The representation is followed by a classification head h : R D → R K , usually consisting of a single linear layer, converting the feature vector into a vector of class scores. The class scores are mapped to class probabilities via the softmax operator:</p><formula xml:id="formula_0">p(y = ·|x i ) = softmax(h • Φ(x i )).</formula><p>The model and head parameters are learned by minimizing the average cross-entropy loss</p><formula xml:id="formula_1">E(p|y 1 , . . . , y N ) = − 1 N N i=1 log p(y i |x i ).<label>(1)</label></formula><p>Training with objective (1) requires a labelled dataset. When labels are unavailable, we require a self-labelling mechanism to assign the labels automatically.</p><p>In semi-supervised learning, self-labelling is often achieved by jointly optimizing (1) with respect to the model h • Φ and the labels y 1 , . . . , y N . This can work if at least part of the labels are known, thus constraining the optimization. However, in the fully unsupervised case, it leads to a degenerate solution: eq. (1) is trivially minimized by assigning all data points to a single (arbitrary) label.</p><p>To address this issue, we first rewrite eq. (1) by encoding the labels as posterior distributions q(y|x i ):</p><formula xml:id="formula_2">E(p, q) = − 1 N N i=1 K y=1 q(y|x i ) log p(y|x i ).<label>(2)</label></formula><p>If we set the posterior distributions q(y|x i ) = δ(y−y i ) to be deterministic, the formulations in eqs. <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_2">(2)</ref> are equivalent, in the sense that E(p, q) = E(p|y 1 , . . . , y N ). In this case, optimizing q is the same as reassigning the labels, which leads to the degeneracy. To avoid this, we add the constraint that the label assignments must partition the data in equally-sized subsets. Formally, the learning objective objective 2 is thus:</p><formula xml:id="formula_3">min p,q E(p, q) subject to ∀y : q(y|x i ) ∈ {0, 1} and N i=1 q(y|x i ) = N K .<label>(3)</label></formula><p>The constraints mean that each data point x i is assigned to exactly one label and that, overall, the N data points are split uniformly among the K classes.</p><p>The objective in eq. (3) is combinatorial in q and thus may appear very difficult to optimize. However, this is an instance of the optimal transport problem, which can be solved relatively efficiently. In order to see this more clearly, let P yi = p(y|x i ) 1 N be the K × N matrix of joint probabilities estimated by the model. Likewise, let Q yi = q(y|x i ) 1 N be K × N matrix of assigned joint probabilities. Using the notation of <ref type="bibr" target="#b5">(Cuturi, 2013)</ref>, we relax matrix Q to be an element of the transportation polytope</p><formula xml:id="formula_4">U (r, c) := {Q ∈ R K×N + | Q1 = r, Q 1 = c}.<label>(4)</label></formula><p>Here 1 are vectors of all ones of the appropriate dimensions, so that r and c are the marginal projections of matrix Q onto its rows and columns, respectively. In our case, we require Q to be a matrix of conditional probability distributions that split the data uniformly, which is captured by:</p><formula xml:id="formula_5">r = 1 K · 1, c = 1 N · 1.</formula><p>With this notation, we can rewrite the objective function in eq. (3), up to a constant shift, as</p><formula xml:id="formula_6">E(p, q) + log N = Q, − log P ,<label>(5)</label></formula><p>where · is the Frobenius dot-product between two matrices and log is applied element-wise. Hence optimizing eq. (3) with respect to the assignments Q is equivalent to solving the problem:</p><formula xml:id="formula_7">min Q∈U (r,c) Q, − log P .<label>(6)</label></formula><p>This is a linear program, and can thus be solved in polynomial time. Furthermore, solving this problem always leads to an integral solution despite having relaxed Q to the continuous polytope U (r, c), guaranteeing the exact equivalence to the original problem.</p><p>In practice, however, the resulting linear program is large, involving millions of data points and thousands of classes. Traditional algorithms to solve the transport problem scale badly to instances of this size. We address this issue by adopting a fast version <ref type="bibr" target="#b5">(Cuturi, 2013)</ref> of the Sinkhorn-Knopp algorithm. This amounts to introducing a regularization term</p><formula xml:id="formula_8">min Q∈U (r,c) Q, − log P + 1 λ KL(Q rc ),<label>(7)</label></formula><p>where KL is the Kullback-Leibler divergence and rc can be interpreted as a K × N probability matrix. The advantage of this regularization term is that the minimizer of eq. (7) can be written as:</p><formula xml:id="formula_9">Q = diag(α)P λ diag(β)<label>(8)</label></formula><p>where exponentiation is meant element-wise and α and β are two vectors of scaling coefficients chosen so that the resulting matrix Q is also a probability matrix (see <ref type="bibr" target="#b5">(Cuturi, 2013)</ref> for a derivation). The vectors α and β can be obtained, as shown below, via a simple matrix scaling iteration.</p><p>For very large λ, optimizing eq. (7) is of course equivalent to optimizing eq. (6), but even for moderate values of λ the two objectives tend to have approximately the same optimizer <ref type="bibr" target="#b5">(Cuturi, 2013)</ref>. Choosing λ trades off convergence speed with closeness to the original transport problem. In our case, using a fixed λ is appropriate as we are ultimately interested in the final clustering and representation learning results, rather than in solving the transport problem exactly.</p><p>Our final algorithm's core can be described as follows. We learn a model h • Φ and a label assignment matrix Q by solving the optimization problem eq. (6) with respect to both Q, which is a probability matrix, and the model h • Φ, which determines the predictions P yi = softmax y (h • Φ(x i )). We do so by alternating the following two steps:</p><p>Step 1: representation learning. Given the current label assignments Q, the model is updated by minimizing eq. (6) with respect to (the parameters of) h • Φ. This is the same as training the model using the common cross-entropy loss for classification.</p><p>Step 2: self-labelling. Given the current model h • Φ, we compute the log probabilities P . Then, we find Q using eq. (8) by iterating the updates <ref type="bibr" target="#b5">(Cuturi, 2013)</ref> ∀y :</p><formula xml:id="formula_10">α y ← [P λ β] −1 y ∀i : β i ← [α P λ ] −1 i .</formula><p>Each update involves a single matrix-vector multiplication with complexity O(N K), so it is relatively quick even for millions of data points and thousands of labels and so the cost of this method scales linearly with the number of images N . In practice, convergence is reached within 2 minutes on ImageNet when computed on a GPU. Also, note that the parameters α and β can be retained between steps, thus allowing a warm start of Step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INTERPRETATION</head><p>As shown above, the formulation in eq. (2) uses scaled versions of the probabilities. We can interpret these by treating the data index i as a random variable with uniform distribution p(i) = 1/N and by rewriting the posteriors p(y|x i ) = p(y|i) and q(y|x i ) = q(y|i) as conditional distributions with respect to the data index i instead of the feature vector x i . With these changes, we can rewrite eq. (5) as</p><formula xml:id="formula_11">E(p, q) + log N = − N i=1 K y=1 q(y, i) log p(y, i) = H(q, p),<label>(9)</label></formula><p>which is the cross-entropy between the joint label-index distributions q(y, i) and p(y, i). The minimum of this quantity w.r.t. q is obtained when p = q, in which case E(q, q) + log N reduces to the entropy H q (y, i) of the random variables y and i. Additionally, since we assumed that q(i) = 1/N , the marginal entropy H q (i) = log N is constant and, due to the equipartition condition q(y) = 1/K, H q (y) = log K is also constant. Subtracting these two constants from the entropy yields:</p><formula xml:id="formula_12">min p E(p, q) + log N = E(q, q) + log N = H q (y, i) = H q (y) + H q (i) − I q (y, i) = const. − I q (y, i).</formula><p>Thus we see that minimizing E(p, q) is the same as maximizing the mutual information between the label y and the data index i.</p><p>In our formulation, the maximization above is carried out under the equipartition constraint. We can instead relax this constraint and directly maximize the information I(y, i). However, by rewriting information as the difference I(y, i) = H(y) − H(y|i), we see that the optimal solution is given by H(y|i) = 0, which states each data point i is associated to only one label deterministically, and by H(y) = ln K, which is another way of stating the equipartition condition.</p><p>In other words, our learning formulation can be interpreted as maximizing the information between data indices and labels while explicitly enforcing the equipartition condition, which is implied by maximizing the information in any case. Compared to minimizing the entropy alone, maximizing information avoids degenerate solutions as the latter carry no mutual information between labels y and indices i. Similar considerations can be found in <ref type="bibr" target="#b20">(Ji et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RELATION TO SIMULTANEOUS REPRESENTATION LEARNING AND CLUSTERING</head><p>In the discussion above, self-labelling amounts to assigning discrete labels to data and can thus be interpreted as clustering. Most of the traditional clustering approaches are generative. For example, K-means takes a dataset x 1 , . . . , x N of vectors and partitions it into K classes in order to minimize the reconstruction error</p><formula xml:id="formula_13">E(µ 1 , . . . , µ K , y 1 , . . . , y N ) = 1 N N i=1 x i − µ yi 2<label>(10)</label></formula><p>where y i ∈ {1, . . . , K} are the data-to-cluster assignments and µ y are means approximating the vectors in the corresponding clusters. The K-means energy can thus be interpreted as the average data reconstruction error.</p><p>It is natural to ask whether a clustering method such as K-means, which is based on approximating the input data, could be combined with representation learning, which uses a discriminative objective. In this setting, the feature vectors x = Φ(I) are extracted by the neural network Φ from the input data I. Unfortunately, optimizing a loss such as eq. (10) with respect to the clustering and representation parameters is meaningless: in fact, the obvious solution is to let the representation send all the data points to the same constant feature vector and setting all the means to coincide with it, in which case the K-means reconstruction error is zero (and thus minimal).</p><p>Nevertheless, DeepCluster  does successfully combine K-means with representation learning. DeepCluster can be related to our approach as follows.</p><p>Step 1 of the algorithm, namely representation learning via cross-entropy minimization, is exactly the same.</p><p>Step 2, namely self-labelling, differs: where we solve an optimal transport problem to obtain the pseudo-labels, they do so by running K-means on the feature vectors extracted by the neural network.</p><p>DeepCluster does have an obvious degenerate solution: we can assign all data points to the same label and learn a constant representation, achieving simultaneously a minimum of the cross-entropy loss in Step 1 and of the K-means loss in Step 2. The reason why DeepCluster avoids this pitfall is due to the particular interaction between the two steps. First, during Step 2, the features x i are fixed so K-means cannot pull them together. Instead, the means spread to cover the features as they are, resulting in a balanced partitioning. Second, during the classification step, the cluster assignments y i are fixed, and optimizing the features x i with respect to the cross-entropy loss tends to separate them. Lastly, the method in  also uses other heuristics such as sampling the training data inversely to their associated clusters' size, leading to further regularization.</p><p>However, a downside of DeepCluster is that it does not have a single, well-defined objective to optimize, which means that it is difficult to characterize its convergence properties. By contrast, in our formulation, both Step 1 and Step 2 optimize the same objective, with the advantage that convergence to a (local) optimum is guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">AUGMENTING SELF-LABELLING VIA DATA TRANSFORMATIONS</head><p>Methods such as DeepCluster extend the training data via augmentations. In vision problems, this amounts to (heavily) distorting and cropping the input images at random. Augmentations are applied so that the neural network is encouraged to learn a labelling function which is transformation invariant.</p><p>In practice, this is crucial to learn good clusters and representations, so we adopt it here. This is achieved by setting</p><formula xml:id="formula_14">P yi = E t [log softmax y h • Φ(tx i )]</formula><p>where the transformations t are sampled at random. In practice, in Step 1 (representation learning), this is implemented via the application of the random transformations to data batches during optimization via SGD, which is corresponds to the usual data augmentation scheme for deep neural networks. As noted in <ref type="bibr" target="#b40">(YM. et al., 2020)</ref>, and as can be noted by an analysis of recent publications <ref type="bibr" target="#b26">Misra &amp; van der Maaten, 2019)</ref>, augmentation is critical for good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">MULTIPLE SIMULTANEOUS SELF-LABELINGS</head><p>Intuitively, the same data can often be clustered in many equally good ways. For example, visual objects can be clustered by color, size, typology, viewpoint, and many other attributes. Since our main objective is to use clustering to learn a good data representation Φ, we consider a multi-task setting in which the same representation is shared among several different clustering tasks, which can potentially capture different and complementary clustering axis.</p><p>In our formulation, this is easily achieved by considering multiple heads <ref type="bibr" target="#b20">(Ji et al., 2018</ref>) h 1 , . . . , h T , one for each of T clustering tasks (which may also have a different number of labels). Then, we optimize a sum of objective functions of the type eq. (6), one for each task, while sharing the parameters of the feature extractor Φ among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the quality of the representations learned by our Self Labelling (SeLa) technique. We first test variants of our method, including ablating its components, in order to find an optimal configuration. Then, we compare our results to the state of the art in self-supervised representation learning, where we find that our method is the best among clustering-based techniques and overall state-of-the-art or at least highly competitive in many benchmarks. In the appendix, we also show qualitatively that the labels identified by our algorithm are usually meaningful and group visually similar concepts in the same clusters, often even capturing whole ImageNet classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SETUP</head><p>Linear probes. In order to quantify if a neural network has learned useful feature representations, we follow the standard approach of using linear probes . This amounts to solving a difficult task, such as ImageNet classification, by training a linear classifier on top of a pre-trained feature representation, which is kept fixed. Linear classifiers heavily rely on the quality of the representation since their discriminative power is low. We apply linear probes to all intermediate convolutional blocks of representative networks. While linear probes are conceptually straightforward, there are several technical details that can affect the final accuracy, so we follow the standard protocol further outlined in the Appendix.</p><p>Data. For training data we consider ImageNet LSVRC-12 <ref type="bibr" target="#b6">(Deng et al., 2009</ref>) and other smaller scale datasets. We also test our features by transferring them to MIT Places <ref type="bibr" target="#b44">(Zhou et al., 2014)</ref>. All of these are standard benchmarks for evaluation in self-supervised learning.</p><p>Architectures. Our base encoder architecture is AlexNet <ref type="bibr" target="#b22">(Krizhevsky et al., 2012)</ref>, since this is the most frequently used in other self-supervised learning works for the purpose of benchmarking. We inject the probes right after the ReLU layer in each of the five blocks, and denote these entry points conv1 to conv5. Furthermore, since the conv1 and conv2 can be learned effectively from data augmentations alone <ref type="bibr" target="#b40">(YM. et al., 2020)</ref>, we focus the analysis on the deeper layers conv2 to conv5 which are more sensitive to the quality of the learning algorithm. In addition to AlexNet, we also test ResNet-50 <ref type="bibr" target="#b15">(He et al., 2016)</ref> models. Further experimental details are given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">OPTIMAL CONFIGURATION AND ABLATIONS</head><p>In tables 1 and 5, we first validate various modelling and configuration choices. Two key hyperparameters are the number of clusters K and the number of clustering heads T , which we denote in the experiments below with the shorthand "SeLa[K × T ]". We run SeLa by alternating steps 1 and 2 as described in section 3.1.</p><p>Step 1 amounts to standard CE training, which we run for a fixed number of epochs.</p><p>Step 2 can be interleaved at any point in the optimization; to amortize its cost, we run it     at most once per epoch, and usually less, with a schedule described and validated below. For these experiments, we train the representation and the linear probes on ImageNet.</p><p>Number of clusters K. <ref type="table" target="#tab_1">Table 2</ref>, compares different values for K: moving from 1k to 3k improves the results, but larger numbers decrease the quality slightly.</p><p>Ablation: number of heads T . <ref type="table" target="#tab_2">Table 3</ref> shows that increasing the number of heads from T = 1 to T = 10 yields a large performance gain: +2% for AlexNet and +10% for ResNet. The latter more expressive model appears to benefit more from a more diverse training signal.</p><p>Ablation: number of self-labelling iterations. First, in table 1, we show that self-labelling (step 2) is essential for good performance, as opposed to only relying on the initial random label assignments and the data augmentations. For this, we vary the number of times the self-labelling algorithm (step 2) is run during training (#opts), from zero to once per step 1 epoch. We see that self-labelling is essential, with the best value around 80 (for 160 step 1 epochs in total).</p><p>Architectures. <ref type="table" target="#tab_3">Table 4</ref> compares a smaller variant of AlexNet which uses <ref type="bibr">(64,</ref><ref type="bibr">192)</ref> filters in its first two convolutional layers <ref type="bibr" target="#b23">(Krizhevsky, 2014)</ref>, to the standard variant with (96, 256) <ref type="bibr" target="#b22">(Krizhevsky et al., 2012)</ref>, all the way to a ResNet-50. SeLa works well in all cases, for large models such as ResNet but also smaller ones such as AlexNet, for which methods such as BigBiGAN  or CPC (Hénaff et al., 2019) are unsuitable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LABEL TRANSFER</head><p>An appealing property of SeLa is that the label it assigns to the images can be used to train another model from scratch, using standard supervised training. For instance, table 5 shows that, given the labels assigned by applying SeLa to AlexNet, we can re-train AlexNet from scratch using a shorter 90-epochs schedule with achieving the same final accuracy. This shows that the quality of the learned representation depends only the final label assignment, not on the fact that the representation is learned jointly with the labels. More interestingly, we can transfer labels between different architectures. For example, the labels obtained by applying SeLa [3k × 1] and SeLa [3k × 10] to ResNet-50 can be used to train a better AlexNet model than applying SeLa to the latter directly. For this reason, we publish on our website the self-labels for the ImageNet dataset in addition to the code and trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SMALL-SCALE DATASETS</head><p>Here, we evaluate our method on relatively simple and small datasets, namely CIFAR-10/100 <ref type="bibr" target="#b24">(Krizhevsky et al., 2009</ref>) and SVHN <ref type="bibr" target="#b29">(Netzer et al., 2011)</ref>. For this, we follow the experimental and evaluation protocol from the current state of the art in self-supervised learning in these datasets, AND . In table 6, we compare our method with the settings [128 × 10] for CIFAR-10, [512 × 10] for CIFAR-100 and [128 × 1] for SVHN to other published methods; details on the evaluation method are provided in the appendix. We observe that our proposed method outperforms the best previous method by 5.8% for CIFAR-10, by 9.5% for CIFAR-100 and by 0.8% for SVHN when training a linear classifier on top of the frozen network. The relatively minor gains on SVHN can be explained by the fact that the gap between the supervised   baseline and the self-supervised results is already very small (&lt; 3%). We also evaluate our method using weighted kNN using an embedding of size 128. We find that the proposed method consistently outperforms the previous state of the art by around 2% across these datasets, even though AND is based on explicitly learning local neighbourhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">IMBALANCED DATA EXPERIMENTS</head><p>In order to understand if our equipartition regularization is affected by the underlying class distribution of a dataset, we perform multiple ablation experiments on artificially imbalanced datasets in table 8. We consider three training datasets based on CIFAR-10. The first is the original dataset with 5000 images for each class (full in table 8). Second, we remove 50% of the images of one class (truck) while the rest remains untouched (light imbalance) and finally we remove 10% of one class, 20% of the second class and so on (heavy imbalance). On each of the three datasets we compare the performance of our method "ours (SK)" with a baseline that replaces our Sinkhorn-Knopp optimization with K-means clustering "ours (K-means)". We also compare the performance to training a network under full supervision. The evaluation follows  and is based on linear probing and kNN classification -both on CIFAR-10 and, to understand feature generalization, on CIFAR-100.   17.7 24.5 31.0 29.9 28.0 22.0 28.7 31.8 31.3 29.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-crop evaluation</head><p>Instance retrieval,  16.8 26.5 31.8 34.1 35.6 18.8 24.3 31.9 34.5 33.6 RotNet,  18.8 31.7 38.7 38.2 36.5 21.5 31.0 35.1 34.6 33.7 AND * ,  15.6 27.0 35.9 39.7 37.9 -----CMC * ,  18.4 33.5 38.1 40.4 42.6 -----AET * ,  19 To our surprise, we find that our method generalizes better to CIFAR-100 than the supervised baseline during kNN evaluation, potentially due to overfitting when training with labels. We also find that using SK optmization for obtaining pseudo-labels is always better than K-means on all metrics and datasets. When comparing the imbalance settings, we find that under the light imbalance scenario, the methods' performances are ranked the same and no method is strongly affected by the imbalance. Under the heavy imbalance scenario, all methods drop in performance. However, compared to full data and light imbalance, the gap between supervised and self-supervised even decreases slightly for both K-means and our method, indicating stronger robustness of self-supervised methods compared to a supervised one.</p><p>In conclusion, our proposed method does not rely on the data to contain the same number of images for every class and outperforms a K-means baseline even in very strong imbalance settings. This confirms the intuition that the equipartioning constraint acts as a regularizer and does not exploit the class distribution of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">LARGE SCALE BENCHMARKS</head><p>To compare to the state of the art and concurrent work, we evaluate several architectures using linear probes on public benchmark datasets.</p><p>AlexNet. The main benchmark for feature learning methods is linear probing of an AlexNet trained on ImageNet. In table 9 we compare the performance across layers also on the Places dataset. We find that across both datasets our method outperforms DeepCluster and local Aggregation at every layer. From our ablation studies in tables 1-5 we also note that even our single head variant [3k × 1] outperforms both methods. Given that our method provides labels for a dataset that can be used for retraining a network quickly, we find that we can improve upon this initial performance. And by adopting a hybrid approach, similar to , of training an AlexNet with 10 heads and one additional head for computing the RotNet loss, we find further improvement. This result (SeLa   ResNet-50 (64.1) (85.4) MoCo,  ResNet-50 60.6 − PIRL, <ref type="bibr" target="#b26">(Misra &amp; van der Maaten, 2019</ref> ResNet. Training better models than AlexNets is not yet standardized in the feature learning community. In <ref type="table" target="#tab_0">Table 10</ref> we compare a ResNet-50 trained with our method to other works. With top-1 accuracy of 61.5, we outperform than all other methods including Local Aggregation, CPCv1 and MoCo that use the same level of data augmentation. We even outperform larger architectures such as BigBiGAN's RevNet-50x4 and reach close to the performance of models using AutoAugment-style transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">FINE-TUNING: CLASSIFICATION, OBJECT DETECTION AND SEMANTIC SEGMENTATION</head><p>Finally, since pre-training is usually aimed at improving down-stream tasks, we evaluate the quality of the learned features by fine-tuning the model for three distinct tasks on the PASCAL VOC benchmark. In <ref type="table" target="#tab_6">Table 7</ref> we compare results with regard to multi-label classification, object detection and semantic segmentation on PASCAL VOC <ref type="bibr" target="#b12">(Everingham et al., 2010)</ref>.</p><p>As in the linear probe experiments, we find our method better than the current state of the art in detection and classification with both fine-tuning only the last fully connected layers and when fine-tuning the whole network ("all". Notably, our fine-tuned AlexNet outperforms its supervised ImageNet baseline on the VOC detection task. Also for segmentation the method is very close (0.2%) to the best performing method. This shows that our trained network does not only learn useful feature representations but is also able to perform well when fine-tuned on actual down-stream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We present a self-supervised feature learning method that is based on clustering. In contrast to other methods, ours optimizes the same objective during feature learning and during clustering. This becomes possible through a weak assumption that the number of samples should be equal across clusters. This constraint is explicitly encoded in the label assignment step and can be solved for efficiently using a modified Sinkhorn-Knopp algorithm. Our method outperforms all other feature learning approaches and achieves SOTA on SVHN, CIFAR-10/100 and ImageNet for AlexNet and ResNet-50. By virtue of the method, the resulting self-labels can be used to quickly learn features for new architectures using simple cross-entropy training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 IMPLEMENTATION DETAILS</head><p>Learning Details Unless otherwise noted, we train all our self-supervised models with SGD and intial learning rate 0.05 for 400 epochs with two learning rate drops where we divide the rate by ten at 150 and 300 and 350 epochs. We spread our pseudo-label optimizations throughout the whole training process in a logarithmic distribution. We optimize the label assignment at</p><formula xml:id="formula_15">t i = i M −1 2 , i ∈ {1, . . . , M },</formula><p>where M is the user-defined number of optimizations and t i is expressed as a fraction of total training epochs. For the Sinkhorn-Knopp optimization we set λ = 25 as in <ref type="bibr" target="#b5">(Cuturi, 2013)</ref>. We use standard data augmentations during training that consist of randomly resized crops, horizontal flipping and adding noise, as in .</p><p>Quantitative Evaluation -Technical Details. Unfortunately, prior work has used several slightly different setups, so that comparing results between different publications must be done with caution.</p><p>In our ImageNet implementation, we follow the original proposal  in pooling each representation to a vector with 9600, 9216, 9600, 9600, 9216 dimensions for conv1-5 using adaptive max-pooling, and absorb the batch normalization weights into the preceding convolutions. For evaluation on ImageNet we follow RotNet to train linear probes: images are resized such that the shorter edge has a length of 256 pixels, random crops of 224×224 are computed and flipped horizontally with 50% probability. Learning lasts for 36 epochs and the learning rate schedule starts from 0.01 and is divided by five at epochs 5, 15 and 25. The top-1 accuracy of the linear classifier is then measured on the ImageNet validation subset by optionally extracting 10 crops for each validation image (four at the corners and one at the center along with their horizontal flips) and averaging the prediction scores before the accuracy is computed or just taking the a centred crop. For CIFAR-10/100 and SVHN we train AlexNet architectures on the resized images with batchsize 128, learning rate 0.03 and also the same image augmentations (random resized crops, color jitter and random grayscale) as is used in prior work . We use the same linear probing protocol as for our ImageNet experiments but without using 10 crops. For the weighted kNN experiments we use k = 50, σ = 0.1 and we use an embedding of size 128 as done in previous works.</p><p>In <ref type="table" target="#tab_8">Table 9</ref>, when retraining an AlexNet using ResNet generated labels, we can apply heavier augmentation strategies as the labels are kept constant. Hence for the experiments denoted by "+ more aug.", in addition to the usual augmentations, we further randomly apply one of equalize, autoconstrast and sharpening. We find that this raises the performance for ImageNet but lowers the performance on Places by a small amount, hence illuminating the need to always also report performance on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 FURTHER DETAILS</head><p>NMI over time In <ref type="figure">fig. A</ref>.1 we find that most learning takes place in the early epochs, and we reach a final NMI value of around 66%. Similarly, we find that due to the updating of the pseudo-labels at regular intervals and our data augmentation, the pseudo-label accuracies keep continuously rising without overfitting to these labels.</p><p>Clustering metrics In table A.1, we report standard clustering metrics (see <ref type="bibr" target="#b37">(Vinh et al., 2010)</ref> for detailed definitions) of our trained models with regards to the ImageNet validation set groundtruth labels. These metrics include chance-corrected metrics which are the adjusted normalized mutual information (NMI) and the adjusted Rand-Index, as well as the default NMI, also reported in DeepCluster .</p><p>Conv1 filters In <ref type="figure" target="#fig_2">fig. A.3</ref> we show the first convolutional filters of two of our trained models. We can find the typical Gabor-like edge detectors as well as color blops and dot-detectors.</p><p>Entropy over time In <ref type="figure">fig. A.4</ref>, we show how the distribution of entropy with regards to the true ImageNet labels changes with training time. We find that while at first, all 3000 pseudo-labels contain random real ImageNet labels, yielding high entropy of around 6 ≈ ln(400) = ln(1.2 · 10 6 /3000). Towards the end of training we arrive at a broad spectrum of entropies with some as low as This measure is not used for training but indicates how good a clustering is. Right: Similarities of consecutive labellings using NMI. Both plots use the [10k × 1] AlexNet for comparability with the DeepCluster paper . <ref type="table">Table A</ref>.1: Clustering metrics that compare with ground-truth labels of the ImageNet validation set (with 1-crop). For reference, we provide the best Top-1 error on ImageNet linear probing (as reported in the main part). * : for the multi-head variants, we simply use predictions of a randomly picked, single head.   AlexNet to compare to the equivalent plot in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 COMPLETE TABLES</head><p>In the following, we report the unabridged tables with all related work. <ref type="table">Table A</ref>.2: Linear probing evaluation -AlexNet. A linear classifier is trained on the (downsampled) activations of each layer in the pretrained model. We bold the best result in each layer and underline the second best. The best layer is highlighted in blue. * denotes a larger AlexNet variant. − refers to AlexNets trained with self-label transfer from a corresponding ResNet-50. "+Rot" refers to retraining using labels and an additional RotNet loss, "+ more aug." includes further augmentation during retraining.  Figure A.8: Here we show a random sample of valdation set images associated to random pseudoclasses. The entropy is given by true image labels which are also shown as a frame around each picture with a random color. This visualization uses ResNet-50 [3k × 1]. Classes with less than 9 images are sampled with repetition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Left: Normalized Mutual Information (NMI) against validation set ImageNet labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure A. 2 :</head><label>2</label><figDesc>Pseudo-label accuracies for the training data versus training time for the [10k × 1] AlexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure A. 3 :</head><label>3</label><figDesc>Visualization of the first convolutional layers of our [3k × 10] AlexNet (left) and the [1k × 1] ResNet-50 (right). The filters are scaled to lie between (0,1) for visualization. 0.07 ≈ ln(1.07) (see Fig. A.5 and A.6 for low entropy label visualizations) and the mean around 4.2 ≈ ln(66) (see Fig. A.7 and A.8 for randomly chosen labels' visualizations). Cross-entropy of the pseudo-labels with the true ImageNet training set labels. This measure is not used for training but indicates how good a clustering is. This plot uses the [10k × 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A. 3 :</head><label>3</label><figDesc>Linear evaluation -ResNet. A linear layer is trained on top of the global average pooled features of ResNets. All evaluations use a single centred crop. We have separated much larger architectures such as RevNet-50×4 and ResNet-161. Methods in brackets use a augmentation policy learned from supervised training and methods with * are not explicit about which further augmentations they use. A.4 LOW ENTROPY PSEUDOCLASSES Figure A.5: Here we show a random sample of images associated to the lowest entropy pseudoclasses. The entropy is given by true image labels which are also shown as a frame around each picture with a random color. This visualization uses ResNet-50 [3k × 1]. The entropy varies from 0.07 − −0.83 Figure A.6: Visualization of pseudoclasses on the validation set. Here we show random samples of validation set images associated to the lowest entropy pseudoclasses of training set. For further details, see Figure A.5. Classes with less than 9 images are sampled with repetition. A.5 RANDOM PSEUDOCLASSES Figure A.7: Here we show a random sample of Imagenet training set images associated to the random pseudoclasses. The entropy is given by true image labels which are also shown as a frame around each picture with a random color. This visualization uses ResNet-50 [3k × 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation: number of self-labelling steps.</figDesc><table><row><cell>Method</cell><cell>#opt. c3 c4 c5</cell></row><row><cell>SeLa [3k × 1]</cell><cell>0 20.8 18.3 13.4</cell></row><row><cell cols="2">SeLa [3k × 1] 40 42.7 43.4 39.2</cell></row><row><cell cols="2">SeLa [3k × 1] 80 43.0 44.7 40.9</cell></row><row><cell cols="2">SeLa [3k × 1] 160 42.4 44.6 40.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Number of clusters K.</figDesc><table><row><cell>Method</cell><cell>c3 c4 c5</cell></row><row><cell cols="2">SeLa [1k × 1] 40.1 42.1 38.8</cell></row><row><cell cols="2">SeLa [3k × 1] 43.0 44.7 40.9</cell></row><row><cell cols="2">SeLa [5k × 1] 42.5 43.9 40.2</cell></row><row><cell cols="2">SeLa [10k × 1] 42.2 43.8 39.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation: number of heads T . (c4 for AlexNet)</figDesc><table><row><cell>Method</cell><cell cols="2">Architecture Top-1</cell></row><row><cell cols="2">SeLa [3k × 1] AlexNet</cell><cell>44.7</cell></row><row><cell cols="2">SeLa [3k × 10] AlexNet</cell><cell>46.7</cell></row><row><cell cols="3">SeLa [3k × 1] ResNet-50 51.8</cell></row><row><cell cols="3">SeLa [3k × 10] ResNet-50 61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Different architectures.</figDesc><table><row><cell>Method</cell><cell>Architecture</cell><cell>Top-1</cell></row><row><cell cols="3">SeLa [3k × 1] AlexNet (small) 41.3</cell></row><row><cell cols="2">SeLa [3k × 1] AlexNet</cell><cell>44.7</cell></row><row><cell cols="2">SeLa [3k × 1] ResNet-50</cell><cell>51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Label transfer.</figDesc><table><row><cell>Method</cell><cell>Source (Top-1) Target (Top-1)</cell></row><row><cell cols="2">SeLa [3k × 10] AlexNet (46.7) AlexNet (46.5)</cell></row><row><cell cols="2">SeLa [3k × 1] ResNet-50 (51.8) AlexNet (45.0)</cell></row><row><cell cols="2">SeLa [3k × 10] ResNet-50 (61.5) AlexNet (48.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Nearest Neighbour and linear classification evaluation on small datasets using AlexNet. Results of previous methods are taken from.</figDesc><table><row><cell></cell><cell></cell><cell>Dataset</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">CIFAR-10 CIFAR-100 SVHN</cell></row><row><cell>Classifier/Feature</cell><cell cols="3">Linear Classifier / conv5</cell></row><row><cell>Supervised</cell><cell>91.8</cell><cell>71.0</cell><cell>96.1</cell></row><row><cell>Counting</cell><cell>50.9</cell><cell>18.2</cell><cell>63.4</cell></row><row><cell>DeepCluster</cell><cell>77.9</cell><cell>41.9</cell><cell>92.0</cell></row><row><cell>Instance</cell><cell>70.1</cell><cell>39.4</cell><cell>89.3</cell></row><row><cell>AND</cell><cell>77.6</cell><cell>47.9</cell><cell>93.7</cell></row><row><cell>SL</cell><cell>83.4</cell><cell>57.4</cell><cell>94.5</cell></row><row><cell>Classifier/Feature</cell><cell cols="3">Weighted kNN / FC</cell></row><row><cell>Supervised</cell><cell>91.9</cell><cell>69.7</cell><cell>96.5</cell></row><row><cell>Counting</cell><cell>41.7</cell><cell>15.9</cell><cell>43.4</cell></row><row><cell>DeepCluster</cell><cell>62.3</cell><cell>22.7</cell><cell>84.9</cell></row><row><cell>Instance</cell><cell>60.3</cell><cell>32.7</cell><cell>79.8</cell></row><row><cell>AND</cell><cell>74.8</cell><cell>41.5</cell><cell>90.9</cell></row><row><cell>SL</cell><cell>77.6</cell><cell>44.2</cell><cell>92.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>denotes a larger AlexNet variant. SeLa * [3k × 10] − +Rot 75.6 77.2 59.2 45.7</figDesc><table><row><cell>:</cell><cell cols="3">PASCAL VOC finetun-</cell></row><row><cell>ing.</cell><cell cols="3">VOC07-Classification %mAP,</cell></row><row><cell cols="4">VOC07-Detection %mAP and VOC12-</cell></row><row><cell cols="4">Segmentation %mIU.  PASCAL VOC Task</cell></row><row><cell>Method</cell><cell></cell><cell>Cls.</cell><cell>Det. Seg.</cell></row><row><cell></cell><cell></cell><cell>fc6-8 all</cell><cell>all</cell><cell>all</cell></row><row><cell cols="2">ImageNet labels</cell><cell cols="2">78.9 79.9 59.1 48.0</cell></row><row><cell>Random</cell><cell></cell><cell cols="2">− 53.3 43.4</cell><cell>−</cell></row><row><cell cols="2">Random Rescaled</cell><cell cols="2">− 56.6 45.6 32.6</cell></row><row><cell>BiGAN</cell><cell></cell><cell cols="2">52.3 60.1 46.9 35.2</cell></row><row><cell>Context  *</cell><cell></cell><cell cols="2">55.1 65.3 51.1</cell><cell>−</cell></row><row><cell>Context 2</cell><cell></cell><cell cols="2">− 69.6 55.8 41.4</cell></row><row><cell>CC+VGG</cell><cell></cell><cell cols="2">− 72.5 56.5 42.6</cell></row><row><cell>RotNet</cell><cell></cell><cell cols="2">70.9 73.0 54.4 39.1</cell></row><row><cell cols="2">DeepCluster  *</cell><cell cols="2">72.0 73.4 55.4 45.1</cell></row><row><cell cols="2">RotNet+retrieval  *</cell><cell cols="2">72.5 74.7 58.0 45.9</cell></row><row><cell cols="2">SeLa  *  [3k × 10]</cell><cell cols="2">73.1 75.3 55.9 43.7</cell></row><row><cell cols="2">SeLa  *  [3k × 10] −</cell><cell cols="2">74.4 75.9 57.8 44.7</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Nearest Neighbour and linear classification evaluation using imbalanced CIFAR-10 training data. We evaluate on the normal CIFAR-10 test set and on CIFAR-100 to analyze the transferability of the features. Difference to the supervised baseline in parentheses. See section 4.5 for details.</figDesc><table><row><cell></cell><cell></cell><cell>kNN</cell><cell cols="2">Linear/conv5</cell></row><row><cell>Training data/Method</cell><cell>CIFAR-10</cell><cell cols="2">CIFAR-100 CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>CIFAR-10, full</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell>92.1</cell><cell>24.0</cell><cell>90.2</cell><cell>54.2</cell></row><row><cell cols="5">ours (K-means) [128 × 1] 64.7 (−17.4) 19.3 (−4.7) 77.5 (−12.7) 45.6 (−8.8)</cell></row><row><cell>ours (SK) [128 × 1]</cell><cell cols="4">72.9 (−9.2) 28.9 (+4.9) 79.8 (−9.4) 49.4 (−4.8)</cell></row><row><cell>CIFAR-10, light imbalance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell>92.0</cell><cell>24.0</cell><cell>90.4</cell><cell>53.6</cell></row><row><cell cols="5">ours (K-means) [128 × 1] 64.2 (−17.8) 18.1 (−5.9) 77.0 (−13.4) 44.8 (−8.8)</cell></row><row><cell>ours (SK) [128 × 1]</cell><cell cols="4">71.7 (−10.3) 28.2 (+4.2) 79.5 (−10.9) 48.6 (−5.0)</cell></row><row><cell>CIFAR-10, heavy imbalance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell>86.7</cell><cell>22.6</cell><cell>86.8</cell><cell>51.4</cell></row><row><cell cols="5">ours (K-means) [128 × 1] 60.7 (−16.0) 17.8 (−4.8) 75.2 (−11.6) 44.3 (−7.1)</cell></row><row><cell>ours (SK) [128 × 1]</cell><cell cols="4">67.6 (−9.1) 26.7 (+3.9) 77.2 (−9.6) 47.5 (−2.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Linear probing evaluation -AlexNet. A linear classifier is trained on the (downsampled) activations of each layer in the pretrained model. We bold the best result in each layer and underline the second best. The best layer is highlighted in blue. * denotes a larger AlexNet variant. − refers to AlexNets trained with self-label transfer from a corresponding ResNet-50. "+Rot" refers to retraining using labels and an additional RotNet loss, "+ more aug." includes further augmentation during retraining. SeeTable A.2 in the Appendix for a full version of this table and details.</figDesc><table><row><cell>ILSVRC-12</cell><cell>Places</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Linear evaluation -ResNet. A linear layer is trained on top of the global average pooled features of ResNets. All evaluations use a single centred crop. We have separated much larger architectures such as RevNet-50×4 and ResNet-161. Methods in brackets use a augmentation policy learned from supervised training and methods with * are not explicit about which further augmentations they use. SeeTable A.3 in the Appendix for a full version of this table.</figDesc><table><row><cell>Method</cell><cell cols="3">Architecture Top-1 Top-5</cell></row><row><cell cols="2">Supervised, (Donahue &amp; Simonyan, 2019) ResNet-50</cell><cell cols="2">76.3 93.1</cell></row><row><cell>Jigsaw, (Kolesnikov et al., 2019)</cell><cell>ResNet-50</cell><cell>38.4</cell><cell>−</cell></row><row><cell>Rotation, (Kolesnikov et al., 2019)</cell><cell>ResNet-50</cell><cell>43.8</cell><cell>−</cell></row><row><cell>CPC, (Oord et al., 2018)</cell><cell>ResNet-101</cell><cell cols="2">48.7 73.6</cell></row><row><cell cols="2">BigBiGAN, (Donahue &amp; Simonyan, 2019) ResNet-50</cell><cell cols="2">55.4 77.4</cell></row><row><cell cols="2">LocalAggregation, (Zhuang et al., 2019) ResNet-50</cell><cell>60.2</cell><cell>−</cell></row><row><cell cols="2">Efficient CPC v2.1, (Hénaff et al., 2019) ResNet-50</cell><cell cols="2">(63.8) (85.3)</cell></row><row><cell>CMC,</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>− +Rot) achieves state of the art in unsupervised representation learning for AlexNet, with a gap of 1.3% to the previous best performance on ImageNet and surpasses the ImageNet supervised baseline transferred to Places by 1.7%.</figDesc><table><row><cell>)  *</cell><cell>ResNet-50</cell><cell>63.6</cell><cell>−</cell></row><row><cell>SeLa [3k × 10]</cell><cell>ResNet-50</cell><cell cols="2">61.5 84.0</cell></row><row><cell>other architectures</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo, (He et al., 2019)</cell><cell cols="2">RevNet-50×4 68.6</cell><cell>−</cell></row><row><cell cols="2">Efficient CPC v2.1, (Hénaff et al., 2019) ResNet-161</cell><cell cols="2">71.5 90.1</cell></row><row><cell>[3k × 10]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We assume for simplicity that K divides N exactly, but the formulation is easily extended to any N ≥ K by setting the constraints to either N/K or N/K + 1, in order to assure that there is a feasible solution.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Yuki Asano gratefully acknowledges support from the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines &amp; Systems (EP/L015897/1). We are also grateful to ERC IDIU-638009, AWS Machine Learning Research Awards (MLRA) and the use of the University of Oxford Advanced Research Computing (ARC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffrac: a discriminative and flexible framework for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaïd</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cliquecnn: Deep unsupervised exemplar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Tikhoncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Conference on Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3846" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2496141.2</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning by rotation feature decoupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272v2</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning by neighbourhood discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on machine learning (ICML)</title>
		<meeting>the International Conference on machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Invariant information distillation for unsupervised image segmentation and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06653</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09005</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Citeseer</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Nguyen Xuan Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised image matching and object discovery as optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8287" to="8296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A critical analysis of self-supervision, or what we can learn from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1esx6EYvr.6" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by auto-encoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imagenet Supervised</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Places supervised</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>22.1 35.1 40.2 43.3 44.6</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Random</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Inpainting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pathak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Bigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donahue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Context</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Colorization</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Jigsaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Counting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noroozi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Splitbrain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cc+vgg-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noroozi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Context 2 (Mundhenk</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Rotnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gidaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Artifacts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>And *</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno>18.4 33.5 38.1 40.4 42.6 - - - - - AET *</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Rotnet+retrieval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Deepcluster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rgb) *</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1830</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Local</forename><surname>Agg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">*</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Rotnet+retrieval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<idno>Top-1 Top-5</idno>
		<title level="m">Method Architecture</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Supervised</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno>ResNet-50 76.3 93.1</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Supervised</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">ResNet-50 42.2 − Exemplar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Jigsaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno>ResNet-101 − 69.3</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Doersch &amp; Zisserman</publisher>
		</imprint>
	</monogr>
	<note>ResNet-50 43.0 − Rotation. ResNet-50 43.8 − Multi-task</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Cpc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Bigbigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno>ResNet-50 55.4 77.4</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">ResNet-50 60.2 − Efficient CPC v2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Localaggregation</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>ResNet-50 (63.8) (85.3</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Cmc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ResNet-50 (64.1) (85.4</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Moco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>ResNet-50</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Misra &amp; van der Maaten</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Rotation</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno>RevNet-50×4 60.8 81.4</idno>
		<title level="m">RevNet-50×4 53.7 − BigBiGAN</title>
		<imprint>
			<publisher>Donahue &amp; Simonyan</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Amdim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bachman</surname></persName>
		</author>
		<idno>103 (67.4) (81.8</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Cmc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<idno>RevNet-50×4 68.4 88.2</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">RevNet-50×4 68.6 − Efficient CPC v2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Moco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>ResNet-161 71.5 90.1</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
