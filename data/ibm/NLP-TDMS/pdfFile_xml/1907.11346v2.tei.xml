<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><forename type="middle">Yong</forename><surname>Chang</surname></persName>
							<email>juyong.chang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ECE</orgName>
								<orgName type="institution">Kwangwoon University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Qualitative results of applying our 3D multi-person pose estimation framework to COCO dataset [25]  which consists of in-the-wild images. Most of the previous 3D human pose estimation studies mainly focused on the root-relative 3D single-person pose estimation. In this study, we propose a general 3D multi-person pose estimation framework that takes into account all factors including human detection and 3D human root localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Although significant improvement has been achieved recently in 3D human pose estimation, most of the previous methods only treat a single-person case. In this work, we firstly propose a fully learning-based, camera distanceaware top-down approach for 3D multi-person pose estimation from a single RGB image. The pipeline of the proposed system consists of human detection, absolute 3D human root localization, and root-relative 3D single-person pose estimation modules. Our system achieves comparable results with the state-of-the-art 3D single-person pose estimation models without any groundtruth information and significantly outperforms previous 3D multi-person pose estimation methods on publicly available datasets. The code is available in 1,2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of 3D human pose estimation is to localize semantic keypoints of single or multiple human bodies in 3D space. It is an essential technique for human behavior understanding and human-computer interaction. Recently, many methods <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b53">52]</ref> utilize deep convolutional neural networks (CNNs) and have achieved noticeable performance improvement on large-scale publicly available datasets <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b29">28]</ref>.</p><p>Most of the previous 3D human pose estimation methods <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b53">52]</ref> are designed for single-person case. They crop the human area in an input image with a groundtruth bounding box or the bounding box that is predicted from a human detection model <ref type="bibr" target="#b12">[11]</ref>. The cropped patch of a human body is fed into the 3D pose estimation module, which then estimates the 3D location of each keypoint. As their models take a single cropped image, estimating the absolute camera-centered coordinate of each keypoint is difficult. To handle this issue, many methods <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b53">52]</ref> estimate the relative 3D pose to a reference point in the body, e.g., the center joint (i.e., pelvis) arXiv:1907.11346v2 [cs.CV] 17 Aug 2019 of a human, called root. The final 3D pose is obtained by adding the 3D coordinates of the root to the estimated rootrelative 3D pose. Prior information on the bone length <ref type="bibr" target="#b38">[37]</ref> or the groundtruth <ref type="bibr" target="#b45">[44]</ref> has been commonly used for the localization of the root.</p><p>Recently, many top-down approaches <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b48">47]</ref> for the 2D multi-person pose estimation have shown noticeable performance improvement. These approaches first detect humans by using a human detection module, and then estimate the 2D pose of each human by a 2D single-person pose estimation module. Although they are straightforward when used in 2D cases, extending them to 3D cases is nontrivial. Note that for the estimation of 3D multi-person poses, we need to know the absolute distance to each human from the camera as well as the 2D bounding boxes. However, existing human detectors provide 2D bounding boxes only.</p><p>In this study, we propose a general framework for 3D multi-person pose estimation. To the best of our knowledge, this study is the first to propose a fully learning-based camera distance-aware top-down approach of which components are compatible with most of the previous human detection and 3D human pose estimation methods. The pipeline of the proposed system consists of three modules. First, a human detection network (DetectNet) detects the bounding boxes of humans in an input image. Second, the proposed 3D human root localization network (RootNet) estimates the camera-centered coordinates of the detected humans' roots. Third, a root-relative 3D single-person pose estimation network (PoseNet) estimates the root-relative 3D pose for each detected human. <ref type="figure" target="#fig_1">Figures 1 and 2</ref> show the qualitative results and overall pipeline of our framework, respectively.</p><p>We show that our approach outperforms previous 3D multi-person pose estimation methods <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b41">40]</ref> on several publicly available 3D single-and multi-person pose estimation datasets <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b30">29]</ref> by a large margin. Also, even without any groundtruth information (i.e., the bounding boxes and the 3D location of the roots), our method achieves comparable performance with the state-of-the-art 3D single-person pose estimation methods that use the groundtruth in the inference time. Note that our framework is new but follows previous conventions of object detection and 3D human pose estimation networks. Thus, previous detection and pose estimation methods can be easily plugged into our framework, which makes the proposed framework quite flexible and generalizable.</p><p>Our contributions can be summarized as follows.</p><p>• We propose a new general framework for 3D multiperson pose estimation from a single RGB image. The framework is the first fully learning-based, camera distance-aware top-down approach, of which components are compatible with most of the previous human detection and 3D human pose estimation models.</p><p>• Our framework outputs the absolute camera-centered coordinates of multiple humans' keypoints. For this, we propose a 3D human root localization network (RootNet). This model makes it easy to extend the 3D single-person pose estimation techniques to the absolute 3D pose estimation of multiple persons.</p><p>• We show that our method significantly outperforms previous 3D multi-person pose estimation methods on several publicly available datasets. Also, it achieves comparable performance with the state-of-the-art 3D single-person pose estimation methods without any groundtruth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>2D multi-person pose estimation. There are two main approaches in the multi-person pose estimation. The first one, top-down approach, deploys a human detector that estimates the bounding boxes of humans. Each detected human area is cropped and fed into the pose estimation network. The second one, bottom-up approach, localizes all human body keypoints in an input image first, and then groups them into each person using some clustering techniques. <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b48">47]</ref> are based on the top-down approach. Papandreou et al. <ref type="bibr" target="#b35">[34]</ref> predicted 2D offset vectors and 2D heatmaps for each joint. They fused the estimated vectors and heatmaps to generate highly localized heatmaps. Chen et al. <ref type="bibr" target="#b7">[6]</ref> proposed a cascaded pyramid network whose cascaded structure refines an initially estimated pose by focusing on hard keypoints. Xiao et al. <ref type="bibr" target="#b48">[47]</ref> used a simple pose estimation network that consists of a deep backbone network and several upsampling layers. <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b39">38]</ref> are based on the bottom-up approach. Cao et al. <ref type="bibr" target="#b4">[3]</ref> proposed the part affinity fields (PAFs) that model the association between human body keypoints. They grouped the localized keypoints of all persons in the input image by using the estimated PAFs. Newell et al. <ref type="bibr" target="#b34">[33]</ref> introduced a pixel-wise tag value to assign localized keypoints to a certain human. Kocabas et al. <ref type="bibr" target="#b22">[21]</ref> proposed a pose residual network for assigning detected keypoints to each person.</p><p>3D single-person pose estimation. Current 3D singleperson pose estimation methods can be categorized into single-and two-stage approaches. The single-stage approach directly localizes the 3D body keypoints from the input image. The two-stage methods utilize the high accuracy of 2D human pose estimation. They initially localize body keypoints in a 2D space and lift them to a 3D space. <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b44">[43]</ref><ref type="bibr" target="#b45">[44]</ref><ref type="bibr" target="#b46">[45]</ref> are based on the single-stage approach. Li et al. <ref type="bibr" target="#b24">[23]</ref> proposed a multi-task framework that jointly trains both the pose regression and body part detectors. Tekin et al. <ref type="bibr" target="#b46">[45]</ref> modeled high-dimensional joint dependencies by adopting an auto-encoder structure. Pavlakos et  al. <ref type="bibr" target="#b38">[37]</ref> extended the U-net shaped network to estimate a 3D heatmap for each joint. They used a coarse-to-fine approach to boost performance. Sun et al. <ref type="bibr" target="#b44">[43]</ref> introduced compositional loss to consider the joint connection structure. Sun et al. <ref type="bibr" target="#b45">[44]</ref> used soft-argmax operation to obtain the 3D coordinates of body joints in a differentiable manner. <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b53">52]</ref> are based on the two-stage approach. Park et al. <ref type="bibr" target="#b36">[35]</ref> estimated the initial 2D pose and utilized it to regress the 3D pose. Martinez et al. <ref type="bibr" target="#b27">[26]</ref> proposed a simple network that directly regresses the 3D coordinates of body joints from 2D coordinates. Zhou et al. <ref type="bibr" target="#b53">[52]</ref> proposed a geometric loss to facilitate weakly supervised learning of the depth regression module with images in the wild. Yang et al. <ref type="bibr" target="#b50">[49]</ref> utilized adversarial loss to handle the 3D human pose estimation in the wild.</p><p>3D multi-person pose estimation. Few studies have been conducted on 3D multi-person pose estimation from a single RGB image. Rogez et al. <ref type="bibr" target="#b41">[40]</ref> proposed a top-down approach called LCR-Net, which consists of localization, classification, and regression parts. The localization part detects a human from an input image, and the classification part classifies the detected human into several anchorposes. The anchor-pose is defined as a pair of 2D and rootrelative 3D pose. It is generated by clustering poses in the training set. Then, the regression part refines the anchorposes. Mehta et al. <ref type="bibr" target="#b30">[29]</ref> proposed a bottom-up approach system. They introduced an occlusion-robust pose-map formulation which supports pose inference for more than one person through PAFs <ref type="bibr" target="#b4">[3]</ref>.</p><p>3D human root localization in 3D multi-person pose estimation. Rogez et al. <ref type="bibr" target="#b41">[40]</ref> estimated both the 2D pose in the image coordinate space and the 3D pose in the cameracentered coordinate space simultaneously. They obtained the 3D location of the human root by minimizing the distance between the estimated 2D pose and projected 3D pose, similar to what Mehta et al. <ref type="bibr" target="#b29">[28]</ref> did. However, this strategy cannot be generalized to other 3D human pose estimation methods because it requires both the 2D and 3D estimations. For example, many works <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b53">52]</ref> estimate the 2D image coordinates and root-relative depth values of keypoints. As their methods do not output root-relative camera-centered coordinates of keypoints, such a distance minimization strategy cannot be used. Moreover, contextual information cannot be exploited because the image feature is not considered. For example, it cannot distinguish between a child close to the camera and an adult far from the camera because their scales in the 2D image is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of the proposed model</head><p>The goal of our system is to recover the absolute camera-centered coordinates of multiple persons' keypoints {P abs j } J j=1 , where J denotes the number of joints. To address this problem, we construct our system based on the top-down approach that consists of DetectNet, Root-Net, and PoseNet. The DetectNet detects a human bounding box of each person in the input image. The RootNet takes the cropped human image from the DetectNet and localizes the root of the human R = (x R , y R , Z R ), in which x R and y R are pixel coordinates, and Z R is an absolute depth value. The same cropped human image is fed to the PoseNet, which estimates the root-relative 3D pose P rel j = (x j , y j , Z rel j ), in which x j and y j are pixel coordinates in the cropped image space and Z rel j is root-relative depth value. We convert Z rel j into Z abs j by adding Z R and transform x j and y j to the original input image space. Then, the final absolute 3D pose {P abs j } J j=1 is obtained by simple back-projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DetectNet</head><p>We use Mask R-CNN <ref type="bibr" target="#b12">[11]</ref> as the framework of De-tectNet. Mask R-CNN <ref type="bibr" target="#b12">[11]</ref> consists of three parts. The first one, backbone, extracts useful local and global features from the input image by using deep residual network (ResNet) <ref type="bibr" target="#b13">[12]</ref> and feature pyramid network <ref type="bibr" target="#b25">[24]</ref>. Based on the extracted features, the second part, region proposal network, proposes human bounding box candidates. The RoIAlign layer extracts the features of each proposal  <ref type="figure">Figure 3</ref>: Correlation between k and real depth value of the human root. Human3.6M <ref type="bibr" target="#b17">[16]</ref> and MuCo-3DHP <ref type="bibr" target="#b30">[29]</ref> datasets were used. r represents Pearson correlation coefficient.</p><p>and passes them to the third part, which is the classification head network. The head network determines whether the given proposal is a human or not and estimates the bounding box refinement offsets. It achieves the state-ofthe-art performance on publicly available object detection datasets <ref type="bibr" target="#b26">[25]</ref>. Due to its high performance and publicly available code <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b28">27]</ref>, we use Mask R-CNN <ref type="bibr" target="#b12">[11]</ref> as a De-tectNet in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RootNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model design</head><p>The RootNet estimates the camera-centered coordinates of the human root R = (x R , y R , Z R ) from a cropped human image. To obtain them, RootNet separately estimates the 2D image coordinates (x R , y R ) and the depth value (i.e., the distance from the camera Z R ) of the human root. The estimated 2D image coordinates are back-projected to the camera-centered coordinate space using the estimated depth value, which becomes the final output.</p><p>Considering that an image provides sufficient information on where the human root is located in the image space, the 2D estimation part can learn to localize it easily. By contrast, estimating the depth only from a cropped human image is difficult because the input does not provide information on the relative position of the camera and human. To resolve this issue, we introduce a new distance measure, k, which is defined as follows:</p><formula xml:id="formula_0">k = α x α y A real A img ,<label>(1)</label></formula><p>where α x , α y , A real , and A img are focal lengths divided by the per-pixel distance factors (pixel) of xand y-axes, the  area of the human in real space (mm 2 ), and image space (pixel 2 ), respectively. k approximates the absolute depth from the camera to the object using the ratio of the actual area and the imaged area of it, given camera parameters. Eq 1 can be easily derived by considering a pinhole camera projection model. The distance d (mm) between the camera and object can be calculated as follows:</p><formula xml:id="formula_1">d = α x l x,real l x,img = α y l y,real l y,img ,<label>(2)</label></formula><p>where l x,real , l x,img , l y,real , l y,img are the lengths of an object in real space (mm) and in image space (pixel), on the x and y-axes, respectively. By multiplying the two representations of d in Eq 2 and taking the square root of it, we can have the 2D extended version of depth measure k in Eq 1. Assuming that A real is constant and using α x and α y from datasets, the distance between the camera and an object can be measured from the area of the bounding box. As we only consider humans, we assume that A real is 2000mm × 2000mm. The area of the human bounding box is used as A img after extending it to fixed aspect ratio (i.e., height:width = 1:1). <ref type="figure">Figure 3</ref> shows that such an approximation provides a meaningful correlation between k and the real depth values of the human root in 3D human pose estimation datasets <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b30">29]</ref>. Although k can represent how far the human is from the camera, it can be wrong in several cases because it assumes that A img is an area of A real (i.e., 2000mm × 2000mm) in the image space when the distance between the human and the camera is k. However, as A img is obtained by extending the 2D bounding box, it can have a different value according to its appearance, although the distance to the camera is the same. For example, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>(a), two humans have different A img although they are at the same distance to the camera. On the other hand, in some cases, A img can be the same, even with different distances from the camera. For example, in <ref type="figure" target="#fig_4">Figure 4</ref>(b), a child and an adult have similar A img however, the child is closer to the camera than the adult. To handle this issue, we design the RootNet to utilize the image feature to correct A img , eventually k. The image feature can give a clue to the RootNet about how much the A img has to be changed. For example, in <ref type="figure" target="#fig_4">Figure 4</ref>(a), the left image can tell the RootNet to increase the area because the human is in a crouching posture. Also, in Figure 4(b), the right image can tell the RootNet to increase the area because the input image contains a child. Specifically, the RootNet outputs the correction factor γ from the image feature. The estimated γ is multiplied by the given A img , which becomes A γ img . From A γ img , k is calculated and it becomes the final depth value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Camera normalization</head><p>Our RootNet outputs correction factor γ only from an input image. Therefore, data from any camera intrinsic parameters (i.e., α x and α y ) can be used during training and testing. We call this property camera normalization, which makes our RootNet very flexible. For example, in the training stage, data from different α x and α y can be used together. Also, in the testing stage, RootNet can be used when α x and α y are not available, likely for in-the-wild images. In this case, α x and α y can be set to any values α x and α y , respectively. Then, estimated Z R represents distance between an object and camera whose α x and α y are α x and α y , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Network architecture</head><p>The network architecture of the RootNet, which comprises three components, is visualized in <ref type="figure" target="#fig_5">Figure 5</ref>. First, a backbone network extracts the useful global feature of the input human image using ResNet <ref type="bibr" target="#b13">[12]</ref>. Second, the 2D image coordinate estimation part takes a feature map from the backbone part and upsamples it using three consecutive deconvolutional layers with batch normalization layers <ref type="bibr" target="#b16">[15]</ref> and ReLU activation function. Then, a 1-by-1 convolution is applied to produce a 2D heatmap of the root. Softargmax <ref type="bibr" target="#b45">[44]</ref> extracts 2D image coordinates x R , y R from the 2D heatmap. The third component is the depth estimation part. It also takes a feature map from the backbone part and applies global average pooling. Then, the pooled feature map goes through a 1-by-1 convolution, which outputs a single scalar value γ. The final absolute depth value Z R is obtained by multiplying k with 1 √ γ . In practice, we implemented the RootNet to output γ = 1 √ γ directly and multiply it with the k to obtain the absolute depth value Z R (i.e., Z R = γ k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Loss function</head><p>We train the RootNet by minimizing the L1 distance between the estimated and groundtruth coordinates. The loss function L root is defined as follows:</p><formula xml:id="formula_2">L root = R − R * 1 ,<label>(3)</label></formula><p>where * indicates the groundtruth.</p><p>6. PoseNet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Model design</head><p>The PoseNet estimates the root-relative 3D pose P rel j = (x j , y j , Z rel j ) from a cropped human image. Many works have been presented for this topic <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b53">52]</ref>. Among them, we use the model of Sun et al. <ref type="bibr" target="#b45">[44]</ref>, which is the current state-of-the-art method. This model consists of two parts. The first part is the backbone, which extracts a useful global feature from the cropped human image using ResNet <ref type="bibr" target="#b13">[12]</ref>. Second, the pose estimation part takes a feature map from the backbone part and upsamples it using three consecutive deconvolutional layers with batch normalization layers <ref type="bibr" target="#b16">[15]</ref> and ReLU activation function. A 1-by-1 convolution is applied to the upsampled feature map to produce the 3D heatmaps for each joint. The soft-argmax operation is used to extract the 2D image coordinates (x j , y j ), and the root-relative depth values Z rel j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Loss function</head><p>We train the PoseNet by minimizing the L1 distance between the estimated and groundtruth coordinates. The loss function L pose is defined as follows:</p><formula xml:id="formula_3">L pose = 1 J J j=1 P rel j − P rel * j 1 ,<label>(4)</label></formula><p>where * indicates groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Implementation details</head><p>Publicly released Mask R-CNN model <ref type="bibr" target="#b28">[27]</ref> pre-trained on the COCO dataset <ref type="bibr" target="#b26">[25]</ref> is used for the DetectNet without fine-tuning on the human pose estimation datasets <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b30">29]</ref>. For the RootNet and PoseNet, PyTorch <ref type="bibr" target="#b37">[36]</ref> is used for implementation. Their backbone part is initialized with the publicly released ResNet-50 <ref type="bibr" target="#b13">[12]</ref> pre-trained on the Ima-geNet dataset <ref type="bibr" target="#b43">[42]</ref>, and the weights of the remaining part are initialized by Gaussian distribution with σ = 0.001. The weights are updated by the Adam optimizer <ref type="bibr" target="#b21">[20]</ref> with a mini-batch size of 128. The initial learning rate is set to 1 × 10 −3 and reduced by a factor of 10 at the 17th epoch. We use 256×256 as the size of the input image of the Root-Net and PoseNet. We perform data augmentation including rotation (±30 • ), horizontal flip, color jittering, and synthetic occlusion <ref type="bibr" target="#b52">[51]</ref> in training. Horizontal flip augmentation is performed in testing for the PoseNet following Sun et al. <ref type="bibr" target="#b45">[44]</ref>. We train the RootNet and PoseNet for 20 epochs with four NVIDIA 1080 Ti GPUs, which took two days, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Dataset and evaluation metric</head><p>Human3.6M dataset. Human3.6M dataset <ref type="bibr" target="#b17">[16]</ref> is the largest 3D single-person pose benchmark. It consists of 3.6 millions of video frames. 11 subjects performing 15 activities are captured from 4 camera viewpoints. The groundtruth 3D poses are obtained using a motion capture system. Two evaluation metrics are widely used. The first one is mean per joint position error (MPJPE) <ref type="bibr" target="#b17">[16]</ref>, which is calculated after aligning the human root of the estimated and groundtruth 3D poses. The second one is MPJPE after further alignment (i.e., Procrustes analysis (PA) <ref type="bibr" target="#b11">[10]</ref>). This metric is called PA MPJPE. To evaluate the localization of the absolute 3D human root, we introduce the mean of the Euclidean distance between the estimated coordinates of the root R and the groundtruth R * , i.e., the mean of the root position error (MRPE), as a new metric:</p><formula xml:id="formula_4">M RP E = 1 N N i=1 ||R (i) − R (i) * || 2 ,<label>(5)</label></formula><p>where superscript i is the sample index, and N denotes the total number of test samples. MuCo-3DHP and MuPoTS-3D datasets. These are the 3D multi-person pose estimation datasets proposed by Mehta et al. <ref type="bibr" target="#b30">[29]</ref>. The training set, MuCo-3DHP, is generated by compositing the existing MPI-INF-3DHP 3D single-person pose estimation dataset <ref type="bibr" target="#b29">[28]</ref>. The test set, MuPoTS-3D dataset, was captured at outdoors and it includes 20 real-world scenes with groundtruth 3D poses for up to three subjects. The groundtruth is obtained with a multi-view marker-less motion capture system. For evaluation, a 3D percentage of correct keypoints (3DPCK rel ) and area under 3DPCK curve from various thresholds (AUC rel ) is used after root alignment with groundtruth. It treats a joint's prediction as correct if it lies within a 15cm from the groundtruth joint location. We additionally define 3DPCK abs which is the 3DPCK without root alignment to evaluate the absolute camera-centered coordinates. To evaluate the localization of the absolute 3D human root, we use  the average precision of 3D human root location (AP root 25 ) which considers a prediction is correct when the Euclidean distance between the estimated and the groundtruth coordinates is smaller than 25cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Experimental protocol</head><p>Human3.6M dataset. Two experimental protocols are widely used. Protocol 1 uses six subjects (S1, S5, S6, S7, S8, S9) in training and S11 in testing. PA MPJPE is used as an evaluation metric. Protocol 2 uses five subjects (S1, S5, S6, S7, S8) in training and two subjects (S9, S11) in testing. MPJPE is used as an evaluation metric. We use every 5th and 64th frames in videos for training and testing, respectively following <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b45">44]</ref>. When training, besides the Human3.6M dataset, we used additional MPII 2D human pose estimation dataset <ref type="bibr" target="#b2">[1]</ref> following <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b53">52]</ref>. Each mini-batch consists of half Human3.6M and half MPII data. For MPII data, the loss value of the z-axis becomes zero for both of the RootNet and PoseNet following Sun et al. <ref type="bibr" target="#b45">[44]</ref>.</p><p>MuCo-3DHP and MuPoTS-3D datasets. Following the previous protocol, we composite 400K frames of which half are background augmented. For augmentation, we use images from the COCO dataset <ref type="bibr" target="#b26">[25]</ref> except for images with humans. We use an additional COCO 2D human keypoint detection dataset <ref type="bibr" target="#b26">[25]</ref> when training our models on the MuCo-3DHP dataset following Mehta et al. <ref type="bibr" target="#b30">[29]</ref>. Each mini-batch consists of half MuCo-3DHP and half COCO data. For COCO data, loss value of z-axis becomes zero for both of the RootNet and PoseNet following Sun et al. <ref type="bibr" target="#b45">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Ablation study</head><p>In this study, we show how each component of our proposed framework affects the 3D multi-person pose estimation accuracy. To evaluate the performance of the Detect-Net, we use the average precision of bounding box (AP box ) following metrics of the COCO object detection benchmark <ref type="bibr" target="#b26">[25]</ref>.   Disjointed pipeline. To demonstrate the effectiveness of the disjointed pipeline (i.e., separated DetectNet, Root-Net, and PoseNet), we compare MRPE, MPJPE, and running time of joint and disjointed learning of the RootNet and PoseNet in <ref type="table" target="#tab_0">Table 1</ref>. The running time includes De-tectNet and is measured using a single TitanX Maxwell GPU. For the joint learning, we combine the RootNet and PoseNet into a single model which shares backbone part (i.e., ResNet <ref type="bibr" target="#b13">[12]</ref>). The image feature from the backbone is fed to each branch of RootNet and PoseNet in a parallel way. Compared with the joint learning, our disjointed learning gives lower error under a similar running time. We believe that this is because each task of RootNet and PoseNet is not highly correlated so that jointly training all tasks can make training harder, resulting in lower accuracy.</p><p>Effect of the DetectNet. To show how the performance of the human detection affects the accuracy of the final 3D human root localization and 3D multi-person pose estimation, we compare AP root 25 , AUC rel , and 3DPCK abs using the DetectNet in various backbones (i.e., ResNet-50 <ref type="bibr" target="#b13">[12]</ref>, ResNeXt-101-32 <ref type="bibr" target="#b49">[48]</ref>) and groundtruth box in the second, third, and fourth row of <ref type="table" target="#tab_1">Table 2</ref>, respectively. The table shows that based on the same RootNet (i.e., Ours), better human detection model improves both of the 3D human root localization and 3D multi-person pose estimation performance. However, the groundtruth box does not improve overall accuracy considerably compared with other Detect-Net models. Therefore, we have sufficient reasons to believe that the given boxes cover most of the person instances with such a high detection AP. We can also conclude that the bounding box estimation accuracy does not have a large impact on the 3D multi-person pose estimation accuracy.</p><p>Effect of the RootNet. To show how the performance of the 3D human root localization affects the accuracy of the 3D multi-person pose estimation, we compare AUC rel and 3DPCK abs using various RootNet settings in <ref type="table" target="#tab_1">Table 2</ref>. The first and second rows show that based on the same DetectNet (i.e., R-50), our RootNet exhibits significantly  higher AP root 25 and 3DPCK abs compared with the setting in which k is directly utilized as a depth value. We use the x and y of the RootNet when the k is used as a depth value. This result demonstrates that the RootNet successfully corrects the k value. The fourth and last rows show that the groundtruth human root provides similar AUC rel , but significantly higher 3DPCK abs compared with our RootNet. This finding shows that better human root localization is required to achieve more accurate absolute 3D multi-person pose estimation results.</p><p>Effect of the PoseNet. All settings in <ref type="table" target="#tab_1">Table 2</ref> provides similar AUC rel . Especially, the first and last rows of the table show that using groundtruth box and human root does not provide significantly higher AUC rel . As the results in the table are based on the same PoseNet, we can conclude that AUC rel , which is an evaluation of the root-relative 3D human pose estimation highly depends on the accuracy of the PoseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Comparison with state-of-the-art methods</head><p>Human3.6M dataset. We compare our proposed system with the state-of-the-art 3D human pose estimation methods on the Human3.6M dataset <ref type="bibr" target="#b17">[16]</ref> in <ref type="table" target="#tab_3">Tables 3 and 4</ref>. As most of the previous methods use the groundtruth information (i.e., bounding boxes or 3D root locations) in inference time, we report the performance of the PoseNet using the groundtruth 3D root location. Note that our full model does not require any groundtruth information in inference time. The tables show that our method achieves comparable performance despite not using any groundtruth information in inference time. Moreover, it significantly outperforms pre-vious 3D multi-person pose estimation methods <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b30">29]</ref>.</p><p>MuCo-3DHP and MuPoTS-3D datasets. We compare our proposed system with the state-of-the-art 3D multi-person pose estimation methods on the MuPoTS-3D dataset <ref type="bibr" target="#b30">[29]</ref> in Tables 5 and 6. The proposed system significantly outperforms them in most of the test sequences and joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Discussion</head><p>Although our proposed method outperforms previous 3D multi-person pose estimation methods by a large margin, room for improvement is substantial. As shown in <ref type="table" target="#tab_1">Table 2</ref>, using the groundtruth 3D root location brings significant 3DPCK abs improvement. Recent advances in depth map estimation from a single RGB image <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b23">22]</ref> can give a clue for improving the 3D human root localization model.</p><p>Our framework can also be used in applications other than 3D multi-person pose estimation. For example, recent methods for 3D human mesh model reconstruction <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b20">19]</ref> reconstruct full 3D mesh model from a single person. Joo et al. <ref type="bibr" target="#b19">[18]</ref> utilized 2D multi-view input for 3D multiperson mesh model reconstruction. In our framework, if the PoseNet is replaced with existing human mesh reconstruction model <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b20">19]</ref>, 3D multi-person mesh model reconstruction can be performed from a single RGB image. This shows our framework can be applied to many 3D instanceaware vision tasks which take a single RGB image as an input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>We propose a novel and general framework for 3D multiperson pose estimation from a single RGB image. Our framework consists of human detection, 3D human root localization, and root-relative 3D single-person pose estimation models. Since any existing human detection and 3D single-person pose estimation models can be plugged into our framework, it is very flexible and easy to use. The proposed system outperforms previous 3D multi-person pose estimation methods by a large margin and achieves compa-rable performance with 3D single-person pose estimation methods without any groundtruth information while they use it in inference time. To the best of our knowledge, this work is the first to propose a fully learning-based camera distance-aware top-down approach whose components are compatible with most of the previous human detection and 3D human pose estimation models. We hope that this study provides a new basis for 3D multi-person pose estimation, which has only barely been explored.</p><formula xml:id="formula_5">l y,real l y,img = α x α y A real A img .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Comparison of 3D human root localization with previous approaches</head><p>We compare previous absolute 3D human root localization methods <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b41">40]</ref> with the proposed RootNet on the Hu-man3.6M dataset <ref type="bibr" target="#b17">[16]</ref> based on protocol 2.</p><p>Previous approaches <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b41">40]</ref> simultaneously estimate 2D image coordinates and 3D camera-centered root-relative coordinates of keypoints. Then, absolute camera-centered coordinates of the human root are obtained by minimizing the distance between 2D predictions and projected 3D predictions. For optimization, linear least-squares formulation is used. To measure the errors of their method, we implemented and used ResNet-152-based model of Sun et al. <ref type="bibr" target="#b45">[44]</ref> as a 2D pose estimator and model of Martinez et al. <ref type="bibr" target="#b27">[26]</ref> as a 3D pose estimator, which are state-of-the-art methods. In addition, to minimize the effect of outliers in 3D-to-2D <ref type="figure">Figure 6</ref>: Visualization of a pinhole camera model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>MRPE MRPE x MRPE y MRPE z Baseline <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b41">40]</ref> 267. <ref type="bibr" target="#b9">8</ref>    fitting, we excluded limb joints when fitting. Also, we performed RANSAC with a various number of joints to get optimal joint set for fitting instead of using heuristically selected joint set. <ref type="table" target="#tab_8">Table 7</ref> shows our RootNet significantly outperforms previous approaches. Furthermore, the RootNet can be designed independently of the PoseNet, giving design flexibility to both models. In contrast, the previous 3D root localization methods <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b41">40]</ref> require both of 2D and 3D predictions for the root localization, which results in lack of generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Running time of the proposed framework</head><p>In <ref type="table" target="#tab_9">Table 8</ref>, we report seconds per frame for each component of our framework. The running time is measured using a single TitanX Maxwell GPU. As the table shows, most of the running time is consumed by DetectNet. It is hard to directly compare running time with previous works <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b41">40]</ref> because they did not report it. However, we guess that there would be no big difference because models of <ref type="bibr" target="#b41">[40]</ref> and <ref type="bibr" target="#b29">[28]</ref> are similar with <ref type="bibr" target="#b40">[39]</ref> and <ref type="bibr" target="#b4">[3]</ref> whose speed is 0.2 and 0.11 Methods S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg Accuracy for all groundtruths Ours 59. <ref type="bibr" target="#b6">5</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Absolute 3D multi-person pose estimation errors</head><p>For the continual study of the 3D multi-person pose estimation, we report 3DPCK abs in <ref type="table" target="#tab_11">Table 9</ref> and 10. As previous works <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b30">29]</ref> did not report 3DPCK abs , we only report our result.  <ref type="bibr" target="#b30">[29]</ref> and COCO <ref type="bibr" target="#b26">[25]</ref> datasets, respectively. Note that COCO dataset consists of in-the-wild images which are hardly included in the 3D human pose estimation training sets <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b30">29]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Qualitative results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall pipeline of the proposed framework for 3D multi-person pose estimation from a single RGB image. The proposed framework can recover the absolute camera-centered coordinates of multiple persons' keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Different area, same distance (b) Same area, different distance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Examples where k fails to represent the distance between a human and the camera because of incorrect A img .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Network architecture of the RootNet. The Root-Net estimates the 3D human root coordinate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figures 7 and 8</head><label>8</label><figDesc>show qualitative results of our 3D multi-person pose estimation framework on the MuPoTS-3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of applying our method on the MuPoTS-3D dataset<ref type="bibr" target="#b30">[29]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results of applying our method on the COCO 2017<ref type="bibr" target="#b26">[25]</ref> validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MRPE, MPJPE, and seconds per frame comparison between joint and disjointed learning on Human3.6M dataset.</figDesc><table><row><cell>Settings</cell><cell></cell><cell></cell><cell>MRPE</cell><cell>MPJPE</cell><cell>Time</cell></row><row><cell cols="2">Joint learning</cell><cell></cell><cell>138.2</cell><cell>116.7</cell><cell>0.132</cell></row><row><cell cols="3">Disjointed learning (Ours)</cell><cell>120.0</cell><cell>57.3</cell><cell>0.141</cell></row><row><cell cols="4">DetectNet RootNet AP box AP root 25</cell><cell cols="2">AUC rel 3DPCK abs</cell></row><row><cell>R-50</cell><cell>k</cell><cell>43.8</cell><cell>5.2</cell><cell>39.2</cell><cell>9.6</cell></row><row><cell>R-50</cell><cell>Ours</cell><cell>43.8</cell><cell>28.5</cell><cell>39.8</cell><cell>31.5</cell></row><row><cell cols="2">X-101-32 Ours</cell><cell>45.0</cell><cell>31.0</cell><cell>39.8</cell><cell>31.5</cell></row><row><cell>GT</cell><cell>Ours</cell><cell>100.0</cell><cell>31.4</cell><cell>39.8</cell><cell>31.6</cell></row><row><cell>GT</cell><cell>GT</cell><cell>100.0</cell><cell>100.0</cell><cell>39.8</cell><cell>80.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall performance comparison for different De-tectNet and RootNet settings on the MuPoTS-3D dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Smo. Phot. Wait Walk WalkD. WalkP. Avg With groundtruth information in inference time Yasin [50] 88.4 72.5 108.5 110.2 97.1 81.6 107.2 119.0 170.8 108.2 142.5 86.9 92.1</figDesc><table><row><cell>Methods</cell><cell>Dir.</cell><cell>Dis.</cell><cell>Eat</cell><cell cols="4">Gre. Phon. Pose Pur.</cell><cell>Sit</cell><cell cols="6">SitD. 165.7</cell><cell cols="2">102.0 108.3</cell></row><row><cell>Chen [5]</cell><cell cols="4">71.6 66.6 74.7 79.1</cell><cell>70.1</cell><cell cols="8">67.6 89.3 90.7 195.6 83.5 93.3 71.2 55.7</cell><cell>85.9</cell><cell>62.5</cell><cell>82.7</cell></row><row><cell>Moreno [32]</cell><cell cols="4">67.4 63.8 87.2 73.9</cell><cell>71.5</cell><cell cols="3">69.9 65.1 71.7</cell><cell>98.6</cell><cell cols="4">81.3 93.3 74.6 76.5</cell><cell>77.7</cell><cell>74.6</cell><cell>76.5</cell></row><row><cell>Zhou [53]</cell><cell cols="4">47.9 48.8 52.7 55.0</cell><cell>56.8</cell><cell cols="3">49.0 45.5 60.8</cell><cell>81.1</cell><cell cols="4">53.7 65.5 51.6 50.4</cell><cell>54.8</cell><cell>55.9</cell><cell>55.3</cell></row><row><cell>Martinez [26]</cell><cell cols="4">39.5 43.2 46.4 47.0</cell><cell>51.0</cell><cell cols="3">41.4 40.6 56.5</cell><cell>69.4</cell><cell cols="4">49.2 56.0 45.0 38.0</cell><cell>49.5</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Sun [43]</cell><cell cols="4">42.1 44.3 45.0 45.4</cell><cell>51.5</cell><cell cols="3">43.2 41.3 59.3</cell><cell>73.3</cell><cell cols="4">51.0 53.0 44.0 38.3</cell><cell>48.0</cell><cell>44.8</cell><cell>48.3</cell></row><row><cell>Fang [7]</cell><cell cols="4">38.2 41.7 43.7 44.9</cell><cell>48.5</cell><cell cols="3">40.2 38.2 54.5</cell><cell>64.4</cell><cell cols="4">47.2 55.3 44.3 36.7</cell><cell>47.3</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell>Sun [44]</cell><cell cols="4">36.9 36.2 40.6 40.4</cell><cell>41.9</cell><cell cols="3">34.9 35.7 50.1</cell><cell>59.4</cell><cell cols="4">40.4 44.9 39.0 30.8</cell><cell>39.8</cell><cell>36.7</cell><cell>40.6</cell></row><row><cell>Ours (PoseNet)</cell><cell cols="4">31.0 30.6 39.9 35.5</cell><cell>34.8</cell><cell cols="3">30.2 32.1 35.0</cell><cell>43.8</cell><cell cols="4">35.7 37.6 30.1 24.6</cell><cell>35.7</cell><cell>29.3</cell><cell>34.0</cell></row><row><cell cols="5">Without groundtruth information in inference time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rogez [41]  *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.7</cell></row><row><cell>Ours (Full)</cell><cell cols="4">32.5 31.5 41.5 36.7</cell><cell>36.3</cell><cell cols="3">31.9 33.2 36.5</cell><cell>44.4</cell><cell cols="4">36.7 38.7 31.2 25.6</cell><cell>37.1</cell><cell>30.5</cell><cell>35.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>PA MPJPE comparison with state-of-the-art methods on the Human3.6M dataset using Protocol 1. * used extra synthetic data for training.</figDesc><table><row><cell>Methods</cell><cell>Dir.</cell><cell>Dis.</cell><cell>Eat</cell><cell cols="3">Gre. Phon. Pose Pur.</cell><cell>Sit</cell><cell cols="2">SitD. Smo. Phot. Wait Walk WalkD. WalkP. Avg</cell></row><row><cell cols="5">With groundtruth information in inference time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chen [5]</cell><cell cols="9">89.9 97.6 90.0 107.9 107.3 93.6 136.1 133.1 240.1 106.7 139.2 106.2 87.0</cell><cell>114.1</cell><cell>90.6 114.2</cell></row><row><cell>Tome [46]</cell><cell cols="4">65.0 73.5 76.8 86.4</cell><cell>86.3</cell><cell cols="4">68.9 74.8 110.2 173.9 85.0 110.7 85.8 71.4</cell><cell>86.3</cell><cell>73.1</cell><cell>88.4</cell></row><row><cell>Moreno [32]</cell><cell cols="9">69.5 80.2 78.2 87.0 100.8 76.0 69.7 104.7 113.9 89.7 102.7 98.5 79.2</cell><cell>82.4</cell><cell>77.2</cell><cell>87.3</cell></row><row><cell>Zhou [53]</cell><cell cols="4">68.7 74.8 67.8 76.4</cell><cell>76.3</cell><cell cols="4">84.0 70.2 88.0 113.8 78.0 98.4 90.1 62.6</cell><cell>75.1</cell><cell>73.6</cell><cell>79.9</cell></row><row><cell>Jahangiri [17]</cell><cell cols="4">74.4 66.7 67.9 75.2</cell><cell>77.3</cell><cell cols="4">70.6 64.5 95.6 127.3 79.6 79.1 73.4 67.4</cell><cell>71.8</cell><cell>72.8</cell><cell>77.6</cell></row><row><cell>Mehta [28]</cell><cell cols="4">57.5 68.6 59.6 67.3</cell><cell>78.1</cell><cell cols="4">56.9 69.1 98.0 117.5 69.5 82.4 68.0 55.3</cell><cell>76.5</cell><cell>61.4</cell><cell>72.9</cell></row><row><cell>Martinez [26]</cell><cell cols="4">51.8 56.2 58.1 59.0</cell><cell>69.5</cell><cell cols="2">55.2 58.1 74.0</cell><cell>94.6</cell><cell>62.3 78.4 59.1 49.5</cell><cell>65.1</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Fang [7]</cell><cell cols="4">50.1 54.3 57.0 57.1</cell><cell>66.6</cell><cell cols="2">53.4 55.7 72.8</cell><cell>88.6</cell><cell>60.3 73.3 57.7 47.5</cell><cell>62.7</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Sun [43]</cell><cell cols="4">52.8 54.8 54.2 54.3</cell><cell>61.8</cell><cell cols="2">53.1 53.6 71.7</cell><cell>86.7</cell><cell>61.5 67.2 53.4 47.1</cell><cell>61.6</cell><cell>63.4</cell><cell>59.1</cell></row><row><cell>Sun [44]</cell><cell cols="4">47.5 47.7 49.5 50.2</cell><cell>51.4</cell><cell cols="2">43.8 46.4 58.9</cell><cell>65.7</cell><cell>49.4 55.8 47.8 38.9</cell><cell>49.0</cell><cell>43.8</cell><cell>49.6</cell></row><row><cell>Ours (PoseNet)</cell><cell cols="4">50.5 55.7 50.1 51.7</cell><cell>53.9</cell><cell cols="2">46.8 50.0 61.9</cell><cell>68.0</cell><cell>52.5 55.9 49.9 41.8</cell><cell>56.1</cell><cell>46.9</cell><cell>53.3</cell></row><row><cell cols="5">Without groundtruth information in inference time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rogez [40]</cell><cell cols="4">76.2 80.2 75.8 83.3</cell><cell>92.2</cell><cell cols="4">79.9 71.7 105.9 127.1 88.0 105.7 83.7 64.9</cell><cell>86.6</cell><cell>84.0</cell><cell>87.7</cell></row><row><cell>Mehta [29]</cell><cell cols="4">58.2 67.3 61.2 65.7</cell><cell>75.8</cell><cell cols="2">62.2 64.6 82.0</cell><cell>93.0</cell><cell>68.8 84.5 65.1 57.6</cell><cell>72.0</cell><cell>63.6</cell><cell>69.9</cell></row><row><cell>Rogez [41]  *</cell><cell cols="4">55.9 60.0 64.5 56.3</cell><cell>67.4</cell><cell cols="2">71.8 55.1 55.3</cell><cell>84.8</cell><cell>90.7 67.9 57.5 47.8</cell><cell>63.3</cell><cell>54.6</cell><cell>63.5</cell></row><row><cell>Ours (Full)</cell><cell cols="4">51.5 56.8 51.2 52.2</cell><cell>55.2</cell><cell cols="2">47.7 50.9 63.3</cell><cell>69.9</cell><cell>54.2 57.4 50.4 42.5</cell><cell>57.5</cell><cell>47.7</cell><cell>54.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>MPJPE comparison with state-of-the-art methods on the Human3.6M dataset using Protocol 2. * used extra synthetic data for training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Sequence-wise 3DPCK rel comparison with state-of-the-art methods on the MuPoTS-3D dataset. * used extra synthetic data for training.</figDesc><table><row><cell>Methods</cell><cell>Hd. Nck. Sho. Elb. Wri. Hip Kn. Ank. Avg</cell></row><row><cell cols="2">Rogez [40] 49.4 67.4 57.1 51.4 41.3 84.6 56.3 36.3 53.8</cell></row><row><cell cols="2">Mehta [29] 62.1 81.2 77.9 57.7 47.2 97.3 66.3 47.6 66.0</cell></row><row><cell>Ours</cell><cell>79.1 92.6 85.1 79.4 67.0 96.6 85.7 73.1 81.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">: Joint-wise 3DPCK rel comparison with state-</cell></row><row><cell>of-the-art methods on the MuPoTS-3D dataset.</cell><cell>All</cell></row><row><cell>groundtruths are used for evaluation.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>MRPE comparisons between previous distance minimization-based approaches<ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b41">40]</ref> and our RootNet on the Human3.6M dataset. MRPE x , MRPE y , and MRPE z represent the mean of the errors in the x, y, and z axes, respectively.</figDesc><table><row><cell>DetectNet</cell><cell>RootNet</cell><cell>PoseNet</cell><cell>Total</cell></row><row><cell>0.120</cell><cell>0.010</cell><cell>0.011</cell><cell>0.141</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Seconds per frame for each component of our framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>44.7 51.4 46.0 52.2 27.4 23.7 26.4 39.1 23.6 18.3 14.9 38.2 26.5 36.8 23.4 14.4 19.7 18.8 25.1 31.5 Accuracy only for matched groundtruths Ours 59.5 45.3 51.4 46.2 53.0 27.4 23.7 26.4 39.1 23.6 18.3 14.9 38.2 29.5 36.8 23.6 14.4 20.0 18.8 25.4 31.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Sequence-wise 3DPCK abs on the MuPoTS-3D dataset. Methods Hd. Nck. Sho. Elb. Wri. Hip Kn. Ank. Avg Ours 37.3 35.3 33.7 33.8 30.4 30.3 31.0 25.0 31.5</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Joint-wise 3DPCK abs on the MuPoTS-3D dataset. All groundtruths are used for evaluation. seconds per frame, respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the Visual Turing Test project (IITP-2017-0-01780) from the Ministry of Science and ICT of Korea.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of "Camera</head><p>Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image"</p><p>In this supplementary material, we present more experimental results that could not be included in the main manuscript due to the lack of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Derivation of Equation 1</head><p>We provide a derivation of Equation 1 of the main manuscript with reference to <ref type="figure">Figure 6</ref> ,which shows a pinhole camera model. The green and blue arrows represent the human root joint centered x and y-axes, respectively. The yellow lines show rays, and c is the hole. d, f , and l sensor are distance between camera and the human root joint (mm), focal length (mm), and the length of human on the image sensor (mm), respectively.</p><p>According to the definition of tan,</p><p>Let p x be per pixel distance factor in x-axis. Then,</p><p>Above equations are also valid in y-axis. Therefore, d = f l y,real l y,sensor = f p y l y,real l y,sensor p y = α y l y,real l y,img , Finally, d = α x α y l x,real l x,img</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehta</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehta</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">2d-3d pose consistency-based conditional random fields for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d human pose esti-mation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generalized procrustes analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A coarsefine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep attention-based classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Hang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-scale aggregation r-cnn for 2d multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihye</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Weaklysupervised transfer for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Georgios Pavlakos, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
