<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangled Non-Local Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>yuecao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangled Non-Local Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics. Code is available at https://github.com/yinmh17/DNL-Semantic-Segmentation, https://github.com/Howal/DNL-Object-Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The non-local block <ref type="bibr" target="#b33">[34]</ref>, which models long-range dependency between pixels, has been widely used for numerous visual recognition tasks, such as object detection, semantic segmentation, and video action recognition. Towards better understanding the non-local block's efficacy, we observe that it can be viewed as a self-attention mechanism for pixel-to-pixel modeling. This self-attention is modeled as the dot-product between the features of two pixels in the embedding space. At first glance, this dot-product formulation represents pairwise relationships. After further consideration, we find that it may encode unary information as well, in the sense that a pixel may have its own independent impact on all other pixels. Based on this perspective, we split the dot-product based attention into two terms: a whitened pairwise term that accounts for the impact of one pixel specifically on another pixel, and a unary term that represents the influence of one pixel generally over all the pixels.</p><p>We investigate the visual properties of each term without interference from the other. Specifically, we train two individual networks, with either the whitened pairwise term or the unary term removed in the standard attention formula of the non-local block. It is found that the non-local variant using the whitened pairwise term alone generally learns within-region relationships (the 2nd row of <ref type="figure" target="#fig_1">Fig. 3</ref>), while the variant using the unary term alone tends to model salient boundaries (the 3rd row of <ref type="figure" target="#fig_1">Fig. 3</ref>). However, the two terms do not learn such clear visual clues when they are both present within a non-local block, as illustrated in the top row of <ref type="figure" target="#fig_0">Fig. 1</ref>. This observation is verified via statistical analysis on the whole validation set. Also, the standard non-local block combining both terms performs even worse than the variant that includes only the unary term (shown in <ref type="table" target="#tab_2">Table 2</ref>). This indicates that coupling the two terms together may be detrimental to the learning of these visual clues, and consequently affects the learning of discriminative features.</p><p>To address this problem, we present the disentangled non-local (DNL) block, where the whitened pairwise and unary terms are cleanly decoupled by using independent Softmax functions and embedding matrices. With this disentangled design, the difficulty in joint learning of the whitened pairwise and unary terms is greatly diminished. As shown in second row of <ref type="figure" target="#fig_0">Fig. 1</ref>, the whitened pairwise term learns clear within-region clues while the unary term learns salient boundaries, even more clearly than what is learned when each term is trained alone.</p><p>The disentangled non-local block is validated through various vision tasks. On semantic segmentation benchmarks, by replacing the standard non-local block with the proposed DNL block with all other settings unchanged, significantly greater accuracy is achieved, with a 2.0% mIoU gain on the Cityscapes validation set, 1.3% mIoU gain on ADE20k, and 3.4% on PASCAL-Context using a ResNet-101 backbone. With few bells and whistles, our DNL obtains state-of-the-art performance on the challenging ADE20K dataset. Also, with a task-specific DNL block, noticeable accuracy improvements are observed on both COCO object detection and Kinetics action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Non-local/self-attention. These terms may appear in different application domains, but they refer to the same modeling mechanism. This mechanism was first proposed and widely used in natural language processing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref> and physical system modeling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. The self-attention / relation module affects an individual element (e.g. a word in a sentence) by aggregating features from a set of elements (e.g. all the words in the sentence), where the aggregation weights are usually determined on embedded feature similarities among the elements. They are powerful in capturing long-range dependencies and contextual information.</p><p>In the computer vision, two pioneering works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> first applied this kind of modeling mechanism to capture the relations between objects and pixels, respectively. Since then, such modeling methods have demonstrated great effectiveness in many vision tasks, such as image classification <ref type="bibr" target="#b21">[22]</ref>, object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15]</ref>, semantic segmentation <ref type="bibr" target="#b38">[39]</ref>, video object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref> and tracking <ref type="bibr" target="#b36">[37]</ref>, and action recognition <ref type="bibr" target="#b33">[34]</ref>. There are also works that propose improvements to self-attention modeling, e.g. an additional relative position term <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, an additional channel attention <ref type="bibr" target="#b12">[13]</ref>, simplification <ref type="bibr" target="#b1">[2]</ref>, and speed-up <ref type="bibr" target="#b22">[23]</ref>.</p><p>This paper also presents an improvement over the basic self-attention / nonlocal neural networks. However, our work goes beyond straightforward application or technical modification of non-local networks in that it also brings a new perspective for understanding this module. Understanding non-local/self-attention mechanisms. Our work is also related to several approaches that analyze the non-local/self-attention mechanism in depth, including the performance of individual terms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">46]</ref> on various tasks. Also, there are studies which seek to uncover what is actually learnt by the non-local/self-attention mechanism in different tasks <ref type="bibr" target="#b1">[2]</ref>.</p><p>This work also targets a deeper understanding of the non-local mechanism, in a new perspective. Beyond improved understanding, our paper presents a more effective module, the disentangled non-local block, that is developed from this new understanding and is shown to be effective on multiple vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Non-local Networks in Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dividing Non-local Block into Pairwise and Unary Terms</head><p>Non-local block <ref type="bibr" target="#b33">[34]</ref> computes pairwise relations between features of two positions to capture long-range dependencies. With x i representing the input features at position i, the output features y i of a non-local block are computed as where Ω denotes the set of all pixels on a feature map of size H × W ; g(·) is the value transformation function with parameter W v ; ω(x i , x j ) is the embedded similarity function from pixel j (referred to as a key pixel) to pixel i (referred to as a query pixel), typically instantiated by an Embedded Gaussian as</p><formula xml:id="formula_0">yi = j∈Ω ω(xi, xj)g (xj) ,<label>(1)</label></formula><formula xml:id="formula_1">ω(xi, xj) = σ q T i kj = exp q T i kj t∈Ω exp (q T i kt) ,<label>(2)</label></formula><p>where q i = W q x i and k j = W k x j denote the query and key embedding of pixel i and j, respectively, and σ(·) denotes the softmax function. At first glance, ω(x i , x j ) (defined in Eq. 2) appears to represent only a pairwise relationship in the non-local block, through a dot product operation. However, we find that it may encode some unary meaning as well. Considering a special case where the query vector is a constant over all image pixels, a key pixel will have global impact on all query pixels. In <ref type="bibr" target="#b1">[2]</ref>, it was found that non-local blocks frequently degenerate into a pure unary term in several image recognition tasks where each key pixel in the image has the same similarity with all query pixels. These findings indicate that the unary term does exist in the non-local block formulation. It also raises a question of how to divide Eq. (2) into pairwise and unary terms, which account for the impact of one key pixel specifically on another query pixel and the influence of one key pixel generally over all the query pixels, respectively.</p><p>To answer this question, we first present a whitened dot product between key and query to represent the pure pairwise term:</p><formula xml:id="formula_2">q i − µ q T (k j − µ k ), where µ q = 1 |Ω| i∈Ω q i and µ k = 1</formula><p>|Ω| j∈Ω k j are the averaged query and key embedding over all pixels, respectively. To remove the unary/global component of key pixels, the whitened dot product is determined by maximizing the normalized differences between query and key pixels. In following proposition, we show how this can be achieved via an optimization objective, which allows for the whitened dot product to be computed. </p><formula xml:id="formula_3">Proposition 1: α * = 1 |Ω| i∈Ω q i , β * = 1</formula><p>|Ω| m∈Ω k m is the optimal solution of the following optimization objective:</p><formula xml:id="formula_4">arg max α,β i,m,n∈Ω (qi − α) T (km − β) − (qi − α) T (kn − β) 2 i∈Ω ((qi − α) T (qi − α)) · m,n∈Ω ((km − kn) T (km − kn)) + m,i,j∈Ω (km − β) T (qi − α) − (km − β) T (qj − α) 2 m∈Ω ((km − β) T (km − β)) · i,j∈Ω ((qi − qj) T (qi − qj))<label>(3)</label></formula><p>Proof sketch: The Hessian of the objective function O with respect to α and β is a non-positive definite matrix. The optimal α * and β * are thus the solutions of the following equations: ∂O ∂α = 0, ∂O ∂β = 0. Solving this yields α * = 1 |Ω| i∈Ω q i , β * = 1 |Ω| m∈Ω k m . Please see the appendix for a detailed proof. By extracting the whitened dot product as the pure pairwise term, we can divide the dot product computation of the standard non-local block as</p><formula xml:id="formula_5">q T i kj = qi − µ q T (kj − µ k ) + µ T q kj + q T i µ k + µ T q µ k .<label>(4)</label></formula><p>Note that the last two terms (q T i µ k and µ T q µ k ) are factors that appear in both the numerator and denominator of Eq. (2). Hence, these two terms can be eliminated (see proof in the Appendix). After this elimination, we reach the following pairwise and unary split of a standard non-local block:</p><formula xml:id="formula_6">ω(xi, xj) = σ(q T i kj) = σ( qi − µ q T (kj − µ k ) pairwise + µ T q kj unary ),<label>(5)</label></formula><p>where the first whitened dot product term represents the pure pairwise relation between a query pixel i and a key pixel j, and the second term represents the unary relation where a key pixel j has the same impact on all query pixels i. To study what visual clues are expected to be learnt by the pairwise and unary terms, respectively, we construct two variants of the non-local block by using either the pairwise or unary term alone, such that the influence of the other term is eliminated. The two variants use the following similarity computation functions instead of the one in Eq. <ref type="formula" target="#formula_1">(2)</ref>:</p><formula xml:id="formula_7">ωp (xi, xj) = σ qi − µ q T (kj − µ k ) ,<label>(6)</label></formula><formula xml:id="formula_8">ωu (xi, xj) = σ(µ T q kj).<label>(7)</label></formula><p>The two variants are denoted as "pairwise NL" and "unary NL", and illustrated in <ref type="figure">Fig. 2</ref>(b) and 2(c), respectively. We apply these two variants of non-local block to the Cityscapes semantic segmentation <ref type="bibr" target="#b7">[8]</ref> (see Section 5.1 for detailed settings), and visualize their learnt attention (similarity) maps on several randomly selected validation images in Cityscapes, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref> (please see more examples in the Appendix). It can be seen that the pairwise NL block tends to learn pixel relationships within the same category region, while the unary NL block tends to learn the impact from boundary pixels to all image pixels. This observation is further verified by quantitative analysis using the groundtruth region and boundary annotations in Cityscapes. Denote P (i) ={ω p (x i , x j )| j ∈ Ω} ∈ R H×W as the attention map of pixel i according to the pairwise term of Eq. (6), U ={ω u (x i , x j )|j ∈ Ω} ∈ R H×W as the attention map for all query pixels according to the unary term of Eq. <ref type="formula" target="#formula_8">(7)</ref>, C (i) ∈ R H×W as the binary withincategory region map of pixel i, and E ∈ R H×W as the binary boundary map indicating pixels with distance to ground truth contour of less than 5 pixels.</p><p>We evaluate the consistency between attention maps A ∈ {P (i) , U } and ground-truth boundary/same-category region G ∈ {C (i) , E} by their overlaps:</p><formula xml:id="formula_9">A ∩ G = j∈Ω A j G j ,<label>(8)</label></formula><p>where A j , G j are the element values of the corresponding attention map and binary map at pixel j, respectively. <ref type="table" target="#tab_1">Table 1</ref> shows the averaged consistency measures of the attention maps in Eq. (6) and Eq. <ref type="formula" target="#formula_8">(7)</ref> to ground-truth region maps (denoted as pairwise NL and unary NL) using all 500 validation images in the Cityscapes datasets. We also report the consistency measures by a random attention map for reference (denoted as random). The following can be seen:</p><p>-The attention map by the pairwise NL block of Eq. (6) has significantly larger overlap with the ground-truth same-category region than the random attention map (0.635 vs. 0.259), but has similar overlap with the groundtruth boundary region (0.141 vs. 0.132), indicating that the pure pairwise term tends to learn relationship between pixels within same-category regions. -The attention map by the unary NL block of Eq. <ref type="formula" target="#formula_8">(7)</ref> has significantly larger overlap with the ground-truth boundary region than the random attention map (0.460 vs. 0.135), indicating that the unary term tends to learn the impact of boundary pixels on all image pixels. This is likely because the image boundary area provides the most informative cues when considering the general effect on all pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Does the Non-local Block Learn Visual Clues Well?</head><p>We then study the learnt pairwise and unary terms by the non-local block. We follow Eq. (5) to split the standard similarity computation into the pairwise and unary terms, and normalize them by a softmax operation separately. After splitting and normalization, we can compute their overlaps with the ground-truth within-category region map and boundary region map, as shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>It can be seen that the pairwise term in the standard NL block which is jointly learnt with the unary term has significantly smaller overlap with the ground-truth within-category region than in the pairwise NL block where the pairwise term is learnt alone (0.318 vs. 0.635). It can be also seen that the unary term in the standard NL block which is jointly learnt with the pairwise term has significantly smaller overlap with the ground-truth boundary region than in the unary NL block where the unary term is learnt alone (0.172 vs. 0.460). These results indicate that neither of the pairwise and unary terms learn the visual clues of within-category regions and boundaries well, as also demonstrated in <ref type="figure" target="#fig_0">Fig. 1 (top)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Why the Non-Local Block Does Not Learn Visual Clues Well?</head><p>To understand why the non-local block does not learn the two visual clues well, while the two terms alone can clearly learn them, we rewrite Eq. (5) as:</p><formula xml:id="formula_10">σ(qi · kj) = σ qi − µ q T (kj − µ k ) + µ T q kj = 1 λi σ qi − µ q T (kj − µ k ) · σ(µ T q kj) = 1 λi ωp(xi, xj) · ωu(xi, xj),<label>(9)</label></formula><p>where λ i is a normalization scalar such that the sum of attention map values over Ω is 1.</p><p>Consider the back-propagation of loss L to the pairwise and unary terms:</p><formula xml:id="formula_11">∂L ∂σ(ωp) = ∂L ∂σ(ω) · ∂σ(ω) ∂σ(ωp) = ∂L ∂σ(ω) · σ(ωu), ∂L ∂σ(ωu) = ∂L ∂σ(ω) · ∂σ(ω) ∂σ(ωu) = ∂L ∂σ(ω) · σ(ωp).</formula><p>It can be seen that both gradients are determined by the value of the other term. When the value of the other term becomes very small (close to 0), the gradient of this term will be also very small, thus inhibiting the learning of this term. For example, if we learn the unary term to well represent the boundary area, the unary attention weights on the non-boundary area will be close to 0 and the pairwise term at the non-boundary area would thus be hard to learn well due to the vanishing gradient issue. On the other hand, if we learn the pairwise term to well represent the within-category area, the unary attention weights on the boundary area will be close to 0 and the pairwise term at the non-boundary area would also be hard to learn well due to the same vanishing gradient issue. Another problem is the shared key transformation W k used in both the pairwise and unary terms, causing the computation of the two terms to be coupled. Such coupling may introduce additional difficulties in learning the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Disentangled Non-local Neural Networks</head><p>In this section, we present a new non-local block, named disentangled non-local (DNL) block, which effectively disentangles the learning of pairwise and unary terms. In the following sections, we first describe how we modify the standard non-local (NL) block into a disentangled non-local (NL) block, such that the two visual clues described above can be learnt well. Then we analyze its actual behavior in learning visual clues using the method in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Formulation</head><p>Our first modification is to change the multiplication in Eq. (9) to addition:</p><formula xml:id="formula_12">ω(xi, xj) = ωp(xi, xj) · ωu(xi, xj) ⇒ ω(xi, xj) = ωp(xi, xj) + ωu(xi, xj).<label>(10)</label></formula><p>The gradients of these two terms are</p><formula xml:id="formula_13">∂L ∂σ(ωp) = ∂L ∂σ(ω) , ∂L ∂σ(ωu) = ∂L ∂σ(ω) .</formula><p>So the gradients of each term will not be impacted by the other. The second modification is to change the transformation W k in unary term to be an independent linear transformation W m with output dimension of 1:</p><formula xml:id="formula_14">µ T q kj = µ T q W k xj ⇒ mj = Wmxj.<label>(11)</label></formula><p>After this modification, the pairwise and unary terms will no longer share the W k transformation, which further decouples them. DNL formulation. With these two modifications, we obtain the following similarity computation for the disentangled non-local (DNL) block:</p><formula xml:id="formula_15">ω D (xi, xj) = σ qi − µ q T (kj − µ k ) + σ(mj).<label>(12)</label></formula><p>The resulting DNL block is illustrated in <ref type="figure">Fig. 2 (d)</ref>. Note that we adopt a single value transform for both pairwise and unary terms, which is similarly effective on benchmarks as using independent value transform but with reduced complexity. DNL variants for diagnostic purposes. To diagnose the effects of the two decoupling modifications alone, we consider the following two variants:</p><formula xml:id="formula_16">ω D * (xi, xj) = σ qi − µ q T (kj − µ k ) + mj ,<label>(13)</label></formula><formula xml:id="formula_17">ω D † (xi, xj) = σ qi − µ q T (kj − µ k ) + σ(µ T q kj),<label>(14)</label></formula><p>which each involves only one of the two modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Behavior of DNL on Learning Visual Clues</head><p>We compute the overlaps of the pairwise and unary attention maps in DNL (Eq. 12) with the ground-truth within-category region map and boundary region map, as shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>It can be seen that the pairwise term in DNL has significantly larger overlap with the ground-truth within-category region than the one in the standard NL block (0.759 vs. 0.318), and the unary term has significantly larger overlap with the boundary region than that in the standard NL block (0.696 vs. 0.172). These results indicate better learning of the two visual clues by the DNL block in comparison to the standard NL block.</p><p>Compared with the blocks which learn the pairwise or unary terms alone (see the "pairwise NL" and "unary NL" rows), such measures are surprisingly 0.124 and 0.236 higher with DNL. We hypothesize that when one term is learned alone, it may encode some portion of the other clue, as it is also useful for inference. By explicitly learning both terms, our disentangling design can separate one from the other, allowing it to better extract these visual clues.</p><p>We then verify the effects of each disentangling modification by these measures. By incorporating the "disentangled transformation" modification alone (ω * ) as in Eq. (13), it achieves 0.446 and 0.305 on within-category modeling and boundary modeling, respectively, which is marginally better than the standard non-local block. By incorporating the "'multiplication to addition" modification alone (ω † ) as in Eq. <ref type="formula" target="#formula_0">(14)</ref>, it achieves 0.679 and 0.657 on within-category modeling and boundary modeling, respectively.</p><p>The results indicate that the two modifications both benefit the learning of two visual clues and work better if combined together. The improvements in visual clue modeling by two disentangling strategies are also illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Note such disentangling strategies also effect on other tasks beyond semantic segmentation. In object detection and action recognition tasks, we also observe clearer learnt visual clues by the DNL block than by the standard NL. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, while in NL the pairwise term is almost hindered by the unary term (also observed by <ref type="bibr" target="#b1">[2]</ref>), the pariwise term in DNL shows clear within-region meaning and appears significant in the final overall attention maps. The unary term in DNL also shows more focus to salient regions (not limited to boundaries which is different from that observed in the semantic segmentation task) than the one in an NL block. More examples will be shown in appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the proposed DNL method on the recognition tasks of semantic segmentation, object detection/instance segmentation, and action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic Segmentation</head><p>Datasets. We use three benchmarks for semantic segmentation evaluation.</p><p>Cityscapes <ref type="bibr" target="#b7">[8]</ref> focuses on semantic understanding of urban street scenes. It provides a total of 5,000 finely annotated images, which is divided into 2,975/500/ 1,525 images for training, validation and testing. Additional 20,000 coarsely annotated images are also provided. The dataset contains annotations for over 30 classes, of which 19 classes are used in evaluation.  <ref type="bibr" target="#b27">[28]</ref> is a set of additional annotations for PASCAL VOC 2010, which label more than 400 categories of 4,998 images for training and 5,105 images for validation. For semantic segmentation, 59 semantic classes and 1 background class are used in training and validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL-Context</head><p>Architecture. We follow recent practice <ref type="bibr" target="#b22">[23]</ref> by using dilated FCN <ref type="bibr" target="#b26">[27]</ref> and a ResNet101 <ref type="bibr" target="#b18">[19]</ref> backbone for our major segmentation experiments. The strides and dilations of 3 × 3 convolutions are set to 1 and 2 for stage4, and 1 and 4 for stage5. The baseline model uses a segmentation head consisting of a 3 × 3 convolution layer to reduce the channels to 512 and a subsequent classifier to predict the final segmentation results. For experiments with a non-local or a disentangled non-local block, the block is inserted right before the final classifier.</p><p>Training and Inference. The implementation and hyper-parameters mostly follow <ref type="bibr" target="#b22">[23]</ref>. The SGD optimizer with poly learning rate policy (1 − ( iter itermax ) 0.9 ) is employed. For Cityscapes, the networks are trained on 4 GPUs with 2 images per GPU for 60K iterations. The initial learning rate is 0.01, the weight decay is 0.0005. Input images are cropped to 769 × 769. For ADE20K, the networks are trained on 8 GPUs with 2 images per GPU for 150K iterations. The initial learning rate is 0.02, and the weight decay is 0.0001. Input images are cropped to 520 × 520. For PASCAL-Context, the network is trained on 4 GPUs with 4 images per GPU for 30K iterations. The initial learning rate is 0.001, and the weight decay is 0.0001. Input images are cropped to 520 × 520. For all datasets, the data is augmented with random horizontal flipping, random scaling within [0.5, 2.0], and random brightness jittering of [−10, 10]. Following <ref type="bibr" target="#b38">[39]</ref>, online hard example mining (OHEM) and an auxiliary loss on the output of conv4 with a weight of 0.5 are employed for Cityscapes and ADE20K, only auxiliary loss is employed for PASCAL-Context.</p><p>We mostly follow <ref type="bibr" target="#b22">[23]</ref> in inference. For Cityscapes, we sample 769 × 769 windows for inference and their results are fused to generate the prediction of an entire image. For other datasets, we resize the image resolution to be the same as in training and a multi-scale test is adopted.  <ref type="bibr" target="#b43">[44]</ref> ResNet-101 80.1 DANet <ref type="bibr" target="#b12">[13]</ref> ResNet-101 81.5 HRNet <ref type="bibr" target="#b30">[31]</ref> HRNetV2-W48 81.9 SeENet <ref type="bibr" target="#b28">[29]</ref> ResNet-101 81.2 SPGNet <ref type="bibr" target="#b6">[7]</ref> ResNet-101 81. Ablation Study We ablate several design components in the proposed disentangled non-local block on the Cityscapes validation set. A ResNet-101 backbone is adopted for all ablations. DNL variants. The disentangled non-local block has two decoupling modifications on the standard non-local block: multiplication to addition, and separate key transformations. In addition to comparing the full DNL model with the standard non-local model, we also conduct experiments for these two variants which include only one of the decoupling modifications.</p><p>The results are shown in <ref type="table" target="#tab_2">Table 2</ref>(a). While the standard non-local model brings 2.7% mIoU gains over a plain ResNet-101 model (78.5% vs. 75.8%), by replacing the standard non-local block by our disentangled non-local block, we achieve an additional 2.0% mIoU gain over the standard non-local block (80.5% vs. 78.5%), with almost no complexity increase.</p><p>The variants that use each decoupling strategy alone achieve 0.5% and 0.7% mIoU gains over the standard non-local block (79.0 vs. 78.5 and 79.2 vs. 78.5), showing that both strategies are beneficial alone. They are also both crucial, as combining them leads to significantly better performance than using each alone. Effects of pairwise and unary term alone. Table 2(b) compares the methods using the pairwise term or unary term alone. Using the pairwise term alone achieves 77.5% mIoU, which is 1.7% better than the baseline plain network without it. Using the unary term alone achieves 79.3% mIoU, which is 3.5% better than the baseline plain network and even 0.8% mIoU better than the standard non-local network which models both pairwise and unary terms. These results indicate that the standard non-local block hinders the effect of the unary term, probably due to the coupling of two kinds of relationships. Our disentangled non-local networks effectively disentangle the two terms, and thus can better exploit their effects to achieve a higher accuracy of 80.5% mIoU.</p><p>Complexities. As discussed in Section 4.1, the time and space complexity of the DNL model over the NL model is tiny. <ref type="table" target="#tab_6">Table 5</ref> show the FLOPs and actual latency (single-scale inference using a single GPU) on semantic segmentation, using a ResNet-101 backbone and input resolution of 769 × 769.</p><p>Comparison with other methods. Results on Cityscapes. <ref type="table" target="#tab_3">Table 3</ref> shows comparison results for the proposed disentangled non-local network on the Cityscapes test set. Using a ResNet-101 backbone, the disentangled non-local network achieves 82.0% mIoU, 1.2% better than that of a standard non-local network. On a stronger backbone of HRNetV2-W48, the disentangled non-local network achieves 0.5% better accuracy than a standard non-local network. Considering that the standard non-local network has 0.6% mIoU improvement over a plain HRNetV2-W48 network, such additional gains are significant. Results on ADE20K. <ref type="table" target="#tab_5">Table 4</ref> shows comparison results of the proposed disentangled non-local network on the ADE20k benchmark. Using a ResNet-101 backbone, the disentangled non-local block achieves 45.97% and 56.23% on the validation and test sets, respectively, which are 1.30% and 0.65% better than the counterpart networks using a standard non-local block. Our result reveals a new SOTA on this benchmark. On a HRNetV2-W48 backbone, the DNL block is 1.0% and 0.38% better than a standard non-local block. Note on ADE20K, HRNetV2-W48 backbone does not perform better than a ResNet-101 backbone, which is different with the other datasets.   Results on PASCAL-Context. <ref type="table" target="#tab_3">Table 3</ref> shows comparison results of the proposed disentangled non-local network on the PASCAL-Context test set. On ResNet-101, our method improves the standard non-local method significantly, by 3.4% mIoU (53.7 vs. 50.3). On HRNetV2-W48, our DNL method is 1.1% mIoU better, which is significant considering that the NL method has 0.2% improvements over the plain counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Object Detection/Segmentation and Action Recognition</head><p>Object Detection and Instance Segmentation on COCO.</p><p>We adopt the open source mmdetection <ref type="bibr" target="#b3">[4]</ref> codebase for experiments. Following <ref type="bibr" target="#b33">[34]</ref>, the nonlocal variants are inserted right before the last residual block of c4. We use the standard configuration of Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> with FPN and ResNet-50 as the backbone architecture, and report the mean average-precision scores at different boxes and the mask IoUs on the COCO2017 validation set. The input images are resized such that their shorter side is 800 pixels <ref type="bibr" target="#b25">[26]</ref>. We trained on 4 GPUs with 4 images per GPU (effective mini batch size of 16). The backbones of all models are pretrained on ImageNet classification <ref type="bibr" target="#b8">[9]</ref>, then all layers except for stage1 and stage2 are jointly fine-tuned. In training, synchronized BatchNorm is adopted, and the learning rate scheduler follows the 1× settings of 12 epochs in <ref type="bibr" target="#b17">[18]</ref> where the initial learning rate is 0.02 and decayed by a factor of 10 at the 8 th and 11 th epoch. The weight decay is 0.0001 and momentum is 0.9. <ref type="table" target="#tab_7">Table 6</ref> shows comparisons of different methods. While the standard non-local block outperforms the baseline counterpart by 0.8% bbox mAP and 0.7% mask mAP, the proposed disentangled non-local block brings an additional 0.7% bbox mAP and 0.6% mask mAP in gains. Please also see Appendix for experiments when stacking more non-local or disentangled non-local blocks. Action Recognition on Kinetics. We adopt the Kinetics <ref type="bibr" target="#b23">[24]</ref> dataset for experiments, which includes ∼240k training videos and 20k validation videos in 400 human action categories. All models are trained on the training set, and we report the top-1 (%) and top-5 (%) accuracy on the validation set. We adopt the slow-only baseline in <ref type="bibr" target="#b11">[12]</ref>, the best single model to date that can utilize weights inflated <ref type="bibr" target="#b2">[3]</ref> from the ImageNet pretrained model. All the experiment settings follow the slow-only baseline in <ref type="bibr" target="#b11">[12]</ref>, where 8 frames (8 × 8) are used as input, and 30-clip validation is adopted. Following <ref type="bibr" target="#b33">[34]</ref>, we insert (disentangled) non-local blocks after every two residual blocks. <ref type="table" target="#tab_8">Table 7</ref> shows the comparison of different blocks. It can be seen that the disentangled design performs 0.36% better than using standard non-local block. Noting the gains of the standard non-local block over the baseline is 1.0, the relative gains of disentangled non-local block over a standard NL block is 36%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we first study the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term and a unary term. Via both intuitive and statistical analysis, we find that the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design for learning visual clues on various vision tasks, such as semantic segmentation, object detection and action recognition.</p><p>45. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More NL/DNL blocks for COCO Object Detection</head><p>In section 5.2 of the main paper, we follow the settings in <ref type="bibr" target="#b33">[34]</ref> where 1 nonlocal (NL) or disentangled non-local (DNL) block is inserted right before the last residual block of c4. In this section, we investigate the performance of NL and DNL when more attention blocks are inserted into the backbone, as shown in <ref type="table" target="#tab_10">Table 8</ref>. While the proposed DNL method outperforms NL method by 0.7% bbox mAP and 0.6% mask mAP when 1 attention block is inserted into the backbone (denoted as "c4 one"), the gains brought by the proposed DNL method over the NL method are enlarged to 1.2% bbox mAP and 0.9% mask mAP, respectively, when every residual block of stage c5 is followed by 1 attention block (denoted as "c4 all"). The gains are further enlarged to 1.3% bbox mAP and 1.3% mask mAP when additionally every residual block of stage c4 is followed by 1 attention block (denoted as "c4 c5 all"). These results indicate that the DNL method can benefit more from increasing block number than the NL method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed Proof of Proposition 1</head><p>The object function O(α, β) in Eq. (3) of the main paper can be rewritten as</p><formula xml:id="formula_18">O(α, β) = i∈Ω (qi − α) T A(qi − α) i∈Ω ((qi − α) T (qi − α)) + m∈Ω (km − β) T B(km − β) m∈Ω ((km − β) T (km − β))<label>(15)</label></formula><p>where A = m,n∈Ω (km − kn)(km − kn) T m,n∈Ω (km − kn) T (km − kn)</p><formula xml:id="formula_19">B = i,j∈Ω (qi − qj)(qi − qj) T i,j∈Ω (qi − qj) T (qi − qj)<label>(16)</label></formula><p>We first prove that all eigenvalues of matrix A and B are smaller or equal than 1. Denote the eigenvalues of matrix A as λ 1 , ..., λ d . According to CauchySchwarz inequality, we have Given Eq. (17), we have:</p><formula xml:id="formula_20">∀1 ≤ i ≤ d, λ i ≤ 1.</formula><p>Similarly, we can prove all eigenvalues of matrix B are smaller or equal than 1. The hessian matrix of Eq. (1) with respect to α and β are non-positive definite matrix. The optimalα * and β * are thus the solutions of the following equations: ∂O ∂α = 0, ∂O ∂β = 0. For α * , we have</p><formula xml:id="formula_21">∂O ∂α * = Np i=1 2 m,n (km − kn)(km − kn) T m,n (km − kn) T (km − kn) − 1 (qi − α * ) = 0, ⇔ m,n (km − kn)(km − kn) T m,n (km − kn) T (km − kn) − 1 Np i=1 2(qi − α * ) = 0.<label>(18)</label></formula><p>To satisfy Eqn. 18, we have:</p><formula xml:id="formula_22">Np i=1 (qi − α * ) = 0.<label>(19)</label></formula><p>The optimal α * is thus</p><formula xml:id="formula_23">α * = 1 Np Np i=1 qi.<label>(20)</label></formula><p>Similarly, the optimal β * is computed as</p><formula xml:id="formula_24">β * = 1 Np Np i=1</formula><p>ki.</p><p>C Proof for Eqn. 4 in the main paper</p><p>Here, we provide a proof for Eqn. 4 in Section 3.1. The dot product of query q i and key k j can be split into several terms by introducing a whitening operation on the key and query:</p><formula xml:id="formula_26">q T i kj = qi − µ q T (kj − µ k ) + µ T q kj + q T i µ k + µ T q µ k ,<label>(22)</label></formula><p>where µ q and µ k denote 1 Np Np i=1 q i and 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Np</head><p>Np i=1 k j , respectively. Note that the last two terms (q T i µ k and µ T q µ k ) are factors in common with both the numerator and denominator of the correlation function f and the normalization factor C, so they can be eliminated as follows:</p><formula xml:id="formula_27">exp q T i kj Np t=1 exp (q T i kt) = exp qi − µ q T (kj − µ k ) + µ T q kj + q T i µ k + µ T q µ k Np t=1 exp qi − µ q T (kt − µ k ) + µ T q kt + q T i µ k + µ T q µ k = exp qi −µ q T (kj −µ k )+µ T q kj exp q T i µ k +µ T q µ k Np t=1 exp qi −µ q T (kt −µ k )+µ T q kt exp q T i µ k +µ T q µ k = exp qi − µ q T (kj − µ k ) + µ T q kj Np t=1 exp qi − µ q T (kt − µ k ) + µ T q kt .<label>(23)</label></formula><p>Finally, we obtain</p><formula xml:id="formula_28">σ(q T i kj) = σ( qi − µ q T (kj − µ k ) pairwise + µ T q kj unary ).<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Examples of Learnt Attention Maps by NL/DNL Methods</head><p>In this section, we show more examples of the learnt attention maps by the NL/DNL methods on the Cityscapes semantic segmentation, COCO object detection/instance segmentation and Kinetics action recognition tasks. <ref type="figure">Fig. 5</ref> show more examples of the learnt attention maps by NL/DNL on Cityscapes. With DNL block, the whitened pairwise term learns clear withinregion clues while the unary term learns salient boundaries, which cannot be observed in that of the original NL block. <ref type="figure">Fig. 6</ref> show more examples of the learnt attention maps by NL/DNL on COCO object detection/instance segmentation. It can be seen that the attention maps of NL block are mainly dominated by the unary term that different query points (marked in red) have similar overall attention maps. In DNL, the pariwise term in DNL shows clear within-region meaning and appears significant in the final overall attention maps, that different query points have different overall attention maps. DNL also shows more focus to salient regions than the one in an NL block. <ref type="figure" target="#fig_6">Fig. 7</ref> show more examples of the learnt attention maps by NL/DNL on Kinetics action recognition task. 4 frames in a video clip are visualized. The unary term of DNL shows better focus to salient regions than the one in an NL block. The pairwise term in DNL also shows clearer within-region meaning than that in an NL block.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Equal contribution. This work is done when Minghao Yin and Zhuliang Yao are interns at MSRA. arXiv:2006.06668v2 [cs.CV] 8 Sep 2020 Visualization of attention maps in the non-local block and our disentangled nonlocal block. With the disentanglement of our non-local block, the whitened pairwise term learns clear within-region clues while the unary term learns salient boundaries, which cannot be observed with the original non-local block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization of attention maps for all variants of the NL block mentioned in this paper. Column 1: image, ground truth and edges of ground truth. Columns 2-5: attention maps of pairwise terms. Column 6: attention maps of unary terms. As NLu has no pairwise attention map, and NLp has no unary attention map, we leave the corresponding spaces empty</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of attention maps in NL and our DNL block on COCO object detection and Kinetics action recognition. The query points are marked in red. Please refer to appendix for more examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Complexity. For an input feature map of C × H × W , we follow [34] by using C/2 dimensional key and query vectors. The space and time complexities are O D (space) = (2C + 1)C and O D (time) = (2C + 1)C + ( 3 2 C + 2)HW HW , respectively. For reference, the space and time complexity of a standard non-local block are O(space) = 2C 2 and O(time) = 2C 2 + ( 3 2 C + 1)HW HW , respectively. The additional space and computational overhead of the disentangled non-local block over a standard non-local block is marginal, specifically 0.1% and 0.15% for C = 512 in our semantic segmentation experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 i d λ 2 i 2 = 2 m</head><label>1222</label><figDesc>= tr(A T A) = tr m,n∈Ω (km − kn)(km − kn) T · s,t∈Ω (ks − kt)(ks − kt) T m,n∈Ω (km − kn) T (km − kn) · s,t∈Ω (ks − k T t )(ks − kt) = m,n,s,t∈Ω (km − kn) T (ks − kt) · tr (km − kn)(ks − kt) T m,n∈Ω (km − kn) T (km − kn) m,n,s,t∈Ω (km − kn) T (ks − kt) ,n∈Ω (km − kn) T (km − kn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Visualization of attention maps of NL block and our DNL block on Cityscapes Dataset. The query points are marked in white cross Visualization of attention maps of NL block and our DNL block on COCO object detection task. The query points are marked in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of attention maps of NL block and our DNL block on Kinetics action recognition. 4 frames of a video clip are visualized. For each sample of each block, two different queries are chosen as the top and bottom rows. The query points are marked in red</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Consistency statistics between attention maps of the non-local variants and the ground-truth within-category and boundary maps on the Cityscapes validation set method pair ∩ within-category pair ∩ boundary unary ∩</figDesc><table><row><cell>boundary</cell></row></table><note>3.2 What Visual Clues are Expected to be Learnt by Pairwise and Unary Terms?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the validation set of Cityscapes</figDesc><table><row><cell cols="3">(a) Decoupling strategy</cell><cell></cell><cell cols="4">(b) Pairwise and unary terms</cell></row><row><cell></cell><cell cols="3">mul → add non-shared Wk mIoU</cell><cell></cell><cell cols="3">pairwise term unary term mIoU</cell></row><row><cell>Baseline NL DNL  † (14) DNL*(13) DNL</cell><cell>-× √ × √</cell><cell>-× × √ √</cell><cell>75.8 78.5 79.2 79.0 80.5</cell><cell>Baseline NL NLp NLu DNL</cell><cell>-√ √ × √</cell><cell>-√ × √ √</cell><cell>75.8 78.5 77.5 79.3 80.5</cell></row><row><cell cols="8">ADE20K [45] was used in the ImageNet Scene Parsing Challenge 2016 and</cell></row><row><cell cols="8">covers a wide range of scenes and object categories. It contains 20K images for</cell></row><row><cell cols="8">training, 2K images for validation, and another 3K images for testing. It includes</cell></row><row><cell cols="4">150 semantic categories for evaluation.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with state-of-the-art approaches on the Cityscapes test set</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>ASPP</cell><cell>Coarse</cell><cell>mIoU (%)</cell></row><row><cell>PSANet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparisons with state-of-the-art approaches on the validation set and test set of ADE20K, and test set of PASCAL-Context</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">ADE20K val mIoU (%) test mIoU (%)</cell><cell>PASCAL-Context test mIoU (%)</cell></row><row><cell>PSANet [44]</cell><cell>ResNet-101</cell><cell>43.77</cell><cell>55.46</cell><cell>-</cell></row><row><cell>CCNet [23]</cell><cell>ResNet-101</cell><cell>45.22</cell><cell>-</cell><cell>-</cell></row><row><cell>OCNet [39]</cell><cell>ResNet-101</cell><cell>45.45</cell><cell>-</cell><cell>-</cell></row><row><cell>SVCNet [11]</cell><cell>ResNet-101</cell><cell>-</cell><cell>-</cell><cell>53.2</cell></row><row><cell>EMANet [25]</cell><cell>ResNet-101</cell><cell>-</cell><cell>-</cell><cell>53.1</cell></row><row><cell cols="2">HRNetV2 [31] HRNetV2-W48</cell><cell>42.99</cell><cell>-</cell><cell>54.0</cell></row><row><cell>EncNet [41]</cell><cell>ResNet-101</cell><cell>44.65</cell><cell>55.67</cell><cell>52.6</cell></row><row><cell>DANet [13]</cell><cell>ResNet-101</cell><cell>45.22</cell><cell>-</cell><cell>52.6</cell></row><row><cell>CFNet [42]</cell><cell>ResNet-101</cell><cell>44.89</cell><cell>-</cell><cell>54.0</cell></row><row><cell>ANN [47]</cell><cell>ResNet-101</cell><cell>45.24</cell><cell>-</cell><cell>52.8</cell></row><row><cell>DMNet [17]</cell><cell>ResNet-101</cell><cell>45.50</cell><cell>-</cell><cell>54.4</cell></row><row><cell>ACNet [14]</cell><cell>ResNet-101</cell><cell>45.90</cell><cell>55.84</cell><cell>54.1</cell></row><row><cell>NL</cell><cell>ResNet-101</cell><cell>44.67</cell><cell>55.58</cell><cell>50.6</cell></row><row><cell>DNL (ours)</cell><cell>ResNet-101</cell><cell>45.97</cell><cell>56.23</cell><cell>54.8</cell></row><row><cell>NL</cell><cell>HRNetV2-W48</cell><cell>44.82</cell><cell>55.60</cell><cell>54.2</cell></row><row><cell>DNL (ours)</cell><cell>HRNetV2-W48</cell><cell>45.82</cell><cell>55.98</cell><cell>55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Complexity</figDesc><table><row><cell></cell><cell></cell><cell cols="2">comparisons</cell></row><row><cell></cell><cell cols="3">#param(M) FLOPs(G) latency(s/img)</cell></row><row><cell>baseline</cell><cell>70.960</cell><cell>691.06</cell><cell>0.177</cell></row><row><cell>NL</cell><cell>71.484</cell><cell>765.07</cell><cell>0.192</cell></row><row><cell>DNL</cell><cell>71.485</cell><cell>765.16</cell><cell>0.194</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Results based on Mask R-CNN, using R50 as backbone with FPN, for object detection and instance segmentation on COCO 2017 validation set AP bbox AP bbox 50</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>AP bbox 75</cell><cell cols="2">AP mask AP mask 50</cell><cell>AP mask 75</cell></row><row><cell cols="2">baseline 38.8</cell><cell>59.3</cell><cell>42.5</cell><cell>35.1</cell><cell>56.2</cell><cell>37.9</cell></row><row><cell>NL</cell><cell>39.6</cell><cell>60.3</cell><cell>43.2</cell><cell>35.8</cell><cell>57.1</cell><cell>38.5</cell></row><row><cell>NLp</cell><cell>39.8</cell><cell>60.4</cell><cell>43.7</cell><cell>35.9</cell><cell>57.3</cell><cell>38.4</cell></row><row><cell>NLu</cell><cell>40.1</cell><cell>60.9</cell><cell>43.8</cell><cell>36.1</cell><cell>57.6</cell><cell>38.7</cell></row><row><cell>DNL</cell><cell>40.3</cell><cell>61.2</cell><cell>44.1</cell><cell>36.4</cell><cell>58.0</cell><cell>39.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Results</figDesc><table><row><cell></cell><cell></cell><cell>based on Slow-</cell></row><row><cell cols="3">only baseline using R50 as back-</cell></row><row><cell cols="3">bone on Kinetics validation set</cell></row><row><cell></cell><cell cols="2">Top-1 Acc Top-5 Acc</cell></row><row><cell>baseline</cell><cell>74.94</cell><cell>91.90</cell></row><row><cell>NL</cell><cell>75.95</cell><cell>92.29</cell></row><row><cell>NLp</cell><cell>76.01</cell><cell>92.28</cell></row><row><cell>NLu</cell><cell>75.76</cell><cell>92.44</cell></row><row><cell>DNL</cell><cell>76.31</cell><cell>92.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 633-641 (2017) 46. Zhu, X., Cheng, D., Zhang, Z., Lin, S., Dai, J.: An empirical study of spatial attention mechanisms in deep networks. In: The IEEE International Conference on Computer Vision (ICCV) (October 2019) 47. Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X.: Asymmetric non-local neural networks for semantic segmentation. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 593-602 (2019)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Results with more NL and DNL blocks based on Mask R-CNN, using R50 as backbone with FPN, for object detection and instance segmentation on COCO 2017 validation set AP bbox AP bbox 50</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>AP bbox 75</cell><cell cols="2">AP mask AP mask 50</cell><cell>AP mask 75</cell></row><row><cell>baseline</cell><cell>38.8</cell><cell>59.3</cell><cell>42.5</cell><cell>35.1</cell><cell>56.2</cell><cell>37.9</cell></row><row><cell>NL (c4 one)</cell><cell>39.6</cell><cell>60.3</cell><cell>43.2</cell><cell>35.8</cell><cell>57.1</cell><cell>38.5</cell></row><row><cell>NL (c5 all)</cell><cell>40.0</cell><cell>62.1</cell><cell>43.5</cell><cell>36.1</cell><cell>58.6</cell><cell>38.6</cell></row><row><cell cols="2">NL (c4c5 all) 40.1</cell><cell>62.3</cell><cell>43.5</cell><cell>36.0</cell><cell>58.9</cell><cell>38.3</cell></row><row><cell cols="2">DNL (c4 one) 40.3</cell><cell>61.2</cell><cell>44.1</cell><cell>36.4</cell><cell>58.0</cell><cell>39.1</cell></row><row><cell cols="2">DNL (c5 all) 41.2</cell><cell>62.7</cell><cell>44.7</cell><cell>37.0</cell><cell>59.5</cell><cell>39.5</cell></row><row><cell cols="2">DNL (c4c5 all) 41.4</cell><cell>63.2</cell><cell>45.3</cell><cell>37.3</cell><cell>59.8</cell><cell>39.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03906</idno>
		<title level="m">Massive exploration of neural machine translation architectures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11492</idno>
		<title level="m">Gcnet: Non-local networks meet squeezeexcitation networks and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory enhanced global-local aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spgnet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5218" to="5228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relation distillation networks for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8885" to="8894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03982</idno>
		<title level="m">Slowfast networks for video recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6748" to="6757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Progressive sparse local attention for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic multi-scale filters for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3562" to="3572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vain: Attentional multi-agent predictive modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2701" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Relation networks for object detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Local relation networks for image recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9167" to="9176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards bridging semantic gap to improve semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4230" to="4239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An analysis of attention mechanisms: The case of word sense disambiguation in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07595</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual interaction networks: Learning a physics simulator from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatial-temporal relation networks for multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6798" to="6807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Psanet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
