<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PENet: Towards Precise and Efficient Image Guided Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuling</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Ning</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
						</author>
						<title level="a" type="main">PENet: Towards Precise and Efficient Image Guided Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image guided depth completion is the task of generating a dense depth map from a sparse depth map and a high quality image. In this task, how to fuse the color and depth modalities plays an important role in achieving good performance. This paper proposes a two-branch backbone that consists of a color-dominant branch and a depth-dominant branch to exploit and fuse two modalities thoroughly. More specifically, one branch inputs a color image and a sparse depth map to predict a dense depth map. The other branch takes as inputs the sparse depth map and the previously predicted depth map, and outputs a dense depth map as well. The depth maps predicted from two branches are complimentary to each other and therefore they are adaptively fused. In addition, we also propose a simple geometric convolutional layer to encode 3D geometric cues. The geometric encoded backbone conducts the fusion of different modalities at multiple stages, leading to good depth completion results. We further implement a dilated and accelerated CSPN++ to refine the fused depth map efficiently. The proposed full model ranks 1st in the KITTI depth completion online leaderboard at the time of submission. It also infers much faster than most of the top ranked methods. The code of this work is available at https://github.com/JUGGHM/PENet_ICRA2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image guided depth completion aims to predict a dense depth map from a sparse one with the guidance of a highresolution color image. This task has been attracting considerable research interest due to its importance in various computer vision applications, such as autonomous driving, 3D reconstruction, and augmented reality. A sparse depth map is usually obtained by projecting 3D point clouds collected by ranging sensors like LiDARs in outdoor environments. However, even if a high-end LiDAR is employed, the projected depth maps are still highly sparse and also noisy around object boundaries. These defects make depth completion a challenging problem.</p><p>To address this problem, a wide variety of methods have been developed. Recent approaches are mainly based on deep convolutional neural networks. Considering that color and depth are two different modalities, most previous methods adopt two-branch network architectures in order to fuse the two modalities. For instance, Jaritz et al. <ref type="bibr" target="#b0">[1]</ref> and Hua et al. <ref type="bibr" target="#b1">[2]</ref> use two encoders to extract features from each modality separately and then fuse them into one decoder. Tang et al. <ref type="bibr" target="#b2">[3]</ref> construct two encoder-decoder networks to extract color and depth features and take a decoder-encoder fusion scheme. In these networks, each branch inputs only one modality and therefore only late fusion is considered.</p><p>Two-branch architectures are also constructed in some works, such as FusionNet <ref type="bibr" target="#b3">[4]</ref> and DeepLiDAR <ref type="bibr" target="#b4">[5]</ref>, to perform both early and late fusion. FusionNet <ref type="bibr" target="#b3">[4]</ref> consists of two branches to extract local and global information respectively. DeepLiDAR <ref type="bibr" target="#b4">[5]</ref> is a network composed of a color pathway and a surface normal pathway. In these networks, each branch takes two modalities as inputs and the multi-modality fusion is performed at multiple stages. By this means, better fusion can be achieved, which further results in better depth completion performance. However, these two methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> require extra datasets, such as Cityscapes <ref type="bibr" target="#b5">[6]</ref> or synthetic data <ref type="bibr" target="#b4">[5]</ref>, to pretrain their networks.</p><p>Inspired by above-mentioned methods, our work constructs a two-branch network, which consists of a colordominant (CD) branch and a depth-dominant (DD) branch, as the backbone. In contrast to FusionNet <ref type="bibr" target="#b3">[4]</ref> and DeepL-iDAR <ref type="bibr" target="#b4">[5]</ref>, we design the branches for different purposes. More specifically, the CD branch aims to extract colordominant information for depth prediction. It inputs a color image and a sparse depth map and produces a dense depth map. Since this branch is color-dominant, the predicted depth map is relatively reliable around object boundaries but may be too sensitive to the change of color or texture. The DD branch takes as inputs a sparse depth map and the CD depth prediction to produce a dense depth map, which overall is reliable but suffered from the heavy noise existing near object boundaries in the sparse input. It means that the depth maps predicted from two branches are complementary to each other. Therefore, we adaptively fuse them with learned confidence weights. This backbone is able to exploit and fuse color and depth modalities thoroughly. It can also be trained from scratch without using extra datasets.</p><p>In addition, we also propose a simple geometric convolutional layer to encode 3D geometric cues. It simply augments a convolutional layer via concatenating a 3D position map to the layer's input. Assisted with this geometric encoding scheme, our backbone achieves promising performance. Considering that the accurate depth values from the sparse input may not be preserved after prediction, we additionally integrate a module based on CSPN++ <ref type="bibr" target="#b6">[7]</ref> to refine the depth map predicted by our backbone. We design a dilated and accelerated implementation of CSPN++ to make the refinement more effective and efficient.</p><p>The main contributions of our work are summarized as follows:</p><p>• We construct a two-branch backbone that produces  dense depth prediction via exploiting color-and depthdominant information, respectively, from two branches. This backbone is able to exploit and fuse color and depth modalities thoroughly. <ref type="bibr">•</ref> We propose a geometric convolutional layer to simply encode 3D geometric cues. The geometric encoded backbone outperforms most top ranked and peerreviewed methods. • We design an implementation way to accelerate the depth refinement technique CSPN++, making it much more efficient. • The proposed full model ranks 1st in the KITTI depth completion online leaderboard 1 at the time of submission. Moreover, it infers much more efficiently than most of the top ranked methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth Completion</head><p>Depth completion aims to produce a dense depth map by completing a sparse depth map, without <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> or with the guidance of a reference image <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The latter takes advantage of structure information from the guidance image to boost performance and therefore attracts more research interests. The image guided depth completion task has specific challenges, including 1) the input depth map is irregularly sparse and noisy; 2) the color image and the depth map are two different modalities. To address these issues, different sparse invariant convolutions <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, uncertainty exploration <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b3">[4]</ref> and multi-modality fusion strategies <ref type="bibr" target="#b2">[3]</ref> have been developed. Besides, various recent methods also exploit multi-scale features <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, surface normal <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref>, semantic information <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, or context affinity <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref> to improve performance further. Among these methods, we take a two-branch architecture similar to <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref> as our backbone. But we construct our branches for different purposes and our network is more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Geometric Encoding</head><p>As pointed out in <ref type="bibr" target="#b19">[20]</ref>, 3D geometric clues are important for depth completion. So far various strategies have been developed to encode geometric cues. For instance, Uber-ATG <ref type="bibr" target="#b19">[20]</ref> applies continuous convolution on 3D points, ACMNet <ref type="bibr" target="#b20">[21]</ref> exploits the graph propagation, DeepLi-DAR <ref type="bibr" target="#b4">[5]</ref> and PwP <ref type="bibr" target="#b16">[17]</ref> use surface normal to introduce geometric constraints. These methods are either complicated in computation or need extra data for learning. In this work, we propose a geometric convolutional layer to encode 3D geometric cues simply. Our method is inspired by Coord-Conv <ref type="bibr" target="#b21">[22]</ref>, which encodes 2D position information by simply augmenting an input of a convolution with extra coordinate channels. CoordConv <ref type="bibr" target="#b21">[22]</ref> has demonstrated its effectiveness in position-sensitive applications such as object segmentation <ref type="bibr" target="#b22">[23]</ref> and semantic segmentation <ref type="bibr" target="#b23">[24]</ref>. Our experiments show that the proposed geometric convolutional layer can considerably improve the depth completion performance but CoordConv <ref type="bibr" target="#b21">[22]</ref> is not helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Propagation Networks</head><p>The spatial propagation network (SPN) is proposed by Liu et al. <ref type="bibr" target="#b24">[25]</ref> to learn local affinities that can be exploited in various high-level vision tasks. However, it propagates in a column-wise and row-wise manner, which is inefficient. Cheng et al. <ref type="bibr" target="#b10">[11]</ref> thereby propose a convolutional spatial propagation network (CSPN) for efficiency and meanwhile apply it to refine depth completion results. These two methods perform propagation within a fixed local neighborhood. To dynamically learn the convolutional kernels, CSPN++ <ref type="bibr" target="#b6">[7]</ref> and NLSPN <ref type="bibr" target="#b18">[19]</ref> are proposed very recently. The former adaptively learns the convolutional kernel size and iteration number for propagation, while the latter learns deformable kernels. These SPN methods are effective to refine depth predictions but still not so efficient. We adopt CSPN++ <ref type="bibr" target="#b6">[7]</ref> for our depth refinement, but we introduce a dilation scheme to enlarge the neighborhoods and implement the propagation in a much more efficient way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>We design an end-to-end learning framework for image guided depth completion. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the entire framework consists of a two-branch backbone and a depth refinement module. In the backbone, one branch is colordominant, which predicts a dense depth map mainly relying on color information. The other is depth-dominant, which also predicts a dense depth map but depending more on depth information. The depth maps predicted from two branchs are adaptively fused with learned confidence weights. The fused map is further fed into the refinement module to enhance the depth quality. In this module, we adopt the CSPN++ <ref type="bibr" target="#b6">[7]</ref> technique but make it more effective and efficient via a dilated and accelerated implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Two-branch Backbone</head><p>The two-branch backbone is designed to thoroughly exploit color-dominant and depth-dominant information from their respective branches and make the fusion of two modalities effective. To this end, we build similar encoder-decoder networks in two branches to perform a color-dominant depth prediction and a depth-dominant depth prediction.</p><p>The color-dominant branch initially aims to predict a dense depth map from a color image. For the purpose of effectiveness, an aligned sparse depth map is also input to assist depth prediction. In this branch, we build an encoder-decoder network with symmetric skip connections. The encoder contains one convolution layer and ten basic residual blocks, i.e. ResBlocks <ref type="bibr" target="#b25">[26]</ref>. The decoder has five deconvolution layers and one convolution layer. Each of all convolutional layers are followed by a BN layer and a ReLU activation. Although both a color image and a sparse depth map are input, this branch extracts color-dominant features for depth prediction so that the depth around object boundaries can be learned by taking advantage of structure information in the color image.</p><p>The depth-dominant branch initially aims to predict a dense depth map by upsampling a sparse one. In this branch, a similar encoder-decoder network is constructed. We additionally adopt a decoder-encoder fusion strategy <ref type="bibr" target="#b2">[3]</ref> to fuse the color-dominant features into this branch. Specifically, the decoder features of the color-dominant branch are concatenated with the corresponding encoder features in the depthdominant branch. In addition, the depth prediction result obtained from the CD branch is also input to this branch. By this means, the features of color and depth modalities are fused at multiple stages.</p><p>Depth fusion. As two dense depth maps are predicted, we fuse them by following the same strategy in FusionNet <ref type="bibr" target="#b3">[4]</ref>. Formally, we denote the depth maps obtained from two branches byD cd andD dd respectively, and the confidence maps by C cd and C dd . The fused depth map is obtained bŷ</p><formula xml:id="formula_0">D f (u, v) = e C cd (u,v) ·D cd (u, v) + e C dd (u,v) ·D dd (u, v) e C cd (u,v) + e C dd (u,v) ,<label>(1)</label></formula><p>in which (u, v) denotes a pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Geometric Convolutional Layer</head><p>As pointed out by <ref type="bibr" target="#b19">[20]</ref>, 3D geometric clues are important for depth completion. In this work, we propose a geometric convolutional layer to encode the 3D geometric information. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, it simply augments a conventional convolutional layer via concatenating a 3D position map to the layer's input. The position map (X,Y, Z) is derived from an original sparse depth map by</p><formula xml:id="formula_1">Z = D, X = (u − u 0 )Z f x , Y = (v − v 0 )Z f y ,<label>(2)</label></formula><p>in which (u, v) are the coordinates of a pixel and u 0 , v 0 , f x , f y are intrinsic parameters of a camera. In this work, we replace each convolutional layer within each of the ResBlocks by the proposed geometric convolutional layer. In addition, the sparse depth map is min-pooled to obtain Z at smaller scales. By this means, 3D geometric information can be better encoded into features in both colorand depth-dominant branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Dilated and Accelerated CSPN++</head><p>As shown in <ref type="bibr" target="#b10">[11]</ref>, depth maps produced by a deep neural network may not preserve the input depth values at valid pixels. To recover the depth values at valid pixels, we adopt CSPN++ <ref type="bibr" target="#b6">[7]</ref> to refine the depth map predicted by our backbone. Based on CSPN++, we design two modifications to make it more effective and efficient. First, we introduce a dilation strategy similar to the well known dilated convolutions <ref type="bibr" target="#b26">[27]</ref> to enlarge the propagation neighborhoods. Second, we design an implementation that makes the propagation from each neighbor truly parallel, which greatly accelerates the propagation procedure.</p><p>Hereby, we briefly introduce our implementation for acceleration. Formally, we denote a coarse depth map by D 0 . The spatial propagation network produces a refined depth map D t after t iterations. For pixel i, at each iteration it aggregates information propagated from pixels within its neighborhood N (i). That is,</p><formula xml:id="formula_2">D t+1 i = W ii D 0 i + ∑ j∈N (i) W ji D t j ,<label>(3)</label></formula><p>where W ji denotes the affinity between pixel i and pixel j. This equation is defined pixel-wise. For the purpose of efficiency, we convert it to a tensor-level operation. Considering a neighborhood of k ×k size, from the network we learn k ×k number of affinity maps, each of which represents the affinity of one certain neighbor to all pixels. Then, each affinity map needs to be translated along the opposite direction of the corresponding neighbor for alignment. Taking a 3 × 3 neighborhood as an example, we use nine one-hot convolutional kernels to implement these translations, as shown in <ref type="figure">Figure 3</ref>. We denote a translation operator by T (A x , x), which moves an affinity map A x along −x direction. Then, the spatial propagation defined in Equation <ref type="formula" target="#formula_2">(3)</ref> is equivalent to the following one:</p><formula xml:id="formula_3">D t+1 = T (A 0 , 0)T (D 0 , 0) + ∑ x∈N T (A x , x)T (D t , x) (4)</formula><p>By using one-hot convolutional kernels, our implementation of the translations can be performed parallelly. Moreover, the transformed propagation defined in Equation <ref type="formula">(4)</ref> can be implemented much more efficient than the pixel-wise one.   <ref type="figure">Fig. 3</ref>: Illustration of our accelerated implementation. Each affinity map is translated by convolving with a corresponding one-hot convolutional kernel. The propagation is then conducted at tensor-level and fully parallel.</p><formula xml:id="formula_4">∑ 0 +1 = (1,1) (1,0) (1, −1) (0,1) (0,0) (0, −1) (−1,1) (−1,0) (−1, −1) A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Training Loss</head><p>We use a 2 loss for training, which is defined by</p><formula xml:id="formula_5">L(D) = (D − D gt ) 1(D gt &gt; 0) 2 .<label>(5)</label></formula><p>Here,D is the predicted depth map, D gt is a ground truth for supervision, 1() is an indicator, and is an element-wise multiplication. Since the ground truth contains invalid pixels, we only consider those having valid depth values.</p><p>In the early epochs of training, supervision is also placed to the intermediate depth prediction results. That is,</p><formula xml:id="formula_6">L = L(D) + λ cd L(D cd ) + λ dd L(D dd ),<label>(6)</label></formula><p>where λ cd and λ dd are two hyper-parameters empirically set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>Dataset:</p><p>We evaluate the proposed model and its variants on the KITTI depth completion dataset <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b7">[8]</ref>. It provides both color images and aligned sparse depth maps that are obtained by projecting 3D LiDAR points to corresponding image frames. The images are in the resolution of 1216 × 352. A sparse depth map has about 5% valid pixels and a ground truth dense depth map have around 16% valid pixels <ref type="bibr" target="#b7">[8]</ref>. The dataset contains 86K frames for training, together with 7K validation frames and 1K test frames. In the validation set, 1K frames are officially selected <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b7">[8]</ref>. We use the 1K validation set for ablation studies.</p><p>Evaluation metrics: As the common practice, we adopt four metrics for performance evaluation, which are root mean squared error (RMSE [mm]), mean absolute error (MAE [mm]), root mean squared error of the inverse depth (iRMSE [1/km]), and mean absolute error of the inverse depth (iMAE [1/km]). Besides, the runtime of inference is also reported.</p><p>Implementation details: The proposed model is implemented with the PyTorch <ref type="bibr" target="#b28">[29]</ref> framework and trained on two NVIDIA GTX 2080Ti GPUs. During training, we use the ADAM optimizer <ref type="bibr" target="#b29">[30]</ref> with β 1 = 0.9, β 2 = 0.99, and the weight decay is 10 −6 . We adopt a multi-stage training strategy to train the backbone, DA-CSPN++, and the full model progressively. First, the backbone is trained with a batch size of 6 and an initial learning rate of 0.001, decayed by { 1 2 , 1 5 , 1 10 } at epoch {10, 15, 25}, for 30 epochs. The loss defined in Equation <ref type="formula" target="#formula_6">(6)</ref> is used, with λ cd = λ dd = 0.2 at initial epochs and reduced to 0 later. Then, we freeze the weights in the backbone and train DA-CSPN++ with a batch size of 6 and a learning rate of of 0.001 for 2 epochs. Finally, we train the full model with an initial learning rate of 0.02 and 0.002, respectively, for the weights in the backbone and DA-CSPN++. In this stage, the training procedure lasts for 75 epochs, and the learning rate is decayed by { 1 2 , 1 5 , 1 10 , 1 20 , 1 50 } at epoch {10, 20, 30, 40, 50}. We randomly crop images to 576 × 160 and set the batch size to 10, making it feasible to train effectively with limited computational resources. In addition, data augmentation techniques including horizontal random flip and color jitter <ref type="bibr" target="#b30">[31]</ref> are adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>We first conduct a series of experiments to validate the effectiveness of each component proposed in our method, including the two-branch backbone, the geometric convolutional layer, and the DA-CPSN++ module.</p><p>The effectiveness of the two-branch backbone. According to whether input a sparse depth map into the color-dominant branch and input the CD-depth prediction into the depthdominant branch or not, we obtain four variants of the backbone. The performance of these variants, denoted by B 1 to B 4 , are presented in <ref type="table" target="#tab_3">Table I</ref>. The results show that the performance is greatly improved when the CD branch is assisted with the sparse depth input and the DD branch is    guided with the CD-depth prediction. In addition, we investigate one more backbone variant (B 5 ), which additionally produces a guidance map from the first branch to guide the second one, as done in FusionNet <ref type="bibr" target="#b3">[4]</ref> and DeepLiDAR <ref type="bibr" target="#b4">[5]</ref>.</p><p>The results show that this additional guidance map is not necessary and it even slightly hurts the performance. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates some typical examples. From the CDand DD-depth predictions and their confidence maps, we make the following observations. 1) Overall, DD-depth maps contribute more to fused depth maps in most regions. 2)</p><p>The CD-depth predictions rely heavily on color information so that they are sensitive to the change of color or texture, as shown in road markings, grass and tree leaves. In these regions, CD-depth predictions have even lower confidences.</p><p>3) Color images have sharp object boundaries while the input sparse depth maps are noisy around object boundaries. These result in CD-depth predictions having higher confidence along object boundaries than other regions. convolutional layer in the ResBlocks by our proposed geometric convolutional layer and get the model B 4 +GCL. Besides, we compare this component to the CoordConv layer <ref type="bibr" target="#b21">[22]</ref> that encodes pixel coordinates, together with another variant that encodes the depth only. These two variants are denoted as B 4 +CCL and B 4 +DCL respectively. As shown in <ref type="table" target="#tab_3">Table I</ref>, our geometric convolutional layer improves the backbone's performance on RMSE by a great margin. B 4 +DCL also helps for performance. However, encoding pixel coordinates (B 4 +CCL) may slightly hurt the performance, contradicting to its performance in other position sensitive vision tasks <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_5">Figure 5</ref> demonstrates a typical example to show the difference of model B 4 +GCL and its counterparts. The model with geometric convolutional layers can infer better depth information than the other models especially when a foreground object looks similar to background in color, as the road sign shown in marked red boxes.</p><p>The effectiveness of DA-CSPN++. Based on the backbone model B 4 , we integrate variants of CSPN++ to compare their performance. The total number of iterations for propagation is 12. C1 stands for original CSPN++, with a dilation rate (dr) of 1 for all iterations. C2 stands for the model that takes dr = 2 for first six iterations and dr = 1 for the remaining iterations. C4 is the model taking dr = {4, 2, 1} for every four iterations, respectively. As shown in <ref type="table" target="#tab_3">Table I</ref>, all variant models can greatly improve the backbone's performance and the model B 4 +C2 slightly outperforms the other two counterparts. <ref type="table" target="#tab_3">Table II</ref> lists the time taken by the spatial propagation in the depth refinement module, tested on our single 2080Ti GPU. The results show that our accelerated implementation greatly reduces the running time.</p><p>SPN Models Acceleration Propagation Time NLSPN <ref type="bibr" target="#b18">[19]</ref> -0.055s C1 <ref type="bibr" target="#b6">[7]</ref> 0.091s C2 0.186s </p><formula xml:id="formula_7">C1 √ 0.014s C2 √ 0.015s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-arts</head><p>The proposed method ranks 1st in the KITTI online leaderboard at the time of submission. In table III, we present the quantitative performance of our full method (referred to as PENet), together with the other top 10 methods that have published or archived papers. The results show that our method has a significant improvement on RMSE, which is the most important metric for evaluation. We also test our geometric encoded backbone without depth refinement (referred to as ENet). The results show that this model outperforms 9 top ranked methods, including all of these <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b31">[32]</ref> using spatial propagation techniques.</p><p>Table III also provides two runtimes. Runtime1 is quoted from the leaderboard. To make a fair comparison, Runtime2 is tested on our single 2080Ti GPU with source codes released by the authors. The results indicate that our full model infers faster than 8 methods. Especially, it runs much faster than those <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b31">[32]</ref> that utilize spatial propagation techniques as well.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a method for image guided depth completion. By revisiting two-branch architectures developed in previous works, we propose a new twobranch architecture that exploit color-and depth-dominant information, respectively, from two branches. The designed backbone, together with the proposed geometric convolutional layer, can exploit and fuse multi-modality information thoroughly. In addition, we integrate a speedup DA-CSPN++ module for further depth refinement. The entire model is precise and efficient, as tested in the KITTI online leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An overview of the proposed framework. It consists of a two-branch backbone and a depth refinement module. The branches predict two dense depth maps, denoted as CD-Depth and DD-Depth respectively, from color-dominant and depth-dominant information. CD-Depth and DD-Depth are adaptively fused and further refined by a dilated and accelerated (DA) CSPN++. Here (1)-(5) denotes multi-scale CD-features which are then concatenated with DD-features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Comparison of convolutional layers. A geometric convolutional layer augments a convolutional layer by concatenating three extra channels, including X, Y , and Z, to the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Illustrations of typical examples. The outputs, including CD-Depth, DD-Depth, CD-Confidence, DD-Confidence, and fused depth maps, are obtained by the geometric encoded backbone (the model B 4 +GCL). We also provide the refined depth maps obtained by our full model B 4 +GCL+C2 for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>The effectiveness of the geometric convolutional layer.Based on the backbone model B 4 , we further replace each A typical example to illustrate the difference of various encoding strategies. Compared to standard convolution(CL) and CoordConv(CCL), our geometric convolutional layer (GCL) infers better depth information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Performance on the KITTI depth completion validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Runtime of the depth refinement modules.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Comparisons to state-of-the-art methods on the KITTI test set, ranked by RMSE.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">M. Hu, S. Wang, B. Li, and X. Gong are with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cvlibs.net/datasets/kitti/eval depth.php?benchmark =depth completion</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A normalized convolutional neural network for guided sparse depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning guided convolutional network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01238</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MAV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uncertainty-aware cnns for depth completion: Uncertainty from beginning to end</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Holmquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guided depth enhancement via anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="3429" to="3441" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale features fusion from sparse lidar data and single image for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="1375" to="1377" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A multiscale guided cascade hourglass network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adaptive contextaware multi-modal network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10833</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeuroIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deformable spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04251</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
