<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2010-10">October 2010. 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<email>phil.blunsom@comlab.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Computing Laboratory University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Sheffield</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<email>t.cohn@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Computing Laboratory University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Sheffield</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing <address><addrLine>MIT, Massachusetts, USA, 9; c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="issue">11</biblScope>
							<biblScope unit="page" from="1204" to="1213"/>
							<date type="published" when="2010-10">October 2010. 2010</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Inducing a grammar directly from text is one of the oldest and most challenging tasks in Computational Linguistics. Significant progress has been made for inducing dependency grammars, however the models employed are overly simplistic, particularly in comparison to supervised parsing models. In this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammar induction is a central problem in Computational Linguistics, the aim of which is to induce linguistic structures from an unannotated text corpus. Despite considerable research effort this unsupervised problem remains largely unsolved, particularly for traditional phrase-structure parsing approaches <ref type="bibr" target="#b2">(Clark, 2001;</ref><ref type="bibr">Klein and Manning, 2002</ref>). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don't know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved more fruitful ( <ref type="bibr">Klein and Manning, 2004</ref>). Dependency grammars <ref type="bibr" target="#b18">(MelčukMelˇMelčuk, 1988)</ref> should be easier to induce from text compared to phrase-structure grammars because the set of labels (heads) are directly observed as the words in the sentence.</p><p>Approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models <ref type="bibr" target="#b2">(Clark, 2001;</ref><ref type="bibr">Klein and Manning, 2004</ref>), especially in comparison to supervised parsing models <ref type="bibr" target="#b10">(Collins, 2003;</ref><ref type="bibr" target="#b1">Clark and Curran, 2004;</ref><ref type="bibr" target="#b17">McDonald, 2006</ref>). Simple models are attractive for grammar induction because they have a limited capacity to overfit, however they are incapable of modelling many known linguistic phenomena. We posit that more complex grammars could be used to better model the unsupervised task, provided that active measures are taken to prevent overfitting. In this paper we present an approach to dependency grammar induction using a tree-substitution grammar (TSG) with a Bayesian non-parametric prior. This allows the model to learn large dependency fragments to best describe the text, with the prior biasing the model towards fewer and smaller grammar productions.</p><p>We adopt the split-head construction <ref type="bibr" target="#b11">(Eisner, 2000;</ref><ref type="bibr">Johnson, 2007)</ref> to map dependency parses to context free grammar (CFG) derivations, over which we apply a model of TSG induction <ref type="bibr" target="#b8">(Cohn et al., 2009)</ref>. The model uses a hierarchical Pitman-Yor process to encode a backoff path from TSG to CFG rules, and from lexicalised to unlexicalised rules. Our best lexicalised model achieves a head attachment accuracy of of 55.7% on Section 23 of the WSJ data set, which significantly improves over state-ofthe-art and far exceeds an EM baseline ( <ref type="bibr">Klein and Manning, 2004</ref>) which obtains 35.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CFG Rule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DMV Distribution Description</head><formula xml:id="formula_0">S → L H H R p(root = H)</formula><p>The head of the sentence is H.</p><formula xml:id="formula_1">L H → H l p(ST OP |dir = L, head = H, val = 0) H has no left children. L H → L 1 H p(CON T |dir = L, head = H, val = 0) H has at least one left child. L * H → H l p(ST OP |dir = L, head = H, val = 1) H has no more left children. L * H → L 1 H p(CON T |dir = L, head = H, val = 1) H has another left child. H R → H r p(ST OP |dir = R, head = H, val = 0)</formula><p>H has no right children.</p><formula xml:id="formula_2">H R → H R 1 p(CON T |dir = R, head = H, val = 0) H has at least one right child. H R * → H r p(ST OP |dir = R, head = H, val = 1)</formula><p>H has no more right children.</p><formula xml:id="formula_3">H R * → H R 1 p(CON T |dir = R, head = H, val = 1) H has another right child. L 1 H → L C C M H * p(C|dir = L, head = H)</formula><p>C is a left child of H. <ref type="table">Table 1</ref>: The CFG-DMV grammar schema. Note that the actual CFG is created by instantiating these templates with part-of-speech tags observed in the data for the variables H and C. Valency (val) can take the value 0 (no attachment in the direction (dir) d) and 1 (one or more attachment). L and R indicates child dependents left or right of the parent; superscripts encode the stopping and valency distributions, X 1 indicates that the head will continue to attach more children and X * that it has already attached a child.</p><formula xml:id="formula_4">H R 1 → H * M C C R p(C|dir = R, head = H) C is a right child of H. C M H * → C R L * H p = 1 Unambiguous H * M C → H R * L C p = 1 Unambiguous</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) ( <ref type="bibr">Klein and Manning, 2004</ref>). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction <ref type="bibr" target="#b3">(Cohen and Smith, 2009;</ref><ref type="bibr" target="#b14">Headden III et al., 2009</ref>). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w| 3 ) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. <ref type="table">Table 1</ref> shows the equivalent CFG grammar for the DMV model (CFG-DMV) using the unfold-fold transformation. The key insight to understanding the non-terminals in this grammar is that the subscripts encode the terminals at the boundaries of the span of that non-terminal. For example the non-terminal L H encodes that the right most terminal spanned by this constituent is H (and the reverse for H R), while A M B encodes that A and B are the left-most and right-most terminals of the span. The * and 1 superscripts are used to encode the valency of the head, both indicate that the head has at least one attached dependent in the specified direction. This grammar allows O(|w| 3 ) parsing complexity which follows from the terminals of the dependency tree being observed, such that each span of the parse chart uniquely specifies its possible heads (either the leftmost, rightmost or both) and therefore the number of possible non-terminals for each span is constant. The transform is illustrated in figures 1a and 1c which show the CFG tree for an example sentence and the equivalent dependency tree.</p><p>Normally DMV based models have been trained on part-of-speech tags of the words in a sentence, rather than the words themselves. <ref type="bibr" target="#b14">Headden III et al. (2009)</ref> showed that performance could be improved by including high frequency words as well as tags in their model. In this paper we refer to such models as lexicalised; words which occur more than one hundred times in the training corpus are represented by a word/tag pair, while those less frequent are represented simply by their tags. We are also able to show that this basic approach to lexicalisation improves the performance of our models.</p><formula xml:id="formula_5">S L hates[V ] L 1 hates[V ] L N N l N M hates[V] * N R N r L * hates[V ] hates[V] l hates[V ] R hates[V ] R 1 hates[V] * M N hates[V ] R * hates[V] r L N N l N R N r</formula><p>(a) A TSG-DMV derivation for the sentence George hates broccoli. George and broccoli occur less than the lexicalisation cutoff and are thus represented by the part-of-speech N, while hates is common and therefore is represented by a word/tag pair. Bold nodes indicate frontier nodes of elementary trees. <ref type="figure" target="#fig_1">Figure 1a</ref>. This rule encodes a dependency between the subject and object of hates that is not present in the CFG-DMV. Note that this rule doesn't restrict hates, or its arguments, to having a single left and right child. More dependents can be inserted using additional rules below the M/L/R frontier non-terminals.  </p><formula xml:id="formula_6">S L hates[V ] L 1 hates[V ] L N N M hates[V] * hates[V ] R hates[V ] R 1 hates[V] * M N N R (b) A TSG-DMV elementary rule from</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lexicalised TSG-DMV</head><p>The models we investigate in this paper build upon the CFG-DMV by defining a Tree Substitution Grammar (TSG) over the space of CFG rules. A TSG is a 4-tuple, G = (T, N, S, R), where T is a set of terminal symbols, N is a set of non-terminal symbols, S ∈ N is the distinguished root non-terminal and R is a set of productions (rules). The productions take the form of elementary trees -tree fragments of height ≥ 1, where each internal node is labelled with a non-terminal and each leaf is labelled with either a terminal or a non-terminal. Nonterminal leaves are called frontier non-terminals and form the substitution sites in the generative process of creating trees with the grammar.</p><p>A derivation creates a tree by starting with the root symbol and rewriting (substituting) it with an elementary tree, then continuing to rewrite frontier non-terminals with elementary trees until there are no remaining frontier non-terminals. We can represent derivations as sequences of elementary trees, e, by specifying that during the generation of the tree each elementary tree is substituted for the left-most frontier non-terminal. <ref type="figure" target="#fig_1">Figure 1a</ref> shows a TSG derivation for the dependency tree in <ref type="figure" target="#fig_1">Figure 1c</ref> where bold nonterminal labels denote substitution sites (root/frontier nodes in the elementary trees).</p><p>The probability of a derivation, e, is the product of the probabilities of its component rules,</p><formula xml:id="formula_7">P (e) = c→e∈e P (e|c) .<label>(1)</label></formula><p>where each rewrite is assumed conditionally independent of all others given its root nonterminal, c = root(e). The probability of a tree, t, and string of words, w, are</p><formula xml:id="formula_8">P (t) = e:tree(e)=t P (e) and P (w) = t:yield(t)=w P (t) ,</formula><p>respectively, where tree(e) returns the tree for the derivation e and yield(t) returns the string of terminal symbols at the leaves of t. A Probabilistic Tree Substitution Grammar (PTSG), like a PCFG, assigns a probability to each rule in the grammar, denoted P (e|c). The probability of a derivation, e, is the product of the probabilities of its component rules. Estimating a PTSG requires learning the sufficient statistics for P (e|c) in (1) based on a training sample. Parsing involves finding the most probable tree for a given string (arg max t P (t|w)). This is typically approximated by finding the most probable derivation which can be done efficiently using the CYK algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in <ref type="table">Table 1</ref>. Our model is a generalisation of that of <ref type="bibr" target="#b8">Cohn et al. (2009)</ref> and <ref type="bibr" target="#b9">Cohn et al. (2011)</ref>. We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language <ref type="bibr" target="#b23">(Teh, 2006;</ref><ref type="bibr" target="#b13">Goldwater et al., 2006</ref>). <ref type="bibr" target="#b23">Teh (2006)</ref> used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules.</p><p>Here we describe our deepest model which has a four level hierarchy, depicted graphically in <ref type="table" target="#tab_0">Table  2</ref>. In Section 5 we evaluate different subsets of this hierarchy. The topmost level of our model describes lexicalised elementary elementary fragments (e) as produced by a PYP,</p><formula xml:id="formula_9">e|c ∼ G c G c |a c , b c , P lcfg ∼ PYP(a c , b c , P lcfg (·|c)) ,</formula><p>where a c and b c control the strength of the backoff distribution P lcfg . The space of lexicalised TSG rules will inevitably be very sparse, so the base distribution P lcfg backs-off to calculating the probability of a TSG rules as the product of the CFG rules it contains, multiplied by a geometric distribution over the size of the rule.</p><formula xml:id="formula_10">P lcfg (e|c) = f ∈F(e) s fc i∈I(e) (1 − s ic ) × A(lex-cfg-rules(e|c)) α|c ∼ A c A c |a lcfg c , b lcfg c , P cfg ∼ PYP(a lcfg c , b lcfg c , P cfg (·|c)),</formula><p>where I(e) are the set of internal nodes in e excluding the root, F (e) are the set of frontier non-terminal nodes, and c i is the non-terminal symbol for node i and s c is the probability of stopping expanding a node labelled c. The function lex-cfg-rules(e|c) returns the CFG rules internal to e, each of the form c → α; each CFG rule is drawn from the backoff distribution, A c . We treat s c as a parameter which is estimated during training, as described in Section 4.2.</p><p>The next level of backoff (P cfg ) removes the lexicalisation from the CFG rules, describing the generation of a lexicalised rule by first generating an unlexicalised rule from a PYP, then generating the lexicalisaton from a uniform distribution over words: 1</p><formula xml:id="formula_11">P cfg (α|c) = B(unlex(α)|unlex(c)) × 1 |w| |α| α |c ∼ B c B c |a cfg c , b cfg c , P sh ∼ PYP(a cfg c , b cfg c , P sh (·|c )),</formula><p>where unlex(·) removes the lexicalisation from nonterminals leaving only the tags. The final base distribution over CFG-DMV rules (P sh ) is inspired by the skip-head smoothing model of <ref type="bibr" target="#b14">Headden III et al. (2009)</ref>. This model showed that smoothing the DMV by removing the heads from the CFG rules significantly improved performance. We replicate this behavior through a final level in our hierarchy which generates the CFG rules without their heads, then generates the heads from a uniform distribution:</p><formula xml:id="formula_12">P sh (α|c) = C(drop-head(c → α)) × 1 |P | α|c ∼ C c C c |a sh c , b sh c ∼ PYP(a sh c , b sh c , Uniform(·|c)),</formula><p>where drop-head(·) removes the symbols that mark the head on the CFG rules, and P is the set of partof-speech tags. Each stage of backoff is illustrated in <ref type="table" target="#tab_0">Table 2</ref>, showing the rules generated from the TSG elementary tree in <ref type="figure" target="#fig_1">Figure 1b</ref>. Note that while the supervised model of <ref type="bibr" target="#b8">Cohn et al. (2009)</ref> used a fixed back-off PCFG distribution, this model implicitly infers this distribution within </p><formula xml:id="formula_13">P lcfg P cfg P sh S L hates[V ] hates[V ] R L hates[V ] L 1 hates[V ] S L V V R L V L 1 V S L · · R L · L 1 · L 1 hates[V ] L N N M hates[V] * hates[V ] R hates[V ] R 1 L 1 V L N N M V * V R V R 1 L 1 · L N N M · * · R · R 1 hates[V ] R 1 hates[V] * M N N R V R 1 V * M N N R · R 1 · * M N N R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>To train our model we use Markov Chain Monte Carlo sampling <ref type="bibr" target="#b12">(Geman and Geman, 1984)</ref>. Where previous supervised TSG models ( <ref type="bibr" target="#b8">Cohn et al., 2009</ref>) permit an efficient local sampler, the lack of an observed parse tree in our unsupervised model makes this sampler not applicable. Instead we use a recently proposed blocked Metroplis-Hastings (MH) sampler <ref type="bibr" target="#b7">(Cohn and Blunsom, 2010</ref>) which exploits a factorisation of the derivation probabilities such that whole trees can be sampled efficiently. See Cohn and Blunsom (2010) for details. That algorithm is applied using a dynamic program over an observed tree, the generalisation to our situation of an inside pass over the space of all trees is straightforward.</p><p>A final consideration is the initialisation of the sampler. <ref type="bibr">Klein and Manning (2004)</ref> emphasised the importance of the initialiser for achieving good performance with their model. We employ the same harmonic initialiser as described in that work. The initial derivations for our sampler are the Viterbi derivations under the CFG parameterised according to this initialiser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling hyperparameters</head><p>We treat the hyper-parameters {(a x c , b x c , s c ) , c ∈ N } as random variables in our model and infer their values during training. We choose quite vague priors for each hyper-parameter, encoding our lack of information about their values.</p><p>We place prior distributions on the PYP discount a c and concentration b c hyperparamters and sample their values using a slice sampler. We use the range doubling slice sampling technique of <ref type="bibr" target="#b19">(Neal, 2003)</ref> to draw a new sample of a c from its conditional distribution. <ref type="bibr">2</ref> For the discount parameters a c we employ a uniform Beta distribution, as we have no strong prior knowledge of what its value should be (a c ∼ Beta <ref type="figure" target="#fig_1">(1, 1)</ref>). Similarly, we treat the concentration parameters, b c , as being generated by a vague gamma prior, b c ∼ Gamma(1, 1), and sample a new value b c using the same slice-sampling approach as for a c :  We use a vague Beta prior for the stopping probabilities in P lcfg , s c ∼ Beta(1, 1). All the hyper-parameters are resampled after every 10th sample of the corpus derivations.</p><formula xml:id="formula_14">P (b c |z) ∝ P (z|b c ) × Gamma(b c |1, 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parsing</head><p>Unfortunately finding the maximising parse tree for a string under our TSG-DMV model is intractable due to the inter-rule dependencies created by the PYP formulation. Previous work has used Monte Carlo techniques to sample for one of the maximum probability parse (MPP), maximum probability derivation (MPD) or maximum marginal parse (MMP) ( <ref type="bibr" target="#b8">Cohn et al., 2009;</ref><ref type="bibr" target="#b0">Bod, 2006</ref>). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV <ref type="bibr" target="#b5">(Cohen et al., 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. <ref type="bibr">Treebank (Marcus et al., 1993</ref>) and reporting head attachment accuracy. Like previous work we pre-process the training and test data to remove punctuation, training our unlexicalised models on the gold-standard part-of-speech tags, and including words occurring more than 100 times in our lexicalised models <ref type="bibr" target="#b14">(Headden III et al., 2009)</ref>. It is very difficult for an unsupervised model to learn from long training sentences as they contain a great deal of ambiguity, therefore the majority of DMV based models have been trained on sentences restricted in length to ≤ 10 tokens. <ref type="bibr">3</ref> This has the added benefit of decreasing the runtime for experiments. We present experiments with this training scenario. The training data comes from sections 2-21, while section 23 is used for evaluation. An advantage of our sampling based approach over previous work is that we infer all the hyperparameters, as such we don't require the use of section 22 for tuning the model.</p><p>The models are evaluated in terms of head attachment accuracy (the percentage of correctly predicted head indexes for each token in the test data), on two subsets of the testing data. Although we can argue that unsupervised models are better learnt from short sentences, it is much harder to argue that we don't then need to be able to parse long sentences with a trained model. The most commonly employed test set mirrors the training data by only including sentences ≤ 10. In this work we focus on the accuracy of our models on the whole of section 23, without any pruning for length. The training and testing corpora statistics are presented in <ref type="table" target="#tab_2">Table 3</ref>. Subsequent to the evaluation reported in <ref type="table" target="#tab_5">Table 4</ref> we use section 22 to report the correlation between heldout accuracy and the model log-likelihood (LLH) for analytic purposes.</p><p>As we are using a sampler during training, the result of any single run is non-deterministic and will exhibit a degree of variance. All our reported results are the mean and standard deviation (σ) from forty sampling runs. <ref type="table" target="#tab_5">Table 4</ref> shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Discussion</head><p>The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w| ≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of <ref type="bibr" target="#b14">Headden III et al. (2009</ref> EM <ref type="bibr">(Klein and Manning, 2004)</ref> 46.1 35.9 Dirichlet <ref type="bibr" target="#b5">(Cohen et al., 2008)</ref> 46.1 36.9</p><p>LN <ref type="bibr" target="#b5">(Cohen et al., 2008)</ref> 59.4 40.5 SLN, TIE V&amp;N <ref type="bibr" target="#b3">(Cohen and Smith, 2009)</ref> 61.3 41.4</p><p>DMV <ref type="bibr" target="#b14">(Headden III et al., 2009)</ref> 55.7σ=8.0 -DMV smoothed <ref type="bibr" target="#b14">(Headden III et al., 2009)</ref> 61.2σ=1.2 -EVG smoothed <ref type="bibr" target="#b14">(Headden III et al., 2009)</ref> 65.0σ=5.7 -L-EVG smoothed <ref type="bibr" target="#b14">(Headden III et al., 2009)</ref> 68.8σ=4.  valency conditioning, interpolated back-off smoothing and a random initialiser. In particular Headden <ref type="bibr" target="#b14">III et al. (2009)</ref> shows that the random initialiser is crucial for good performance, however this initialiser requires training 1000 models to select a single best model for evaluation and results in considerable variance in test set performance. Note also that our model exhibits considerably less variance than those induced using this random initialiser, suggesting that the combination of the harmonic initialiser and blocked-MH sampling may be a more practicable training regime.</p><p>The recently proposed Adaptor Grammar DMV model of <ref type="bibr" target="#b6">Cohen et al. (2010)</ref> is similar in many way to our TSG model, incorporating a Pitman Yor prior over units larger than CFG rules. As such it is surprising that our model is performing significantly better than this model. We can identify a number of differences that may impact these results: the Adaptor Grammar model is trained using variational inference with the space of tree fragments truncated, while we employ a sampler which can nominally explore the full space of tree fragments; and the adapted tree fragments must be complete subtrees (i.e. they don't contain variables), whereas our model can make use of arbitrary tree fragments. An interesting avenue for further research would be to extend the variational algorithm of <ref type="bibr" target="#b6">Cohen et al. (2010)</ref> to our TSG model, possibly speeding inference and allowing easier parallelisation.</p><p>In <ref type="figure" target="#fig_3">Figure 2a</ref> we graph the model LLH on the training data versus the head attachment accuracy on the heldout set. The graph was generated by running 160 models for varying numbers of samples and evaluating their accuracy. This graph indicates that the improvements in the posterior probability of the model are correlated with the evaluation, though the correlation is not as high as we might require in order to use LLH as a model selection criteria similar to <ref type="bibr" target="#b14">Headden III et al. (2009)</ref>. Further refinements to the model could improve this correlation.</p><p>The scaling perfomance of the model as the number of samples is increased is shown in <ref type="figure" target="#fig_3">Figure 2b</ref>. Performance improves as the training data is sampled for longer, and continues to trend upwards beyond 1000 samples (the point for which we've reported results in <ref type="table" target="#tab_5">Table 4</ref>). This suggests that longer sampling runs -and better inference techniquescould yield further improvements.</p><p>For further analysis <ref type="table" target="#tab_7">Table 5</ref> shows the accuracy of the model at predicting the head for frequent types, while <ref type="table">Table 6</ref> shows the performance on dependencies of various lengths. We emphasise that these results are for the single best performing sampler run on the heldout corpus and there is considerable variation in the analyses produced by each sampler. Unsurprisingly, the model appears to be more accurate when predicting short dependencies, a result that is also reflected in the per type accuracies. The model is relatively good at identifying the root verb in each sentence, especially those headed by past tense verbs (VBD, was), and to a lesser degree VBPs (are). Conjunctions such as and pose a particular difficulty when evaluating dependency models as the correct modelling of these remains a  contentious linguistic issue and it's not clear what the 'correct' analysis should be. Our model gets a respectable 75% accuracy for and conjunctions, but for conjunctions (CC) as a whole, the model performs poorly (39%). <ref type="table" target="#tab_6">Table 7</ref> list the most frequent TSG rules lexicalised with has. The most frequent rule is simply the single level equivalent of the DMV terminal rule for has. Almost as frequent is rule 3, here the grammar incorporates the terminal into a larger elementary fragment, encoding that it is the head of the past participle occuring immediately to it's right. This shows the model's ability to learn the verb's argument position conditioned on both the head and child type, something lacking in DMV. Rule 7 further refines this preferred analysis for has been by lexicalising both the head and child. Rules (4,5,8,10) employ similar conditioning for proper and ordinary nouns heading noun phrases to the left of has. We believe that it is the ability of the TSG to encode stronger constraints on argument positions that leads to the model's higher accuracy on longer sentences, while other models do well on shorter sentences but relatively poorly on longer ones ( <ref type="bibr" target="#b22">Spitkovsky et al., 2010c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have made two significant contributions to probabilistic modelling and grammar induction. We have shown that it is possible to successfully learn hierarchical Pitman-Yor models that encode deep and complex backoff paths over highly structured latent spaces. By applying these models to the induction of dependency grammars we have also been able to advance the state-of-the-art, increasing the head attachment accuracy on section 23 of the Wall Street Journal Corpus by more than 5%.</p><p>Further gains in performance may come from an exploration of the backoff paths employed within the model. In particular more extensive experimentation with alternate priors and larger training data may allow the removal of the lexicalisation cutoff which is currently in place to counter sparsity.</p><p>We envisage that in future many grammar formalisms that have been shown to be effective in supervised parsing, such as categorial, unification and tree adjoining grammars, will prove amenable to unsupervised induction using the hierarchical nonparametric modelling approaches we have demonstrated in this paper.  94</p><formula xml:id="formula_15">L * has−V BZ → (L * has−V BZ has-VBZ l ) 2 74 L 1 has−V BZ → (L 1 has−V BZ (L N N L 1 N N ) N N M has−V BZ * ) 3 71 has−V BZ * M V BN → ( has−V BZ * M V BN ( has−V BZ R * has-VBZ r ) L V BN ) 4 54 N N M has−V BZ * → ( N N M has−V BZ * N N R (L * has−V BZ has-VBZ l )) 5 36 N N M has−V BZ * → ( N N M has−V BZ * N N R L * has−V BZ ) 6 36 has−V BZ R * → ( has−V BZ R * ( has−V BZ R 1 has−V BZ * M V BN ( V BN R VBN r ))) 7 30 has−V BZ * M been−V BN → ( has−V BZ * M been−V BN ( has−V BZ R * has-VBZ r ) L been−V BN ) 8</formula><p>27 N N P M has−V BZ * → ( N N P M has−V BZ * N N P R (L * has−V BZ has-VBZ l )) 9 25 has−V BZ R → ( has−V BZ R ( has−V BZ R 1 has−V BZ * M N N S ( N N S R N N S R 1 ))) <ref type="bibr">10</ref> 18 L 1 has−V BZ → (L 1 has−V BZ L N N P N N P M has−V BZ * )   <ref type="table">Table 6</ref>: Link distance precision, recall and f-score, on the WSJ Section 22 |w| ≤ 10 heldout set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>George hates broccoli ROOT (c) A traditional dependency tree representation of the parse tree in Figure 1a before applying the lexicalisation cutoff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TSG-DMV representation of dependency trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Perplexity vs.</head><label></label><figDesc>Accuracy Correlation PYP.LLH Directed.Attachment.Accuracy (a) Correlation (R 2 = 0.2) between the training LLH of the PYP Model and heldout directed head attachment accuracy (WSJ Section 22, |w| ≤ 10) for LexTSG-DMV (P lcfg , P cfg , P sh ).(b) Mean heldout directed head attachment accuracy (WSJ Sec- tion 22, |w| ≤ 10) versus the number of samples used during training for LexTSG-DMV (P lcfg , P cfg , P sh ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>Figure 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Backoff trees for the elementary tree in Figure 1b. 

its hierarchy, essentially learning the DMV model 
embedded in the TSG. 
In this application to dependency grammar our 
model is capable of learning tree fragments which 
group CFG parameters. As such the model can learn 
to condition dependency links on the valence, e.g. by 
combining L H → L 1 
H and L 1 
H → L C C M H  *  rules 
into a single fragment the model can learn a pa-
rameter that the leftmost child of H is C. By link-
ing together multiple L 1 
H or H R 1 non-terminals the 
model can learn groups of dependencies that occur 
together, e.g. tree fragments representing the com-
plete preferred argument frame of a verb. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). 2 We made use of the slice sampler included in Mark Johnson's Adaptor Grammar implementation http://www.cog.brown.edu/ ˜ mj/Software.htm.</figDesc><table>Corpus 

Words Sentences 

Sections 2-21 (|x| ≤ 10) 42505 
6007 

Section 22 (|x| ≤ 10) 
1805 
258 

Section 23 (|x| ≤ 10) 
2649 
398 
Section 23 (|x| ≤ ∞) 
49368 
2416 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Corpus statistics for the training and testing data 
for the TSG-DMV model. All models are trained on the 
gold standard part-of-speech tags after removing punctu-
ation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>). The L-EVG model extends DMV by adding additional lexicalisation,</figDesc><table>Directed Attachment 
Accuracy on WSJ23 

Model 
|w| ≤ 10 |w| ≤ ∞ 

Attach-Right 
38.4 
31.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Mean and variance for the head attachment accu-
racy of our TSG-DMV models (highlighted) with varying 
backoff paths, and many other high performing models. 
Citations indicate where the model and result were re-
ported. Our models labelled TSG used an unlexicalised 
top level G c PYP, while those labelled LexTSG used the 
full lexicalised G c . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 : The ten most frequent LexTSG-DMV rules in a final training sample that contain has.</head><label>7</label><figDesc></figDesc><table>Child Tag 

Predicted 
Accuracy 
Head Correct 
(%) 

NN 
181 
0.64 
NNP 
130 
0.71 
DT 
127 
0.87 
NNS 
108 
0.72 
VBD 
108 
0.81 
JJ 
106 
0.80 
IN 
81 
0.55 
RB 
65 
0.61 
PRP 
64 
0.97 
VBZ 
47 
0.80 
VBN 
36 
0.86 
VBP 
30 
0.77 
CD 
26 
0.23 
VB 
25 
0.68 

the 
42 
0.88 
was 
29 
0.97 
The 
25 
0.83 
of 
18 
0.78 
a 
18 
0.90 
to 
17 
0.50 
in 
16 
0.89 
is 
15 
0.79 
n't 
15 
0.83 
were 
12 
0.86 
are 
11 
0.92 
It 
11 
1.00 
for 
9 
0.64 
and 
9 
0.75 
's 
9 
1.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Per tag type predicted count and accuracy, 
for the most frequent 15 un/lexicalised tokens on the 
WSJ Section 22 |w| ≤ 10 heldout set (LexTSG-DMV 
(P lcfg , P cfg , P sh )). 

Mark Johnson. 2007. Transforming projective bilexical 
dependency grammars into efficiently-parsable CFGs 
with unfold-fold. In Proceedings of the 45th Annual 
Meeting of the Association of Computational Linguis-
tics, pages 168-175, Prague, Czech Republic, June. 
Association for Computational Linguistics. 
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar 
induction. In Proceedings of 40th Annual Meeting of 
the Association for Computational Linguistics, pages 
128-135, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics. 
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL '04: Proceedings 

</table></figure>

			<note place="foot" n="1"> All unlexicalised words are actually given the generic UNK symbol as their lexicalisation.</note>

			<note place="foot" n="3"> See Spitkovsky et al. (2010a) for an exception to this rule.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An all-subtrees approach to unsupervised parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006)</title>
		<meeting>of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parsing the WSJ using CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 42nd Annual Meeting of the ACL (ACL-2004)</title>
		<meeting>of the 42nd Annual Meeting of the ACL (ACL-2004)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised induction of stochastic context-free grammars using distributional clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ConLL &apos;01: Proceedings of the 2001 workshop on Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL &apos;09: Proceedings of Human Language Technologies</title>
		<imprint>
			<publisher>The</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Logistic normal priors for unsupervised probabilistic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Daphne Koller, Dale Schuurmans, Yoshua Bengio, and Lon Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational inference for adaptor grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blocked inference in Bayesian tree substitution grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>page To Appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inducing compact but accurate tree-substitution grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL &apos;09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on ZZZ</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inducing tree-substitution grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilexical grammars and their cubictime parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Probabilistic and Other Parsing Technologies</title>
		<editor>Harry Bunt and Anton Nijholt</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000-10" />
			<biblScope unit="page" from="29" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpolating between types and tokens by estimating power-law generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Weiss, B. Schölkopf, and J. Platt</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The</title>
		<meeting>Human Language Technologies: The</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Boulder, Colorado, June</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">478</biblScope>
		</imprint>
	</monogr>
	<note>of the 42nd Annual Meeting on Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Discriminative Training and Spanning Tree Algorithms for Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dependency Syntax: theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">A</forename><surname>Melčukmelˇmelčuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<pubPlace>Albany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>State University of New York Press</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radford</forename><surname>Neal</surname></persName>
		</author>
		<title level="m">Slice sampling. Annals of Statistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="705" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From Baby Steps to Leapfrog: How &quot;Less is More&quot; in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010)</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Profiting from mark-up: Hyper-text annotations for guided parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian language model based on Pitman-Yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
