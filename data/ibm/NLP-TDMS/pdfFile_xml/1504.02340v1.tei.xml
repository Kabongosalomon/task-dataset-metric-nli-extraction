<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
							<email>wongun@nec-labs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NEC Laboratories America</orgName>
								<address>
									<addrLine>10080 N. Wolfe Rd</addrLine>
									<settlement>Cupertino</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on the two key aspects of multiple target tracking problem: 1) designing an accurate affinity measure to associate detections and 2) implementing an efficient and accurate (near) online multiple target tracking algorithm. As the first contribution, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a pair of temporally distant detections using long term interest point trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections regardless of the application scenarios. As another contribution, we present a Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is formulated as a data-association between targets and detections in a temporal window, that is performed repeatedly at every frame. While being efficient, NOMT achieves robustness via integrating multiple cues including ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization into the model. Our ablative analysis verifies the superiority of the ALFD metric over the other conventional affinity metrics. We run a comprehensive experimental evaluation on two challenging tracking datasets, KITTI [15]  and MOT [1]  datasets. The NOMT method combined with ALFD metric achieves the best accuracy in both datasets with significant margins (about 10% higher MOTA) over the state-of-the-arts. arXiv:1504.02340v1 [cs.CV] 9 Apr 2015 at t 1 at t 2 at t 3 t 1 t--τ Detec)ons not observed yet t 2 t 2 --τ Associa,on Error t 3 t 3 --τ</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of multiple target tracking is to automatically identify objects of interest and reliably estimate the motion of targets over the time. Thanks to the recent advancement in image-based object detection methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>, tracking-by-detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> has become a popular framework to tackle the multiple target tracking problem. The advantages of the framework are that it naturally identifies new objects of interest entering the scene, that it can handle video sequences recorded using mobile plat-Distance: similar Appearance: similar Flows: dis4nc4ve t t + Δt <ref type="bibr">Figure 1</ref>. Bounding box distance and appearance similarity are popularly used affinity metrics in the multiple target tracking literature. However, in real-world crowded scenes, they are often ambiguous to successfully distinguish adjacent or similar looking targets. Yet, the optical flow trajectories provide more reliable measure to compare different detections across time. Although individual trajectory may be inaccurate (red line), collectively they provide strong information to measure the affinity. We propose a novel Aggregated Local Flow Descriptor that exploits the optical flow reliably in the multiple target tracking problem. The figure is best shown in color. forms, and that it is robust to a target drift. The key challenge in this framework is to accurately group the detections into individual targets with high accuracy (data association), so one target could be fully represented by a single estimated trajectory. Mistakes made in the identity maintenance could result in a catastrophic failure in many high level reasoning tasks, such as future motion prediction, target behavior analysis, etc.</p><p>To implement a highly accurate multiple target tracking algorithm, it is important to have a robust data association model and an accurate measure to compare two detections across time (pairwise affinity measure). Recently, much work is done in the design of the data association algorithm using global (batch) tracking framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. Compared to the online counterparts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>, these <ref type="bibr">Figure 2</ref>. Our NOMT algorithm solves the global association problem at every time frame t with a temporal window τ . Solid circles represent associated targets, dashed circles represent unobserved detections, dashed lines show finalized target association before the temporal window, and solid lines represent the (active) association made in the current time frame. Due to the limited amount of observation, the tracking algorithm may produce an erroneous association at t2. But once more observation is provided at t3, our algorithm is capable of fixing the error made in t2. In addition, our method automatically identifies new targets on the fly (red circles). The figure is best shown in color.</p><p>methods have a benefit of considering all the detections over entire time frames. With a help of clever optimization algorithms, they achieve higher data association accuracy than traditional online tracking frameworks. However, the application of these methods is fundamentally limited to post-analysis of video sequences. On the other hand, the pairwise affinity measure is relatively less investigated in the recent literature despite its importance. Most methods adopt weak affinity measures (see <ref type="figure">Fig. 1</ref>) to compare two detections across time, such as spatial affinity (e.g. bounding box overlap or euclidean distance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28]</ref>) or simple appearance similarity (e.g. intersection kernel with color histogram <ref type="bibr" target="#b28">[29]</ref>). In this paper, we address the two key challenging questions of the multiple target tracking problem: 1) how to accurately measure the pairwise affinity between two detections (i.e. likelihood to link the two) and 2) how to efficiently apply the ideas in global tracking algorithms into an online application.</p><p>As the first contribution, we present a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between two detection boxes in different time frames (Sec. 3). By aggregating multiple local interest point trajectories (IPTs), the descriptor encodes how the IPTs in a detection moves with respect to another detection box, and vice versa. The main intuition is that although each individual IPT may have an error, collectively they provide a strong information for comparing two detections. With a learned model, we observe that ALFD provides strong affinity measure, thereby providing strong cues for the association algorithm.</p><p>As the second contribution, we propose an efficient Near-Online Multi-target Tracking (NOMT) algorithm. Incorporating the robust ALFD descriptor as well as longterm motion/appearance models motivated by the success of modern batch tracking methods, the algorithm produces highly accurate trajectories, while preserving the causality property and running in real-time (∼ 10 FPS). In every time frame t, the algorithm solves the global data association problem between targets and all the detections in a temporal window [t−τ, t] of size τ (see <ref type="figure">Fig. 2</ref>). The key property is that the algorithm is able to fix any association error made in the past when more detections are provided. In order to achieve both accuracy and efficiency, the algorithm generates candidate hypothetical trajectories using ALFD driven tracklets and solve the association problem with a parallelized junction tree algorithm (Sec. 4).</p><p>We perform a comprehensive experimental evaluation on two challenging datasets: KITTI <ref type="bibr" target="#b14">[15]</ref> and MOT Challenge <ref type="bibr" target="#b0">[1]</ref> datasets. The proposed algorithm achieves the best accuracy with a large margin over the state-of-the-arts (including batch algorithms) in both datasets, demonstrating the superiority of our algorithm. The rest of the paper is organized as follows. Sec. 2 discusses the background and related work in multiple target tracking literature. Sec. 3 describes our newly proposed ALFD. Sec. 4 presents overview of NOMT data association model and the algorithm. Sec. 5 discusses the details of model design. We show the analysis and experimental evaluation in Sec. 6, and finally conclude with Sec. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Given a video sequence V T 1 = {I 1 , I 2 , ..., I T } of length T and a set of detection hypotheses <ref type="bibr" target="#b0">1</ref> , and the score s i , the goal of multiple target tracking is to find a coherent set of targets (associations) A = {A 1 , A 2 , ..., A M }, where each target A m are parameterized by a set of detection indices (e.g. A 1 = {d 1 , d 10 , d 23 }) during the time of presence; i.e.</p><formula xml:id="formula_0">D T 1 = {d 1 , d 2 , ..., d N }, where d i is parameterized by the frame number t i , a bound- ing box (d i [x], d i [y], d i [w], d i [h])</formula><formula xml:id="formula_1">(V T 1 , D T 1 ) → A.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Association Models</head><p>Most of multiple target tracking algorithms/systems can be classified into two categories: online method and global (batch) method.</p><p>Online algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref> are formulated to find the association between existing targets and detections in the current time frame:</p><formula xml:id="formula_2">(V t t , D t t , A t−1 ) → A t .</formula><p>The advantages of online formulation are: 1) it is applicable to online/real-time scenario and 2) it is possible to take advantage of targets' dynamics information available in A t−1 . Such methods, however, are often prone to association errors since they consider only one frame when making the association. Solving the problem based on (temporally) local information can fundamentally limit the association accuracy. To avoid such errors, <ref type="bibr" target="#b4">[5]</ref> adopts conservative association threshold together with detection confidence maps, or <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref> model interactions between targets.</p><p>Recently, global algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref> became much popular in the community, as more robust association is achieved when considering long-term information in the association process. One common approach is to formulate the tracking as the network flow problem to directly obtain the targets from detection hypothesis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref></p><formula xml:id="formula_3">; i.e. (V T 1 , D T 1 ) → A T .</formula><p>Although they have shown promising accuracy in multiple target tracking, the methods are often over-simplified for the tractability concern. They ignore useful target level information, such as target dynamics and interaction between targets (occlusion, social interaction, etc). Instead of directly solving the problem at one step, other employ an iterative algorithm that progressively refines the target association <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref></p><formula xml:id="formula_4">; i.e. (V T 1 , D T 1 , A T i ) → A T i+1</formula><p>, where i represent an iteration. Starting from short trajectories (tracklet), <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> associate them into longer targets in a hierarchical fashion. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref> iterate between two modes, association and continuous estimation. Since these methods obtain intermediate target information, targets' dynamics, interaction and high-order statistics on the trajectories could be accounted that can lead to a better association accuracy. However, it is unclear how to seamlessly extend such models to an online application.</p><p>We propose a novel framework that can fill in the gap between the online and global algorithms. The task is defined as to solve the following problem:</p><formula xml:id="formula_5">(V t 1 , D t t−τ , A t−1 ) → A t in each time frame t,</formula><p>where τ is pre-defined temporal window size. Our algorithm behaves similar to the online algorithm in that it outputs the association in every time frame. The critical difference is that any decision made in the past is subject to change once more observations are available. The association problems in each temporal window are solved using a newly proposed global association algo-rithm. Our method is also reminiscent of iterative global algorithm, since we augment all the track iteratively (one iteration per frame) considering multiple frames, that leads to a better association accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Affinity Measures in Visual Tracking</head><p>The importance of a robust pairwise affinity measure (i.e. likelihood of d i and d j being the same target) is relatively less investigated in the multi-target tracking literature. Most of the recent literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29</ref>] employs a spatial distance and/or an appearance similarity with simple features (such as color histograms). In order to learn a discriminative affinity metric, Kuo et al. <ref type="bibr" target="#b22">[23]</ref> introduces an online appearance learning with boosting algorithm using various feature inputs such as HoG <ref type="bibr" target="#b7">[8]</ref>, texture feature, and RGB color histogram. Milan et al. <ref type="bibr" target="#b24">[25]</ref> and Zamir et al. <ref type="bibr" target="#b28">[29]</ref> proposed to use a global appearance consistency measure to ensure a target has a similar (or smoothly varying) appearance over a long term. Although there have been many works exploiting appearance information or spatial smoothness, we are not aware of any work employing optical flow trajectories to define a likelihood of matching detections. Recently, Fragkiadaki et al. <ref type="bibr" target="#b12">[13]</ref> introduced a method to track multiple targets while jointly clustering optical flow trajectories. The work presents a promising result, but the model is complicated due to the joint inference on both target and flow level association. In contrast, our ALFD provides a strong pairwise affinity measure that is generally applicable in any tracking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Aggregated Local Flow Descriptor</head><p>The Aggregated Local Flow Descriptor (ALFD) encodes the relative motion pattern between two bounding boxes in a temporal distance (∆t = |t i − t j |) given interest point trajectories <ref type="bibr" target="#b30">[31]</ref>. The main intuition in ALFD is that if the two boxes belong to the same target, we shall observe many supporting IPTs in the same relative location with respect to the boxes. In order to make it robust against small localization errors in detections, targets' orientation change, and outliers/errors in the IPTs, we build the ALFD using spatial histograms. Once the ALFD is obtained, we measure the affinity between two detections using the linear product of a learned model parameter w ∆t and ALFD, i.e.</p><formula xml:id="formula_6">a A (d i , d j ) = w ∆t · ρ(d i , d j ).</formula><p>In the following subsections, we discuss the details of the design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Interest Point Trajectories</head><p>We obtain Interest Point Trajectories using a local interest point detector <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref> and optical flow algorithm <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. The algorithm is designed to produce a set of long and accurate point trajectories, combining various well-known computer vision techniques. Given an image I t , we run the FAST interest point detector <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref> to identify "good In the top figure, we show detections as colored bounding boxes (d red , d blue , and dgreen). A pair of circles with connecting lines represent IPTs that are existing in both t and t + t and located inside of the d red at t. We draw the accurate (green), outlier (black), and erroneous (red) IPTs. In the bottom figure, we show two exemplar unidirectional ALFDs ρ for (d red , d blue ) and (d red , dgreen). The red grids (2 × 2) represent the IPTs' location at t relative to d red . The blue and green grids inside of each red bin (2×2+2 external bins) shows the IPTs' location at t+ t relative to the corresponding boxes. IPTs in the grid bins with a red box are the one observed in the same relative location. Intuitively, the more IPTs are observed in the bins, the more likely the two detections belong to the same target. In contrast, wrong matches will have more supports in the outside bins. The illustration is shown using 2 × 2 grids to avoid clutter. We use 4 × 4 in practice. The figure is best shown in color.</p><p>points" to track. In order to avoid having redundant points, we compute the distance between the newly detected interest points and the existing IPTs and keep the new points sufficiently far from the existing IPTs (&gt; 4 px). The new points are assigned unique IDs. For all the IPTs in t, we compute the forward (t → t + 1) and backward (t + 1 → t) optical flow using <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. The starting points of backward flows are given by the forward flows' end point. Any IPT having a large disagreement between the two (&gt; 10 px) is terminated.  <ref type="figure">Figure 4</ref>. Visualization of two learned model weights w∆1 and w∆20. Having a higher ρ value in the bright (white) bins yields a higher affinity measure. As the temporal distance increase, the model weights tend to spread out to the adjacent bins to account for a possible targets' orientation change and higher IPT errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ALFD Design</head><p>Let us define the necessary notations to discuss ALFD.</p><formula xml:id="formula_7">κ id ∈ K represents an IPT with a unique id that is param- eterized by pixel locations (κ id (t)[x], κ id (t)[y])</formula><p>during the time of presence. κ id (t) denotes the pixel location at the frame t. If κ id does not exist at t (terminated or not initiated), ø is returned.</p><p>We first define a unidirectional ALFD ρ (d i , d j ), i.e. from d i to d j , by aggregating the information from all the IPTs that are located inside of d i box and existing at t j . Formally, we define the IPT set as</p><formula xml:id="formula_8">K(d i , d j ) = {κ id |κ id (t i ) ∈ d i &amp; κ id (t j ) = ø}. For each κ id ∈ K(d i , d j ), we com- pute the relative location r i (κ id ) = (x, y) of each κ id at t i by r i (κ id )[x] = (κ id (t i )[x]−d i [x])/d i [w] and r i (κ id )[y] = (κ id (t i )[y]−d i [y])/d i [h]</formula><p>. We compute r j (κ id ) similarly. Notice that r i (κ id ) are bounded between [0, 1], but r j (κ id ) are not bounded since κ id can be outside of d j . Given the r i (κ id ) and r j (κ id ), we compute the corresponding spatial grid bin indices as shown in the <ref type="figure" target="#fig_0">Fig. 3</ref> and accumulate the count to build the descriptor. We define 4 × 4 grids for r i (κ id ) and 4 × 4 + 2 grids for r j (κ id ) where the last 2 bins are accounting for the outside region of the detection. The first outside bin defines the neighborhood of the detection (&lt; width/4 &amp; &lt; height/4), and the second outside bin represents any farther region.</p><p>Using a pair of unidirectional ALFDs, we define the ALFD as ρ(</p><formula xml:id="formula_9">d i , d j ) = (ρ (d i , d j ) + ρ (d j , d i )) / n(d i , d j ), where n(d i , d j ) is a normalizer. The normalizer n is defined as n(d i , d j ) = |K(d i , d j )| + |K(d j , d i )| + λ,</formula><p>where |K(·)| is the count of IPTs and λ is a constant. λ ensures that the L1 norm of the ALFD increases as we have more supporting K(d i , d j ) and converges to 1. We use λ = 20 in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning the Model Weights</head><p>We learn the model parameters w ∆t from a training dataset with a weighted voting. Given a set of detections D T 1 and corresponding ground truth (GT) target annotations, we first assign the GT target id to each detections. For each detection d i , we measure the overlap with all the GT boxes in t i . If the best overlap o i is larger than 0.5, the corresponding target id (id i ) is assigned. Otherwise, −1 is assigned. For all detections that has id i ≥ 0 (positive detections), we collect a set of detections P ∆t</p><formula xml:id="formula_10">i = {d j ∈ D T 1 |t j − t i = ∆t}.</formula><p>For each pair, we compute the margin m ij as follows:</p><formula xml:id="formula_11">if id i and id j are identical, m ij = (o i − 0.5) · (o j − 0.5). Other- wise, m ij = −(o i − 0.5) · (o j − 0.5).</formula><p>Intuitively, m ij shall have a positive value if the two detections are from the same target, while m ij will have a negative value, if the d i and d j are from different targets. The magnitude is weighted by the localization accuracy. Given all the pairs and margins, we learn the model w ∆t as follows:</p><formula xml:id="formula_12">w∆t = {i∈D T 1 |id i ≥0} j∈P ∆t i mij (ρ (di, dj ) + ρ (dj , di)) {i∈D T 1 |id i ≥0} j∈P ∆t i |mij |(ρ (di, dj ) + ρ (dj , di))<label>(1)</label></formula><p>The algorithm computes a weighted average with a sign over all the ALFD patterns, where the weights are determined by the overlap between targets and detections. Intuitively, the ALFD pattern between detections that matches well with GT contributes more on the model parameters. The advantage of the weighted voting method is that each element in w ∆t are bounded in <ref type="figure">Fig. 4</ref> shows two learned model using our method. One can adopt alternative learning algorithms like SVM <ref type="bibr" target="#b5">[6]</ref>.</p><formula xml:id="formula_13">[−1, 1], thus the ALFD metric, a A (d i , d j ), is also bounded by [−1, 1] since ||ρ(d i , d j )|| 1 ≤ 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Properties</head><p>In this section, we discuss the properties of ALFD affinity metric a A (d i , d j ). Firstly, unlike appearance or spatial metrics, ALFD implicitly exploit the information in all the images between t i and t j through IPTs. Secondly, thanks to the collective nature of ALFD design, it provides strong affinity metric over arbitrary length of time. We observe a significant benefit over the appearance or spatial metric especially over a long temporal distance (see Sec. 6.1 for the analysis). Thirdly, it is generally applicable to any scenarios (either static or moving camera) and for any object types (person or car). A disadvantage of the ALFD is that it may become unreliable when there is an occlusion. When an occlusion happens to a target, the IPTs initiated from the target tend to adhere to the occluder. It motivates us to combine target dynamics information discussed in Sec. 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Near Online Multi-target Tracking (NOMT)</head><p>We employ a near-online multi-target tracking framework that updates and outputs targets A t in each time frame considering inputs in a temporal window [t−τ, t]. We implement the NOMT algorithm with a hypothesis generation and selection scheme. For the convenience of discussion, we define clean targets A * t−1 = {A * t−1 . Each H t m,k may contain 0 to τ detections (at one time frame, there can be 0 or 1 detection). Given the set of hypotheses for all the existing and new targets, the algorithm finds the most consistent set of hypotheses (MAP) for all the targets (one for each) using a graphical model (sec. 4.3). As the key characteristic, our algorithm is able to fix any association error (for the detections within the temporal window [t−τ, t] ) made in the previous time frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Representation</head><p>Before going into the details of each step, we discuss our underlying model representation. The model is formulated as an energy minimization framework;</p><formula xml:id="formula_14">x = argmin x E(A * t−1 , H t (x), D t t−τ , V t 1 )</formula><p>, where x is an integer state vector indicating which hypothesis is chosen for a corresponding target, H t is the set of all the hypotheses</p><formula xml:id="formula_15">{H t 1 , H t 2 , ...}, and H t (x) is a set of selected hypothesis {H t 1,x1 , H t 2,x2 , ...}.</formula><p>Solving the optimization, the updated targets A t can be uniquely identified by augmenting A * t−1 with the selected hypothesis H t (x). Hereafter, we drop V t 1 and D t t−τ to avoid clutters in the equations. The energy is defined as follows:</p><formula xml:id="formula_16">E(A * t−1 , H t (x)) = m∈A * t−1 Ψ(A * t−1 m , H t m,xm ) + (m,l)∈A * t−1 Φ(H t m,xm , H t l,x l )<label>(2)</label></formula><p>where Ψ(·) encodes individual target's motion, appearance, and ALFD metric consistency, and Φ(·) represent an exclusive relationship between different targets (e.g. no two targets share the same detection). If there are hypotheses for newly entering targets, we define the corresponding target as an empty set,</p><formula xml:id="formula_17">A * t−1 m = ø.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Target Consistency</head><p>The potential measures the compatibility of a hypothesis H t m,xm to a target A * t−1 m . Mathematically, this can be decomposed into unary, pairwise and high order terms as follows:     </p><formula xml:id="formula_18">Ψ(A * t−1 m , H</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual Exclusion</head><p>This potential penalizes choosing two targets with large overlap in the image plane (repulsive force) as well as duplicate assignments of a detection. Instead of using "hard" exclusion constraints as in the Hungarian Algorithm <ref type="bibr" target="#b21">[22]</ref>, we use "soft" cost function for flexibility and computational simplicity. If the single target consistency is strong enough, soft penalization cost could be overcome. Also, this formulation makes it possible to reuse popular graph inference algorithms discussed in Sec. 4.3. The potential can be written as follows:</p><formula xml:id="formula_20">Φ(H t m,xm , H t l,x l ) = t f =t−τ α · o 2 (d(H t m,xm , f ), d(H t l,x l , f )) + β · I(d(H t m,xm , f ), d(H t l,x l , f ))<label>(4)</label></formula><p>where d(H t m,xm , f ) gives the associated detection of</p><formula xml:id="formula_21">H t m,xm at time f (if none, ø is returned), o 2 (d i , d j ) = 2 * IoU (d i , d j ) 2 , and I(d i , d j )</formula><p>is an indicator function. The former penalizes having too much overlap between hypotheses and the later penalizes duplicate assignments of detections. We use α = 0.5 and β = 100 (large enough to avoid duplicate assignments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hypothesis Generation</head><p>Direct optimization over the aforementioned objective function (eq. 2) is infeasible since the space of H t is huge in practice. To cope with the challenge, we first propose a set of candidate hypotheses H m for each target independently ( <ref type="figure" target="#fig_4">Fig. 5(b)</ref>) and find a coherent solution (MAP) using a CRF inference algorithm (sec. 4.3). As all the subsequent steps depend on the generated hypotheses, it is critical to have a comprehensive set of target hypotheses. We generates the hypotheses of existing and new targets using tracklets. Notice that following steps could be done in parallel since we generate the hypotheses set per target independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracklet Generation</head><p>For all the confident detections (∀d i ∈ D t t−τ , s.t. s i &gt; 0), we build a tracklet using the ALFD metric a A . Starting from one detection tracklet T i = {d i }, we grow the tracklet by greedily adding the best matching detection d k such that</p><formula xml:id="formula_22">k = argmax k∈D t t−τ \Ti max j∈Ti a A (d j , d k ), where D t t−τ \T i is the set of detections in [t−τ, t]</formula><p>excluding the frames already included in T i . If the best ALFD metric is lower than 0.4 or T i is full (has τ number of detections), the iteration is terminated. In addition, we also extracts the residual detections from each A t−1 m in [t−τ, t] to obtain additional tracklets (i.e. ∀m, A t−1 m \A * t−1 m ). Since there can be identical tracklets, we keep only unique tracklets in the output set T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypotheses for Existing Targets</head><p>We generate a set of target hypotheses H t m for each existing target A * t−1 m using the tracklets T. In order to avoid having unnecessarily large number of hypotheses, we employ a gating strategy. For each target A * t−1 m , we obtain a target predictor using the least square algorithm with polynomial function <ref type="bibr" target="#b23">[24]</ref>. We vary the order of the polynomial depending on the dataset (1 for MOT and 2 for KITTI). If there is an overlap (IoU) larger than a certain threshold between the prediction and the detections in the tracklet T i at any frame in [t−τ, t], we add T i to the hypotheses set H t m . In practice, we use a conservative threshold 0.1 to have a rich set of hypotheses. Too old targets (having no associated detection in [t−τ −T active , t]) are ignored to avoid unnecessary computational burden. We use T active = 1 sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New Target Hypotheses</head><p>Since new targets can enter the scene at any time and at any location, it is desirable to automatically identify new targets. Our algorithm can naturally identify the new targets by treating any tracklet in the set T as a potential new target. We use a non-maximum suppression on tracklets to avoid having duplicate new targets. For each tracklet T i , we simply add an empty target A * t−1 m = ø to A * t−1 with an associated hypotheses set H t m = {ø, T i }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference with Dynamic Graphical Model</head><p>Once we have all the hypotheses for all the new and existing targets, the problem (eq. 2) can be formulated as an inference problem with an undirected graphical model, where one node represents a target and the states are hypothesis indices as shown in <ref type="figure" target="#fig_4">Fig. 5 (c)</ref>. The main challenges in this problem are: 1) there may exist loops in the graphical model representation and 2) the structure of graph is different depending on the hypotheses at each circumstance. In order to obtain the exact solution efficiently, we first analyze the structure of the graph on the fly and apply appropriate inference algorithms based on the structure analysis. Given the graphical model, we find independent subgraphs (shown as dashed boxes in <ref type="figure" target="#fig_4">Fig. 5 (c)</ref>) using connected component analysis <ref type="bibr" target="#b16">[17]</ref> and perform individual inference algorithm per each subgraph in parallel. If a subgraph is composed of more than one node, we use junctiontree algorithm <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref> to obtain the solution for corresponding subgraph. Otherwise, we choose the best hypothesis for the target.</p><p>Once the states x are found, we can uniquely identify the new set of targets by augmenting A * t−1 with H t (x):</p><formula xml:id="formula_23">A * t−1 + H t (x) → A t .</formula><p>This process allows us to adjust any associations of A t−1 in [t−τ, t] (i.e. addition, deletion, replacement, or no modification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model Details</head><p>In this section, we discuss the details of the potentials described in the Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Unary potential</head><p>As discussed in the previous sections, we utilize the ALFD metric as the main affinity metric to compare detections. The unary potential for each detection in the hypothesis is measured by:</p><formula xml:id="formula_24">µ A (A * t−1 m , d i ) = − ∆t∈N a A (d(A * t−1 m , t i − ∆t), d i )<label>(5)</label></formula><p>where N is a predefined set of neighbor frame distances and d(A * t−1 m , t i ) gives the associated detection of A * t−1 m at t i . Although we can define an arbitrarily large set of N , we choose N = {1, 2, 5, 10, 20} for computational efficiency while modeling long term affinity measures.</p><p>Although ALFD metric provides very strong information in most of the cases, there are few failure cases including occlusions, erroneous IPTs, etc. To complement such cases, we design an additional Target Dynamics (TD) feature µ T (A * t−1 m , d i ). Using the same polynomial least square predictor discussed in Sec. 4.2, we define the feature as follows:</p><formula xml:id="formula_25">µ T (A * t−1 m , d i ) = ∞, if o 2 (p(A * t−1 m , t i ), d i ) &lt; 0.5 −η t i −f (A * t−1 m ) o 2 (p(A * t−1 m , t i ), d i ), otherwise<label>(6)</label></formula><p>where η is a decay factor (0.98) that discounts long term prediction, f (A * t−1 m ) denotes the last associated frame of A * t−1 m , o 2 represents IoU 2 discussed in the Sec. 4.1, and p is the polynomial least square predictor described in Sec. 4.2.</p><p>Using the two measures, we define the unary potential ψ u (A * t−1 m , d i ) as:</p><formula xml:id="formula_26">ψu(A * t−1 m , d i ) = min(µ A (A * t−1 m , d i ), µ T (A * t−1 m , d i )) − s i<label>(7)</label></formula><p>where s i represents the detection score of d i . The min operator enables us to utilize the ALFD metric in most cases, but activate the TD metric only when it is very confident (more than 0.5 overlap between the prediction and the detection). If A * t−1 m is empty, the potential becomes −s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Pairwise potential</head><p>The pairwise potential ψ p (·) is solely defined by the ALFD metric. Similarly to the unary potential, we define the pairwise relationship between detections in H t m,xm ,</p><formula xml:id="formula_27">ψp(d i , d j ) = −a A (d i , d j ), if |d i − d j | ∈ N 0, otherwise<label>(8)</label></formula><p>It measures the self-consistency of a hypothesis H t m,xm .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">High-order potential</head><p>We incorporate a high-order potential to regularize the target association process with a physical feasibility and appearance similarity. Firstly, inspired by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>, we implement the physical feasibility by penalizing the hypotheses that present an abrupt motion. Secondly, we encodes long term appearance similarity between all the detections in A * t−1 m and H t m,xm similarly to <ref type="bibr" target="#b28">[29]</ref>. The intuition is encoded by the following potential:</p><formula xml:id="formula_28">ψ h (A * t−1 m , H t m,xm ) = γ · i∈H t m,xm ξ(p(A * t−1 m ∪ H t m,xm , ti), di) + · (i,j)∈A * t−1 m ∪H t m,xm θ − K(di, dj )<label>(9)</label></formula><p>where γ, , θ are scalar parameters, ξ(a, b) measures the sum of squared distances in (x, y, height) of the two boxes, that is normalized by the mean height of p in [t−τ, t], and K(d i , d j ) represents the intersection kernel for color histograms associated with the detections. We use a pyramid of LAB color histogram where the first layer is the full box and the second layer is 3 × 3 grids. Only the A and B channels are used for the histogram with 4 bins per each channel (resulting in 4 × 4 × (1 + 9) bins). We use (γ, , θ) = (20, 0.4, 0.8) in practice.  <ref type="table">Table 1</ref>. AUC of affinity metrics for varying t. Notice that ALFD provides a robust affinity metric even at 20 frames distance. The results verify that ALFD provides stable affinity measure regardless of object type or the camera motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Evaluation</head><p>In order to evaluate the proposed algorithm, we use the KITTI object tracking benchmark <ref type="bibr" target="#b14">[15]</ref> and MOT challenge dataset <ref type="bibr" target="#b0">[1]</ref>. KITTI tracking benchmark is composed of about 19, 000 frames (∼ 32 minutes). The dataset is composed of 21 training and 29 testing video sequences that are recorded using cameras mounted on top of a moving vehicle. Each video sequence has a variable number of frames from 78 to 1176 frames having a variable number of target objects (Car, Pedestrian, and Cyclist). The videos are recorded at 10 FPS. The dataset is very challenging since 1) the scenes are crowded (occlusion and clutter), 2) the camera is not stationary, and 3) target objects appears in arbitrary location with variable sizes. Many conventional assumptions/techniques adopted in multiple target tracking with a surveillance camera is not applicable in this case (e.g. fixed entering/exiting location, background subtraction, etc). MOT challenge is composed of 11, 286 frames (∼ 16.5 minutes) with varying FPS. The dataset is composed of 11 training and 11 testing video sequences. Some of the videos are recorded using mobile platform and the others are from surveillance videos. All the sequences contain only Pedestrians. As it is composed of videos with various configuration, tracking algorithms that are particularly tuned for a specific scenario would not work well in general. For the evaluation, we adopt the widely used CLEAR MOT tracking metrics <ref type="bibr" target="#b18">[19]</ref>. For a fair comparison to the other methods, we use the reference object detections provided by the both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">ALFD Analysis</head><p>We first run an ablative analysis on our ALFD affinity metric. We choose two sequences, KITTI's 0001 and MOT's PETS09-S2L1 both from the training sets, for the analysis. Given all the detections and the ground truth annotations, we first find the label association between detections and annotations. For each detection, we assign ground truth id if there is larger than 0.5 overlap. We collect all possible pairs of detections in 1, 2, 5, 10, 20 frame distance   <ref type="table">Table.</ref> 1. X axis is False-Positive-Rate and Y axis is True-Positive-Rate. Notice that NDist2 measure becomes quickly unreliable as the temporal distance increases, when the camera is moving.</p><p>(∆t), to obtain the positive and negative pairs. As the baseline affinity measures, we use the L2 distance between bottom center of the detections that is normalized by the mean height of the two (NDist2) and the intersection kernel between the color histograms of the two (HistIK). <ref type="figure" target="#fig_7">Fig. 6</ref> and <ref type="table">Table.</ref> 1 show the ROC curve and AUC of each affinity metric. We observe that ALFD affinity metric performs the best in all temporal distance regardless of the camera configuration and object type. As the temporal distance increases, the other metrics become quickly unreliable as expected, whereas our ALFD metric still provides strong cue to compare different detections. <ref type="table">Table.</ref> 2 summarizes the evaluation accuracy of our method (NOMT) and the other state-of-the-art algorithms on the whole 28 test video sequences 2 . We also implemented an online tracking algorithm with the Hungarian method <ref type="bibr" target="#b21">[22]</ref> (HM) using our unary match function. Any match cost larger than −0.5 is set to be an invalid match. In following evaluations, we set the temporal window τ = 10 and filter out targets that either have only one detection or a median detection score lower than 0. We use the Kalman Filter <ref type="bibr" target="#b32">[33]</ref> to obtain continuous trajectories out of discrete detection sets A. Since the KITTI evaluation system does not provide results on Cyclist category (due to lack of sufficient data), we report the accuracy of Car and Pedestrian categories. We also run the experiments with more advanced detection results (HM+ <ref type="bibr" target="#b31">[32]</ref> and NOMT+ <ref type="bibr" target="#b31">[32]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">KITTI Testing Benchmark Evaluation</head><p>As shown in the table, we observe that our algorithm (NOMT) outperforms the other state-of-the-art methods in most of the metrics with significant margins. Our method   produces much larger numbers of mostly tracked targets (MT) in both Car and Pedestrian experiments with smaller numbers of mostly lost targets (ML). This is thanks to the highly accurate identity maintenance capability of our algorithm demonstrated in the low number of identity switch (IDS) and fragmentation (FRAG). In turn, our method achieves highest MOTA compared to other state-of-the-arts (&gt; 10% for Car and &gt; 8% for Pedestrian), which summarize all aspects of tracking evaluation. Notice that the higher tracking accuracy results in the higher detection accuracy as shown in Recall, Precision, and F1 metrics. Our own HM baseline also performs better than the other state-of-the-art methods, which demonstrates the robustness of ALFD metric. However, due to the nature of pure online association and lack of high order potential, it ends up missing more targets as shown in the MT and ML measures. <ref type="table">Table.</ref> 3 summarizes the evaluation accuracy of our method (NOMT) and the other state-of-the-art algorithms on the MOT test video sequences <ref type="bibr" target="#b2">3</ref> . The website provides a set of reference detections obtained using <ref type="bibr" target="#b8">[9]</ref>.</p><formula xml:id="formula_29">Method Rec. ↑ Prec. ↑ F1 ↑ MOTA ↑ MOTP ↑ MT ↑ ML ↓ IDS ↓ FRAG ↓</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">MOT Challenge Evaluation</head><p>Similarly to the KITTI experiment, we observe that our algorithm outperforms the other state-of-the-art methods <ref type="bibr" target="#b2">3</ref> The comparison is also available at http://nyx.ethz.ch/ view_results.php?chl=2.  <ref type="table">Table 4</ref>. Computation time on KITTI and MOT test datasets. The total number of images is shown in parentheses. We report the average FPS (images/total) and the time (seconds) spent in IPT computation (IPT), Color Histogram extraction (CHist), Hypothesis generation (Hypos) that includes all the potential computations, and the CRF inference (Infer). Total time includes file IO (reading images). The main bottleneck is the optical flow computation in IPT module, that can be readily improved using a GPU architecture.</p><p>with significant margins. Our method achieves the lowest identity switch and fragmentation while achieving the highest detection accuracy (lowest False Positives (FP) and False Negatives (FN)). In turn, our method records the highest MOTA compared to the other state-of-the-arts with a significant margin (&gt; 14%). The two experiments demonstrate that our ALFD metric and NOMT algorithm is generally applicable to any application scenario. <ref type="figure">Fig. 7</ref> shows some qualitative examples of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Timing Analysis</head><p>In order to understand the timeliness of the NOMT method, we measure the latency by computing the difference between detection time (t i of d i in A T ) and the last MOT : AVG-TownCentre @ 237 MOT : TUD-Crossing @ 70 MOT : PETS09-S2L1 @ 306 MOT : PETS09-S2L2 @ 140 KITTI Train 0001 @ 225 KITTI Train 0009 @ 147 KITTI Train 0017 @ 34 KITTI Test 0007 @ 78 KITTI Test 0010 @ 73 KITTI Test 0016 @ 340 <ref type="figure">Figure 7</ref>. Qualitative examples of the tracking results. We show the bounding boxes together with the past trajectories (last 30 and 10 frames for MOT and KITTI, respectively). The color of the boxes and trajectories represents the identity of the targets. Notice that our method can generate long trajectories with consistent IDs in challenging situations, such as occlusion, fast camera motion, etc. The figure is best shown in color.</p><p>association time. The last association time is defined as: if a detection d i is newly added to a target A t m or replace any other detection d j (e.g. t i = t j ) in A t−1 m at t, t is recorded as the last association time for d i . If d i was in the A t−1 m , no change is made to the last association time of d i . The last association time tells us when the algorithm first recognizes the d i as a part of A T m (the final trajectory output for the target m). The mean and standard deviation are 0.59 ± 1.75 and 0.66 ± 1.87 with <ref type="bibr" target="#b31">[32]</ref> for the KITTI test set (84.7% and 83.9% with no latency) and 0.87 ± 2.04 for the MOT test set (77.6% with no latency). It shows that NOMT is indeed a near online method.</p><p>Our algorithm is not only highly accurate, but also very efficient. Leveraging on the parallel computation, we achieve a real-time efficiency (∼ 10F P S) using a 2.5GHz CPU with 16 cores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a novel Aggregated Local Flow Descriptor that enables us to accurately measure the affinity between a pair of detections and a Near Online Mutitarget Tracking that takes the advantages of both the pure online and global tracking algorithms. Our controlled experiment demonstrates that ALFD based affinity metric is significantly better than other conventional affinity metrics. Equipped with ALFD, our NOMT algorithm generates significantly better tracking results on two challenging largescaler datasets. In addition, our method runs in real-time that enables us to apply the method in a variety of applications including autonomous driving, real-time surveillance, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Illustrative figure for unidirectional ALFDs ρ (di, dj).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>j)∈H t m,xm ψp(di, dj ) + ψ h (A * t−1 m , H t m,xm )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Schematic illustration of NOMT algorithm. (a) Given a set of existing targets A t−1 and detections D t t−τ , (b) our method generates a set of candidate hypotheses H t using tracklets T . Constructing a CRF model with the hypotheses, (c) we select the most consistent solution x using our inference algorithm and (d) output targets A t are obtained by augmenting previous targets A t−1 with the solution H t (x). See text for the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ψ</head><label></label><figDesc>u encodes the compatibility of each detection d i in the target hypothesis H t m,xm using the ALFD affinity metric and Target Dynamics feature (Sec. 5.1). ψ p measures the pairwise compatibility (self-consistency of the hypothesis) between detections within H t m,xm (Sec. 5.2) using the ALFD metric. Finally, ψ h implements a long-term smoothness constraint and appearance consistency (Sec. 5.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Corresponding ROC curves for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1</head><label></label><figDesc>, A * t−1 2 , ...} that exclude all the associated detections in [t−τ, t−1]. Given a set of detections in [t−τ, t] and clean targets A * t−1 , we generate multiple target hypotheses H t m = {H t m,1 = ø, H t m,2 , H t m,3 ...} for each target A * t−1 m as well as newly entering targets, where ø (empty hypothesis) represents the termination of the target and each H t m,k indicates a set of candidate detections in [t−τ, t] that can be associated to a target (Sec. 4.2)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Multiple Target tracking accuracy for KITTI Car/Pedestrian tracking benchmark. ↑ represents that high numbers are better for the metric and ↓ means the opposite. The best numbers in each column are bold-faced. We use τ = 10 for NOMT and NOMT+<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell></cell><cell>Method</cell><cell>FP ↓</cell><cell>FN ↓</cell><cell>MOTA ↑</cell><cell>MOTP ↑</cell><cell>MT ↑</cell><cell>ML ↓</cell><cell>IDS ↓</cell><cell>FRAG ↓</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Pedestrian Tracking Benchmark</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DP [28]</cell><cell>Batch</cell><cell>13,171</cell><cell>34,814</cell><cell>14.5 %</cell><cell>70.8 %</cell><cell>6.0 %</cell><cell>40.8 %</cell><cell>4,537</cell><cell>3,090</cell></row><row><cell>TBD [14]</cell><cell>Batch</cell><cell>14,943</cell><cell>34,777</cell><cell>15.9 %</cell><cell>70.9 %</cell><cell>6.4 %</cell><cell>47.9 %</cell><cell>1,939</cell><cell>1,963</cell></row><row><cell>RMOT [34]</cell><cell>Online</cell><cell>12,473</cell><cell>36,835</cell><cell>18.6 %</cell><cell>69.6 %</cell><cell>5.3 %</cell><cell>53.3 %</cell><cell>684</cell><cell>1,282</cell></row><row><cell>CEM [25]</cell><cell>Batch</cell><cell>14,180</cell><cell>34,591</cell><cell>19.3 %</cell><cell>70.7 %</cell><cell>8.5 %</cell><cell>46.5 %</cell><cell>813</cell><cell>1,023</cell></row><row><cell>HM</cell><cell>Online</cell><cell>11,162</cell><cell>33,187</cell><cell>26.7 %</cell><cell>71.5 %</cell><cell>11.2 %</cell><cell>47.9 %</cell><cell>669</cell><cell>916</cell></row><row><cell>NOMT</cell><cell>Online</cell><cell>7,762</cell><cell>32,547</cell><cell>33.7 %</cell><cell>71.9 %</cell><cell>12.2 %</cell><cell>44.0 %</cell><cell>442</cell><cell>823</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Multiple Target tracking accuracy for MOT Challenge. ↑ represents that high numbers are better for the metric and ↓ means the opposite. The best numbers in each column are bold-faced. We use τ = 10 for NOMT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table. 4 summarizes the time spent in each computational module.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">[x], [y], [w], [h] operators represent the x, y, width and height value, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t t + Δt v.s.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The comparison is also available at http://www.cvlibs.net/ datasets/kitti/eval_tracking.php that includes other anonymous submissions.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://nyx.ethz.ch/" />
		<title level="m">MOT challenge</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discretecontinuous optimization for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple object tracking using k-shortest paths optimization. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning OpenCV: Computer vision with the OpenCV library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Reilly Media, Inc</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust tracking-by-detection using a detector confidence particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/˜cjlin/libsvm.5" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A general framework for tracking multiple people from a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust multi-person tracking from a mobile platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Very high accuracy velocity estimation using orientation tensors, parametric motion, and simultaneous segmentation of the motion field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Twogranularity tracking: Mediating trajectory and detection graphs for tracking under occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (5)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="552" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">3d traffic scene understanding from movable platforms. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algorithm 447: Efficient algorithms for graph manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Robust object tracking by hierarchical association of detection responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Keni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">MCMC-based particle filtering for tracking a variable number of interacting targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-target tracking by on-line learned discriminative appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Linear algebra with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Leon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Macmillan</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">libDAI: A free and open source C++ library for discrete approximate inference in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<ptr target="https://staff.fnwi.uva.nl/j.m.mooij/libDAI/.7" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2169" to="2173" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Globallyoptimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gmcp-tracker: Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Machine learning for highspeed corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Detection and tracking of point features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An introduction to the kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian multi-object tracking using motion context from multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
