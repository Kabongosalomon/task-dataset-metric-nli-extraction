<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNSUPERVISED LEARNING OF SEMANTIC AUDIO REPRESENTATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-06">6 Nov 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
							<email>arenjansen@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Mountain View, New York</settlement>
									<region>CA, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Plakal</surname></persName>
							<email>plakal@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Mountain View, New York</settlement>
									<region>CA, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratheet</forename><surname>Pandya</surname></persName>
							<email>ratheet@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Mountain View, New York</settlement>
									<region>CA, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Mountain View, New York</settlement>
									<region>CA, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
							<email>shershey@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Mountain View, New York</settlement>
									<region>CA, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Mountain View, New York</settlement>
									<region>CA, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Channing</forename><surname>Moore</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Mountain View, New York</settlement>
									<region>CA, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Mountain View, New York</settlement>
									<region>CA, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNSUPERVISED LEARNING OF SEMANTIC AUDIO REPRESENTATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-06">6 Nov 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Unsupervised learning</term>
					<term>triplet loss</term>
					<term>sound clas- sification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several classagnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The last few years have seen great advances in nonspeech audio processing, as popular deep learning architectures developed in the speech and image processing communities have been ported to this relatively understudied domain <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. However, these datahungry neural networks are not always matched to the available training data in the audio domain. While unlabeled audio is easy to collect, manually labeling data for each new sound application remains notoriously costly and time consuming. We seek to alleviate this incongruity by developing alternative learning strategies that exploit basic semantic properties of sound that are not grounded to an explicit labeling.</p><p>Recent efforts in the computer vision community have identified several class-independent constraints on natural images and videos that can be used to learn semantic representations <ref type="bibr" target="#b4">[5]</ref>. For example, object categories are invariant to camera angle, and tracking unknown objects in videos can provide novel examples for the same unknown category <ref type="bibr" target="#b5">[6]</ref>. For audio, we can identify several analogous constraints, which are not tied to any particular inventory of sound categories. First, we can apply category-preserving transformations to individual events of unknown type, such as adding Gaussian noise, translation in time within the analysis window, and small perturbations in frequency. Second, pairs of unknown sounds can be mixed to provide new, often natural sounding examples of both. Finally, Portions of this paper were submitted to ML4Audio 2017 workshop. sounds from within the same vicinity in a recording likely contain multiple examples of the same (or related) unknown categories.</p><p>If provided a set of labeled sound events of the form X is an example of category C, applying the above semantic constraints is interpretable as regular labeled data augmentation. However, when each example has unknown categorical assignment, standard classification loss functions can not be applied. We instead resort to deep metric learning using triplet loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, which finds a nonlinear mapping into a low dimensional space where simple Euclidean distance can express any desired relationship between examples of the form X is more like Y than like Z. Critically, while labeled examples can be converted to triplets to explicitly learn a semantic embedding (i.e., X has same class as Y , but different than Z), a triplet relationship need not be anchored to an explicit categorical assignment. This makes it a natural fit for our set of semantic constraints; indeed, a noisy version of a sound event is more semantically similar to the clean recording than another arbitrary sound. Moreover, since we can generate as many triplets from as much unlabeled data as we wish, we can support arbitrarily complex neural architectures.</p><p>To validate these ideas, we train embeddings using state-of-theart convolutional architectures on millions of triplets sampled from the AudioSet dataset <ref type="bibr" target="#b9">[10]</ref>, both with and without using the label information. We evaluate the learned embeddings as features for query-by-example sound retrieval and supervised sound event classification. Our results demonstrate that highly complex models can be trained from unlabeled triplets alone to produce representations that recover up to 84% of the performance gap between using the raw log mel spectrogram inputs and using fully-supervised embeddings trained on millions of labeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>There have been multiple past efforts to perform unsupervised deep representation learning on non-speech audio. Lee et al. <ref type="bibr" target="#b10">[11]</ref> applied convolutional deep belief networks to extract a representation for speech and music, but not general purpose nonspeech audio. More recently, a denoising autoencoder variant was used to extract features for environmental sound classification <ref type="bibr" target="#b11">[12]</ref>. While both approaches produced useful representations for their respective tasks, neither explicitly introduced training mechanisms to elicit semantic structure in their learned embeddings. Classical distance metric learning has also been applied to music in the past <ref type="bibr" target="#b12">[13]</ref>.</p><p>Recent zero-resource efforts in the speech processing community have explicitly aimed to learn meaningful linguistic units from untranscribed speech <ref type="bibr" target="#b13">[14]</ref>. With this goal, several weak supervision mechanisms have been proposed that are analogous to what we attempt to achieve for nonspeech audio. For speech, the relevant constraints are derived from the inherent linguistic hierarchy: repeated unknown words have the same unknown phonetic structure <ref type="bibr" target="#b14">[15]</ref>, conversations with same unknown topic have shared unknown words <ref type="bibr" target="#b15">[16]</ref>, etc. Various forms of deep metric learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> have been successfully applied using these speech-specific constraints.</p><p>Finally, so-called self-supervised approaches in the computer vision community are analogous to what we propose in this paper for audio. There, constraints based on egomotion <ref type="bibr" target="#b20">[21]</ref>, spatial/compositional context <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, object tracking <ref type="bibr" target="#b5">[6]</ref>, and colorization <ref type="bibr" target="#b4">[5]</ref> have all been evaluated. Recent efforts have extended this principle of self-supervision to joint audio-visual models that learn speech or audio embeddings using semantic constraints imposed by the companion visual signal <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LEARNING ALGORITHM</head><p>Our training procedure consists of two stages: (i) sampling training triplets from a collection of unlabeled audio recordings, and (ii) learning a map from input context windows extracted from spectrograms (matrices with F frequency channels and T frames) to a lower d-dimensional vector space using triplet loss optimization of convolutional neural networks. We summarize the triplet loss metric learning framework, and then formally define each of our triplet sampling strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Metric Learning with Triplet Loss</head><p>The goal of triplet loss-based metric learning is to estimate a map g : R F ×T → R d such that simple (e.g.) Euclidean distance in the target space correspond to highly complex geometric relationships in the input space. Training data is provided as a set T = {ti} N i=1 of example triplets of the form ti = (x</p><formula xml:id="formula_0">(i) a , x (i) p , x (i) n ), where x (i) a , x (i) p , x (i)</formula><p>n ∈ R F ×T are commonly referred to as the anchor, positive, and negative, respectively. The loss is given by</p><formula xml:id="formula_1">L(T ) = N i=1 g(x (i) a )−g(x (i) p ) 2 2 − g(x (i) a )−g(x (i) n ) 2 2 + δ + ,<label>(1)</label></formula><p>where · 2 is L2 norm, [·]+ is standard hinge loss, and δ is a nonnegative margin hyperparameter. Intuitively, the optimization is attempting to learn an embedding of the input data such that positive examples end up closer to their anchors than the corresponding negatives do, by some margin. Notice that the loss is identically zero when all training triplets satisfy the inequality (dropping index)</p><formula xml:id="formula_2">g(xa) − g(xp) 2 2 + δ ≤ g(xa) − g(xn) 2 2 .<label>(2)</label></formula><p>Thus we may also view the triplets as a collection of hard constraints on the inputs. This is an extremely flexible construct: any pairwise relationship between input examples that permits a relative ranking (i.e., (xa, xp) are more similar than (xa, xn)) complies. The learned distance then becomes a proxy for that pairwise relationship. The map g can be defined by a fully-connected, d-unit output layer of any modern deep learning architecture. The optimization is performed with stochastic gradient descent, though training time is greatly decreased with the use of within-batch semi-hard negative mining <ref type="bibr" target="#b27">[28]</ref>. Here, all examples in the batch are transformed under the current state of g, and the available negatives are reassigned to the anchor-positive pairs to make more difficult triplets. Specifically, we choose the closest negative to the anchor that is still further away than the positive (the absolute closest is vulnerable to label noise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Triplet Sampling Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Explicitly Labeled Data</head><p>In standard supervised learning, we are provided a set of labeled examples of the form Z = {(xi, yi)}, where each xi ∈ R F ×T and yi ∈ C for some set C of semantic categories. Triplet lossbased metric learning was originally formulated for this setting, and converting Z to a set of triplets is straightforward. For each c ∈ C, we randomly sample anchor-positive pairs (xa, xp) from Z such that ya = yp = c. Then, for each sampled pair, we attach as the triplet's negative an example (xn, yn) ∈ Z such that yn = c. This procedure sets the supervised performance topline in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Gaussian Noise</head><p>Since the introduction of the denoising autoencoders, learning representations that are invariant to small perturbations in the original data space has been a standard tool for unsupervised learning. However, when more complex convolutional architectures with pooling are desired, inverting the encoder function is complicated <ref type="bibr" target="#b28">[29]</ref>. However, since we are not interested in actually reconstructing the inputs, we can use triplet loss to effect similar representational properties using an arbitrary deep learning architecture. For each xi ∈ R F ×T in the provided set of unlabeled examples X , we simply sample one or more anchor-positive pairs of the form (xi, xp), where element</p><formula xml:id="formula_3">x p,tf = x i,tf (1+|ǫ tf |) for ǫ tf ∼ N (0, σ 2 )</formula><p>, a Gaussian distribution with mean 0 and standard deviation σ (a model hyperparameter). For each sampled anchor-positive pair, we simply choose another example in X as the negative to complete the triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Time and Frequency Translation</head><p>When processing long context windows of spectrogram, we are provided snapshots of the contained sound events with arbitrary temporal offsets and clipping. A transient sound event with unknown category that starts at the left edge of the window maintains its semantic assignment if it begins somewhere in the center. However, in the input space, this simple translation in time can produce dramatic transformations of the training data. Similarly, small translations in frequency may leave the semantics unchanged while greatly perturbing the input space. To exploit this we generate training triplets as follows. For each xi ∈ R F ×T in the provided set of unlabeled examples X , we sample one or more anchor-positive pairs of the form (xi, xp) where xp = TruncS(CircT (xi)). Here, CircT is a circular shift in time by an integer number of frames sampled uniformly from [0, T − 1], where T is the number of frames in the example. TruncS is a truncated shift in frequency by an integer number of bins sampled uniformly from [−S, S] (missing values after shift are set to zero energy). We again choose another example in X as the negative to complete the triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Example Mixing</head><p>Sound is often referred to as transparent, since we can superimpose sound recordings and still hear the constituents to some degree. We can use this intuition to construct triplets by mixing sounds together. One approach to this is forming a positive by mixing a random example with the anchor. However, if we subsequently attach a random negative, we can not guarantee we want to satisfy the inequality of Eq. 2. For example, if the anchor is a dog bark, and we mix it with a siren, we cannot assume a dog growl as the negative should be mapped further from the anchor than the mixture.</p><p>To solve this, we can mix random anchor and negative examples to form the positives. Given a random anchor xa and negative xn containing energies in each time-frequency cell, we construct positive xp = xa + α[E(xa)/E(xn)]xn, where E(x) is the total energy of example x, and α is a hyperparameter. Note that this is the only triplet type considered not strictly compatible with semi-hard negative mining, since negatives are not randomly sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">Temporal Proximity</head><p>Audio recordings from real world environments do not consist of events drawn completely at random from the space of all possible sounds. Instead, a given environment has limited subset of sound creating objects that are often closely, or even causally, related. As such, two events in the same recording more likely to be of the same, or at least related, event categories than any two random events in a large audio collection. We can use this intuition to sample triplets of the form (xa, xp, xn) where xa and xp are from the same recording, xn is from a different recording. We can further impose the constraint that |time(xa) − time(xp)| &lt; ∆t, where time(x) is the start time of example x, and ∆t is a hyperparameter. Note that if overlapping context windows and sufficiently small values of ∆t are used, this method is functionally similar to the time translation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6.">Joint Training</head><p>In supervised learning settings, it is not always trivial to combine multiple data sources from separate domains with distinct label inventories. For cases where simply using a classification layer that is the union of the categories is not possible (e.g. mutual exclusivity is not implied across the two class sets), we must resort to multi-task training objectives. For triplet loss, this is not a problem. All triplet sets produced using the sampling methods outlined in this section can be simply mixed together for training a joint embedding that reflects them all to whatever degree possible. In general, if we have preconceived notions of each constraint's importance, we can either introduce a source-dependent weight to each triplet's contribution to the loss function in Eq. 1 or, alternatively, use varying triplet sample sizes for each source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We evaluate embeddings that result from the triplet sampling methods of Section 3.2 in two downstream tasks: (i) query-by-example semantic retrieval of sound segments, and (ii) training shallow fullyconnected sound event classifiers. The query-by-example task does not involve any subsequent supervised training and thus directly measures the intrinsic semantic consistency of the learned representation. The shallow model measures how easily a relatively simple, non-convolutional classifier network can predict the sound event categories given the labeled data. Finally, we also perform a lightlysupervised classification experiment, where we repeat the shallow model evaluation with only a small fraction of the labeled data. This allows us to measure the utility of unlabeled data in reducing annotation requirements for any sound event classification application where unlabeled data is plentiful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Features</head><p>We use Google's recently released AudioSet database of manually annotated sound events <ref type="bibr" target="#b9">[10]</ref> for both training and evaluation. Au-dioSet consists of over 2 million 10-second audio segments from YouTube videos, each labeled using a comprehensive ontology of 527 sound event categories (minimum 120 segments/class). We use an internal version of the unbalanced training set (50%-larger than the released set), which we split into train and development subsets. We report all performance metrics on the released evaluation set. We compute 64-channel mel-scale spectrograms using an FFT window size of 25 ms with 10 ms step. Triplets are sampled in this energy domain since the some of our triplet sampling mechanisms require an energy interpretation, but a stabilized logarithm is applied before input to our models. We then process these spectrograms into nonoverlapping 0.96 second context windows, such that each training example is an F =64 by T =96 matrix. Each embedding model was trained on order 10 million triplets (40 million for joint model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Architecture</head><p>Given its impressive performance on previous large-scale sound classification evaluations <ref type="bibr" target="#b1">[2]</ref>, we use the ResNet-50 convolutional neural network architecture. Each input 64×96 context window is first processed by a layer of 64 convolutional 7×7 filters, followed by a 3×3 max pool with stride 2 in both dimensions. This is followed by 4 standard ResNet blocks and a final average pool over time and frequency to a 2048-dimensional representation. Instead of the classification output layer used in <ref type="bibr" target="#b1">[2]</ref>, all of our triplet models use a 128-unit fully-connected linear output layer. This produces a vector of dimension d =128, which represents a factor of 48 reduction from the original input dimensionality of 64×96. We also employ the standard practice of length normalizing the network output before input to the loss function (i.e., g = h/ h 2 , where h is the output embedding layer of the network). This normalization means the squared Euclidean distance used in the loss is proportional to using cosine distance, a common choice for learned representations. All training is performed using Adam, with hyperparameters optimized on the development set. We use semi-hard negative mining and a learning rate of 10 −4 for supervised, temporal proximity, and combined unsupervised models; otherwise 10 −6 is used with mining disabled. The margin hyperparameter δ is set to 0.1 in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Query-by-Example Retrieval</head><p>Our first evaluation task is query-by-example (QbE) segment retrieval. Here, no additional training is performed, making it a direct measurement of inherent semantic representational quality. We begin by mapping each 0.96 second context window in the evaluation set to its corresponding 128-dimensional embedding vector and average these across each AudioSet segment to arrive at a segmentlevel embedding. For each sound event category, from the AudioSet evaluation set we sample 100 segments where it is present, and 100 segments where it is not. We then compute the cosine distance between all 4,950 within-class pairs as target trials, and all 10,000 (present,not-present) pairs as nontarget trials. We sort this set of pairs by ascending distance and compute the average precision (AP) of ranking target over nontarget trials (random chance gives 0.33). We repeat this for each class and average the per-class AP score to produce the reported mean average precision (mAP). <ref type="table" target="#tab_0">Table 1</ref> shows the retrieval mAP for three of the triplet sampling mechanisms as a function of their associated hyperparameters. In each case, the optimal performing setting on the validation set also was optimal for eval (listed for each hyperparameter in bold). For the Gaussian noise and example mixing, we observed relatively weak dependence on sampling hyperparameter values. However, we found for the translation method that allowing larger shifts in frequency produce substantial improvements over time translations alone. While this may be surprising from a signal processing point of view, two-dimensional translations help to force increased spectral localization in the early layers' filters of the convolutional network, which is observed in fully-supervised models. Note that since all AudioSet clips are limited to at most 10 seconds, we did not explore additional limiting of proximity (i.e., ∆t is effectively clip duration). <ref type="table" target="#tab_1">Table 2</ref> shows the retrieval performance for each of the evaluated representations. The fully-supervised topline uses explicitly labeled data to sample the triplets. As a baseline, we evaluate the retrieval performance achieved using the raw log mel spectrogram features (each 64×96 context window is treated as a 6144-dimensional vector before segment-level averaging). For each of the unsupervised methods, we tuned on the development set and the reported performance here is on the separate evaluation set. At the bottom, we also list the performance of the joint embedding, trained on a mixture of all four unsupervised triplet types (approximately equal number of triplets from each). Alongside each mAP value, we also list the percentage of the baseline-to-topline performance gap recovered using each given unsupervised triplet embedding. We find that each unsupervised triplet method significant improves retrieval performance over the input features, with the joint unsupervised model improving mAP by 15% absolute over the input spectrogram features, and recovering more than 40% of the performance gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Sound Classification</head><p>While the retrieval task measures how the geometric structure of the representation mirrors AudioSet classes, we are also interested how our unsupervised methods aid an arbitrary downstream supervised task over the same or similar data. To test this, we use our various embeddings to train shallow, fully-connected networks using labeled AudioSet segments. For each feature, we consider classifiers with 1 and 2 hidden layers of 512 units each. The output layer consists of independent logistic regression models for the 527 classes. For each class, we compute segment-level scores (average of the frame-level predictions) for the evaluation set and compute average precision. We again report the mean average precision over the classes. <ref type="table" target="#tab_1">Table 2</ref> shows the classification performance for each of representation types. We again find substantial improvement over the input features in all cases, with temporal proximity the clear standout. Combining triplet sets provides additional gains, indicating the learned representation's ability to encode multiple types of semantic constraints for downstream tasks. Notice that our approach performs fully-unsupervised training of a ResNet-50 triplet embedding model that achieves 85% (0.244/0.288) the mAP of a fullysupervised ResNet-50 triplet embedding model, when both are coupled to a single hidden layer downstream classifier.</p><p>Finally, <ref type="table">Table 3</ref> shows performance of lightly-supervised classifiers trained on just 20 examples per class. To account for the variability in sample selection, we generate 3 random training samples, run the experiment separately on each, and report average performance. Here we evaluate three models: (i) a ResNet-50 classifier model, (ii) a fully-connected model trained from log mel spectrograms, and (iii) a fully-connected model trained on the joint unsupervised triplet embedding (last line of <ref type="table" target="#tab_1">Table 2</ref>). Since our unsupervised triplet embeddings are derived from the full AudioSet train set (as unlabeled data), a single layer classifier trained on top doubles the mAP of a full ResNet-50 classifier trained from raw inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have presented a new approach to unsupervised audio representation learning that explicitly elicits semantic structure. By sampling triplets using a variety of audio-specific semantic constraints that do not require labeled data, we learn a representation that greatly outperforms the raw inputs on both sound event retrieval and classification task. We found that the various semantic constraints are complementary, producing improvements when combined to train a joint triplet loss embedding model. Finally, we demonstrated that our best unsupervised embedding provides great advantage when training sound event classifiers in limited supervision scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Segment retrieval mean average precision (mAP) as function of: (left) Gaussian width σ for Gaussian noise triplets; (middle) frequency shift range S for translation triplets; (right) and mixing weight α for mixed example triplets. Best results in bold.</figDesc><table><row><cell>σ</cell><cell>mAP</cell><cell>S</cell><cell>mAP</cell><cell>α</cell><cell>mAP</cell></row><row><cell>0.1</cell><cell>0.453</cell><cell>0</cell><cell>0.461</cell><cell>0.1</cell><cell>0.483</cell></row><row><cell cols="2">0.25 0.466</cell><cell>2</cell><cell>0.492</cell><cell cols="2">0.25 0.489</cell></row><row><cell>0.5</cell><cell>0.478</cell><cell>5</cell><cell>0.493</cell><cell>0.5</cell><cell>0.487</cell></row><row><cell>1.0</cell><cell>0.478</cell><cell cols="2">10 0.508</cell><cell>1.0</cell><cell>0.476</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mean average precision for segment retrieval and shallow model classification using original log mel spectrogram and triplet embeddings as features. All embedding models use the same ResNet-50 architecture with a 128-dimensional linear output layer.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Classification</cell><cell cols="2">Classification</cell></row><row><cell></cell><cell></cell><cell cols="2">QbE Retrieval</cell><cell cols="4">(1 layer, 512 units) (2 layer, 512 units)</cell></row><row><cell></cell><cell>Representation</cell><cell cols="3">mAP recovery mAP</cell><cell>recovery</cell><cell>mAP</cell><cell>recovery</cell></row><row><cell cols="2">Explicit Label Triplet (topline)</cell><cell>0.790</cell><cell>100%</cell><cell>0.288</cell><cell>100%</cell><cell>0.289</cell><cell>100%</cell></row><row><cell cols="3">Log Mel Spectrogram (baseline) 0.423</cell><cell>0%</cell><cell>0.065</cell><cell>0%</cell><cell>0.102</cell><cell>0%</cell></row><row><cell cols="2">Gaussian Noise (σ = 0.5)</cell><cell>0.478</cell><cell>15%</cell><cell>0.096</cell><cell>14%</cell><cell>0.114</cell><cell>6%</cell></row><row><cell cols="2">T/F Translation (S = 10)</cell><cell>0.508</cell><cell>23%</cell><cell>0.108</cell><cell>19%</cell><cell>0.125</cell><cell>12%</cell></row><row><cell cols="2">Mixed Example (α = 0.25)</cell><cell>0.489</cell><cell>18%</cell><cell>0.103</cell><cell>17%</cell><cell>0.122</cell><cell>11%</cell></row><row><cell cols="3">Temporal Proximity (∆t = 10s) 0.562</cell><cell>38%</cell><cell>0.226</cell><cell>72%</cell><cell>0.241</cell><cell>74%</cell></row><row><cell cols="2">Joint Unsupervised Triplet</cell><cell>0.575</cell><cell>41%</cell><cell>0.244</cell><cell>80%</cell><cell>0.259</cell><cell>84%</cell></row><row><cell cols="4">Table 3. Lightly-supervised classifier performance averaged over</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">three trials, each trained with a different random draw of 20 seg-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ments/class (totaling 0.5% of labeled data).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Representation</cell><cell>Classifier Architecture</cell><cell>mAP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Log Mel Spectrogram</cell><cell cols="2">Fully Connected (4x512) 0.032</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Log Mel Spectrogram</cell><cell>ResNet-50</cell><cell>0.072</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Joint Unsupervised Triplet Fully Connected (1x512) 0.143</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep convolutional neural networks and data augmentation for acoustic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07160</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for polyphonic sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Cakır</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giambattista</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heikki</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06286</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A first attempt at polyphonic sound event detection using connectionist temporal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio set: A strongly labeled dataset of audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning based on deep models for environmental audio tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Sigtia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1230" to="1241" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a metric for music similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Society of Music-Information Retrieval</title>
		<meeting>the International Society of Music-Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Zero Resource Speech Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Thiolliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Nga</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3169" to="3173" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Weak top-down constraints for unsupervised acoustic model training.,&quot; in ICASSP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hynek</forename><surname>Hermansky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8091" to="8095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weak semantic context helps phonetic learning in a model of infant language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Phonetics embedding learning with side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="106" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised neural network based feature extraction using weak top-down constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5818" to="5822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint learning of speaker and phonetic similarities with Siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1295" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional acoustic word embeddings using word-pair side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4950" to="4954" />
		</imprint>
	</monogr>
	<note>2016 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08168</idno>
		<title level="m">Look, listen and learn</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="892" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spoken language with visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1858" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visually grounded learning of keyword prediction from untranscribed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08136</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
