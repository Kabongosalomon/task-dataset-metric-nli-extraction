<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Transformer Network for Utterance-level Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbiao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Wu</surname></persName>
							<email>wuchunhua@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangfeng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<email>wangxiaozhe@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Transformer Network for Utterance-level Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While there have been significant advances in detecting emotions in text, in the field of utterancelevel emotion recognition (ULER), there are still many problems to be solved. In this paper, we address some challenges in ULER in dialog systems.</p><p>(1) The same utterance can deliver different emotions when it is in different contexts or from different speakers.</p><p>(2) Long-range contextual information is hard to effectively capture. (3) Unlike the traditional text classification problem, this task is supported by a limited number of datasets, among which most contain inadequate conversations or speech. To address these problems, we propose a hierarchical transformer framework (apart from the description of other studies, the "transformer" in this paper usually refers to the encoder part of the transformer) with a lower-level transformer to model the word-level input and an upper-level transformer to capture the context of utterance-level embeddings. We use a pretrained language model bidirectional encoder representations from transformers (BERT) as the lower-level transformer, which is equivalent to introducing external data into the model and solve the problem of data shortage to some extent. In addition, we add speaker embeddings to the model for the first time, which enables our model to capture the interaction between speakers. Experiments on three dialog emotion datasets, Friends, Emotion-Push, and EmoryNLP, demonstrate that our proposed hierarchical transformer network models achieve 1.98%, 2.83%, and 3.94% improvement, respectively, over the state-of-the-art methods on each dataset in terms of macro-F1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis, considered one of the most important methods for analyzing real-world communication, is a kind of classification task for extracting emotion from language. It can help us progress in many fields, such as data mining and developing empathetic machines for people. In this paper, we consider one of the tasks in this research direction, utterance-level emotion recognition (ULER) . In ULER, an utterance <ref type="bibr" target="#b1">[Olson, 1977]</ref> is a unit of speech bounded by breathes or pauses, and its goal is to tag each utterance in a dialog with the indicated emotion (e.g., happy, sad, or angry). Traditional sentiment analysis methods are confined to analyzing only a single sentence or document, regardless of its surrounding information. However, in the field of ULER, contextual information is indispensable in emotional discrimination. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the utterance "Yes, I agree with this point." can deliver different emotions in different contexts. To identify a speaker's emotion precisely, <ref type="bibr" target="#b1">[Hazarika et al., 2018]</ref> produced contextual representations for prediction with a recurrent neural network (RNN), where each utterance is represented by a feature vector extracted by convolutional neural networks (CNN) at an earlier stage. Similarly, <ref type="bibr" target="#b1">[Jiao et al., 2019]</ref> proposed a hierarchical gated recurrent unit (HiGRU) framework with a lowerlevel GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings.</p><p>Theoretically, RNNs such as long short-term memory (LSTM) and gated recurrent units (GRUs) should propagate long-term contextual information. However, in practice, this is not always the case <ref type="bibr" target="#b1">[Bradbury et al., 2017]</ref>. In cases where the input sequence is long, RNNs may experience an exploding gradient or vanishing gradient. Unlike traditional text classification problems, in the field of ULER, there are a limited number of datasets, and most datasets contain inadequate conversations. This issue limits the possibility of obtaining larger models for this task. To solve this issue, <ref type="bibr" target="#b1">[Zhong et al., 2019]</ref> proposed a knowledge-enriched transformer (KET) to effectively incorporate contextual information and external knowledge bases, but this model structure is complex, and the running speed is not high. <ref type="bibr" target="#b1">[Jiao et at., 2019]</ref> proposed pretraining a context-dependent encoder (CoDE) for ULER by learning from unlabeled conversation data to address the aforementioned challenge, but the model did not perform better in the word-level embedding phase.</p><p>In this task, we propose a hierarchical transformer framework to solve the above issues. First, we use a transformer <ref type="bibr" target="#b1">[Vaswani et at., 2017]</ref> to model the word-level input and capture the contexts of utterance-level embeddings, which has been shown to be a powerful representation learning model in many NLP tasks and can exploit contextual information more efficiently than RNNs and CNNs. Second, for the data scarcity issue, we use a pretrained language model, bidirectional encoder representations from transformers (BERT) <ref type="bibr" target="#b1">[Devlin et al., 2018]</ref> as the lower-level transformer, which is equivalent to introducing external data into the model and helps our model obtain better utterance embedding. Third, the same utterance can deliver different emotions in the same context. For example, in <ref type="figure" target="#fig_1">Figure 2</ref>, the utterance "Yes, I agree. I think so, too." can deliver different emotions, joy and sadness. However, previous studies have not addressed this situation because those models did not capture the interaction between the speakers, and did not consider the emotional dynamics of the speakers in a dialog. To solve the problem, we introduce speaker embedding into our model. To the best of our knowledge, this is the first model for ULER with speaker embedding. After obtaining the contextual utterance embedding vectors with a hierarchical transformer framework, we feed them into the fully connected layers for classification. We employ dropout on the fully connected layers to prevent overfitting. Finally, we obtain an utterance category with a softmax layer. We summarize our contributions as follows:</p><p>• We propose a hierarchical transformer framework to better learn both the individual utterance embeddings and the contextual information of utterances.</p><p>• We use a pretrained language model, BERT, to obtain better dialog embedding, which is equivalent to introducing external data into the model and solve the problem of data shortage to some extent.</p><p>• For the first time, we use speaker embedding in the model for the ULER task, which allows our model to capture the interaction between speakers and better understand emotional dynamics in dialog systems.</p><p>• Our model outperforms state-of-the-art models on three benchmark datasets, Friends, EmotionPush, and EmoryNLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Text-based emotion recognition is a long-standing research topic, and there have been many excellent studies. However, these models do not perform well in the field of ULER because they treat texts independently and thus cannot capture the interdependence of utterances in dialogs. To capture the contexts of utterance-level embeddings more effectively, we propose a hierarchical transformer framework, which is mainly explored in the following topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Individual Utterance Information Extraction</head><p>In traditional methods, a common method of expressing text is the bag-of-words method. However, the bag-of-words method loses the order of the words. The n-gram model is a very popular statistical language model and usually performs well <ref type="bibr" target="#b2">[Thorsten, 1998]</ref>. However, the n-gram model has a large defect in that it is affected by data sparsity <ref type="bibr" target="#b2">[Bengio et at., 2013]</ref>. Recently, neural network methods have become increasingly popular. There is a trend moving from traditional methods to deep learning methods to obtain better text representations. Some prominent models include recursive autoencoders (RAEs) <ref type="bibr" target="#b3">[Socher et al., 2011]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b4">[Kim, 2014]</ref>, and recurrent neural networks (RNNs) [Abdul-Mageed and Ungar, 2017]. Although we can train a more complex model with a neural network, when the quantity of data is small, it does not perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pretrained Language Models</head><p>Unsupervised pretraining is a special case of semisupervised learning where the goal is to find a good initialization point. Pretrained language models, such as ELMo <ref type="bibr" target="#b4">[Peters et al.,2018]</ref>, OpenAI <ref type="bibr">GPT [Radford et al., 2018]</ref>, and BERT <ref type="bibr" target="#b1">[Devlin et al., 2018]</ref>, have achieved great success in a variety of NLP tasks, such as sentiment analysis and textual classification. They can generate deep contextualized embeddings since they are pretrained on a massive unlabeled corpus (i.e., English Wikipedia). Some proposed models <ref type="bibr" target="#b5">[Sun et al., 2019]</ref> with pretrained language models have obtained outstanding results on the sentiment analysis task of individual sentences.</p><p>[Reimers et at., 2019] proposed Siamese BERT-networks (SBERT) to obtain sentence embeddings and proved that their model outperforms other state-of-the-art sentence embedding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contextual Information Extraction</head><p>The RNN architecture is a standard method for capturing the sequential relationship of data. <ref type="bibr" target="#b5">[Poria et at., 2015]</ref> captured the contextual information with a bidirectional long shortterm memory (BiLSTM) network and obtained great </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed Forward Network</head><p>Multi-Head Self-Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed Forward Network</head><p>Multi-Head Self-Attention ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed Forward Network</head><p>Multi-Head Self-Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed Forward Network</head><p>Multi-Head Self-Attention ... ...  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax</head><formula xml:id="formula_0">p i N u -1 i N u -2 i N u -3 i N u -4 i N u -5 i N u 5 u 4 u 3 u 2 u 1 u 6 u j u f ( ) 1 p 2 p 3 p 4 p 5 p 6 p i N p -1 i N p -2 i N p -3 i N p -4 i N p -5 i N p             f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Transformer</head><p>The transformer learns the dependencies between words based entirely on self-attention without any recurrent or convolutional layers. Due to its rich representation and fast computation, it has been applied to many NLP tasks, e.g., response matching in dialog systems <ref type="bibr" target="#b6">[Zhou et al., 2018]</ref> and language modeling <ref type="bibr" target="#b7">[Dai et al.,2019]</ref>. The success of transformer has raised a large body of follow-up work. Therefore, some transformer variations have also been proposed, such as <ref type="bibr">GPT [Radford et al., 2018]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we present the task definition and our proposed hierarchical transformer (HiTransformer) network. In addition, we propose a variation in HiTransformer by adding speaker embedding, named HiTransformer-s. The overall architecture of our models is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Let there be a set of speakers, = { } =1 , where is the number of speakers, and a set of emotions, = { } =1 , where is the number of emotions, such as anger, joy, sadness, and neutral. Assume we are given a set of dialogs, = { } =1 , where is the number of dialogs. In each dialog, = {( , , )} =1 is a sequence of utterances, where the utterance is spoken by ∈ with an emotion ∈ . Our goal is to train a model to find the most likely emotion from for each new utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HiTransformer: Hierarchical Transformer</head><p>Our HiTransformer consists of two-level transformers: the lower-level transformer models the word-level input and obtains the individual utterance embedding. The upper-level transformer captures the contextual information and obtains utterance-level embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Individual Utterance Embedding</head><p>For the input utterance = { } =1 , where is the − ℎ utterance in and is the number of words in the utterance . First, the utterance is lower-cased and tokenized according to a byte pair encoding (BPE) algorithm. If there are tokens exceeding the preset maximum length of input tokens, those tokens are excluded from the list. Then, we embed those tokens through WordPiece embeddings <ref type="bibr" target="#b8">[Wu et at., 2016]</ref> and obtained the token embeddings = { } =1 . Finally, the input embeddings = { } =1 are the summation of the token embeddings and the positional embeddings = { } =1 :</p><formula xml:id="formula_1">= ⊙ ( ϵ[1, ])<label>(1)</label></formula><p>where ⊙ denotes element − wise addition. We feed the input embeddings into the lower-level transformer to learn the individual utterance embedding. We adopt the transformer-based pretrained language model BERT (illustrated in <ref type="figure" target="#fig_4">Figure 4)</ref> as the lower-level transformer, which is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning both the left and right contexts in all layers. The detailed structure is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The language model converts input embeddings into contextual word embedding = { } =1 .</p><formula xml:id="formula_2">= ( )<label>(2)</label></formula><p>The individual utterance embedding is then obtained by max-pooling on the contextual word embeddings within an utterance, which can assist in retaining important information in each dimension:</p><formula xml:id="formula_3">( ) = ( )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Utterance Embedding</head><p>For the − ℎ dialog in , = {( , , )} =1 , the individual utterance embedding is { ( )} =1 . We concatenate the individual embeddings with the position embeddings to obtain = { ( ) ⊙ } =1 , where is the embedding of position . Then, we feed into the upper-level transformer to capture the sequential and contextual relationship of utterances in a dialog and obtain the contextual utterance embed-</p><formula xml:id="formula_4">ding = { } =1 . = ( )<label>(4)</label></formula><p>Then, we feed the contextual utterance embedding vector into the classifier, which consists of two linear layers, one activation function and dropout. Finally, we obtain the predicted vector over all emotions with a softmax function.</p><formula xml:id="formula_5">( ) = { &gt; 0 − ≤ 0 (5) ̂= ( 2 ( 1 + 1 ) + 2 )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HiTransformer-s: Hierarchical Transformer with speaker embeddings</head><p>The HiTransformer contains a main issue that it cannot capture the interaction of speakers in a dialog. For example, in <ref type="figure" target="#fig_1">Figure 2</ref>, the utterance "Yes, I agree. I think so, too." delivers different emotions, sadness and joy. However, the Hi-Transformer cannot tag it exactly. To solve this problem, we propose hierarchical transformer with speaker embeddings (HiTransformer-s), which can model the interaction of speakers in a dialog.</p><p>For the − ℎ dialog in , = {( , , )} =1 , the individual utterance embedding is { ( )} =1 and ( ) is the number of speakers in . We first encode all the speakers in with one-hot encoding and then pad them to the dimension of { ( )} =1 ) with 0 to obtain the speaker embeddings { ( )} =1 .</p><formula xml:id="formula_6">{ ( )} =1 = ( ℎ ({ } =1 ), { ( )} =1 ) (7)</formula><p>Finally, we concatenate the summation of the individual utterance embeddings and the embeddings of position with the speaker embeddings of every utterance as the input of the upper-level transformer.</p><formula xml:id="formula_7">= {( ( ) ⊙ ) ⊕ ( )} =1 (8)</formula><p>Where ⊙ denotes element − wise addition, and ⊕ is the concatenation operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>To solve the issue of class imbalance, following the above research <ref type="bibr" target="#b8">[Khosla, 2018]</ref>, we use weighted cross entropy as the training loss to weight the samples of minority classes as below.</p><formula xml:id="formula_8">= − 1 ∑ =0 ∑ ∑ 1 ∑ 2 (̂) ∈ =1 =1 (9) = ∑ ∈<label>(10)</label></formula><p>where denotes the number of utterances with emotion in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head><p>In this section, we present the datasets, evaluation metrics, baselines and experimental results of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Friends <ref type="bibr" target="#b9">[Hsu and Ku, 2018]</ref>: The dataset is annotated from the Friends TV Scripts, and each dialog in the dataset consists of a scene of multiple speakers. In total, there are 1,000 dialogs, which are split into three parts: 720 for training, 80 for validation, and 200 dialogs for testing. Each utterance is tagged with an emotion in a set of emotions, {anger, joy, sadness, neutral, surprise, disgust, fear, and nonneutral}.  EmotionPush <ref type="bibr" target="#b9">[Hsu and Ku, 2018]</ref>: The dataset consists of private conversations between friends on Facebook include 1,000 dialogs, which are split into 720, 80, and 200 dialogs for training, validation and testing, respectively. Each utterance is tagged with an emotion in a set of emotions as in the Friends dataset.</p><p>EmoryNLP <ref type="bibr" target="#b5">[Zahiri and Choi, 2018]</ref>: The dataset is annotated from the Friends TV Scripts as well. It includes 713 dialogs for training, 99 dialogs for validation and 85 dialogs for testing. The emotion labels include neutral, sad, mad, scared, powerful, peaceful, and joyful.</p><p>For the first two datasets, we follow previous works <ref type="bibr" target="#b1">[Jiao et at., 2019]</ref> to consider only four emotion classes, i.e., anger, joy, sadness, and neutral, and consider all the emotion classes for EmoryNLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Following <ref type="bibr" target="#b1">[Jiao et at., 2019]</ref>, which achieved the best performance on several ULER datasets, we choose macro-averaged F1-score as the primary metric for evaluating the performance of our models.</p><formula xml:id="formula_9">− 1 = ∑ 1 ∈ | |<label>(11)</label></formula><p>where 1 is the F1-score of emotion . We also report the weighted accuracy (WA) and unweighted accuracy (UWA), which were adopted in a previous work <ref type="bibr" target="#b9">[Hsu and Ku, 2018]</ref>.</p><formula xml:id="formula_10">WA = ∑ ∈ (12) UWA = ∑ ∈ | |<label>(13)</label></formula><p>where is the percentage of class in the testing set, and is the corresponding accuracy. As shown in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref>, most of the datasets in this paper have an imbalanced emotion distribution, so the F1-score is better for measuring the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Methods</head><p>We compare our model HiTransformer and HiTransformer-s with the following state-of-the-art baselines: <ref type="bibr" target="#b5">[Luo et at., 2018]</ref>: A self-attentive bidirectional LSTM model, an efficient model that achieved second place in the EmotionX Challenge <ref type="bibr" target="#b9">[Hsu and Ku, 2018]</ref>; <ref type="bibr" target="#b8">[Khosla, 2018]</ref>: A convolutional-deconvolutional autoencoder with more handmade features, and the winner of the EmotionX Challenge <ref type="bibr" target="#b9">[Hsu and Ku, 2018]</ref>; <ref type="bibr">et at., 2019]</ref>: A model with a 1-D CNN to extract the utterance embeddings, and a bidirectional LSTM to model the relationship of utterances; bcGRU <ref type="bibr" target="#b1">[Jiao et at., 2019]</ref>: A variant of + with a BiGRU to capture the utterance-level context; <ref type="bibr" target="#b1">[Jiao et at., 2019]</ref>: is a context-dependent encoder (CoDE) model with a bidirectional GRU that extracts the utterance embeddings and a bidirectional GRU that models the relationship of utterances;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SA-BiLSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-DCNN</head><formula xml:id="formula_11">+ [Jiao</formula><p>− <ref type="bibr" target="#b1">[Jiao et at., 2019]</ref>: A variant of that pretrains a context-dependent encoder (CoDE) for ULER by learning from unlabeled conversation data; HiGRU <ref type="bibr" target="#b1">[Jiao et al., 2019]</ref>: A hierarchical gated recurrent unit (HiGRU) framework with a lower-level GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings; HiGRU-f <ref type="bibr" target="#b1">[Jiao et al., 2019]</ref>: A variant of HiGRU with individual feature fusion; HiGRU-sf <ref type="bibr" target="#b1">[Jiao et al., 2019]</ref>: A variant of HiGRU with selfattention and feature fusion; SCNN <ref type="bibr" target="#b5">[Zahiri and Choi, 2018]</ref>: A sequence-based convolutional neural networks that utilizes the emotion sequence from the previous utterances for detecting the emotion of the current utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameters</head><p>We adopt the pretrained uncased BERT-Base 1 model as the lower-level transferable language model, where the maximum input length is 512. The number of combination layers of a multi-head attention and a feedforward neural network is 12. For the upper-level transformer layers, the number of transformer layers is 4 and the number of heads in the multihead attention is 8. For the classification layer, the internal hidden size of the classification layer is set to 300, and the dropout rate is 0.5 to prevent overfitting. We adopt Adam <ref type="bibr">[Kingma and Ba, 2015]</ref> as the optimizer with a batch size of 1 and a learning rate of 1 × 10 −5 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result Analysis</head><p>We report the empirical results in <ref type="table" target="#tab_5">Table 3</ref>, which present the overall performance of our models on all datasets. From these results, we make the following observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Baselines</head><p>Our proposed HiTransformer-s outperforms the state-of-theart methods with significant margins on all the datasets in terms of macro-F1 score. Specifically, HiTransformer-s obtains 1.98%, 2.83%, and 3.94% absolute improvement on Friends, EmotionPush, and EmoryNLP, respectively. In addition, for Friends, HiTransformer-s obtains 0.88% improvement compared with the best performance in the past in terms of WA, and 0.12% less than the best performance from HiGRU-sf in terms of UWA. However, HiTransformer-s obtains an 8.18% improvement compared with HiGRU-sf in terms of WA. For EmotionPush, although HiTransformer-s is 0.78% lower than SA-BiLSTM in terms of WA, HiTransformer-s is 8.03% above SA-BiLSTM in terms of WA. Similarly, HiTransformer-s is 5.07% lower than HiGRU-sf in terms of UWA and 13.92% above HiGRU-sf in terms of WA. For EmoryNLP, HiTransformer-s obtains 1.88% and 2.37% absolute improvement in terms of WA and UWA, respectively. The HiTransformer outperforms the state-of-the-art methods on all the datasets in terms of the macro-F1 score as well. The above results demonstrate the superior power of HiTransformer-s and HiTransformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">HiTransformer vs. HiTransformer-s</head><p>By analyzing ULER, we find that speaker information plays an important role in utterance classification. Therefore, we proposed HiTransformer-s on the basis of HiTransformer.</p><p>From <ref type="table" target="#tab_5">Table 3</ref>, we observe that HiTransformer-s outperforms HiTransformer on all three datasets in terms of macro-F1, WA, and UWA. Specifically, on Friends, HiTransformer-s attains 1.22%, 0.07% and 5.07% improvement over HiTransformer in terms of macro-F1, WA, and UWA, respectively.</p><p>On EmotionPush, HiTransformer-s attains 1.53%, 0.05% and 1.52% improvement over HiTransformer in terms of macro-F1, WA, and UWA. On EmoryNLP, HiTransformer-s attains 1.68%, 0.73% and 3.43% improvement over HiTransformer in terms of macro-F1, WA, and UWA. The results demonstrate that HiTransformer-s including speaker information is indeed capable of boosting the performance of the HiTransformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, to address utterance-level emotion recognition in dialog systems, we propose a hierarchical transformer (Hi-Transformer) framework with a lower-level transformer to model word-level input and an upper-level transformer to capture the contexts of utterance-level embeddings. To obtain better individual utterance embeddings, we adopt BERT, which is pretrained on a massive unlabeled corpus as the lower-level transformer. To enable HiTransformer to obtain speaker information, we propose HiTransformer-s. Our proposed hierarchical transformer models outperform the stateof-the-art methods on all three datasets, which demonstrates that hierarchical transformer models can sufficiently capture the available utterance information in a dialog. In the future, we plan to pretrain a transformer model to capture the relationship of utterances, similar to BERT, and adopt it as the upper-level transformer to capture the textual information more sufficiently, which can also address the problem of data scarcity in ULER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The utterance "Yes, I agree with this point." can deliver different emotions in different contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The utterance "Yes, I agree. I think so, too." delivers different emotions, joy and sadness, when the previous sentence is from Person A and Person B, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of our proposed HiTransformer-s. By removing the "Speaker Embedding" layer, we attain HiTransformer.performance. Similarly,<ref type="bibr" target="#b1">[Jiao et al., 2019]</ref> applied bidirectional GRU to model contextual information. In addition, they placed a self-attention layer in the hidden states of GRU and fused the attention outputs with the individual utterance embeddings to learn the contextual utterance embeddings.<ref type="bibr" target="#b5">[Luo et al., 2019]</ref> applied self-attention to model the context of textual features extracted by BiLSTM.<ref type="bibr" target="#b5">[Zahiri and Choi, 2018]</ref> proposed sequence-based convolutional neural networks (SCNN) that utilize emotion sequences from previous utterances to detect the emotion of the current utterance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Structure of BERT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>I like kittens very much, because they are so cute. [joy] I disagree with you. I hate kittens very much, because they are too noisy. [sad] I like kittens very much, because they are so cute. [joy] I disagree with you. I hate kittens very much, because they are too noisy. [sad]</figDesc><table><row><cell>Time</cell><cell></cell><cell>Time</cell><cell></cell></row><row><cell>Person A</cell><cell></cell><cell>Person A</cell><cell></cell></row><row><cell>Person B</cell><cell></cell><cell>Person B</cell><cell></cell></row><row><cell>Person A</cell><cell>Hello, Mary. Do you agree with me? [neu]</cell><cell>Person B</cell><cell>Hello, Mary. Do you agree with me? [neu]</cell></row><row><cell>Person C</cell><cell>Yes, I agree. I think so, too [joy]</cell><cell>Person C</cell><cell>Yes, I agree. I think so, too [sad]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Detailed descriptions of Friends and EmotionPush</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Dialog (#Utterance) Train Val</cell><cell>Test</cell><cell>Neutral</cell><cell>Joyful</cell><cell cols="2">Emotion Peaceful Powerful</cell><cell>Scared</cell><cell>Mad</cell><cell>Sad</cell></row><row><cell>EmoryNLP</cell><cell>713 (9934)</cell><cell>99 (1344)</cell><cell cols="2">85 (1328)</cell><cell>3776</cell><cell>2755</cell><cell>1191</cell><cell>1063</cell><cell>1645</cell><cell>1332</cell><cell>844</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Detailed descriptions of EmoryNLP</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Testing results on Friend, EmotionPush, and EmoryNLP</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hazarika</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08916</idno>
		<idno>arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<editor>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert</editor>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The bias of language in speech and writing. Harvard educational review</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Thorsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten ; Yoshua</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning. Pages 137-142</title>
		<imprint>
			<date type="published" when="1998-02" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
		</imprint>
	</monogr>
	<note>A neural probabilistic language model</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emonet: Fine-grained emotion detection with gated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kim ; Yoon Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Mageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,; Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Karthik Narasimhan,Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training[J</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-as-sets/researchcovers/languageunsupervised/languageun-derstandingpaper.pdf" />
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
	<note>ACL. Zahiri and Choi. Emotion detection on TV show transcripts with sequencebased convolutional neural networks. In AAAI</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
	</analytic>
	<monogr>
		<title level="m">Shuaichen Chang, Xuanjing Huang, Jian Tang, and Jackie Chi Kit Cheung</title>
		<imprint>
			<publisher>Pengfei Liu</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6762" to="6769" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Khosla</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Emotionx-ar: CNN-DCNN autoencoder based emotion classifier</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Socialnlp 2018 emotionx challenge overview: Recognizing emotions in dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ku ; Chao-Chun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Diederik P. Kingma and Jimmy Ba</editor>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
