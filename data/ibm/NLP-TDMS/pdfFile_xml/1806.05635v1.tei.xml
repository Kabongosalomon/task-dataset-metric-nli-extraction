<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Imitation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
						</author>
						<title level="a" type="main">Self-Imitation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The trade-off between exploration and exploitation is one of the fundamental challenges in reinforcement learning (RL). The agent needs to exploit what it already knows in order to maximize reward. But, the agent also needs to explore new behaviors in order to find a potentially better policy. The resulting performance of an RL agent emerges from this interaction between exploration and exploitation. This paper studies how exploiting the agent's past experiences improves learning in RL. More specifically, we hypothesize that learning to reproduce past good experiences can indirectly lead to deeper exploration depending on the domain. A simple example of how this can occur can be seen through our results on an example Atari game, Montezuma's Revenge (see <ref type="figure" target="#fig_0">Figure 1</ref>). In this domain, the first and more proximal source of reward is obtained by picking up the key. Obtaining the key is a precondition of the second and more distal source of reward (i.e., opening the door with the key). Many existing methods occasionally generate experiences that pick up the key and obtain the first reward, but fail to exploit these experiences often enough to learn how to open the door by exploring after picking up The baseline (A2C) often picks up the key as shown by the best episode reward in 100K steps (A2C (Best)), but it fails to consistently reproduce such an experience. In contrast, self-imitation learning (A2C+SIL) quickly learns to pick up the key as soon as the agent experiences it, which leads to the next source of reward (door).</p><p>the key. Thus, they end up with a poor policy (see A2C in <ref type="figure" target="#fig_0">Figure 1</ref>). On the other hand, by exploiting the experiences that pick up the key, the agent is able to explore onwards from the state where it has the key to successfully learn how to open the door (see A2C+SIL in <ref type="figure" target="#fig_0">Figure 1</ref>). Of course, this sort of exploitation can also hurt performance in problems where there are proximal distractor rewards and repeated exploitation of such rewards does not help in learning about more distal and higher rewards; in other words, these two aspects may both be present. In this paper we will empirically investigate many different domains to see how exploiting past experiences can be beneficial for learning agents.</p><p>The main contributions of this paper are as follows: (1) To study how exploiting past good experiences affects learning, we propose a Self-Imitation Learning (SIL) algorithm which learns to imitate the agent's own past good decisions. In brief, the SIL algorithm stores experiences in a replay buffer, learns to imitate state-action pairs in the replay buffer only when the return in the past episode is greater than the agent's value estimate. (2) We provide a theoretical justification of the SIL objective by showing that the SIL objective is derived from the lower bound of the optimal Q-function. <ref type="formula" target="#formula_0">(3)</ref> The SIL algorithm is very simple to implement and can be applied to any actor-critic architecture. (4) We demonstrate that SIL combined with advantage actor-critic (A2C) is competitive to the state-of-the-art count-based exploration actor-critic methods (e.g., Reactor-PixelCNN <ref type="bibr" target="#b23">(Ostrovski et al., 2017)</ref>) on several hard exploration Atari games <ref type="bibr" target="#b2">(Bellemare et al., 2013)</ref>; SIL also improves the overall performance of A2C across 49 Atari games. Finally, SIL improves the performance of proximal policy optimization (PPO) on MuJoCo continuous control tasks <ref type="bibr" target="#b4">(Brockman et al., 2016;</ref><ref type="bibr" target="#b37">Todorov et al., 2012)</ref>, demonstrating that SIL may be generally applicable to any actor-critic architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Exploration There has been a long history of work on improving exploration in RL, including recent work that can scale up to large state spaces <ref type="bibr" target="#b32">(Stadie et al., 2015;</ref><ref type="bibr" target="#b22">Osband et al., 2016;</ref><ref type="bibr" target="#b1">Bellemare et al., 2016;</ref><ref type="bibr" target="#b23">Ostrovski et al., 2017)</ref>. Many existing methods use some notion of curiosity or uncertainty as a signal for exploration <ref type="bibr" target="#b28">(Schmidhuber, 1991;</ref><ref type="bibr" target="#b33">Strehl &amp; Littman, 2008)</ref>. In contrast, this paper focuses on exploiting past good experiences for better exploration. Though the role of exploitation for exploration has been discussed <ref type="bibr" target="#b36">(Thrun, 1992)</ref>, prior work has mostly considered exploiting what the agent has learned, whereas we consider exploiting what the agent has experienced, but has not yet learned.</p><p>Episodic control Episodic control <ref type="bibr" target="#b11">(Lengyel &amp; Dayan, 2008)</ref> can be viewed as an extreme way of exploiting past experiences in the sense that the agent repeats the same actions that gave the best outcome in the past. MFEC  and NEC <ref type="bibr" target="#b26">(Pritzel et al., 2017)</ref> scaled up this idea to complex domains. However, these methods are slow during test-time because the agent needs to retrieve relevant states for each step and may generalize poorly as the resulting policy is non-parametric.</p><p>Experience replay Experience replay <ref type="bibr" target="#b14">(Lin, 1992)</ref> is a natural way of exploiting past experiences for parametric policies. Prioritized experience replay <ref type="bibr" target="#b17">(Moore &amp; Atkeson, 1992;</ref><ref type="bibr" target="#b27">Schaul et al., 2016)</ref> proposed an efficient way of learning from past experiences by prioritizing them based on temporal-difference error. Our self-imitation learning also prioritizes experiences based on the full episode rewards. Optimality tightening <ref type="bibr" target="#b8">(He et al., 2017)</ref> introduced an objective based on the lower/upper bound of the optimal Qfunction, which is similar to a part of our theoretical result. These recent advances in experience replay have focused on value-based methods such as Q-learning, and are not easily applicable to actor-critic architectures.</p><p>Experience replay for actor-critic In fact, actor-critic framework <ref type="bibr" target="#b34">(Sutton et al., 1999;</ref><ref type="bibr" target="#b10">Konda &amp; Tsitsiklis, 2000)</ref> can also utilize experience replay. Many existing methods are based on off-policy policy evaluation <ref type="bibr" target="#b25">(Precup et al., 2001;</ref>, which involves importance sampling. For example, ACER <ref type="bibr" target="#b39">(Wang et al., 2017)</ref> and Reactor <ref type="bibr" target="#b6">(Gruslys et al., 2018)</ref> use Retrace  to evaluate the learner from the behavior policy. Due to importance sampling, this approach may not benefit much from the past experience if the policy in the past is very different from the current policy. Although DPG <ref type="bibr" target="#b31">(Silver et al., 2014;</ref><ref type="bibr" target="#b13">Lillicrap et al., 2016)</ref> performs experience replay without importance sampling, it is limited to continuous control. Our self-imitation learning objective does not involve importance sampling and is applicable to both discrete and continuous control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection between policy gradient and Q-learning</head><p>The recent studies on the relationship between policy gradient and Q-learning have shown that entropy-regularized policy gradient and Q-learning are closely related or even equivalent depending on assumptions <ref type="bibr" target="#b19">(Nachum et al., 2017;</ref><ref type="bibr" target="#b21">O'Donoghue et al., 2017;</ref><ref type="bibr" target="#b29">Schulman et al., 2017a;</ref><ref type="bibr" target="#b7">Haarnoja et al., 2017)</ref>. Our application of self-imitation learning to actor-critic (A2C+SIL) can be viewed as an instance of PGQL <ref type="bibr" target="#b21">(O'Donoghue et al., 2017)</ref> in that we perform Qlearning on top of actor-critic architecture (see Section 4). Unlike Q-learning in PGQL, however, we use the proposed lower bound Q-learning to exploit good experiences.</p><p>Learning from imperfect demonstrations A few studies have attempted to learn from imperfect demonstrations, such as DQfD <ref type="bibr" target="#b9">(Hester et al., 2018)</ref>, Q-filter <ref type="bibr" target="#b20">(Nair et al., 2017)</ref>, and normalized actor-critic <ref type="bibr" target="#b40">(Xu et al., 2018)</ref>. Our self-imitation learning has a similar flavor in that the agent learns from imperfect demonstrations. However, we treat the agent's own experiences as demonstrations without using expert demonstrations. Although a similar idea has been discussed for program synthesis <ref type="bibr" target="#b12">(Liang et al., 2016;</ref><ref type="bibr" target="#b0">Abolafia et al., 2018)</ref>, this prior work used classification loss without justification. On the other hand, we propose a new objective, provide a theoretical justification, and systematically investigate how it drives exploration in RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-Imitation Learning</head><p>The goal of self-imitation learning (SIL) is to imitate the agent's past good experiences in the actor-critic framework. To this end, we propose to store past episodes with cumulative rewards in a replay buffer: D = {(s t , a t , R t )}, where s t , a t are a state and an action at time-step t, and R t = ∞ k=t γ k−t r k is the discounted sum of rewards with a discount factor γ. To exploit only good state-action pairs in the replay buffer, we propose the following off-policy actor-critic loss:</p><formula xml:id="formula_0">L sil = E s,a,R∈D L sil policy + β sil L sil value (1) L sil policy = − log π θ (a|s) (R − V θ (s)) + (2) L sil value = 1 2 (R − V θ (s)) + 2<label>(3)</label></formula><p>where (·) + = max(·, 0), and π θ , V θ (s) are the policy (i.e., actor) and the value function parameterized by θ. β sil ∈ R + Algorithm 1 Actor-Critic with Self-Imitation Learning Initialize parameter θ Initialize replay buffer D ← ∅ Initialize episode buffer E ← ∅ for each iteration do # Collect on-policy samples for each step do Execute an action s t , a t , r t , s t+1 ∼ π θ (a t |s t )</p><formula xml:id="formula_1">Store transition E ← E ∪ {(s t , a t , r t )} end for if s t+1 is terminal then # Update replay buffer Compute returns R t = ∞ k γ k−t r k for all t in E D ← D ∪ {(s t , a t , R t )} for all t in E Clear episode buffer E ← ∅ end if # Perform actor-critic using on-policy samples θ ← θ − η∇ θ L a2c (Eq. 4) # Perform self-imitation learning for m = 1 to M do Sample a mini-batch {(s, a, R)} from D θ ← θ − η∇ θ L sil (Eq. 1) end for end for</formula><p>is a hyperparameter for the value loss.</p><p>Note that L sil policy can be viewed as policy gradient using the value V θ (s) as the state-dependent baseline except that we use the off-policy Monte-Carlo return (R) instead of on-policy return. L sil policy can also be interpreted as cross entropy loss (i.e., classification loss for discrete action) with sample weights proportional to the gap between the return and the agent's value estimate (R − V θ ). If the return in the past is greater than the agent's value estimate (R &gt; V θ ), the agent learns to choose the action chosen in the past in the given state. Otherwise (R ≤ V θ ), and such a stateaction pair is not used to update the parameter due to the (·) + operator. This encourages the agent to imitate its own decisions in the past only when such decisions resulted in larger returns than expected. L sil value updates the value estimate towards the off-policy return R.</p><p>Prioritized Replay The proposed self-imitation learning objective L sil is based on our theoretical result discussed in Section 4. In theory, the replay buffer (D) can be any trajectories from any policies. However, only good state-action pairs that satisfy R &gt; V θ can contribute to the gradient during self-imitation learning (Eq. 1). Therefore, in order to get many state-action pairs that satisfy R &gt; V θ , we propose to use the prioritized experience replay . More specifically, we sample transitions from the replay buffer using the clipped advantage (R − V θ (s)) + as priority (i.e., sampling probability is proportional to (R − V θ (s)) + ). This naturally increases the proportion of valid samples that satisfy the constraint (R − V θ (s)) + in SIL objective and thus contribute to the gradient.</p><p>Advantage Actor-Critic with SIL (A2C+SIL) Our selfimitation learning can be combined with any actor-critic method. In this paper, we focus on the combination of advantage actor-critic (A2C) <ref type="bibr" target="#b16">(Mnih et al., 2016)</ref> and selfimitation learning (A2C+SIL), as described in Algorithm 1. The objective of A2C (L a2c ) is given by <ref type="bibr" target="#b16">(Mnih et al., 2016)</ref>:</p><formula xml:id="formula_2">L a2c = E s,a∼π θ L a2c policy + β a2c L a2c value (4) L a2c policy = − log π θ (a t |s t )(V n t − V θ (s t )) − αH π θ t (5) L a2c value = 1 2 V θ (s t ) − V n t 2<label>(6)</label></formula><p>where H π t = − a π(a|s t ) log π(a|s t ) denotes the entropy in simplified notation, and α is a weight for entropy regularization.</p><formula xml:id="formula_3">V n t = n−1 d=0 γ d r t+d + γ n V θ (s t+n ) is the n-step bootstrapped value.</formula><p>To sum up, A2C+SIL performs both on-policy A2C update (L a2c ) and self-imitation learning from the replay buffer M times (L sil ) to exploit past good experiences. A2C+SIL is relatively simple to implement as it does not involve importance sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Justification</head><p>In this section, we justify the following claim. Claim. The self-imitation learning objective (L sil in Eq. 1) can be viewed as an implementation of lower-bound-soft-Q-learning (Section 4.2) under the entropy-regularized RL framework.</p><p>To show the above claim, we first introduce the entropyregularized RL <ref type="bibr" target="#b7">(Haarnoja et al., 2017)</ref> in Section 4.1. Section 4.2 introduces lower-bound-soft-Q-learning, an offpolicy Q-learning algorithm, which learns the optimal action-value function from good state-action pairs. Section 4.3 proves the above claim by showing the equivalence between self-imitation learning and lower-bound-soft-Qlearning. Section 4.4 further discusses the relationship between A2C and self-imitation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Entropy-Regularized Reinforcement Learning</head><p>The goal of entropy-regularized RL is to learn a stochastic policy which maximizes the entropy of the policy as well as the γ-discounted sum of rewards <ref type="bibr" target="#b7">(Haarnoja et al., 2017;</ref><ref type="bibr" target="#b42">Ziebart et al., 2008)</ref>:</p><formula xml:id="formula_4">π * = argmax π E π ∞ t=0 γ t (r t + αH π t )<label>(7)</label></formula><p>where H π t = − log π(a t |s t ) is the entropy of the policy π, and α ≥ 0 represents the weight of entropy bonus. Intu-itively, in the entropy-regularized RL, a policy that has a high entropy is preferred (i.e., diverse actions chosen given the same state).</p><p>The optimal soft Q-function and the optimal soft value function are defined as:</p><formula xml:id="formula_5">Q * (s t , a t ) = E π * r t + ∞ k=t+1 γ k−t (r k + αH π * k ) (8) V * (s t ) = α log a exp (Q * (s t , a)/α) .<label>(9)</label></formula><p>It is shown that the optimal policy π * has the following form (see <ref type="bibr" target="#b41">Ziebart (2010)</ref>; <ref type="bibr" target="#b7">Haarnoja et al. (2017)</ref> for the proof):</p><formula xml:id="formula_6">π * (a|s) = exp((Q * (s, a) − V * (s))/α).<label>(10)</label></formula><p>This result provides the relationship among the optimal Qfunction, the optimal policy, and the optimal value function, which will be useful in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Lower Bound Soft Q-Learning</head><p>Lower bound of optimal soft Q-value Let π * be an optimal policy in entropy-regularized RL (Eq. 7). It is straightforward that the expected return of any behavior policy µ can serve as a lower bound of the optimal soft Q-value as follows:</p><formula xml:id="formula_7">Q * (s t , a t ) = E π * r t + ∞ k=t+1 γ k−t (r k + αH π * k ) (11) ≥ E µ r t + ∞ k=t+1 γ k−t (r k + αH µ k ) ,<label>(12)</label></formula><p>because the entropy-regularized return of the optimal policy is always greater or equal to that of any other policies.</p><p>Lower bound soft Q-learning Suppose that we have full episode trajectories from a behavior policy µ, which consists of state-action-return triples:</p><formula xml:id="formula_8">(s t , a t , R t ) where R t = r t + ∞ k=t+1 γ k−t (r k + αH µ k )</formula><p>is the entropy-regularized return. We propose lower bound soft Q-learning which updates Q θ (s, a) parameterized by θ towards the optimal soft Qvalue as follows (t is omitted for brevity):</p><formula xml:id="formula_9">L lb = E s,a,R∼µ 1 2 (R − Q θ (s, a)) + 2 ,<label>(13)</label></formula><p>where (·) + = max(·, 0). Intuitively, we update the Q-value only when the return is greater than the Q-value estimate (R &gt; Q θ (s, a)). This is justified by the fact that the lower bound (Eq. 12) implies that the estimated Q-value is lower than the optimal soft Q-value: Q * (s, a) ≥ R &gt; Q θ (s, a) when the environment is deterministic. Otherwise (R ≤ Q θ (s, a)), such state-action pairs do not provide any useful information about the optimal soft Q-value, so they are not used for training. We call this lower-bound-soft-Q-learning as it updates Q-values towards the lower bounds of the optimal Q-values observed from the behavior policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Connection between SIL and Lower Bound Soft Q-Learning</head><p>In this section, we derive an equivalent form of lower-boundsoft-Q-learning (Eq. 13) for the actor-critic architecture and show a connection to self-imitation learning objective.</p><p>Suppose that we have a parameterized soft Q-function Q θ .</p><p>According to the form of optimal soft value function and optimal policy in the entropy-regularized RL (Eq. 9-10), it is natural to consider the following forms of a value function V θ and a policy π θ :</p><formula xml:id="formula_10">V θ (s) = α log a exp(Q θ (s, a)/α) (14) π θ (a|s) = exp((Q θ (s, a) − V θ (s))/α).<label>(15)</label></formula><p>From these definitions, Q θ can be written as:</p><formula xml:id="formula_11">Q θ (s, a) = V θ (s) + α log π θ (a|s).<label>(16)</label></formula><p>For convenience, let us define the following:</p><formula xml:id="formula_12">R = R − α log π θ (a|s) (17) ∆ = R − Q θ (s, a) =R − V θ (s).<label>(18)</label></formula><p>By plugging Eq. 16 into Eq. 13, we can derive the gradient estimator of lower-bound-soft-Q-learning for the actor-critic architecture as follows:</p><formula xml:id="formula_13">∇ θ E s,a,R∼µ 1 2 (R − Q θ (s, a)) + 2 (19) = E [−∇ θ Q θ (s, a)∆ + ] (20) = E [−∇ θ (α log π θ (a|s) + V θ (s)) ∆ + ] (21) = E [−α∇ θ log π θ (a|s)∆ + − ∇ θ V θ (s)∆ + ]<label>(22)</label></formula><formula xml:id="formula_14">= E α∇ θ L lb policy − ∇ θ V θ (s)∆ + (23) = E α∇ θ L lb policy − ∇ θ V θ (s)(R − Q θ (s, a)) + (24) = E α∇ θ L lb policy − ∇ θ V θ (s)(R − V θ (s)) + (25) = E α∇ θ L lb policy + ∇ θ 1 2 (R − V θ (s)) + 2 (26) = E α∇ θ L lb policy + ∇ θ L lb value .<label>(27)</label></formula><p>Each loss term in Eq. 27 is given by:</p><formula xml:id="formula_15">L lb policy = − log π θ (a|s) R − V θ (s) +<label>(28)</label></formula><formula xml:id="formula_16">L lb value = 1 2 (R − V θ (s)) + 2 .<label>(29)</label></formula><p>Thus, L lb policy = L sil policy and L lb value = L sil value as α → 0 (see Eq. 2-3). This shows that the proposed self-imitation learning objective L sil (Eq. 1) can be viewed as a form of lower-bound-soft-Q-learning (Eq. 13), but without explicitly optimizing for entropy bonus reward as α → 0.</p><p>Since the lower-bound-soft-Q-learning directly updates the Q-value towards the lower bound of the optimal Q-value, self-imitation learning can be viewed as an algorithm that updates the policy (π θ ) and the value (V θ ) directly towards the optimal policy and the optimal value respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Relationship between A2C and SIL</head><p>Intuitively, A2C updates the policy in the direction of increasing the expected return of the learner policy and enforces consistency between the value and the policy from on-policy trajectories. On the other hand, SIL updates each of them directly towards optimal policies and values respectively from off-policy trajectories. In fact, <ref type="bibr" target="#b19">Nachum et al. (2017)</ref>; <ref type="bibr" target="#b7">Haarnoja et al. (2017)</ref>; <ref type="bibr" target="#b29">Schulman et al. (2017a)</ref> have recently shown that entropy-regularized A2C can be viewed as n-step online soft Q-learning (or path consistency learning). Therefore, both A2C and SIL objectives are designed to learn the optimal soft Q-function in the entropy-regularized RL framework. Thus, we claim that both objectives can be complementary to each other in that they share the same optimal solution as discussed in PGQL <ref type="bibr" target="#b21">(O'Donoghue et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>The experiments are designed to answer the following:</p><p>• Is self-imitation learning useful for exploration?</p><p>• Is self-imitation learning complementary to count-based exploration methods?</p><p>• Does self-imitation learning improve the overall performance across a variety of tasks?</p><p>• When does self-imitation learning help and when does it not?</p><p>• Can other off-policy actor-critic methods also exploit good experiences (e.g., ACER <ref type="bibr" target="#b39">(Wang et al., 2017)</ref>)?</p><p>• Is self-imitation learning useful for continuous control and compatible with other learning algorithms like PPO <ref type="bibr" target="#b30">(Schulman et al., 2017b)</ref>?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>For Atari experiments, we used a 3-layer convolutional neural network used in DQN <ref type="bibr" target="#b15">(Mnih et al., 2015)</ref> with last 4 stacked frames as input. We performed 4 self-imitation learning updates per on-policy actor-critic update (M = 4 in Algorithm 1). Instead of treating losing a life as episode termination as typically done in the previous work, we terminated episodes when the game ends, as it is the true definition of episode termination. For MuJoCo experiments, we used an MLP which consists of 2 hidden layers with 64 units as in <ref type="bibr" target="#b30">Schulman et al. (2017b)</ref>. We performed 10 selfimitation learning updates per each iteration (batch). More details of the network architectures and hyperparameters are described in the Appendix. Our implementation is based on OpenAI's baseline implementation <ref type="bibr" target="#b5">(Dhariwal et al., 2017</ref>). 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Key-Door-Treasure Domain</head><p>To investigate how self-imitation learning is useful for exploration and whether it is complementary to count-based exploration method, we compared different methods on a gridworld domain, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. More specifically, we implemented a count-based exploration method <ref type="bibr" target="#b33">(Strehl &amp; Littman, 2008</ref>) that gives an exploration bonus reward: r exp = β/ N (s), where N (s) is the visit count of state s and β is a hyperparameter. We also implemented a combination with self-imitation learning shown as 'A2C+SIL+EXP' in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>In the first domain (Key-Door-Treasure), the chance of picking up the key followed by opening the door and obtaining the treasure is low due to the sequential dependency between them. We found that the baseline A2C tends to get stuck at a sub-optimal policy that only opens the door for a long time. A2C+EXP learns faster than A2C because exploration bonus encourages the agent to collect the treasure more often. Interestingly, A2C+SIL and A2C+SIL+EXP learn most quickly. We observed that once the agent opens the door with the key by chance, our SIL helps exploit such good experiences and quickly learns to open the door with the key. This increases the chance of getting the next reward (i.e., treasure) and helps learn the optimal policy. This is an example showing that self-imitation learning can drive deep exploration.</p><p>In the second domain (Apple-Key-Door-Treasure), collecting apples near the agent's initial location makes it even more challenging for the agent to learn the optimal policy, which collects all of the objects within the time limit (50 steps). In this domain, many agents learned a sub-optimal policy that only collects two apples as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. On the other hand, only A2C+SIL+EXP consistently learned the optimal policy because count-based exploration increases the chance of collecting the treasure, while self-imitation learning can quickly exploit such a good experience as soon as the agent collects it. This result shows that self-imitation learning and count-based exploration methods can be complementary to each other. This result also suggests that while exploration is important for increasing the chance/frequency of getting a reward, it is also important to exploit such rare experiences to learn a policy to consistently achieve it especially when the reward is sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Hard Exploration Atari Games</head><p>We investigated how useful our self-imitation learning is for several hard exploration Atari games on which recent advanced exploration methods mainly focused. <ref type="figure" target="#fig_2">Figure 3</ref> shows that A2C with our self-imitation learning (A2C+SIL) outperforms A2C on six hard exploration games. A2C failed to learn a better-than-random policy, except for Hero, whereas our method learned better policies and achieved humanlevel performances on Hero and Freeway. We observed that <ref type="table">Table 1</ref>. Comparison to count-based exploration actor-critic agents on hard exploration Atari games. A3C+ and Reactor+ correspond to A3C-CTS  and Reactor-PixelCNN respectively <ref type="bibr" target="#b23">(Ostrovski et al., 2017)</ref>. SimHash represents TRPO-AE-SimHash . † Numbers are taken from plots. even a random exploration occasionally leads to a positive reward on these games, and self-imitation learning helps exploit such an experience to learn a good policy from it. This can drive deep exploration when the improved policy gets closer to the next source of reward. This result supports our claim that exploiting past experiences can often help exploration.</p><p>We further compared our method against the state-ofthe-art count-based exploration actor-critic agents (A3C-CTS , Reactor-PixelCNN <ref type="bibr" target="#b23">(Ostrovski et al., 2017)</ref>, and SimHash ). These methods learn a density model of the observation or a hash function and use it to compute pseudo visit count, which is used to compute an exploration bonus reward. Even though our method does not have an explicit exploration bonus that encourages exploration, we were curious how well our selfimitation learning approach performs compared to these exploration approaches.  Interestingly, <ref type="table">Table 1</ref> shows that A2C with our self-imitation learning (A2C+SIL) achieves better results on 6 out of 7 hard exploration games without any technique that explicitly encourages exploration. This result suggests that it is important to exploit past good experiences as well as efficiently explore the environment to drive deep exploration.</p><p>On the other hand, we found that A2C+SIL never receives a positive reward on Venture during training. This makes it impossible for our method to learn a good policy because there is no good experience to exploit, whereas one of the count-based exploration methods (Reactor-PixelCNN) achieves a better performance, because the agent is encouraged to explore different states even in the absence of reward signal from the environment. This result suggests that an advanced exploration method is essential in such environments where a random exploration never generates a good experience within a reasonable amount of time. Combining self-imitation learning with state-of-the-art exploration methods would be an interesting future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Overall Performance on Atari Games</head><p>To see how useful self-imitation learning is across various types of environments, we evaluated our self-imitation learn- ing method on 49 Atari games. It turns out that our method (A2C+SIL) significantly outperforms A2C in terms of median human-normalized score as shown in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> shows the relative performance of A2C+SIL compared to A2C using the measure proposed by <ref type="bibr" target="#b38">Wang et al. (2016)</ref>. It is shown that our method (A2C+SIL) improves A2C on 35 out of 49 games in total and 11 out of 14 hard exploration games defined by <ref type="bibr" target="#b1">Bellemare et al. (2016)</ref>. It is also shown that A2C+SIL performs significantly better on many easy exploration games such as Time Pilot as well as hard exploration games. We observed that there is a certain learning stage where the agent suddenly achieves a high score by chance on such games, and our self-imitation learning exploits such experiences as soon as the agent experiences them.</p><p>On the other hand, we observed that our method often learns faster at the early stage of learning, but sometimes gets stuck at a sub-optimal policy on a few games, such as James Bond and Star Gunner. This suggests that excessive exploitation at the early stage of learning can hurt the performance. We found that reducing the number of SIL updates per iteration or using a small weight for the SIL objective in a later learning stage indeed resolves this issue and even improve the performance on such games, though the reported numbers are based on the single best hyperparameter. Thus, automatically controlling the degree of self-imitation learning would be an interesting future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effect of Lower Bound Soft Q-Learning</head><p>A natural question is whether existing off-policy actor-critic methods can also benefit from past good experiences by exploiting them. To answer this question, we trained ACPER (A2C with prioritized experience replay) which performs off-policy actor-critic update proposed by ACER <ref type="bibr" target="#b39">(Wang et al., 2017)</ref> by using the same prioritized experience replay as ours, which uses (R−V θ ) + as sampling priority. ACPER can also be viewed as the original ACER with our proposed prioritized experience replay. <ref type="table" target="#tab_2">Table 2</ref> shows that ACPER performs much worse than our A2C+SIL and is even worse than A2C. We observed that ACPER also benefits from good episodes on a few hard exploration games (e.g., Freeway) but was very unstable on many other games. We conjecture that this is due to the fact that the ACER objective has an importance weight term (π(a|s)/µ(a|s)). This approach may not benefit much from the good experiences in the past if the current policy deviates too much from the decisions made in the past. On the other hand, the proposed self-imitation learning objective (Eq. 1) does not have an importance weight and can learn from any behavior, as long as the behavior policy performs better than the learner. This is because our gradient estimator can be interpreted as lower-bound-soft-Q-learning, which updates the parameter directly towards the optimal Q-value regardless of the similarity between the behavior policy and the learner as discussed in Section 4.2. This result shows that our selfimitation learning objective is suitable for exploiting past good experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Performance on MuJoCo</head><p>This section investigates whether self-imitation learning is beneficial for continuous control tasks and whether it can be applied to other types of policy optimization algorithms, such as proximal policy optimization (PPO) <ref type="bibr" target="#b30">(Schulman et al., 2017b)</ref>. Note that unlike A2C, PPO does not have a strong theoretical connection to our SIL objective. However, we claim that they can still be complementary to each other in that both PPO and SIL try to update the policy and the value towards the optimal policy and value. To empirically verify this, we implemented PPO+SIL, which updates the parameter using both the PPO algorithm and our SIL algorithm and evaluated it on 6 MuJoCo tasks in OpenAI Gym <ref type="bibr" target="#b4">(Brockman et al., 2016)</ref>.</p><p>The result in <ref type="figure" target="#fig_4">Figure 5</ref> shows that our self-imitation learning improves PPO on Swimmer, Walker2d, and Ant tasks. Unlike Atari games, the reward structure in this benchmark is smooth and dense in that the agent always receives a reasonable amount of reward according to its continuous progress. We conjecture that the agent has a relatively low chance to occasionally perform well and learn much faster by exploiting such an experience in this type of domain. Nevertheless, the overall improvement suggests that selfimitation learning can be generally applicable to actor-critic architectures and a variety of tasks.</p><p>To verify our conjecture, we further conducted experiments by delaying reward the agent gets from the environment. More specifically, the modified tasks give an accumulated reward after every 20 steps (or when the episode terminates). This makes it more difficult to learn a good policy because the agent does not receive a reward signal for every step. The result is shown in the bottom row in <ref type="figure" target="#fig_4">Figure 5</ref>. Not surprisingly, we observed that both PPO and PPO+SIL perform worse on the delayed-reward tasks than themselves on the standard OpenAI Gym tasks. However, it is clearly shown that the gap between PPO+SIL and PPO is larger on delayedreward tasks compared to standard tasks. Unlike the standard OpenAI Gym tasks where reward is well-designed and dense, we conjecture that the chance of achieving high overall rewards is much low in the delayed-reward tasks. Thus, the agent can benefit more from self-imitation learning because self-imitation learning captures such rare experiences and learn from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed self-imitation learning, which learns to reproduce the agent's past good experiences, and showed that self-imitation learning is very helpful on hard exploration tasks as well as a variety of other tasks including continuous control tasks. We also showed that a proper level of exploitation of past experiences during learning can drive deep exploration, and that self-imitation learning and exploration methods can be complementary. Our results suggest that there can be a certain learning stage where exploitation is more important than exploration or vice versa. Thus, we believe that developing methods for balancing between exploration and exploitation in terms of collecting and learning from experiences is an important future research direction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance on Atari Games</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s). Learning curves on Montezuma's Revenge. (Left) The agent needs to pick up the key in order to open the door. Picking up the key gives a small reward. (Right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Key-Door-Treasure domain. The agent should pick up the key (K) in order to open the door (D) and collect the treasure (T) to maximize the reward. In the Apple-Key-Door-Treasure domain (bottom), there are two apples (A) that give small rewards (+1). 'SIL' and 'EXP' represent our self-imitation learning and a count-based exploration method respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Learning curves on hard exploration Atari games. X-axis and y-axis represent steps and average reward respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Relative performance of A2C+SIL over A2C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Performance on OpenAI Gym MuJoCo tasks (top row) and delayed-reward versions of them (bottom row). The learning curves are averaged over 10 random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of agents on 49 Atari games after 50M steps (200M frames) of training. 'ACPER' represents A2C with prioritized replay using ACER objective. 'Median' shows median of human-normalized scores. '&gt;Human' shows the number of games where the agent outperforms human experts.</figDesc><table><row><cell>AGENT</cell><cell cols="2">MEDIAN &gt;HUMAN</cell></row><row><cell>A2C</cell><cell>96.1%</cell><cell>23</cell></row><row><cell>ACPER</cell><cell>46.8%</cell><cell>18</cell></row><row><cell cols="2">A2C+SIL 138.7%</cell><cell>29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>A2C+SIL hyperparameters on Atari games. PPO+SIL hyperparameters on MuJoCo.</figDesc><table><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>Architecture</cell><cell>Conv(32-8x8-4)</cell></row><row><cell></cell><cell>-Conv(64-4x4-2)</cell></row><row><cell></cell><cell>-Conv(64-3x3-1)</cell></row><row><cell></cell><cell>-FC(512)</cell></row><row><cell>Learning rate</cell><cell>0.0007</cell></row><row><cell>Number of environments</cell><cell>16</cell></row><row><cell>Number of steps per iteration</cell><cell>5</cell></row><row><cell>Entropy regularization (α)</cell><cell>0.01</cell></row><row><cell>SIL update per iteration (M )</cell><cell>4</cell></row><row><cell>SIL batch size</cell><cell>512</cell></row><row><cell>SIL loss weight</cell><cell>1</cell></row><row><cell>SIL value loss weight (β s il)</cell><cell>0.01</cell></row><row><cell>Replay buffer size</cell><cell>10 5</cell></row><row><cell>Exponent for prioritization</cell><cell>0.6</cell></row><row><cell cols="2">Bias correction for prioritized replay 0.1 for hard exploration experiment (Section 5.3)</cell></row><row><cell></cell><cell>0.4 for overall evaluation (Section 5.4)</cell></row><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>Architecture</cell><cell>FC(64)-FC(64)</cell></row><row><cell>Learning rate</cell><cell>Best chosen from {0.0003, 0.0001, 0.00005, 0.00003}</cell></row><row><cell>Horizon</cell><cell>2048</cell></row><row><cell>Number of epochs</cell><cell>10</cell></row><row><cell>Minibatch size</cell><cell>64</cell></row><row><cell>Discount factor (γ)</cell><cell>0.99</cell></row><row><cell>GAE parameter (λ)</cell><cell>0.95</cell></row><row><cell>Entropy regularization (α)</cell><cell>0</cell></row><row><cell>SIL update per batch</cell><cell>10</cell></row><row><cell>SIL batch size</cell><cell>512</cell></row><row><cell>SIL loss weight</cell><cell>0.1</cell></row><row><cell>SIL value loss weight (β)</cell><cell>Best chosen from {0.01, 0.05}</cell></row><row><cell>Replay buffer size</cell><cell>50000</cell></row><row><cell>Exponent for prioritization</cell><cell>Best chosen from {0.6, 1.0}</cell></row><row><cell cols="2">Bias correction for prioritized replay 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Performances on 49 Atari games with 30 random no-op after 50M steps of training (200M frames).</figDesc><table><row><cell></cell><cell cols="2">A2C ACPER</cell><cell>A2C+SIL</cell></row><row><cell>Alien</cell><cell>1859.2</cell><cell>390.2</cell><cell>2242.2</cell></row><row><cell>Amidar</cell><cell>739.9</cell><cell>424.8</cell><cell>1362.0</cell></row><row><cell>Assault</cell><cell>1981.4</cell><cell>818.2</cell><cell>1812.0</cell></row><row><cell>Asterix</cell><cell>16083.3</cell><cell>3533.1</cell><cell>17984.2</cell></row><row><cell>Asteroids</cell><cell>2056.0</cell><cell>1780.1</cell><cell>2259.4</cell></row><row><cell>Atlantis</cell><cell cols="3">3032444.2 58012.5 3084781.7</cell></row><row><cell>BankHeist</cell><cell>1333.7</cell><cell>1203.2</cell><cell>1137.8</cell></row><row><cell>BattleZone</cell><cell cols="2">10683.3 15025.0</cell><cell>25075.0</cell></row><row><cell>BeamRider</cell><cell>3931.7</cell><cell>2602.4</cell><cell>2366.2</cell></row><row><cell>Bowling</cell><cell>31.2</cell><cell>59.3</cell><cell>31.1</cell></row><row><cell>Boxing</cell><cell>99.7</cell><cell>100.0</cell><cell>99.6</cell></row><row><cell>Breakout</cell><cell>501.6</cell><cell>118.5</cell><cell>452.0</cell></row><row><cell>Centipede</cell><cell>3857.8</cell><cell>7790.1</cell><cell>7559.5</cell></row><row><cell>ChopperCommand</cell><cell>3464.2</cell><cell>1307.5</cell><cell>6710.0</cell></row><row><cell>CrazyClimber</cell><cell cols="2">129715.8 19918.8</cell><cell>130185.8</cell></row><row><cell>DemonAttack</cell><cell>18331.4</cell><cell>4777.5</cell><cell>10140.5</cell></row><row><cell>DoubleDunk</cell><cell>-0.5</cell><cell>-9.8</cell><cell>21.5</cell></row><row><cell>Enduro</cell><cell>0.0</cell><cell>3113.3</cell><cell>1205.1</cell></row><row><cell>FishingDerby</cell><cell>39.1</cell><cell>59.8</cell><cell>55.8</cell></row><row><cell>Freeway</cell><cell>0.0</cell><cell>31.4</cell><cell>32.2</cell></row><row><cell>Frostbite</cell><cell>339.5</cell><cell>2342.5</cell><cell>6289.8</cell></row><row><cell>Gopher</cell><cell>9358.5</cell><cell>3919.5</cell><cell>23304.2</cell></row><row><cell>Gravitar</cell><cell>329.2</cell><cell>627.5</cell><cell>1874.2</cell></row><row><cell>Hero</cell><cell cols="2">28008.1 13299.1</cell><cell>33156.7</cell></row><row><cell>IceHockey</cell><cell>-4.3</cell><cell>0.0</cell><cell>-2.4</cell></row><row><cell>Jamesbond</cell><cell>399.2</cell><cell>598.1</cell><cell>310.8</cell></row><row><cell>Kangaroo</cell><cell>1563.3</cell><cell>5875.0</cell><cell>2888.3</cell></row><row><cell>Krull</cell><cell cols="2">8883.9 11323.2</cell><cell>10614.6</cell></row><row><cell>KungFuMaster</cell><cell cols="2">32507.5 20485.0</cell><cell>34449.2</cell></row><row><cell>MontezumaRevenge</cell><cell>5.8</cell><cell>0.0</cell><cell>1100.0</cell></row><row><cell>MsPacman</cell><cell>2843.4</cell><cell>1016.0</cell><cell>4025.1</cell></row><row><cell>NameThisGame</cell><cell>11174.2</cell><cell>2888.0</cell><cell>14958.2</cell></row><row><cell>Pong</cell><cell>20.8</cell><cell>20.9</cell><cell>20.9</cell></row><row><cell>PrivateEye</cell><cell>210.8</cell><cell>100.0</cell><cell>661.2</cell></row><row><cell>Qbert</cell><cell>17605.2</cell><cell>657.2</cell><cell>104975.6</cell></row><row><cell>Riverraid</cell><cell>13036.0</cell><cell>2224.5</cell><cell>14306.1</cell></row><row><cell>RoadRunner</cell><cell>39874.2</cell><cell>8925.0</cell><cell>57071.7</cell></row><row><cell>Robotank</cell><cell>3.2</cell><cell>7.7</cell><cell>10.5</cell></row><row><cell>Seaquest</cell><cell>1795.2</cell><cell>804.5</cell><cell>2456.5</cell></row><row><cell>SpaceInvaders</cell><cell>2466.1</cell><cell>729.5</cell><cell>2951.7</cell></row><row><cell>StarGunner</cell><cell>57371.7</cell><cell>1107.5</cell><cell>31309.2</cell></row><row><cell>Tennis</cell><cell>-10.3</cell><cell>-17.0</cell><cell>-17.3</cell></row><row><cell>TimePilot</cell><cell>5346.7</cell><cell>3952.5</cell><cell>10811.7</cell></row><row><cell>Tutankham</cell><cell>305.6</cell><cell>270.7</cell><cell>340.5</cell></row><row><cell>UpNDown</cell><cell>48131.8</cell><cell>9562.5</cell><cell>53314.6</cell></row><row><cell>Venture</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>VideoPinball</cell><cell cols="2">391241.6 21797.7</cell><cell>461522.4</cell></row><row><cell>WizardOfWor</cell><cell>4196.7</cell><cell>1550.0</cell><cell>7088.3</cell></row><row><cell>Zaxxon</cell><cell>124.2</cell><cell>4278.8</cell><cell>9164.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available on https://github.com/ junhyukoh/self-imitation-learning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by NSF grant IIS-1526059. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsor.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Abolafia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03526</idno>
		<title level="m">Neural program synthesis with priority queue training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04460</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Model-free episodic control. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaremba</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">W. OpenAI gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ope-Nai Baselines</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The reactor: A sample-efficient actor-critic architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reinforcement learning with deep energy-based policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to play in a day: Faster deep reinforcement learning by optimality tightening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep q-learning from demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sendonaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hippocampal contributions to control: the third way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-improving reactive agents based on reinforcement learning, planning and teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="293" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memory-based reinforcement learning: Efficient computation with prioritized sweeping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bridging the gap between value and policy based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Overcoming exploration in reinforcement learning with demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.10089</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining policy gradient and q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped dqn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Count-based exploration with neural density models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Off-policy temporal difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Puigdomènech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive confidence and adaptive curiosity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Institut fur Informatik</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Technische Universitat Munchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Equivalence between policy gradients and soft q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06440</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Incentivizing exploration in reinforcement learning with deep predictive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00814</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An analysis of modelbased interval estimation for markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1331" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">#ex-ploration: A study of count-based exploration for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deturck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The role of exploration in learning control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reinforcement learning from imperfect demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
