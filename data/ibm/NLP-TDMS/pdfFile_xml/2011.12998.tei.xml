<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörgen</forename><surname>Valk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tallinn University of Technology</orgName>
								<address>
									<country key="EE">Estonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Alumäe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tallinn University of Technology</orgName>
								<address>
									<country key="EE">Estonia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Spoken language recognition</term>
					<term>web scraping</term>
					<term>x- vectors</term>
					<term>crowd-sourcing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semirandom search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Spoken language recognition (SLR) is the task of automatically classifying an utterance based on the spoken language. SLR is used as a pre-processing step in several applications, such as automatic call routing, multilingual spoken translation and human-machine communication systems, multilingual speech transcription systems and spoken document retrieval. SLR is also often used in the area of intelligence and security.</p><p>In the past 20 years, development of SLR technology has been largely fostered through NIST Language Recognition Evaluations (LREs). As a result, the most popular benchmarks for evaluating new SLR models and methods are NIST LRE evaluation datasets <ref type="bibr" target="#b0">[1]</ref>. The NIST LRE evaluations datasets contain mostly narrowband conversational telephone speech. In order to build competitive systems for NIST LREs, large amounts of conversational telephone data from the particular languages are used. Such datasets are typically distributed by the Linguistic Data Consortium (LDC) and cost thousands of dollars. For example, the standard Kaldi <ref type="bibr" target="#b1">[2]</ref> recipe for LRE07 2 relies on 18 LDC SLR datasets that cost $15400 to LDC non-members <ref type="bibr" target="#b2">3</ref> . This makes it difficult for new research groups to enter the academic field of SLR. Furthermore, the NIST LRE evaluations focus mostly on telephone speech. There are not many resources for objectively evaluating SLR technology for other kinds of speech, such as conversational speech "from the wild", i.e., from 1 http://bark.phon.ioc.ee/voxlingua107/ 2 https://github.com/kaldi-asr/kaldi/blob/master/ egs/lre07/v2/run.sh <ref type="bibr" target="#b2">3</ref> The datasets were provided for free to the LRE07 particpants.</p><p>various internet resources. Another problem lies in language coverage: NIST LREs have put a lot of focus on challenging classification tasks (e.g., fine-grained distinction of Arabic and Spanish dialects), but achieving a very large language coverage is not its main goal. The aim of this work is to investigate, whether automatically scraped and labelled speech data from the web can be used for building SLR systems. Our goal is to target various wide-band acoustic conditions and provide a large language coverage. We extract audio data from YouTube videos that are retrieved using random languagespecific search phrases. If the language of the video title and description matches with the language of the search phrase, the audio in the video is likely to be in that particular language. This allows to collect large amounts of somewhat noisy data relatively cheaply.</p><p>There are several previous works that use an approach that is similar to ours. The KALAKA-3 database <ref type="bibr" target="#b2">[3]</ref> contains varying amounts of data from YouTube for several European languages. The YouTube data was retrieved semi-automatically: for each language, a list of search words was created based on the language dictionary, and YouTube was searched using single words from this list. Geographical location tags associated to some of the videos were also used to increase the chances of finding speech in a particular language. All the retrieved videos were hand-validated. Another dataset that uses YouTube audio is ADI17 <ref type="bibr" target="#b3">[4]</ref>, built for the MGB-5 Challenge <ref type="bibr" target="#b4">[5]</ref> Arabic dialect identification task. The dataset contains 3000 hours of speech from 17 Arabic dialects and is collected by manually compiling lists of popular YouTube channels in specific countries, and then retrieving videos from the corresponding channels. Using large amounts of YouTube audio is common in speaker recognition. The Speakers in the Wild (SITW) database <ref type="bibr" target="#b5">[6]</ref> contains speech samples of nearly 300 well-known public figures from open-source media. The research group behind SITW also describe a similar in-house dataset for language recognition which contains 2180 audio files sourced from open-source videos and represents 29 languages <ref type="bibr" target="#b6">[7]</ref>, but it has never been publicly released. The VoxCeleb 1&amp;2 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> corpora are popular free datasets for training speaker recognition models, containing automatically retrieved data from YouTube. The datasets have been compiled by using a phrase "&lt;celebrity name&gt; interview" as a YouTube search phrase and then extracting the segments from the retrieved videos where the corresponding celebrity is speaking, using face identification and lip synchronization detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SCRAPING FOR SPOKEN LANGUAGE DATA</head><p>This section covers the process and tools that were used for collecting the speech data that was used to build the SLR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Method</head><p>In order to build SLR models, we need a large corpus of speech utterances that is labeled according to the spoken language. In this  work, we use two different data sources to automatically collect such data: Wikipedia and YouTube. We rely on Wikipedia to extract random search phrases for each language, and on YouTube for retrieving videos whose title or metadata matches the search phrase and which is thus likely in the particular language. Such automatic data collection process will undeniably not be 100% accurate and there will be false positive video results that have to be dealt with.</p><p>The data collection process can be divided into multiple steps. A diagram describing the process and steps is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Generation of search phrases</head><p>We use Wikipedia dumps 4 for generating random language-specific search phrases. The number and quality of articles in different language's Wikipedia has a lot of variety. For some smaller or less active language communities there may be less than 1 000 articles. After some initial experiments it was decided that only languages that have more than 10 000 articles are appropriate for this method. Out of the more than 300 languages available in Wikipedia, 149 have more than 10 000 articles. However, in some cases the limit of 10 000 may not be enough because in some languages, a large amount of articles are automatically generated, translated, or the contents are mostly about certain kind of entities (such as geographical locations) which are not suitable for random search phrase generation. We filtered out articles shorter than 3 000 characters and the ones containing just a title. This improved the results in the next phrase generation step.</p><p>There are several different approaches that could be used to generate the search phrases from Wikipedia data. The method that was used in this work is based on Term Frequency Inverse Document Frequency (TF-IDF). TF-IDF is a well known method that helps to evaluate how important a word or phrase in a document is. We applied TF-IDF on each language's Wikipedia dataset and compiled a long list of the "most important" phrases for each of the languages.</p><p>After doing some initial experiments with the collected datasets in different languages, it was concluded that a good and universal phrase length to use in the YouTube search engine is three words. Using shorter phrases resulted in vague results that also had many false positives. Longer phrases, on the other hand, decreased the amount of search matches too much.</p><p>However, even with three word search phrases, there were still many false positives. Many of the false positives were caused by phrases that were in other language than expected or that contained too many numbers, stop-words, etc. To alleviate the main problem of the phrases being in the wrong language, we used a text-based language identification model on the generated phrases to filter out all phrases whose language was unknown or did not match the expected language. We used the Polyglot 5 Python package for language identification which supports 165 languages. This step removed a relatively large portion of the generated phrases and resulted in the final set of phrases that deemed usable for the next step. A randomly picked sample of the generated phrases for eight languages can be seen in <ref type="table" target="#tab_1">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Retrieving audio data</head><p>The next step was using the generated phrases for collecting the videos from which the audio could be extracted. An overview of the process is given in <ref type="figure" target="#fig_1">Figure 2</ref>. All of the previously generated phrases were used one by one to find matching videos from YouTube. Even though the phrases were heavily processed and filtered, this still resulted in many false positives, i.e., videos not in the expected language. There are also cases where the video title is in the expected language but the content is in another language. To decrease the number of false positive video results, we applied the text based language identification model to the video title, description (which usually reflects the video content and language most closely) and other metadata, if available. All results with unknown or not expected language were filtered out, similarly to the search phrase generation process before. This decreased the amount of false positive results noticeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E s to n ia n</head><p>A rm e n ia n E n g li s h G e rm a n F in n is h A ra b ic R u s s ia n D u tc h F re n c h L a tv ia n S w e d is h U rd u P e rs ia n S p a n is h C h in e s e D a n is h T u rk is h N o rw e g ia n A z e rb a ij a n i S lo v e n ia n U k ra in ia n S e rb ia n Ja p a n e s e Ic e la n d ic L it h u a n ia n It a li a n P a s h to S lo v a k A fr ik a a n s  <ref type="figure">Fig. 3</ref>. Number of human-provided labels for languages with more than 50 labels.</p><formula xml:id="formula_0">H in d i Y id d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Audio segmentation</head><p>As the final step in the dataset generation process, audio files were split into shorter utterance-like segments and the resulting segments containing mostly noise, music or silence were removed. The LIUM SpkDiarization toolkit <ref type="bibr" target="#b9">[10]</ref> was applied for this. We imposed a limit of a minimium clip length of 2 seconds and a maximum clip length of 20 seconds. The majority of the resulting speech segments had a duration between 4 to 10 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Scraping results</head><p>After the several filtering steps, we were able to download at least some videos for 107 languages. In total, audio was collected from close to 78 000 videos, resulting in a dataset of 14 044 hours of audio content (before segmentation and non-speech data removal).</p><p>All of the collected videos have a maximum duration of one hour. The majority of the dataset consists of relatively short clips between 1-10 minutes. This is positive, as it increases the variety of speakers, acoustic conditions and topics.</p><p>After segmentation and speech/non-speech classification, the the number of utterances in the dataset is about 3.5 million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Validation using crowd-sourcing</head><p>Even after several filtering steps, the data collected using this method still contains false positive results. False positives mostly occur when the language of the speech in the video does not match with the language of the video title and description.</p><p>In order to assess the ratio of false positives in the dataset and to produce human-labelled development data for subsequent SLR experiments, a crowd-sourcing based validation experiment was carried out. For this, a custom web application was built that allowed to share the validation task to a wide group of people who are proficient in different languages. The application first displays a list of all the collected languages and the user can pick a language to validate. The user's language proficiency on a scale of 1 to 5 is also asked. After selecting a language to validate, a random selection of ten audio clips is presented to the user to work on. If possible, then some of the ten clips are selected from the ones that have already been annotated once by another user. This would later allow us to analyze inter-annotator agreement. For each presented clip, the user is asked to decide, whether the clip (1) contains speech in the given language, (2) contains speech in some other language, (3) doesn't contain speech or (4) the user cannot provide a definite answer. Google R u s s ia n A r m e n ia n C h in e s e A z e r b a ij a n i H in d i G e r m a n F r e n c h It a li a n T u r k is h P e r s ia n P a s h t o S p a n is h A f r ik a a n s E s t o n ia n L it h u a n ia n F in n is h A r a b ic E n g li s h S w e d is h D a n is h Ja p a n e s e D u t c h L a t v ia n Ic e la n d ic account based authentication was added to the application in order to reduce the amount of misbehaviour.</p><p>The main validation process was started in February 2020. We asked volunteers via social media to contribute to our research. As of May 2020, over 14 000 unique audio segments have been labelled, with over 18 000 labels in total. The amount of validation data per language varies a lot: for some languages only a few audio clips have been validated. For 31 languages, more than 50 labels have been collected. These 31 languages together with the corresponding counts are shown in <ref type="figure">Figure 3</ref>.</p><p>On the average, 85.3% of the segments in our dataset contain speech in the expected language. 5.8% utterances were in another language, 7.5% contained non-speech and for 1.4% utterances no definite answer was provided. When not counting non-speech segments (which could be eliminated from the dataset using better speech/non-speech classification models) and answers where no definitive answer could be provided, the share of segments in the expected language increases to 93.6%. <ref type="figure">Figure 3</ref> shows the proportion of different validation labels per language.</p><p>Average inter-annotator agreement of the validation results is 97.0%, when ignoring the answers where no definitive answer was provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Data-driven filtering</head><p>As mentioned in the previous section, only an estimated 85% of the segments in the automatically compiled dataset are in the expected language. To address this issue, we experimented with data-driven filtering. The filtering aims to remove segments that are probably not in the given language from the dataset, while keeping most of the valid segments.</p><p>We used a recently proposed technique called Robust Generative classifier (RoG) <ref type="bibr" target="#b10">[11]</ref> for filtering. First, we trained an x-vector <ref type="bibr" target="#b11">[12]</ref> language identification model on the original noisy data. The model was used to extract x-vector embeddings for all utterances. On top of the extracted features, we trained a generative classifier which utilizes the minimum covariance determinant (MCD) estimator to estimate its parameters, as proposed in <ref type="bibr" target="#b10">[11]</ref>. Finally, we applied the resulting generative classifier on the full dataset and chose a posterior probability threshold which roughly equalizes the false positive rate (i.e., segments with false labels that are above the threshold) and the false negative rate (segments with correct labels falling below the threshold), using the small subset of the dataset with crowd-sourced labels. All segments which fell under the threshold were removed from dataset. The filtering reduced the proportion of incorrectly labeled segments in the data from 15% to 2%, while retaining 90% of the correctly labeled segments. In the next section we show that such filtering had a positive effect on the accuracy of the resulting language identification models.</p><p>We split the filtered dataset into official training and evaluation set. The evaluation set was created by selecting at most 100 utterances from each language that had been confirmed by at least two separate volunteers to be in the expected language. This amounts to 1609 segments from 33 languages (not all languages had validation data from two persons). To avoid data leakage, all utterances of the videos with at least one segment in the evaluation set were removed from the training set. Note that this does not remove the possibility of data leakage completely: the dataset might contain several videos from the same YouTube channel(s), with one or more of the videos in the training set and one in the evaluation set. This would allow the language identification model to use speaker recognition cues to perform well in the experiment.</p><p>The total amount of data for each language in the cleaned training set is given in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LANGUAGE IDENTIFICATION EXPERIMENTS</head><p>We conducted several experiments to assess the usefulness of the retrieved data for SLR. In all experiments, we use language embedding models derived from the x-vector paradigm <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, with several enhancements. During training, we apply on-the fly data augmentation using AugMix <ref type="bibr" target="#b13">[14]</ref>, by randomly distorting the training data using a mix of reverberation and noise augmentation. For frame-level feature extraction, we use the Resnet34 <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> architecture where the basic convolutional blocks with residual connections are replaced with squeeze-and-attention modules <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. For temporal pooling, we use multi-head attention, similarly to the method described in <ref type="bibr" target="#b18">[19]</ref>. The models are implemented in PyTorch <ref type="bibr" target="#b19">[20]</ref> using a framework developed in our lab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">In-domain data, 107-language classification</head><p>For the SLR experiment with in-domain data, we trained two systems: one on the noisy dataset and one on the filtered dataset. For Dutch − → Afrikaans English − → Welsh Estonian − → Finnish both systems, we followed the same procedure: first, a backend xvector model was trained on the corresponding training dataset. The model was used to extract utterance embeddings for all training and evaluation data. The frontend LDA/PLDA based generative classifier was trained on the extracted x-vectors, using Kaldi's <ref type="bibr" target="#b1">[2]</ref> implementation.</p><p>The classification error rates of the two models is shown in Table 3. We also calculated the error rates for short utterances (less than 5 seconds) and long utterances (5 to 20 seconds). Our dataset does not contain utterances longer than 20 seconds since the preprocessing pipeline splits speech into to utterances with a maximum duration of 20 seconds. As can be seen, the data-driven cleaning step does not improve classification performance on in-domain data. <ref type="table">Table 4</ref> shows most common classification errors encountered in the evaluation data. Most of the errors occur between closely related languages (in fact, Urdu and Hindi are mutually intelligible as spoken languages), although the substitution of English with Welsh is more likely caused by the bad quality of the retrieved Welsh data.</p><p>This system is available for experimentation at https:// bark.phon.ioc.ee/lid_demo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">KALAKA-3</head><p>The KALAKA-3 dataset <ref type="bibr" target="#b2">[3]</ref> consists of three partitions: training data, containing mostly broadcast speech for a 6 language subset (Basque, Catalan, English, Galician, Portugese and Spanish) <ref type="bibr" target="#b20">[21]</ref>, development data and test data, containing YouTube audio for 21 We experimented with several methods in this experiment. We trained two language embedding models, one on the noisy data and the other on the automatically filtered (cleaned) data. Both models were trained on a 21 language subset of our 107 language dataset, covering all languages of the KALAKA-3 development and evaluation data (both evaluation and out-of-set languages). For training the final logistic regression classifier, we experimented with two approaches. In the first one, we trained the classifiers for each subtask on the x-vectors extracted from our training dataset (noisy or cleaned). In the second approach, we used the KALAKA-3 development data (around 3 hours per in-set language) to train the final classifier. We didn't use the official KALAKA-3 training data in either of the approaches. <ref type="table" target="#tab_4">Table 5</ref> lists the evaluation results, using KALAKA-3 official performance metric Fact and equal error rate (EER). As baseline, we use a fusion of various i-vector and phonotactic systems reported in <ref type="bibr" target="#b2">[3]</ref>. The results show that on the 4-language tasks (EC and EO), all of our approaches give relatively similar scores. On the 6 language task that mainly contains languages spoken in the Iberian peninsula (PC and PO), using the KALAKA-3 development data for training the final classifier gives notably better results. We suspect that this is because our data for the the four official languages in Spain is probably relatively noisy (e.g., our scraped Galician data likely contains a lot of Spanish, in spite of cleaning) and even very little amount of hand-labeled development data allows to build more accurate classifiers. We also see that also in this experiment, using filtered trainining data improves results. Note that our results are not directly comparable to the given baseline, since KALAKA-3 official rules allowed using only the provided training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">LRE07</head><p>The LRE07 dataset contains 14 languages that are used as detection targets <ref type="bibr" target="#b21">[22]</ref>. LRE07 dataset contains conversational telephone speech, which has considerably different characteristics than the YouTube audio data collected in this work.</p><p>We trained x-vector models on the 14 language subset of our scraped data that covers all the LRE07 languages, with audio resampled to 8 kHz. We used the resulting model to extract embeddings for our 14 language training data subset and then built a logistic regression classifier, using centered and length-normalized embeddings. Since the audio in LRE07 evaluation data contains a lot of non-speech regions, we removed long regions of non-speech using a Kaldi speech activity detection model trained for the IARPA ASpIRE challenge by the JHU team <ref type="bibr" target="#b22">[23]</ref>, prior to extracting embeddings. <ref type="table">Table 6</ref> shows the results of different systems. All the <ref type="table">Table 6</ref>. Results on LRE07 closed-set task, in Cavg (lower is better). System 3 sec 10 sec 30 sec Average Trained on in-domain data (telephone speech) GMM-MMI <ref type="bibr" target="#b23">[24]</ref> 17.28 5.90 2.10 8.42 Fusion of models <ref type="bibr" target="#b23">[24]</ref> 13.32 3.55 0.97 5.95 Phonotactic <ref type="bibr" target="#b24">[25]</ref> 18.59 6.28 1.34 8.73 Fusion of models <ref type="bibr" target="#b24">[25]</ref> 15 It can be seen that our Resnetbased models, trained on out-of-domain data, achieve better average results than GMM, phonotactic and i-vector based models trained on in-domain data. Our models perform particularly well on short utterances. However, modern systems trained on in-domain data result in better performance. Our model architecture is very similar to the CNN-SAP model <ref type="bibr" target="#b14">[15]</ref> which gives around 16% relative improvement compared to our system that is trained on automatically filtered data. Using the cleaned dataset shows benefits also in this experiment, except for the 30 second subset where the model trained on noisy data performs slightly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ANALYSIS OF LANGUAGE EMBEDDINGS</head><p>After training an x-vector language recognition system, we extracted the embeddings of all utterances in the training data. LDA was used to reduce the dimensionality of the embeddings to 250. Language embeddings were then calculated by averaging language-specific utterances, transforming them to lower-dimensional space using LDA and applying length normalization. <ref type="figure" target="#fig_4">Figure 5</ref> shows the T-SNE plot of the resulting language embeddings. It demonstrates that the learned language embeddings represent the structure of the language families remarkably well, although there are obvious deviations from the linguistic hierarchy, probably caused by cultural and geographical influences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ALTERNATIVE APPROACHES</head><p>Using YouTube as the data source is not the only option for building a large-scale dataset for SLR. The largest and most well-known free multilingual speech dataset is Mozilla Common Voice 8 . It contains prompted speech utterances donated by volunteers. Utterance metadata contains speaker identity, text of the prompt and language. Many utterances also include demographic metadata like age, sex, and accent. The dataset currently consists of 5671 validated hours in 54 languages. The main benefit of the this dataset over ours is that all the recordings in the validated corpus are confirmed by volunteers to really contain the given utterance. The main shortcoming of the corpus, with regard to using it for language recognition "in the wild", is that it contains only read speech. There is also a lot of variety in the amount available of data per language, and the language coverage is lower than in our data. Nevertheless, in the future we plan to experiment with combining Common Voice with our dataset. Another practical alternative to our method is using podcasts as training data. Podcasts usually contain spontaneous speech which is beneficial for training SLR models. Unfortunately, it seems that finding podcasts by language is not an easy task. There exists some domain-specific podcast listings by language (e.g., "geek" podcasts in 15 languages 9 ), but none of the large podcast directories allow browsing for podcasts by the language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">AVAILABILITY</head><p>The dataset is publicly available at http://bark.phon.ioc. ee/voxlingua107/. Similarly to the VoxCeleb and ADI17 datasets, the VoxLingua107 dataset is distributed under the Creative Commons Attribution 4.0 International License. The copyright remains with the original owners of the video.</p><p>While YouTube users own the copyright to their own videos, using the audio in the vidoes for training language identification models has very limited and transformative purpose and qualifies thus as 9 http://github.com/ayr-ton/awesome-geek-podcasts "fair use" of copyrighted materials. YouTube's terms of service forbid downloading, storing and distribution of videos. However, the aim of this rule is clearly to forbid unfair monetization of the content by third-party sites and applications. Our dataset contains the videos in segmented audio-only form that makes the monetization of the actual distributed content extremely difficult.</p><p>We also point out that the distribution of languages, accents, dialects, genders, races and societal factors in this dataset is not representative of the global population. Using this dataset for training and deploying models may thus introduce unintended biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We described a new speech dataset VoxLingua107 that is compiled from YouTube data. We showed that the dataset is suitable for training spoken language recognition models for classifying data "from the wild". Experiments with the KALAKA-3 dataset showed that the dataset can be used for building both a backend feature extractor and a frontend classifier, or only for training the backend, while the frontend is trained on small amounts of hand-labeled data. Experiments on the NIST LRE07 evaluation data showed that using VoxLinugua107 data results in a classifier that is not far in accuracy from a model trained on large amounts of in-domain data.</p><p>Future work includes using the VoxLingua107 dataset together with in-domain training data, and augmenting input filterbank features with multilingual bottleneck features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>High level overview of the data collection process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the process of retrieving and filtering of videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>N o r w e g ia n S lo v e n ia n S e r b ia n U k r a in ia n U r d u Y id d is h S lo Distribution of crowd-sourced labels per language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>T-SNE plot of the language embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Sample random search phrases.</figDesc><table><row><cell>Language</cell><cell>Random search phrase</cell></row><row><cell>English</cell><cell>the northern territory</cell></row><row><cell>Estonian</cell><cell>ameerikaühendriikide relvajõud</cell></row><row><cell>Finnish</cell><cell>hiileen liittynyt hydroksyyliryhmä</cell></row><row><cell>German</cell><cell>abgesetzten enameloliden zahnkappen</cell></row><row><cell>Latvian</cell><cell>starptautiskajāšaha turnīrā</cell></row><row><cell>Russian</cell><cell>совета рабочих депутатов</cell></row><row><cell>Spanish</cell><cell>administración del estado</cell></row><row><cell>Urdu</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Total duration of cleaned data for each language in the training set, in hours (h).</figDesc><table><row><cell>Language</cell><cell>h Language</cell><cell>h</cell><cell>Language</cell><cell>h Language</cell><cell>h</cell><cell>Language</cell><cell>h Language</cell><cell>h</cell><cell>Language</cell><cell>h</cell></row><row><cell>Abkhazian</cell><cell>10 Catalan</cell><cell>88</cell><cell>German</cell><cell>39 Kannada</cell><cell>46</cell><cell>Marathi</cell><cell>85 Shona</cell><cell>30</cell><cell>Tibetan</cell><cell>101</cell></row><row><cell>Afrikaans</cell><cell>108 Cebuano</cell><cell>6</cell><cell>Greek</cell><cell>66 Kazakh</cell><cell>78</cell><cell>Mongolian</cell><cell>71 Sindhi</cell><cell>84</cell><cell>Turkish</cell><cell>59</cell></row><row><cell>Albanian</cell><cell>71 C. Khmer</cell><cell>41</cell><cell>Guarani</cell><cell>2 Korean</cell><cell>77</cell><cell>Nepali</cell><cell>72 Sinhala</cell><cell>67</cell><cell>Turkmen</cell><cell>85</cell></row><row><cell>Amharic</cell><cell>81 Chinese</cell><cell>44</cell><cell>Gujarati</cell><cell>46 Lao</cell><cell>42</cell><cell>Norwegian</cell><cell>107 Slovak</cell><cell>40</cell><cell>Ukrainian</cell><cell>52</cell></row><row><cell>Arabic</cell><cell>59 Croatian</cell><cell>118</cell><cell>Haitian</cell><cell>96 Latin</cell><cell>67</cell><cell>Nynorsk</cell><cell>57 Slovenian</cell><cell>121</cell><cell>Urdu</cell><cell>42</cell></row><row><cell>Armenian</cell><cell>69 Czech</cell><cell>67</cell><cell>Hausa</cell><cell>93 Latvian</cell><cell>42</cell><cell>Occitan</cell><cell>15 Somali</cell><cell>103</cell><cell>Uzbek</cell><cell>45</cell></row><row><cell>Assamese</cell><cell>155 Danish</cell><cell>28</cell><cell>Hawaiian</cell><cell>12 Lingala</cell><cell>90</cell><cell>Panjabi</cell><cell>54 Spanish</cell><cell>39</cell><cell>Vietnamese</cell><cell>64</cell></row><row><cell>Azerbaijani</cell><cell>58 Dutch</cell><cell>40</cell><cell>Hebrew</cell><cell>96 Lithuanian</cell><cell>82</cell><cell>Persian</cell><cell>56 Sundanese</cell><cell>64</cell><cell>Waray</cell><cell>11</cell></row><row><cell>Bashkir</cell><cell>58 English</cell><cell>49</cell><cell>Hindi</cell><cell>81 Luxembourg.</cell><cell>75</cell><cell>Polish</cell><cell>80 Swahili</cell><cell>64</cell><cell>Welsh</cell><cell>76</cell></row><row><cell>Basque</cell><cell>29 Esperanto</cell><cell>10</cell><cell>Hungarian</cell><cell>73 Macedonian</cell><cell>112</cell><cell>Portuguese</cell><cell>64 Swedish</cell><cell>34</cell><cell>Yiddish</cell><cell>46</cell></row><row><cell>Belarusian</cell><cell>133 Estonian</cell><cell>38</cell><cell>Icelandic</cell><cell>92 Malagasy</cell><cell>109</cell><cell>Pushto</cell><cell>47 Tagalog</cell><cell>93</cell><cell>Yoruba</cell><cell>94</cell></row><row><cell>Bengali</cell><cell>55 Faroese</cell><cell>67</cell><cell cols="2">Indonesian 40 Malay</cell><cell>83</cell><cell>Romanian</cell><cell>65 Tajik</cell><cell>64</cell><cell>Total</cell><cell>6628</cell></row><row><cell>Bosnian</cell><cell>105 Finnish</cell><cell>33</cell><cell>Interlingua</cell><cell>3 Malayalam</cell><cell>47</cell><cell>Russian</cell><cell>73 Tamil</cell><cell>51</cell><cell>Average</cell><cell>62</cell></row><row><cell>Breton</cell><cell>44 French</cell><cell>67</cell><cell>Italian</cell><cell>51 Maltese</cell><cell>66</cell><cell>Sanskrit</cell><cell>15 Tatar</cell><cell>103</cell><cell></cell><cell></cell></row><row><cell>Bulgarian</cell><cell>50 Galician</cell><cell>72</cell><cell>Japanese</cell><cell>56 Manx</cell><cell>4</cell><cell>Scots</cell><cell>3 Telugu</cell><cell>77</cell><cell></cell><cell></cell></row><row><cell>Burmese</cell><cell>41 Georgian</cell><cell>98</cell><cell>Javanese</cell><cell>53 Maori</cell><cell>34</cell><cell>Serbian</cell><cell>50 Thai</cell><cell>61</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results on in-domain data.</figDesc><table><row><cell>Training data</cell><cell></cell><cell>Error rate (%)</cell><cell></cell></row><row><cell></cell><cell cols="3">0..5 sec 5..20 sec Average</cell></row><row><cell>Noisy</cell><cell>12.3</cell><cell>6.1</cell><cell>7.1</cell></row><row><cell>Cleaned</cell><cell>13.4</cell><cell>6.6</cell><cell>7.6</cell></row><row><cell cols="3">Table 4. Most common errors.</cell><cell></cell></row><row><cell></cell><cell cols="2">Urdu − → Hindi</cell><cell></cell></row><row><cell></cell><cell cols="2">Spanish − → Galician</cell><cell></cell></row><row><cell cols="3">Norwegian − → Nynorsk</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Results on KALAKA-3, in Fact/EER (lower is better).</figDesc><table><row><cell>Embedding</cell><cell>LR</cell><cell>PC</cell><cell>PO</cell><cell>EC</cell><cell>EO</cell></row><row><cell>training</cell><cell>train</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>data</cell><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline [3]</cell><cell></cell><cell cols="4">.079/5.74 .115/6.67 .104/6.16 .169/6.96</cell></row><row><cell>Noisy</cell><cell>Noisy</cell><cell cols="4">.124/7.23 .150/8.32 .028/0.32 .083/3.08</cell></row><row><cell>Cleaned</cell><cell cols="5">Cleaned .106/5.95 .112/6.50 .028/0.16 .046/0.86</cell></row><row><cell>Noisy</cell><cell cols="5">Kalaka3 .055/4.36 .083/5.95 .033/0.32 .059/3.68</cell></row><row><cell>Cleaned</cell><cell cols="5">Kalaka3 .041/3.51 .056/5.21 .022/0.00 .058/3.51</cell></row><row><cell cols="6">European languages. There are four evaluation sets: Plenty-Closed</cell></row><row><cell cols="6">(PC) and Plenty-Open (PO), handling closed and open set classifica-</cell></row><row><cell cols="6">tion for the 6 listed languages, and Empty-Closed (EC) and Empty-</cell></row><row><cell cols="6">Open (EO), handling closed and open set classification for German,</cell></row><row><cell cols="3">Greek, French and Italian.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://dumps.wikimedia.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://polyglot.readthedocs.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/kaldi-asr/kaldi/blob/master/ egs/lre07/v1/run.sh 7 https://github.com/kaldi-asr/kaldi/blob/master/ egs/lre07/v2/run.sh 8 http://commonvoice.mozilla.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The 2017 NIST language recognition evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Seyed Omid Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrey</forename><surname>Kheyrkhah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hernandez-Cordero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2018 The Speaker and Language Recognition Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Silovsky, Georg Stemmer, and Karel Vesely</title>
		<imprint>
			<publisher>ASRU</publisher>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">KALAKA-3: a database for the assessment of spoken language recognition technology on YouTube audios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Javier Rodríguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="243" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ADI17: A fine-grained Arabic dialect identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suwon</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younes</forename><surname>Samih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The MGB-5 challenge: Recognition and dialect identification of dialectal Arabic speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suwon</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younes</forename><surname>Samih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Speakers in the Wild (SITW) speaker recognition database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciana</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Castan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Lawson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approaches to multi-domain language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><forename type="middle">Kumar</forename><surname>Nandwana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Castán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciana</forename><surname>Ferrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2018 The Speaker and Language Recognition Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">VoxCeleb: a largescale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">VoxCeleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An open-source state-of-the-art toolbox for broadcast news diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mickael</forename><surname>Rouvier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégor</forename><surname>Dupuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teva</forename><surname>Merlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Meignier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5329" to="5333" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spoken language recognition using x-vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2018 The Speaker and Language Recognition Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the encoding layer and loss function in end-to-end speaker and language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2018: The Speaker and Language Recognition Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7132" to="7141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep speaker embedding extraction with channel-wise feature responses and additive supervision softmax loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyang</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2883" to="2887" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-attentive speaker embeddings for textindependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Albayzin 2012 language recognition evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis Javier Rodríguez-Fuentes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Bordel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">The 2007 NIST language recognition evaluation plan (LRE07)</title>
		<imprint/>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<title level="m">JHU ASpIRE system: Robust LVCSR with TDNNs, iVector adaptation and RNN-LMS</title>
		<imprint>
			<publisher>ASRU</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sturim</surname></persName>
		</author>
		<title level="m">The MITLL NIST LRE 2007 language recognition system,&quot; in Interspeech</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A divide-and-conquer approach for language identification based on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Bac Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelkhalek</forename><surname>Messaoudi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
