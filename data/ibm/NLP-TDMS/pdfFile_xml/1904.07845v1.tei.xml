<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Speech Separation with Time-and-Frequency Cross-domain Joint Embedding and Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene-Ping</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Networking and Multimedia</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-I</forename><surname>Tuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Networking and Multimedia</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Networking and Multimedia</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
							<email>lslee@gate.sinica.edu.tw4</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Networking and Multimedia</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Speech Separation with Time-and-Frequency Cross-domain Joint Embedding and Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Speech separation</term>
					<term>Cocktail party problem</term>
					<term>deep clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech separation has been very successful with deep learning techniques. Substantial effort has been reported based on approaches over spectrogram, which is well known as the standard time-and-frequency cross-domain representation for speech signals. It is highly correlated to the phonetic structure of speech, or "how the speech sounds" when perceived by human, but primarily frequency domain features carrying temporal behaviour. Very impressive work achieving speech separation over time domain was reported recently, probably because waveforms in time domain may describe the different realizations of speech in a more precise way than spectrogram. In this paper, we propose a framework properly integrating the above two directions, hoping to achieve both purposes. We construct a time-and-frequency feature map by concatenating the 1-dim convolution encoded feature map (for time domain) and the spectrogram (for frequency domain), which was then processed by an embedding network and clustering approaches very similar to those used in time and frequency domain prior works. In this way, the information in the time and frequency domains, as well as the interactions between them, can be jointly considered during embedding and clustering. Very encouraging results (state-of-the-art to our knowledge) were obtained with WSJ0-2mix dataset in preliminary experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human beings are able to focus on the voice produced by a single speaker when conversing with a particular individual in a crowded and noisy environment. This requires the ability to extract the desired voice from some mixed audio signal. This so-called cocktail party problem has been shown to be difficult for computers. Very impressive results have been achieved when the speakers are known in advance, but the task remains challenging when the speakers in the mixed voice are not known, referred as the speaker-independent source separation problem. Substantial effort has been made on this problem, obviously because good solutions to it may lead to good contributions to many downstream tasks such as speech recognition <ref type="bibr" target="#b0">[1]</ref>, speaker identification <ref type="bibr" target="#b1">[2]</ref>, and audio classification <ref type="bibr" target="#b2">[3]</ref> in noisy environment.</p><p>Deep learning techniques have accomplished a big step forward on this speech separation task. The Deep Clustering (DPCL) technique <ref type="bibr" target="#b4">[4]</ref> was a good such example. It successfully coped with this problem to a good extent by projecting each element in the mixture magnitude spectrogram to a high-dimensional embedding space which is more discriminative for speaker partitioning. Various related deep learning approaches have been proposed and showed great success in enhancement <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7]</ref> and separation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b13">12]</ref> tasks, although many techniques reported at that time consisted of multiple stages separately optimized under different criteria, such as signal representation and embeddings, embedding clustering for speaker assignments <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b14">13]</ref>, mask generation over mixture magnitude spectrogram <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b15">14]</ref>, and phase approximation approaches <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b18">17]</ref>. End-to-end approaches then became popular later on, in which all different stages with different functions were jointly trained <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b20">19]</ref>.</p><p>For most methods performed over the magnitude spectrogram, the ignorance of the phase of the individual sources inevitably distorted the time domain signals. Moreover, predicting masks for each source also caused mismatch for the individual signals. These problems remained even with great effort made. A very impressive phase estimation approach was proposed recently <ref type="bibr" target="#b18">[17]</ref> based on a trigonometric perspective over the magnitude spectrogram. This approach achieved great progress over previous methods, offering a signal-to-distortion-ratio improvement SDRi = 15.6 on the publicly available datasets WSJ0-2mix <ref type="bibr" target="#b4">[4]</ref>, which seems to be the recent state-of-the-art on the task.</p><p>On the other hand, different from the above mentioned methods operated over the spectrograms, a surprisingly successful approach was reported recently called TasNet <ref type="bibr" target="#b21">[20]</ref>, which directly handled the problem over the time domain signals and achieved superior performance with SDRi = 15.0 dB on WSJ0-2mix dataset. It contained an encoder module, a separation module and a decoder module, where the separation module consisted of multiple blocks of dilated convolutional layers similar to Wavenet <ref type="bibr" target="#b22">[21]</ref>, but with fewer parameters and less computation due to the adoption of depthwise separable convolution previously proposed <ref type="bibr" target="#b23">[22]</ref>.</p><p>To the best of our knowledge, existing approaches for the considered problem have taken either time or frequency domain representations as input, both achieving very good and very close performance. Obviously, both representations possess their respective advantages: the robustness of spectrograms in frequency domain, and the sophisticated but fine structures of time domain signals. The spectrogram is highly correlated to the phonetic structure of speech, or " how the speech sounds" when perceived by human. But the waveform in time domain describes the various realizations of the sound in a more precise way.</p><p>In this paper, we try to integrate both time and frequency domain features together, with the hope to take the advantages of both domains. We construct a time-and-frequency feature map by concatenating features for both time and frequency domains, and perform cross-domain joint embedding and clustering over this feature map, so the model can learn signal behavior in both domains as well as the cross-domain correlations. We make part of the approach similar to TasNet <ref type="bibr" target="#b21">[20]</ref>, which has fewer parameters with large receptive field due to the dilated convolutional layers. We also adopt the previously proposed clustering method for mask estimation <ref type="bibr" target="#b15">[14]</ref>, which directly predicts masks for each source in the mixture from feature embeddings. Such a model structure is also in good parallel to the insight offered by a recently reported work <ref type="bibr" target="#b24">[23]</ref>. As will be shown below, very encouraging performance (state-of-the-art to our knowledge) was obtained on WSJ0-2mix dataset in preliminary experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview of the Proposed Approach</head><p>The overview of the proposed approach is in section 2.1, while the details are in sections 2.2-2.5 . The proposed approach consists of three processing modules as shown in <ref type="figure" target="#fig_0">Figure 1</ref>: an encoder on the left, a separator in the middle, and a decoder on the right.</p><p>• The Encoder on the left of <ref type="figure" target="#fig_0">Figure 1</ref> encodes the input mixture x into a hybrid-domain 2-dim feature map H with F = Fconv + Fspec channels and T time frames, where Fconv and Fspec are respectively the dimensionality of the time and frequency domain features, both of which at each time frame correspond to features extracted from a given small segment of the mixture signal.</p><formula xml:id="formula_0">H = Encode(x)<label>(1)</label></formula><p>• The Separator in the middle of <ref type="figure" target="#fig_0">Figure 1</ref> consists of two parts, an Embedding Network and Clustering plus Mask Estimation. The Embedding Network projects each element in the feature map H onto a D-dimensional space, forming the embeddings V . Clustering plus Mask Estimation then follows, from which the masks M for the different speakers are generated, and each element of H is assigned to the speakers based on these masks.</p><formula xml:id="formula_1">V = Embed(H) (2) M = Clust-M ask(V )<label>(3)</label></formula><p>• The Decoder decodes the masked feature map into the estimated time domain signalsŝ, which is the weighted sum of two signal components respectively obtained from the estimated time and frequency domain features, and denotes element-wise multiplication.</p><formula xml:id="formula_2">s = Decode(M H)<label>(4)</label></formula><p>So in this approach we aim to reconstruct time domain signals from both domain features, and directly optimize the signal-todistortion ratio on the estimated waveform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Encoder</head><p>In our cross-domain setting, we utilize both time domain signals and the magnitude spectrogram jointly. The input is the mixture signal x(t) produced by N speakers s1(t), ..., sN (t), and the corresponding frequency domain representations are obtained by Short-Time Fourier Transform.</p><formula xml:id="formula_3">x(t) = N i=1 si(t) (5) X(f, t) = N i=1</formula><p>Si(f, t) (</p><p>As shown in the left part of <ref type="figure" target="#fig_0">Figure 1</ref>, the encoder is composed of two parallel procedures: a 1-dim convolutional block and the Short-Time Fourier Transform. To properly integrating the two extracted features from different domains, we set the same window size and the same striding for both domains. This gives a 2-dim feature map Mconv with Fconv channels from the convolutional block and a spectrogram Mspec of Fspec frequency channels. We concatenate Mconv and Mspec along the channel/frequency axis while aligning the time frames, giving a hybrid-domain feature map H with F = (Fconv + Fspec) channels and T time frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Separator</head><p>The separator has two parts, an embedding network and clustering plus mask estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Embedding Network</head><p>In order to estimate the speaker assignment for each T-F index on the hybrid-domain feature map H, we seek a D-dimensional embedding representing each T-F index. We project the elements in the hybrid-domain feature H to D-dimension embeddings through multiple layers of 1-d dilated convolutional blocks, as shown in the middle block in <ref type="figure" target="#fig_0">Figure 1</ref>. Here the "1-d Conv" block in the figure is actually a residual block consisting of a 1x1-conv, a dilated depthwise convolution and a 1x1-conv module, following the prior work <ref type="bibr" target="#b21">[20]</ref>. We stack B residual blocks with the dilation factors of 1,2,...,2 B−1 and repeat these blocks for R times, followed by a linear layer with a D-dimension vector output for each T-F index on the hybrid-domain feature map H. This gives the embeddings V for H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Clustering and Mask Estimation</head><p>We follow the clustering algorithm previously proposed <ref type="bibr" target="#b15">[14]</ref> as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, starting with K initial centers e1, e2, ..., eK <ref type="figure" target="#fig_1">(Figure 2(a)</ref>). By arbitrarily choosing N (number of speakers in the mixture) out of the K initial centers <ref type="figure" target="#fig_1">(Figure 2(b)</ref>), we can get a set of N new centroids by performing k-means clustering for I iterations on the embeddings V . Considering all K N possible selections out of the K initial centers, we can obtain a total of K N sets of centroids after performing k-means <ref type="figure" target="#fig_1">(Figure 2(c)</ref>), among which we choose the set of centroids A with the largest in-set distance <ref type="figure" target="#fig_1">(Figure 2(d)</ref>). The in-set distance is the minimum distance among all pairs of centroids if N &gt; 2. The masks for each speaker M ∈ R T F ×N is then estimated by the dot product of the chosen centroids ai ∈ A and the embeddings V .</p><p>Mi t,f = V t,f · ai for ai ∈ A (7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Decoder</head><p>After multiplying the hybrid-domain feature map H by masks M , we disassemble the masked encoded features into their original components: convolutional feature mapMconv and frequency domain spectrogramMspec. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we reconstruct the original signal waveforms from each individual domain: s deconv from convolutional feature map andŝ istf t from the spectrogram, the former through a deconvolution layer followed by overlap-add method to reconstruct the signalsŝ deconv ; the latter taking the phase of the mixture signal for inverse Fourier Transform to deriveŝ istf t . Weighted sum of the two components with a weight parameter α is then taken as the estimated separated signalŝ s.ŝ</p><formula xml:id="formula_5">= α * ŝ deconv + (1 − α) * ŝ istf t<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Training Objective</head><p>We take the negative signal-to-distortion ratio as our training objective. = −10 log 10 s,ŝ 2 ||s|| 2 ||ŝ|| 2 − s,ŝ∠ 2 <ref type="bibr" target="#b9">(9)</ref> where ·, · represents dot product and ||s|| 2 = s, s denotes the signal power. The training is performed end-to-end, so all components are jointly learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We evaluated the proposed method on publicly avaliable dataset WSJ0-2mix <ref type="bibr" target="#b4">[4]</ref>, which was derived from WSJ0 corpus. The 30hrs of training set and 10hrs of validation set consisted of two-speaker mixtures generated by different speakers from WSJ0 training set si tr s mixed at various signal-to-noise ratio between -5 dB and 5 dB. The 5hrs of testing set was similarly generated from WSJ0 validation set si dt 05 and evaluation set si et 05 produced by 16 speakers. All waveforms were resampled to 8 kHz.</p><p>We used in addition environmental sounds from Diverse Environments Multichannel Acoustic Noise Database (DEMAND) <ref type="bibr" target="#b25">[24]</ref> in the test. We resampled all types of background noise from 16kHz to 8kHz, and mixed one arbitrarily chosen type of noise into the mixtures in WSJ0-2mix test set with given signal-to-noise ratio (SNR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setup</head><p>The window size of the Short-time Fourier Transform (STFT) and the kernel size for the first convolution layer were both 2.5ms, and the square root Hann window was used for STFT. 20-point DFT was performed to extract the 11-dimensional log magnitude fea-ture, combined with the 256-dimensional feature extracted by 1-d conv, and formed 267-dimensional features in the feature map H.</p><p>For the separator, the feature map H first went through a 1x1 Conv block with 256 filters, followed by 8 residual 1-d Conv blocks, with dilated rate of 1,2,...,128, repeated for 4 times. D = 20 was chosen to be the embedding dimension following the prior works <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b18">17]</ref> working on speech separation for better comparison. We set N = 4 initial centers and I = 1 iteration for kmeans following the prior work <ref type="bibr" target="#b15">[14]</ref>. We also tested the approach without clustering by passing the output of the last 1-d conv in the embedding net through an additional 1x1 convolution block to estimate N masks as done in TasNet <ref type="bibr" target="#b21">[20]</ref>. Since the weight α in <ref type="bibr" target="#b8">(8)</ref> show minor difference in preliminary experiments, we set α = 1 in most of our experiments. The networks are trained from scratch on 4-second segments for 100 epochs using Adam algorithm with permutation invariant training <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b26">25]</ref>.</p><p>We evaluate the approach by the signal-to-distortion ratio improvement (SDRi) <ref type="bibr" target="#b27">[26]</ref> and the scale-invariant signal-to-noise ratio improvement (SI-SNRi) <ref type="bibr" target="#b15">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>We report the performance tested on the WSJ0-2mix test set compared to prior works in <ref type="table">Table 1</ref>. We see the approach proposed here (row (g)) offered significant improvement with SDRi = 16.9 dB and SI-SNRi = 16.6 dB compared to all prior works including the previous state-of-the-art methods on time domain SDRi = 15.0 dB (TasNet <ref type="bibr" target="#b21">[20]</ref> in row (e)) and frequency domain SDRi = 15.6 dB (Sign Prediction Net <ref type="bibr" target="#b18">[17]</ref> in row (f)). In addition, it is worth mentioning that in contrast to the bidirectional LSTM consisting of 600 units on each direction used in the prior works <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b18">17]</ref>, our implementation of depthwise dilated convolution significantly lowered the parameter size by a factor near 1/3.  <ref type="table">Table 2</ref> shows the ablation studies for the proposed approach. The upper section (I) is for the separator without clustering, so the masks were directly generated from the embeddings. Here in row (1) the Encoder generated only the time domain features (very similar to TasNet <ref type="bibr" target="#b21">[20]</ref> in row (e) of <ref type="table">Table 1</ref>), while in rows (2)(3) the spectrogram was included in addition. From rows (1)(2) we see the spectrogram improved SDRi by 0.45 dB (from 15.82 to 16.27 dB, first column of rows (1)(2)). Comparing rows (2)(3) we see the value of α didn't make too much difference, or α = 1 is about good enough. This implied the cross-domain features were very useful in jointly learning the various modules, but the time domain features alone were about adequate to generate the precise waveforms.</p><p>The lower Section (II) of <ref type="table">Table 2</ref> is for the separator including clustering, rows (4)(5) with time domain features only, and rows (6)(7) with cross-domain features. We see adding the clustering module offered improvement in SDRi of 0.46 dB (from 15.82 to 16.28 dB, rows (4)(5) vs. (1)) for time domain feature alone, and 0.23 to 0.60 dB (from 16.27 to 16.50 or 16.87 dB, rows (6)(7) vs. (2)) for cross-domain features. The spectrogram was certainly useful here too (rows (6)(7) vs. (4)(5)). We also see the choice of the parameter K made the difference for cross-domain features (rows (6)(7)), although this was not clear for time domain features alone (rows (4)(5)). With joint learning including clustering on cross-domain features, we achieve the best result (state-of-the-art to our knowledge) of SDRi = 16.87 dB in row <ref type="bibr" target="#b7">(7)</ref>, or 16.9 dB in row (g) of <ref type="table">Table 1</ref>.</p><p>The results for noisy mixture test data (clean data training) with SNRs = 20, 15 dB are listed in the middle and the right columns of <ref type="table">Table 2</ref>. We see all trends observed above remained true, and a degradation of roughly 1.7 dB or less for 20 dB of SNR, and roughly extra 2 dB for extra 5 dB (20-15) of noise. This showed the robustness of the proposed approach. These results are also shown in <ref type="figure">Figure 3</ref>, for all rows (1)- <ref type="bibr" target="#b7">(7)</ref> of <ref type="table">Table 2</ref>, plus results for SNR = 10 dB which we were not able to include in <ref type="table">Table 2</ref>. <ref type="table">Table 1</ref>: SI-SNRi and SDRi comparison to different prior works tested on WSJ0-2mix dataset. "*" indicates our estimation not written in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approaches</head><p>Params SI-SNRi SDRi prior works (a) DPCL++ <ref type="bibr" target="#b14">[13]</ref> 13.6M 10.8 dB -(b) uPIT-ST <ref type="bibr" target="#b26">[25]</ref> 92.7M -10.0 dB (c) ADANet <ref type="bibr" target="#b15">[14]</ref> 9.1M 10.4 dB 10.8 dB (d) Chimera++ <ref type="bibr" target="#b16">[15]</ref> 32.9M 11.5 dB 12.0 dB (e) TasNet <ref type="bibr" target="#b21">[20]</ref> 8  <ref type="table">Table 2</ref>: SDRi (dB) performance of the proposed approach when the separator included clustering or not (section (I)(II)) and the encoder generated time domain features alone or cross-domain features, with clean or noisy input for different parameters (K: number of cluster centers in <ref type="figure" target="#fig_1">Fig 2(a)</ref>, α: weight in <ref type="formula" target="#formula_5">(8)</ref>).  <ref type="table">Table 2</ref> row (1) row (2) row (3) row (4) row (5) row (6) row (7) <ref type="figure">Figure 3</ref>: Performance degradation with noise level for different configurations in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this paper, we propose to integrate the time and frequency domain features and perform cross-domain joint learning for speech separation. State-of-the-art performance of SDRi = 16.9 dB was achieved on the WSJ0-2mix dataset. This verified that the different advantages of the two domains can be well taken, not to mention the correlations between them are useful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed approach. The encoder extracts features in both time and frequency domains from input signals. The separator includes an embedding network projecting each element of the feature map to a D-dim vector, followed by clustering and mask estimation for each source speaker. The decoder reconstructs the signal waveforms of each source from the masked features. Loss = −SDR(s,ŝ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Clustering and mask estimation procedure ( K = 4 initial centers and N = 2 speakers shown here).</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based endto-end speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="776" to="780" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09452</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkeRTsAcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and language processing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="246" to="250" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep clustering and conventional networks for music separation: Stronger together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02173</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speaker-independent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">End-toend speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09010</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Listen, attend and spell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01211</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tasnet: Surpassing ideal time-frequency masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07454</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00158</idno>
		<title level="m">Speaker recognition from raw waveform with sincnet</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Diverse environments multichannel acoustic noise database (demand)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterancelevel permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
