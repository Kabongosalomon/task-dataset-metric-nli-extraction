<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iterative Neural Autoregressive Distribution Estimator (NADE-k)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">KyungHyun Cho Université de Montréal Yoshua Bengio Université de Montréal, CIFAR Senior Fellow</orgName>
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Iterative Neural Autoregressive Distribution Estimator (NADE-k)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in k steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-prediction training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested.</p><p>1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional building blocks for deep learning have some unsatisfactory properties. Boltzmann machines are, for instance, difficult to train due to the intractability of computing the statistics of the model distribution, which leads to the potentially high-variance MCMC estimators during training (if there are many well-separated modes ) and the computationally intractable objective function. Autoencoders have a simpler objective function (e.g., denoising reconstruction error <ref type="bibr" target="#b18">(Vincent et al., 2010)</ref>), which can be used for model selection but not for the important choice of the corruption function. On the other hand, this paper follows up on the Neural Autoregressive Distribution Estimator (NADE, <ref type="bibr" target="#b12">Larochelle and Murray, 2011)</ref>, which specializes previous neural auto-regressive density estimators <ref type="bibr" target="#b1">(Bengio and Bengio, 2000)</ref> and was recently extended <ref type="bibr" target="#b17">(Uria et al., 2014)</ref> to deeper architectures. It is appealing because both the training criterion (just log-likelihood) and its gradient can be computed tractably and used for model selection, and the model can be trained by stochastic gradient descent with backpropagation. However, it has been observed that the performance of NADE has still room for improvement.</p><p>The idea of using missing value imputation as a training criterion has appeared in three recent papers. This approach can be seen either as training an energy-based model to impute missing values well <ref type="bibr" target="#b4">(Brakel et al., 2013)</ref>, as training a generative probabilistic model to maximize a generalized pseudo-log-likelihood <ref type="bibr" target="#b7">(Goodfellow et al., 2013)</ref>, or as training a denoising autoencoder with a masking corruption function <ref type="bibr" target="#b17">(Uria et al., 2014)</ref>. Recent work on generative stochastic networks (GSNs), which include denoising auto-encoders as special cases, justifies dependency networks <ref type="bibr" target="#b9">(Heckerman et al., 2000)</ref> as well as generalized pseudo-log-likelihood <ref type="bibr" target="#b7">(Goodfellow et al., 2013)</ref>, but have the disadvantage that sampling from the trained "stochastic fill-in" model requires a Markov chain (repeatedly resampling some subset of the values given the others). In all these cases, learning progresses by back-propagating the imputation (reconstruction) error through inference steps of the model. This allows the model to better cope with a potentially imperfect inference algorithm. This learning-to-cope was introduced recently in 2011 by <ref type="bibr" target="#b16">Stoyanov et al. (2011)</ref> and <ref type="bibr" target="#b6">Domke (2011)</ref>.  <ref type="figure">Figure 1</ref>: The choice of a structure for NADE-k is very flexible. The dark filled halves indicate that a part of the input is observed and fixed to the observed values during the iterations. Left: Basic structure corresponding to Equations (6-7) with n = 2 and k = 2. Middle: Depth added as in NADE by <ref type="bibr" target="#b17">Uria et al. (2014)</ref> with n = 3 and k = 2. Right: Depth added as in Multi-Prediction Deep Boltzmann Machine by <ref type="bibr" target="#b7">Goodfellow et al. (2013)</ref> with n = 2 and k = 3. The first two structures are used in the experiments.</p><formula xml:id="formula_0">v &lt;0&gt; v &lt;1&gt; v &lt;2&gt; h &lt;1&gt; h &lt;1&gt; W V W V v &lt;0&gt; v &lt;1&gt; h &lt;1&gt; h &lt;1&gt; [1] [2] U W V h &lt;2&gt; h &lt;2&gt; [1] [2] U W V v &lt;2&gt; v &lt;0&gt; v &lt;1&gt; v &lt;2&gt; h &lt;1&gt; W W W W v &lt;3&gt; W W T T T V V V V T T h &lt;2&gt; h &lt;3&gt; h &lt;1&gt; h &lt;2&gt; [</formula><p>The NADE model involves an ordering over the components of the data vector. The core of the model is the reconstruction of the next component given all the previous ones. In this paper we reinterpret the reconstruction procedure as a single iteration in a variational inference algorithm, and we propose a version where we use k iterations instead, inspired by <ref type="bibr" target="#b7">(Goodfellow et al., 2013;</ref><ref type="bibr" target="#b4">Brakel et al., 2013)</ref>. We evaluate the proposed model on two datasets and show that it outperforms the original NADE <ref type="bibr" target="#b12">(Larochelle and Murray, 2011)</ref> as well as NADE trained with the order-agnostic training algorithm <ref type="bibr" target="#b17">(Uria et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method: NADE-k</head><p>We propose a probabilistic model called NADE-k for D-dimensional binary data vectors x. We start by defining p θ for imputing missing values using a fully factorial conditional distribution:</p><formula xml:id="formula_1">p θ (x mis | x obs ) = i∈mis p θ (x i | x obs ),<label>(1)</label></formula><p>where the subscripts mis and obs denote missing and observed components of x. From the conditional distribution p θ we compute the joint probability distribution over x given an ordering o (a permutation of the integers from 1 to D) by</p><formula xml:id="formula_2">p θ (x | o) = D d=1 p θ (x o d | x o &lt;d ),<label>(2)</label></formula><p>where o &lt;d stands for indices o 1 . . . o d−1 .</p><p>The model is trained to minimize the negative log-likelihood averaged over all possible orderings o</p><formula xml:id="formula_3">L(θ) = E o∈D! [E x∈data [− log p θ (x | o)]] .<label>(3)</label></formula><p>using an unbiased, stochastic estimator of L(θ)</p><formula xml:id="formula_4">L(θ) = − D D − d + 1 log p θ (x o ≥d | x o &lt;d )<label>(4)</label></formula><p>by drawing o uniformly from all D! possible orderings and d uniformly from 1 . . . D <ref type="bibr" target="#b17">(Uria et al., 2014)</ref>. Note that while the model definition in Eq.</p><p>(2) is sequential in nature, the training criterion (4) involves reconstruction of all the missing values in parallel. In this way, training does not involve picking or following specific orders of indices.</p><p>In this paper, we define the conditional model p θ (x mis | x obs ) using a deep feedforward neural network with nk layers, where we use n weight matrices k times. This can also be interpreted as running k successive inference steps with an n-layer neural network.</p><p>The input to the network is</p><formula xml:id="formula_5">v 0 = m E x∈data [x] + (1 − m) x<label>(5)</label></formula><p>where m is a binary mask vector indicating missing components with 1, and is an elementwise multiplication. E x∈data [x] is an empirical mean of the observations. For simplicity, we give  <ref type="formula">(7)</ref>). equations for a simple structure with n = 2. See <ref type="figure">Fig. 1 (left)</ref> for the illustration of this simple structure.</p><p>In this case, the activations of the layers at the t-th step are</p><formula xml:id="formula_6">h t = φ(Wv t−1 + c) (6) v t = m σ(Vh t + b) + (1 − m) x (7)</formula><p>where φ is an element-wise nonlinearity, σ is a logistic sigmoid function, and the iteration index t runs from 1 to k. The conditional probabilities of the variables (see Eq.</p><p>(1)) are read from the output v k as Once the parameters θ are learned, we can define a mixture model by using a uniform probability over a set of orderings O. We can compute the probability of a given vector x as a mixture model</p><formula xml:id="formula_7">p θ (x i = 1 | x obs ) = v k i .<label>(8)</label></formula><formula xml:id="formula_8">p mixt (x | θ, O) = 1 |O| o∈O p θ (x | o)<label>(9)</label></formula><p>with Eq. (2). We can draw independent samples from the mixture by first drawing an ordering o and then sequentially drawing each variable using</p><formula xml:id="formula_9">x o d ∼ p θ (x o d | x o &lt;d )</formula><p>. Furthermore, we can draw samples from the conditional p(x mis | x obs ) easily by considering only orderings where the observed indices appear before the missing ones.</p><p>Pretraining It is well known that training deep networks is difficult without pretraining, and in our experiments, we train networks up to kn = 7 × 3 = 21 layers. When pretraining, we train the model to produce good reconstructions v t at each step t = 1 . . . k. More formally, in the pretraining phase, we replace Equations <ref type="formula" target="#formula_4">(4)</ref> and <ref type="formula" target="#formula_7">(8)</ref> bŷ</p><formula xml:id="formula_10">L pre (θ) = − D D − d + 1 1 k k t=1 log i∈o ≥d p t θ (x i | x o &lt;d ) (10) p t θ (x i = 1 | x obs ) = v t i .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Methods and Approaches</head><p>Order-agnostic NADE The proposed method follows closely the order-agnostic version of NADE <ref type="bibr" target="#b17">(Uria et al., 2014)</ref>, which may be considered as the special case of NADE-k with k = 1. On the other hand, NADE-k can be seen as a deep NADE with some specific weight sharing (matrices W and V are reused for different depths) and gating in the activations of some layers (See Equation <ref type="formula">(7)</ref>).</p><p>Additionally, <ref type="bibr" target="#b17">Uria et al. (2014)</ref> found it crucial to give the mask m as an auxiliary input to the network, and initialized missing values to zero instead of the empirical mean (See Eq. <ref type="formula" target="#formula_5">(5)</ref>). Due to these differences, we call their approach NADE-mask. One should note that NADE-mask has more parameters due to using the mask as a separate input to the network, whereas NADE-k is roughly k times more expensive to compute.</p><p>Probabilistic Inference Let us consider the task of missing value imputation in a probabilistic latent variable model. We get the conditional probability of interest by marginalizing out the latent variables from the posterior distribution:</p><formula xml:id="formula_11">p(x mis | x obs ) = h p(h, x mis | x obs )dh.<label>(12)</label></formula><p>Accessing the joint distribution p(h, x mis | x obs ) directly is often harder than alternatively updating h and x mis based on the conditional distributions p(h | x mis , x obs ) and p(x mis | h). 1 Variational inference is one of the representative examples that exploit this.</p><p>In variational inference, a factorial distribution q(h,</p><formula xml:id="formula_12">x mis ) = q(h)q(x mis ) is iteratively fitted to p(h, x mis | x obs ) such that the KL-divergence between q and p KL[q(h, x mis )||p(h, x mis | x obs )] = − h,xmis q(h, x mis ) log p(h, x mis | x obs ) q(h, x mis ) dhdx mis<label>(13)</label></formula><p>is minimized. The algorithm alternates between updating q(h) and q(x mis ), while considering the other one fixed.</p><p>As an example, let us consider a restricted Boltzmann machine (RBM) defined by</p><formula xml:id="formula_13">p(v, h) ∝ exp(b v + c h + h Wv).<label>(14)</label></formula><p>We can fit an approximate posterior distribution parameterized as q(v i = 1) =v i and q(h j = 1) = h j to the true posterior distribution by iteratively computinḡ</p><formula xml:id="formula_14">h ← σ(Wv + c) (15) v ← m σ(W h + b) + (1 − m) v.<label>(16)</label></formula><p>We notice the similarity to Eqs. (6)- <ref type="formula">(7)</ref>: If we assume φ = σ and V = W , the inference in the NADE-k is equivalent to performing k iterations of variational inference on an RBM for the missing values <ref type="bibr" target="#b15">(Peterson and Anderson, 1987)</ref>. We can also get variational inference on a deep Boltzmann machine (DBM) using the structure in <ref type="figure">Fig. 1 (right)</ref>. <ref type="bibr" target="#b7">Goodfellow et al. (2013)</ref> and <ref type="bibr" target="#b4">Brakel et al. (2013)</ref> use backpropagation through variational inference steps to train a deep Boltzmann machine. This is very similar to our work, except that they approach the problem from the view of maximizing the generalized pseudo-likelihood <ref type="bibr" target="#b11">(Huang and Ogata, 2002)</ref>. Also, the deep Boltzmann machine lacks the tractable probabilistic interpretation similar to NADE-k (See Eq. <ref type="formula" target="#formula_2">(2)</ref>) that would allow to compute a probability or to generate independent samples without resorting to a Markov chain. Also, our approach is somewhat more flexible in the choice of model structures, as can be seen in <ref type="figure">Fig. 1</ref>. For instance, in the proposed NADE-k, encoding and decoding weights do not have to be shared and any type of nonlinear activations, other than a logistic sigmoid function, can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Prediction Deep Boltzmann Machine</head><p>Product and Mixture of Experts One could ask what would happen if we would define an ensemble likelihood along the line of the training criterion in Eq. (3). That is,</p><formula xml:id="formula_15">− log p prod (x | θ) ∝ E o∈D! [− log p(x | θ, o)] .</formula><p>(17) Maximizing this ensemble likelihood directly will correspond to training a product-of-experts model <ref type="bibr" target="#b10">(Hinton, 2000)</ref>. However, this requires us to evaluate the intractable normalization constant during training as well as in the inference, making the model not tractable anymore.</p><p>On the other hand, we may consider using the log-probability of a sample under the mixture-ofexperts model as the training criterion  <ref type="table">Table 1</ref>: Results obtained on MNIST using various models and number of hidden layers (1HL or 2HL). "Ords" is short for "orderings". These are the average log-probabilities of the test set. EoNADE refers to the ensemble probability (See Eq. <ref type="formula" target="#formula_8">(9)</ref>). From here on, in all figures and tables we use "HL" to denote the number of hidden layers and "h" for the number of hidden units.</p><formula xml:id="formula_16">− log p mixt (x | θ) = − log E o∈D! [p(x | θ, o)] .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We study the proposed model with two datasets: binarized MNIST handwritten digits and Caltech 101 silhouettes.</p><p>We train NADE-k with one or two hidden layers (n = 2 and n = 3, see <ref type="figure">Fig. 1</ref>, left and middle) with a hyperbolic tangent as the activation function φ(·). We use stochastic gradient descent on the training set with a minibatch size fixed to 100. We use AdaDelta <ref type="bibr" target="#b20">(Zeiler, 2012)</ref> to adaptively choose a learning rate for each parameter update on-the-fly. We use the validation set for earlystopping and to select the hyperparameters. With the best model on the validation set, we report the log-probability computed on the test set. We have made our implementation available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MNIST</head><p>We closely followed the procedure used by <ref type="bibr" target="#b17">Uria et al. (2014)</ref>, including the split of the dataset into 50,000 training samples, 10,000 validation samples and 10,000 test samples. We used the same version where the data has been binarized by sampling.</p><p>We used a fixed width of 500 units per hidden layer. The number of steps k was selected among {1, 2, 4, 5, 7}. According to our preliminary experiments, we found that no separate regularization was needed when using a single hidden layer, but in case of two hidden layers, we used weight decay with the regularization constant in the interval e −5 , e −2 . Each model was pretrained for 1000 epochs and fine-tuned for 1000 epochs in the case of one hidden layer and 2000 epochs in the case of two.</p><p>For both NADE-k with one and two hidden layers, the validation performance was best with k = 5. The regularization constant was chosen to be 0.00122 for the two-hidden-layer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We report in <ref type="table">Table 1</ref> the mean of the test log-probabilities averaged over randomly selected orderings. We also show the experimental results by others from <ref type="bibr" target="#b17">(Uria et al., 2014;</ref><ref type="bibr" target="#b8">Gregor et al., 2014)</ref>. We denote the model proposed in <ref type="bibr" target="#b17">(Uria et al., 2014)</ref> as a NADE-mask.</p><p>From <ref type="table">Table 1</ref>, it is clear that NADE-k outperforms the corresponding NADE-mask both with the individual orderings and ensembles over orderings using both 1 or 2 hidden layers. NADE-k with two hidden layers achieved the generative performance comparable to that of the deep belief network (DBN) with two hidden layers. <ref type="figure">Fig. 3</ref> shows training curves for some of the models. We can see that the NADE-1 does not perform as well as NADE-mask. This confirms that in the case of k = 1, the auxiliary mask input is indeed useful. Also, we can note that the performance of NADE-5 is still improving at the end of the preallocated 2000 epochs, further suggesting that it may be possible to obtain a better performance simply by training longer. (b) <ref type="figure">Figure 3</ref>: NADE-k with k steps of variational inference helps to reduce the training cost (a) and to generalize better (b). NADE-mask performs better than NADE-1 without masks both in training and test.   <ref type="figure" target="#fig_2">Fig. 4 (a)</ref> shows the effect of the number of iterations k during training. Already with k = 2, we can see that the NADE-k outperforms its corresponding NADE-mask. The performance increases until k = 5. We believe the worse performance of k = 7 is due to the well known training difficulty of a deep neural network, considering that NADE-7 with two hidden layers effectively is a deep neural network with 21 layers.</p><p>At inference time, we found that it is important to use the exact k that one used to train the model. As can be seen from <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>, the assigned probability increases up to the k, but starts decreasing as the number of iterations goes over the k. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Qualitative Analysis</head><p>In <ref type="figure" target="#fig_0">Fig. 2</ref>, we present how each iteration t = 1 . . . k improves the corrupted input (v t from Eq. (5)).</p><p>We also investigate what happens with test-time k being larger than the training k = 5. We can see that in all cases, the iteration -which is a fixed point update -seems to converge to a point that is in most cases close to the ground-truth sample. <ref type="figure" target="#fig_2">Fig. 4 (b)</ref> shows however that the generalization performance drops after k = 5 when training with k = 5. <ref type="figure" target="#fig_0">From Fig. 2</ref>, we can see that the reconstruction continues to be sharper even after k = 5, which seems to be the underlying reason for this phenomenon. From the samples generated from the trained NADE-5 with two hidden layers shown in <ref type="figure">Fig. 5 (a)</ref>, we can see that the model is able to generate digits. Furthermore, the filters learned by the model show that it has learned parts of digits such as pen strokes (See <ref type="figure">Fig. 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Variability over Orderings</head><p>In Section 2, we argued that we can perform any inference task p(x mis | x obs ) easily and efficiently by restricting the set of orderings O in Eq. (9) to ones where x obs is before x mis . For this to work well, we should investigate how much the different orderings vary.  In <ref type="table" target="#tab_5">Table 2</ref>, the variability over the orderings is clearly much smaller than that over the samples. Furthermore, the variability over orderings tends to decrease with the better models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Caltech-101 silhouettes</head><p>We also evaluate the proposed NADE-k on Caltech-101 Silhouettes <ref type="bibr" target="#b13">(Marlin et al., 2010)</ref>, using the standard split of 4100 training samples, 2264 validation samples and 2307 test samples. We demonstrate the advantage of NADE-k compared with NADE-mask under the constraint that they have a matching number of parameters. In particular, we compare NADE-k with 1000 hidden units with NADE-mask with 670 hiddens. We also compare NADE-k with 4000 hidden units with NADE-mask with 2670 hiddens.</p><p>We optimized the hyper-parameter k ∈ {1, 2, . . . , 10} in the case of NADE-k. In both NADE-k and NADE-mask, we experimented without regularizations, with weight decays, or with dropout. Unlike the previous experiments, we did not use the pretraining scheme (See Eq. (10)). As we can see from <ref type="table" target="#tab_6">Table 3</ref>, NADE-k outperforms the NADE-mask regardless of the number of parameters. In addition, NADE-2 with 1000 hidden units matches the performance of an RBM with the same number of parameters. Futhermore, NADE-5 has outperformed the previous best result obtained with the RBMs in <ref type="bibr" target="#b5">(Cho et al., 2013)</ref>, achieving the state-of-art result on this dataset. We can see from the samples generated by the NADE-k shown in <ref type="figure">Fig. 5 (b)</ref> that the model has learned the data well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Discussion</head><p>In this paper, we proposed a model called iterative neural autoregressive distribution estimator (NADE-k) that extends the conventional neural autoregressive distribution estimator (NADE) and its order-agnostic training procedure. The proposed NADE-k maintains the tractability of the original NADE while we showed that it outperforms the original NADE as well as similar, but intractable generative models such as restricted Boltzmann machines and deep belief networks.</p><p>The proposed extension is inspired from the variational inference in probabilistic models such as restricted Boltzmann machines (RBM) and deep Boltzmann machines (DBM). Just like an iterative mean-field approximation in Boltzmann machines, the proposed NADE-k performs multiple iterations through hidden layers and a visible layer to infer the probability of the missing value, unlike the original NADE which performs the inference of a missing value in a single iteration through hidden layers.</p><p>Our empirical results show that this approach of multiple iterations improves the performance of a model that has the same number of parameters, compared to performing a single iteration. This suggests that the inference method has significant effect on the efficiency of utilizing the model parameters. Also, we were able to observe that the generative performance of NADE can come close to more sophisticated models such as deep belief networks in our approach.</p><p>In the future, more in-depth analysis of the proposed NADE-k is needed. For instance, a relationship between NADE-k and the related models such as the RBM need to be both theoretically and empirically studied. The computational speed of the method could be improved both in training (by using better optimization algorithms. See, e.g., <ref type="bibr" target="#b14">(Pascanu and Bengio, 2014)</ref>) and in testing (e.g. by handling the components in chunks rather than fully sequentially). The computational efficiency of sampling for NADE-k can be further improved based on the recent work of <ref type="bibr" target="#b19">Yao et al. (2014)</ref> where an annealed Markov chain may be used to efficiently generate samples from the trained ensemble. Another promising idea to improve the model performance further is to let the model adjust its own confidence based on d. For instance, in the top right corner of <ref type="figure" target="#fig_0">Fig. 2</ref>, we see a case with lots of missing values values (low d), where the model is too confident about the reconstructed digit 8 instead of the correct digit 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The inner working mechanism of NADE-k. The left most column shows the data vectors x, the second column shows their masked version and the subsequent columns show the reconstructions v 0 . . . v 10 (See Eq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>shows examples of how v t evolves over iterations, with the trained model. The parameters θ = {W, V, c, b} can be learned by stochastic gradient descent to minimize −L(θ) in Eq. (3), or its stochastic approximation −L(θ) in Eq. (4), with the stochastic gradient computed by back-propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) The generalization performance of different NADE-k models trained with different k. (b) The generalization performance of NADE-5 2h, trained with k=5, but with various k in test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Samples generated from NADE-k trained on (a) MNIST and (b) Caltech-101 Silhouettes. Filters learned from NADE-5 2HL. (a) A random subset of the encodering filters. (b) A random subset of the decoding filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This criterion resembles clustering, where individual models may specialize in only a fraction of the data. In this case, however, the simple estimator such as in Eq. (4) would not be available.</figDesc><table><row><cell>Model</cell><cell cols="2">Log-Prob. Model</cell><cell>Log-Prob.</cell></row><row><cell>NADE 1HL(fixed order) NADE 1HL NADE 2HL NADE-mask 1HL NADE-mask 2HL</cell><cell>-88.86 -99.37 -95.33 -92.17 -89.17</cell><cell>RBM (500h, CD-25) DBN (500h+2000h) DARN (500h) DARN (500h, adaNoise) NADE-5 1HL</cell><cell>≈ -86.34 ≈ -84.55 ≈ -84.71 ≈ -84.13 -90.02</cell></row><row><cell>NADE-mask 4HL</cell><cell>-89.60</cell><cell>NADE-5 2HL</cell><cell>-87.14</cell></row><row><cell>EoNADE-mask 1HL(128 Ords)</cell><cell>-87.71</cell><cell>EoNADE-5 1HL(128 Ords)</cell><cell>-86.23</cell></row><row><cell>EoNADE-mask 2HL(128 Ords)</cell><cell>-85.10</cell><cell>EoNADE-5 2HL(128 Ords)</cell><cell>-84.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>To measure the variability over orderings, we computed the variance of log p(x | o) for 128 randomly chosen orderings o with the trained NADE-k's and NADE-mask with a single hidden layer. For comparison, we computed the variance of log p(x | o) over the 10,000 test samples.</figDesc><table><row><cell cols="2">E o,x [·] NADE-mask 1HL -92.17 log p(x | o)</cell><cell>E x Var o [·] 3.5</cell><cell>E o Var x [·] 23.5</cell></row><row><cell>NADE-5 1HL</cell><cell>-90.02</cell><cell>3.1</cell><cell>24.2</cell></row><row><cell>NADE-5 2HL</cell><cell>-87.14</cell><cell>2.4</cell><cell>22.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>The variance of log p(x | o) over orderings o and over test samples x.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Average log-probabilities of test samples of Caltech-101 Silhouettes. ( ) The results are from<ref type="bibr" target="#b5">Cho et al. (2013)</ref>. The terms in the parenthesis indicate the number of hidden units, the total number of parameters (M for million), and the L2 regularization coefficient. NADE-mask 670h achieves the best performance without any regularizations.</figDesc><table><row><cell>Model</cell><cell cols="2">Test LL Model</cell><cell>Test LL</cell></row><row><cell>RBM</cell><cell>-108.98</cell><cell>RBM</cell><cell>-107.78</cell></row><row><cell>(2000h, 1.57M)</cell><cell></cell><cell>(4000h, 3.14M)</cell><cell></cell></row><row><cell>NADE-mask</cell><cell>-112.51</cell><cell>NADE-mask</cell><cell>-110.95</cell></row><row><cell>(670h, 1.58M)</cell><cell></cell><cell>(2670h, 6.28M, L2=0.00106)</cell><cell></cell></row><row><cell>NADE-2</cell><cell>-108.81</cell><cell>NADE-5</cell><cell>-107.28</cell></row><row><cell>(1000h, 1.57M, L2=0.0054)</cell><cell></cell><cell>(4000h, 6.28M, L2=0.0068)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We make a typical assumption that observations are mutually independent given the latent variables.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">git@github.com:yaoli/nade k.git</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In the future, one could explore possibilities for helping better converge beyond step k, for instance by using costs based on reconstructions at k − 1 and k even in the fine-tuning phase.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to acknowledge the support of NSERC, Calcul Québec, Compute Canada, the Canada Research Chair and CIFAR, and developers of Theano <ref type="bibr" target="#b3">(Bergstra et al., 2010;</ref><ref type="bibr" target="#b0">Bastien et al., 2012)</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling high-dimensional discrete data with multi-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;99</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="400" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Better mixing via deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML&apos;13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML&apos;13)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Oral Presentation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training energy-based models for time-series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stroobandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2771" to="2797" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced gradient for training restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="805" to="831" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parameter learning with truncated message-passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2937" to="2943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-prediction deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dependency networks for inference, collaborative filtering, and data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rounthwaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="75" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>GCNU TR 2000-004</idno>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Gatsby Unit, University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized pseudo-likelihood estimates for Markov random fields on lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the Institute of Statistical Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive principles for restricted Boltzmann machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS&apos;10)</title>
		<meeting>The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting natural gradient for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>Conference Track</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A mean field theory learning algorithm for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="995" to="1019" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ropson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="725" to="733" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML&apos;14)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Res</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the equivalence between deep nade and generative stochastic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning (ECML/PKDD&apos;14)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno>arXiv 1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
