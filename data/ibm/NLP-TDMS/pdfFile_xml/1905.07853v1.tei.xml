<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Video Representations from Correspondence Proposals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Video Representations from Correspondence Proposals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Correspondences between frames encode rich information about dynamic content in videos. However, it is challenging to effectively capture and learn those due to their irregular structure and complex dynamics. In this paper, we propose a novel neural network that learns video representations by aggregating information from potential correspondences. This network, named CPNet, can learn evolving 2D fields with temporal consistency. In particular, it can effectively learn representations for videos by mixing appearance and long-range motion with an RGB-only input. We provide extensive ablation experiments to validate our model. CPNet shows stronger performance than existing methods on Kinetics and achieves the state-of-the-art performance on Something-Something and Jester. We provide analysis towards the behavior of our model and show its robustness to errors in proposals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video modality can be viewed as a sequence of images evolving over time. A good model for learning video representations should be able to learn from both the static appearance of images as well as the dynamic change of images over time. The dynamic nature of video is described by temporal consistency, which says an object in one frame usually has its correspondence in other frames and its semantic features are carried along the way. Analysis of these correspondences, either fine-grained or coarse-grained, can lead to valuable information for video recognition, such as how objects move or how viewpoints changes, which can further benefit high-level understanding tasks such as action recognition and action prediction.</p><p>Unlike static images where there is a standard representation learning approach of convolutional neural networks (CNNs), the correspondence of objects in videos has entirely different pattern and is more challenging to learn. For example, the corresponding objects can be arbitrarily far away, may deform or change their pose, or may not even exist in other frames. Previous methods rely on op- * Majority of the work done as an intern at Adobe Research. semantic feature space -NN Conv <ref type="figure">Figure 1</ref>: We view video representation tensor as a point cloud of features with T × H × W points. For each point (e.g. the purple point), its k potentially corresponding points are the k-NN in Cdimensional semantic space from other frames. Our CP module will learn and aggregate all these potential correspondences.</p><p>erations within a local neighborhood (e.g. convolution) or global feature re-weighting (e.g. non-local means) for interframe relation reasoning thus cannot effectively capture correspondence: stacking local operations for wider coverage is inefficient or insufficient for long-range correspondences while global feature re-weighting fails to include positional information which is crucial for correspondence.</p><p>In this paper, we present a novel method of learning representations for videos from correspondence proposals. Our intuition is that, the corresponding objects of a given object in other frames typically only occupy a limited set of regions, thus we need to focus on those regions during learning. In practice, for each position (a pixel or a feature), we only consider the few other positions that is most likely to be the correspondence.</p><p>The key of our approach is a novel neural network module for video recognition named CP module. This module views the video representation tensor as a point cloud in semantic space. As illustrated in <ref type="figure">Figure 1</ref>, for every feature in video representation, the module finds and groups its k nearest neighbors in other frames in the semantic space as potential correspondence. Each of the k feature pairs is processed identically and independently by a neural network. Then max pooling is applied to select the strongest response. The module effectively learns a set of functions that embeds and selects the most interesting information among the k pairs and encode the reason for such selection. The output of CP module is the encoded representation of correspondence, i.e. dynamic component in videos, and can be used in subsequent parts of an end-to-end architecture and other applications. Ordered spatiotemporal location information is included in the CP module so that motion can be modelled. We integrate the proposed CP module into CNN so that both static appearance feature and dynamic motion feature of videos are mixed and learned jointly. We name the resulting deep neural network CPNet. We constructed a toy dataset and showed that CPNet is the only RGB-only video recognition architecture that can effectively learn long-range motion. On real datasets, we show the robustness of the max pooling in CP module: it can filter out clearly wrong correspondence proposals and only select embeddings from reasonable proposals.</p><p>We showcase CPNet in the application of video classification. We experimented with it on action recognition dataset Kinetics <ref type="bibr" target="#b17">[18]</ref> and compared it against existing methods. It beats previous methods and achieves leading performance. It also achieves state-of-the-art results among published methods on action-centric datasets Something-Something <ref type="bibr" target="#b11">[12]</ref> and Jester <ref type="bibr" target="#b29">[30]</ref> with fewer parameters. We expect that our CPNet and the ideas behind it can benefit video applications and research in related domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Representation Learning for Videos. Existing approaches of video representation learning can generally be categorized by how dynamic component is modelled. The first family of approaches extract a global feature vector for each frame with a shared CNN and use recurrent neural nets to model temporal relation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref>. Though recurrent architectures can efficiently capture temporal relations, it is harder to train and results in low performance on the latest benchmarks. The second family of approaches learn dynamic changes from offline-estimated optical flow <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref> or online learned optical flow <ref type="bibr" target="#b7">[8]</ref> with a separate branch of the network. The optical flow branch may share the same architecture as the static appearance branch. Though optical flow field bridges consecutive frames, the question of how to learn multiple evolving 2D fields is still not answered.</p><p>The third family of approaches use single-stream 3D CNN with RGB-only inputs and learn dynamic changes jointly and implicitly with static appearance <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>. These architectures are usually built with local operations such as convolution so cannot learn long-range dependencies. To address this problem, non-local neural nets (NL Nets) <ref type="bibr" target="#b32">[33]</ref> was proposed. It adopted non-local operations where features are globally re-weighted by their pairwise feature distance. Our network consumes RGB-only inputs and explicitly computes correspondence proposals in a non-local fashion. Different from NL Net, our architecture focuses on only top correspondences and considers pairwise positional information, thus it can effectively learn not only appearance but also dynamic motion features.</p><p>Deep Learning on Unstructured Point Data. The pioneering work of PointNet <ref type="bibr" target="#b22">[23]</ref> proposed a class of deep learning methods on unordered point sets. The core idea is a symmetric function constructed with shared-weight deep neural networks followed by an element-wise max pooling. Due to the symmetry of pooling, it is invariant to the order of input points. This idea can also be applied to learning functions on generic orderless sets <ref type="bibr" target="#b36">[37]</ref>. Follow-up work of PointNet++ <ref type="bibr" target="#b23">[24]</ref> extracts local features in local point sets within a neighborhood in the Euclidean space and hierarchically aggregates features. Dynamic graph CNN <ref type="bibr" target="#b33">[34]</ref> proposed similar idea, the difference is that the neighborhood is determined in the semantic space and the neural network processes point pairs instead of individual points. Inspired by these works, we treat correspondence candidates as an unordered set. Through a shared-weight MLP and max pooling, our network will learn informative representations about appearance and motion in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning for Correspondence and Relation</head><p>Reasoning. Capturing relation is an essential task in computer vision and machine learning. A common approach to learn relation is letting extracted feature interact through a designed or learned function and discover similarity from the output. This is the general idea behind previous works on stereo matching <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref> and flow estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref>. The learned relation can also be used later in learning highlevel semantic information such as video relational reasoning <ref type="bibr" target="#b38">[39]</ref> and visual question answering <ref type="bibr" target="#b25">[26]</ref>. Compared to these works, we focus on learning video representation from long-range feature correspondences over time and space. <ref type="figure">Figure 3</ref>: Correspondence Embedding layer architecture. f i j s are semantic vectors with length C and the ij-th row of input T HW × C feature tensor. g i 0 is a semantic vector with length C and the i0-th row in the output T HW × C feature tensor. We made C = C so that the output can be added back to the main stream CNN. t i j , h i j , w i j are the spatiotemporal normalized locations.</p><formula xml:id="formula_0">... 0 1 ( 1 -0 , h 1 -h 0 , 1 -0 ) 0 2 ( 2 -0 , h 2 -h 0 , 2 -0 ) 0 ( -0 , h -h 0 , -0 ) MAX MLP MLP MLP shared ( 2 , h 2 , 2 , 2 ) ... 0 ₀ ′ ... ₁ ₀ (index) indexing ( 1 , h 1 , 1 , 1 ) ( , h , , ) ( , h , , ) ( 0 , h 0 , 0 , 0 ) ₂ ₀</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Correspondence Proposals</head><p>Our proposed method is inspired by the following three properties of correspondences in videos:</p><p>1. Corresponding positions have similar visual or semantic features. This is the assumption underlying many computer vision tasks related to correspondence, such as image matching, relation reasoning or flow estimation.</p><p>2. Corresponding positions can span arbitrarily long ranges, spatially or temporally. In the case of fast motion or low frame rate, displacements along spatial dimensions can be large within small frame steps. Objects that disappear and then re-appear in videos across a long time can span arbitrarily long temporal range.</p><p>3. Potential correspondence positions in other frames are small in percentage. Given a pixel/feature, usually only very small portion of pixels/features in other frames could be the potential correspondence. Other apparently dissimilar pixels/features can be safely ignored.</p><p>A good video representation model should at least address the above three properties: it should be able to capture potential pixel/feature correspondence pairs at arbitrary locations and learn from the pairs. It poses huge challenges to the design of the deep architecture, since most deep learning methods work on regular structured data. Inspired by recent work on deep learning on point clouds <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref> and their motion <ref type="bibr" target="#b20">[21]</ref>, we develop an architecture that addresses the above three properties.</p><p>In this section, we first briefly review point cloud deep learning techniques and their theoretical foundations. Then we explain Correspondence Proposal (CP) module, the core of our architecture. Finally we describe how it is integrated into the entire deep neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of Point Cloud Deep Learning</head><p>Qi et al. <ref type="bibr" target="#b22">[23]</ref> recently proposed PointNet, a neural network architecture for deep learning in point clouds. Its theoretical foundation was proven in <ref type="bibr" target="#b22">[23]</ref>: given a set of point clouds X ⊆ {{x 1 , x 2 , . . . , x n } | n ∈ Z + , x i ∈ [0, 1] d } and any continuous set function f : X → R c w.r.t Hausdorff distance, symmetric function g :</p><formula xml:id="formula_1">X → R c g(x 1 , x 2 , . . . , x n ) = γ • M AX{ζ(x 1 ), ζ(x 2 ), . . . , ζ(x n )}</formula><p>can arbitrarily closely approximate f on X , where ζ : R d → R r and γ : R r → R c are two continuous functions and M AX is the element-wise maximum operation. In practice, ζ and γ are instantiated to be multi-layer perceptron (MLP) as learnable functions with universal approximation potential. The symmetry of max pooling ensures the output to be invariant to the ordering of the points.</p><p>While PointNet was originally proposed to learn geometric representation for 3D point clouds, it has been shown that the MLP can take mixed types of modalities as input to learn other tasks. For example, the MLP can take learned geometric representation and displacement in 3D Euclidean space as input to estimate scene flow <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CP Module</head><p>In this subsection, we explain the architectures of CP module. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the input and output to CP module are both video representation tensors with shape T HW ×C, where T denotes the number of frames, H ×W denotes the spatial dimension and C denotes the number of channels. CP module treats the input video tensor as a point cloud of features with T HW points and accomplishes two tasks: 1) k-NN grouping; 2) correspondence embedding.</p><p>k-NN grouping. For each feature in the video representation tensor output by a CNN, CP module selects its k most likely corresponding features in other frames. The selection is solely based on semantic similarity to ensure correspondence can be across arbitrarily long spatiotemporal ranges. Features within the same frame are excluded because temporal consistency should be between different frames.</p><p>The first step is to calculate the semantic similarity of all features point pairs. We use the negative L 2 distance as our similarity metric. It can be done efficiently with matrix multiply operations and produces a matrix of shape T HW × T HW . The next step is to set the values of the elements in the T diagonal block matrices of shape HW × HW to be −∞. With this operation, the features within the same frame will be excluded from potential correspondences of each other. The final step is to apply an arg top-k operation along the row dimension of the matrix. It outputs a tensor of shape T HW × k, where the i-th row are the indices of the k nearest neighbors of the feature i. The workflow is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Correspondence Embedding layer. The goal of this layer is for each feature, learn a representation from its proposed correspondences. The features' motion to their corresponding position in other frames can be learned during this process. The top-1 correspondence candidate can only give the information from one frame so it cannot capture the full correspondence information of the entire video. Besides, there may be more than one qualified correspondence candidates in a frame. So we use a larger k, process k pairs identically and independently, aggregate information from k outputs. This is the general idea behind Correspondence Embedding layer, the core of our CP module.</p><p>Correspondence Embedding layer is located in the dashed box of <ref type="figure" target="#fig_0">Figure 2</ref> and illustrated in detail in <ref type="figure">Figure  3</ref>. Suppose the spatiotemporal location and semantic vec-</p><formula xml:id="formula_2">tor of input feature i 0 is (t i0 , h i0 , w i0 , f i0 ), its j-th k-NN is (t ij , h ij , w ij , f ij ) where j ∈ {1, 2, . . . , k}.</formula><p>For each of the k pairs, we pass the semantic vectors of two features, i.e. f i0 , f ij ∈ R C , and their relative spatiotemporal displace-</p><formula xml:id="formula_3">ments, i.e. [t ij − t i0 , h ij − h i0 , w ij − w i0 ] ∈ R 3 , to an MLP with shared weights. All three dimensions of the spatiotem- poral locations, i.e. t ij , h ij , w ij ∈ R, are normalized to [0, 1] from [0, T ), [0, H) and [0, W ) before sent into MLP.</formula><p>Then the k outputs are aggregated by an element-wise max pooling operation to produce g i0 ∈ R C , the semantic vector of output feature i 0 . During the process, the most informative signals about correspondence, i.e. entangled representation from mixing displacement and two semantic vectors, can be selected from k pairs and the output will implicitly encode motion information. Mathematically, the operation of Correspondence Embedding layer can be written as:</p><formula xml:id="formula_4">g i0 = M AX j∈{1,2,...,k} {ζ(f i0 , f ij , t ij −t i0 , h ij −h i0 , w ij −w i0 )} (1)</formula><p>where ζ is the function computed by MLP and M AX is element-wise max pooling.</p><p>There are other design choices for Correspondence Embedding layer as well. For example, instead of sending both features directly to the MLP, one can first compute a certain distance between two features. However, as discussed in <ref type="bibr" target="#b20">[21]</ref>, sending both features into MLP is a more general form and yields better performance in motion learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overall Network Architecture</head><p>Our CP module are inserted into CNN architecture and are interleaved with convolution layers, which enables the static image features extracted from convolution layers and correspondence signals extracted from CP module be mixed and learned jointly. Specifically, the CP modules are inserted into the ResNet <ref type="bibr" target="#b13">[14]</ref> architectures and is located right after a residual block but before ReLU. We initialize the convolution part of our architecture with a pre-trained Im-ageNet model. The MLPs in CP modules are randomly ARTNet <ref type="bibr" target="#b31">[32]</ref> TRN <ref type="bibr" target="#b38">[39]</ref> C2D CPNet (ours)  initialized with MSRA initialization <ref type="bibr" target="#b12">[13]</ref>, except for the gamma parameter of the last batch normalization layer <ref type="bibr" target="#b15">[16]</ref> being initialized with all zeros. This ensures identity mapping at the start of training so pre-trained model can be used.</p><formula xml:id="formula_5">conv1 3 × 3 × 1,16 3 × 3 × 3,16 3 × 3 × 1,16 3 × 3 × 1,16 NL block - - CP module conv2 1 × 1 × 3,16 3 × 3 × 1,16 SMART- 3 × 3 × 3,16 3 × 3 × 1,16 3 × 3 × 1,</formula><p>In this paper, we only explore CP modules with k nearest neighbors in other frames in L 2 semantic space. In general, however, the nearest neighbors of CP modules can be determined in other metric space as well, such as temporalonly space, spatiotemporal space or joint spatiotemporalsemantic space. We call such convolutional architecture inserted with generic CP module as CPNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A Failing of Several Previous Methods</head><p>We constructed a toy video dataset where previous RGBonly methods fail in learning long-range motion. Through this extremely simple dataset, we show the drawbacks of previous methods and the advantage of our architecture.</p><p>The dataset consists of videos of a 2 × 2 white square moving on a black canvas. The videos have 4 frames and the spatial size is 32 × 32. There are four labels of the moving direction of the square: "left", "right", "up" and "down". The square's moving distance per step is random between 7 and 9 pixels. The dataset has 1000 training and 200 validation videos, both have an equal number of videos for each label. <ref type="figure" target="#fig_1">Figure 4</ref> illustrated an example of videos in our dataset.</p><p>We inserted the core module of several previous RGBonly deep architectures for video recognition, i.e. I3D NL Net <ref type="bibr" target="#b32">[33]</ref>, ARTNet <ref type="bibr" target="#b31">[32]</ref>, TRN <ref type="bibr" target="#b38">[39]</ref>, as well as our CPNet, into a toy CNN with two 3 × 3 convolution layers. We listed the architectures used this experiment in <ref type="table" target="#tab_0">Table 1</ref>. The convolution parts of all models have small spatial receptive fields of 5 × 5. The dataset-model settings are designed to simulate long-range motion situations where stacking convolution layers to increase receptive field is insufficient or inefficient. No data augmentation is used. </p><formula xml:id="formula_6">res 2 56 × 56 × 8 3 × 3, 64 3 × 3, 64 × 2 3 × 3, 64 3 × 3, 64 × 2 res 3 28 × 28 × 8 3 × 3, 128 3 × 3, 128 × 2   3 × 3, 128 3 × 3, 128 CP module   × 2 res 4 14 × 14 × 8 3 × 3, 256 3 × 3, 256 × 2   3 × 3, 256 3 × 3, 256 CP module   × 2 res 5 7 × 7 × 8 3 × 3, 512 3 × 3, 512 × 2   3 × 3, 512 3 × 3, 512 CP module   × 2 1 × 1 × 1 global average pooling, fc 400</formula><p>The training and validation results are listed in <ref type="table" target="#tab_0">Table 1</ref>. Our model can overfit the toy dataset, while other models simply generate random guesses and fail in learning the motion. It's easy to understand that ARTNet and TRN have insufficient convolution receptive fields to cover the step of the motion of the square. However, it's intriguing that NL Net, which should have a global receptive field, also fails.</p><p>We provide an explanation as follows. Though the toy NL Net gets by the problem of insufficient convolution receptive fields, its NL block fails to include positional information thus can't learn long-range motion. However, it's not straightforward to directly add pairwise positional information to NL block without significantly increasing the memory and computation workload to an intractable amount. Through this experiment, we show another advantage of our CPNet: by only focusing on top k potential correspondences, memory and computation can be saved significantly thus allow positional information and semantic feature be learned together with more a complicated method such as a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment Results</head><p>To validate the choice of our architecture for data in the wild, we first did a sequence of ablation studies on Kinetics dataset <ref type="bibr" target="#b17">[18]</ref>. Then we re-implemented several recently published and relevant architectures with the same dataset and experiment settings to produce results as good as we can and compare with our results. Next, we experiment with very large models and compare with the state-of-theart methods on Kinetics validation set. Finally, we did experiments on action-centric datasets Something-something v2 <ref type="bibr" target="#b11">[12]</ref> and Jester v1 <ref type="bibr" target="#b29">[30]</ref> and report our results on both validation and testing sets. Visualizations are also provided to help the understanding of our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Studies</head><p>Kinetics <ref type="bibr" target="#b17">[18]</ref> is one of the largest well-labelled datasets for human action recognition from videos in the wild. Its classification task involves 400 action classes. It contains around 246,000 training videos and 20,000 validation videos. We used C2D ResNet-18 as backbone for all ablation experiments. The architectures we used are derived from the last column of <ref type="table" target="#tab_2">Table 2</ref>. We included C2D baseline for comparison. We downsampled the video frames to be only 1/12 of the original frame rate and used only 8 frames for each clip. This ensures that the clip duration are long enough to cover a complete action while still maintain fast iteration of experiment. The single-clip single-center-crop validation results are shown in <ref type="table" target="#tab_3">Table 3</ref></p><formula xml:id="formula_7">(a)(b)(c).</formula><p>Ablation on the Number of CP modules. We explored the effect of the number of CP modules on the accuracy. We experimented with adding one or two CP modules to the res 4 group, two CP modules to each of res 3 and res 4 groups, and two CP modules to each of res 3 , res 4 and res 5 groups. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>(a). As the number of CP modules increases, the accuracy gain is consistent.</p><p>Ablation on k. We explored the the combination of training-testing time k values and compared the results in <ref type="table" target="#tab_3">Table 3</ref>(b). When ks are the same during training and testing, highest validation accuracy are achieved. It suggests that using different k forces the architecture to learn different distribution and highest accuracy are achieved only when training and test distribution are similar.</p><p>We also notice that the highest accuracy are achieved at a sweet point when both k = 8. An explanation is that when k is too small, CP module can't get enough correspondence candidates to select from; when k is too large, clearly unrelated elements are also included and introduce noise.</p><p>Ablation on the position of CP modules. We explored effect of the position of CP modules. We added two CP modules to three different groups: res 3 , res 4 and res 5 , respectively. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>(c). The highest accuracy are achieved when adding two CP modules to res 4 group. A possible explanation is that res 3 doesn't contain enough semantic information for finding correct k-NN while resolution of res 5 is too low (7 × 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Other Architectures</head><p>We compare our architecture with C2D/C3D baselines, C2D NL Networks <ref type="bibr" target="#b32">[33]</ref> and ARTNet <ref type="bibr" target="#b31">[32]</ref>, on Kinetics. We did two sets of experiments, with frame rate downsampling ratio of 12 and 4 respectively. Both experiment sets used 8 frames per clip. The settings enable us to compare the performance under both low and high frame rates. The architecture used in the experiments are illustrated in <ref type="table" target="#tab_2">Table 2</ref>. We experimented with two inference methods: 25-clip 10crop with averaged softmax score as in <ref type="bibr" target="#b31">[32]</ref> and single-clip single-center-crop. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>(d).</p><p>Our architecture outperforms C2D/C3D baselines by a significant margin, which proves the efficacy of CP module. It also outperforms NL Net and ARTNet given fewer  parameters, further showing the superiority of our CPNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Large Models on Kinetics</head><p>We train a large model with C2D ResNet-101 as backbone. We applied three phases of training where we progressively increase the number of frames in a clip from 8 to 16 and then to 32. We freeze batch normalization layers starting the second phase. During inference, we use 10-clip in time dimension, 3-crop spatially fully-convolutional inference. The results are illustrated in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Compared with large models of several previous RGBonly architectures, our CPNet achieves higher accuracy with fewer parameters. We point out that Kinetics is an appearance-centric dataset where static appearance information dominates the classification. We will show later that our CPNet has larger advantage on other action-centric datasets where dynamic component more important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results on Something-Something</head><p>Something-Something <ref type="bibr" target="#b11">[12]</ref> is a recently released dataset for recognizing human-object interaction from video. It has 220,847 videos in 174 categories. This challenging dataset is action-centric and especially suitable for evaluating recognition of motion components in videos. For example its categories are in the form of "Pushing some-thing from left to right". Thus solely recognizing the object doesn't guarantee correct classification in this dataset.</p><p>We trained two different CPNet models with ResNet-18 and -34 C2D as backbone respectively. We applied two phases of training where we increase the number of frames in a clip from 12 to 24. We freeze batch normalization layers in the second phase. The clip length are kept to be 2s 1 . During inference, we use 6-crop spatially fullyconvolutional inference. We sample 16 clips evenly in temporal dimension from a full-length video and compute the averaged softmax scores over 6 × 16 clips. The results are listed in <ref type="table" target="#tab_5">Table 5</ref>(a).</p><p>Our CPNet model with ResNet-34 backbone achieves the state-of-the-art results on both validation and testing accuracy. Our model size is less than half but beat Two-stream TRN <ref type="bibr" target="#b38">[39]</ref> by more than 2% in validation accuracy and more than 1% testing accuracy. Our CPNet model with ResNet-18 also achieves competing results. With fewer than half parameters, it beats MultiScale TRN <ref type="bibr" target="#b38">[39]</ref> by more than 5% in validation and more than 2% in testing accuracy. Besides, we also showed the effect of CP modules by comparing against respective ResNet C2D baselines. Although parameter size increase due to CP module is tiny, the validation accuracy gain is significant (&gt;14%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Results on Jester</head><p>Jester <ref type="bibr" target="#b29">[30]</ref> is a dataset for recognizing hand gestures from video. It has 148,092 videos in 27 categories. This dataset is also action-centric and especially suitable for evaluating recognizing motion components in video recognition models. One example of its categories is "Turning Hand Clockwise": solely recognizing the static gesture doesn't guarantee correct classification in this dataset. We used the same CPNet with ResNet-34 C2D backbone and the same training strategy as subsection 5.4. During inference, we use 6-crop spatially fully-convolutional inference. We sample 8 clips evenly in temporal dimension from a fulllength video and compute the averaged softmax scores over 6 × 8 clips. The results are listed in <ref type="table" target="#tab_5">Table 5</ref>(b).</p><p>Our CPNet model outperforms all published results on both validation and testing accuracy, while having the smallest parameter size. The effect of CP modules is also shown by comparing against ResNet-34 C2D baselines. Again, although parameter size increase due to CP module is tiny, the validation accuracy gain is significant (≈12%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Visualization</head><p>To understand the behavior of CP module and demystify why it works, we provide visualization in three aspects with the datasets used in previous experiments as follows.</p><p>What correspondences are proposed? We are interested to see whether CP module is able to learn to propose reasonable correspondences purely based on semantic feature similarity. As illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>, in general CP module can find majority of reasonable correspondences. Due to k being a fixed hyperparameter, its k-NN in semantic space may also include wrong correspondences.</p><p>Which of proposed correspondences activate output neurons? We are curious about CP module's robustness to wrong proposals. We trace which of the k proposed correspondence pairs affect the value of output neurons after max pooling. Mathematically, let g i0 c and ζ <ref type="formula">(1)</ref> respectively, we are interested in the set</p><formula xml:id="formula_8">(i0,ij ) c be the dimen- sion c of g i0 and ζ(f i0 , f ij , t ij − t i0 , h ij − h i0 , w ij − w i0 ) from Equation</formula><formula xml:id="formula_9">A i0 = {j ∈ {1, . . . , k} | ∃c ∈ {1, . . . , C}, ζ (i0,ij ) c = g i0 c }</formula><p>(2) associated with a feature i 0 , where j not being in A i0 means pair (i 0 , i j ) is entirely overwhelmed by other proposed correspondence pairs and thus filtered by max pooling when calculating output feature i 0 . We illustrate A i0 of several selected features in <ref type="figure" target="#fig_4">Figure 5</ref> and show that CP module is robust to incorrectly proposed correspondences. We found that CP modules make more changes to features that correspond to moving pixels. Besides, CP modules on a later stage focus more on the moving parts with specific semantic information that helps final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Relation to Other Single-stream Architectures</head><p>Note that since the MLPs in CP modules can potentially learn to approximate any continuous set functions, CPNet can be seen as a generalization of several previous RGBonly architectures for video recognition.</p><p>CPNet can be reduced to a C3D <ref type="bibr" target="#b27">[28]</ref> with kernel size u × v × w, if we set the k of CP modules to be uvw − 1, determine the k nearest neighbors in spatiotemporal space with L 1 distance and let the MLP learn to compute inner product operation within the u × v × w neighborhood.</p><p>CPNet can also be reduced to an NL Net <ref type="bibr" target="#b32">[33]</ref>, if we set the k of CP modules to be maximum T HW − 1 and let the MLP learn to perform the same distance and normalization functions as the NL block.</p><p>CPNet can also be reduced to a TRN <ref type="bibr" target="#b38">[39]</ref>, if we put one final CP module at the end of C2D, determine the k nearest neighbors in temporal-only space, and let the MLP learn to perform the same g θ and h φ functions defined in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Pixel-level Motion vs. Feature-level Motion</head><p>In two-stream architectures, motion in pixel level, i.e. optical flow fields, are first estimated before sent into deep networks. In contrast, CP modules captures motion in semantic feature level. We point out that, though CP module process positional information at a lower spatial resolution (e.g. 14 × 14), detailed motion feature can still be captured, since the semantic features already encode rich information within the receptive fields <ref type="bibr" target="#b21">[22]</ref>.</p><p>In fact, migrating positional reasoning from the original input data to semantic representation has contributed to several successes in computer vision research. For example, in  the realm of object detection, moving the input and/or output of ROI proposal from the original image to the pooled representation tensor is the core of progress from RCNN <ref type="bibr" target="#b9">[10]</ref> to Fast-RCNN <ref type="bibr" target="#b8">[9]</ref> and to Faster-RCNN <ref type="bibr" target="#b24">[25]</ref>; in the realm of flow estimation, successful architectures also calculate displacements within feature representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we presented a novel neural network architecture to learn representation for video. We propose a new CP model that computes k correspondence propos-als for each feature and feeds each of proposed pair to a shared neural network followed by max pooling to learn a new feature tensor. We show that the module can effectively capture motion correspondence information in videos. The proposed CP module can be integrated with most existing frame-based or clip-based video architectures. We show our proposed architecture achieves strong performance on standard video recognition benchmarks. In terms of future work, we plan to investigate this new architecture for problems beyond video classification.</p><p>In this document, we provide more details to the main paper and show extra results on per-class accuracy and visualizations.</p><p>In section B, we provide more details on the Kinetics/ResNet-18 ablation experiments (main paper section 5.1). In section C, we provide more details on the baseline architectures in Kinetics/ResNet-18 comparison experiments (main paper section 5.2). In section D, we provide details on the CPNet architecture used in Kinetics/ResNet-101 experiment (main paper section 5.3). In section E, we provide details on the architecture used in Something-Something and Jester experiments (main paper section 5.4 and 5.5). In section F we report the per-class accuracy of C2D model and our CPNet model on Something-Something and Jester datasets. Lastly in section G we provide time complexity of our model and in section H we provide more visualization results on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CPNet Architecture in Kinetics/ResNet-18 Experiments</head><p>Our CPNet is instantiated by adding a CP module after the last convolution layer of a residual group but before ReLU, as illustrated in <ref type="figure" target="#fig_5">Figure 6</ref>. For Kinetics/ResNet-18 experiments in main paper section 5.1 and 5.2, each CP module has MLP with two hidden layers. Suppose the number of channels of the input tensor of CP module is C. The number of channels of the hidden layers in the MLPs is then [C/4, C/2]. The number of nearest neighbors k is set to 8 for the results in <ref type="table" target="#tab_3">Table 3</ref>(a)(c)(d) of the main paper. k varies for the results in <ref type="table" target="#tab_3">Table 3</ref>(b). The location of CP module is deduced from the last column of <ref type="table" target="#tab_7">Table 6</ref> for different experiments in section 5.1 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Architectures in Kinetics/ResNet-18 Comparison Experiment</head><p>In <ref type="table" target="#tab_7">Table 6</ref>, we listed all the architectures used in Kinetics/ResNet-18 comparison experiments, as a supplementary to <ref type="table" target="#tab_2">Table 2</ref> of the main paper. C2D/C3D are vanilla 2D or 3D CNN. ARTNet is pulled directly from <ref type="bibr" target="#b31">[32]</ref>. It was designed to have the same number of parameters as its C3D counterpart. NL Net model is adapted from <ref type="bibr" target="#b32">[33]</ref>, by adding an NL block at the end of each residual group of C2D ResNet-18. CPNet is instantiated in the same way as illustrated in <ref type="figure" target="#fig_5">Figure 6</ref>. Combined with results in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CPNet Architecture in Kinetics/ResNet-101 Experiment</head><p>We listed CPNet architecture used in Kinetics/ResNet-101 experiment in <ref type="table" target="#tab_9">Table 7</ref>. Each residual group in ResNet-101 has three convolution layers. Our CPNet is instantiated by adding a CP module after the last convolution layer of a residual group but before ReLU, as illustrated in <ref type="figure" target="#fig_6">Figure 7</ref>. Suppose the number of channels of the input tensor of CP module is C. The number of channels of the hidden layers in the MLPs is then [C/16, C/8]. The number of nearest neighbors k is set to 4.</p><p>We used five CP modules in the architecture. Two CP modules are in res 3 groups with spatial resolution of 28×28 and the rest three are in res 4 groups with spatial resolution 14×14. Such mixed usage of CP modules at residual groups of different spatial resolutions enables correspondence and motion in different semantic level to be learned jointly. We only listed the case of using 8 frames as input. For 32frame input, all "8" in the second column of <ref type="table" target="#tab_9">Table 7</ref> should be replaced by 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Architecture used in Something-Something and Jester Experiments</head><p>We listed the CPNet architectures used in Something-Something <ref type="bibr" target="#b11">[12]</ref> and Jester <ref type="bibr" target="#b29">[30]</ref> experiments in <ref type="table" target="#tab_10">Table 8</ref>. CP-Net is instantiated in the same way as illustrated in <ref type="figure" target="#fig_5">Figure  6</ref>. Suppose the number of channels of the input tensor of CP module is C. The number of channels of the hidden layers in the MLPs is then [C/4, C/2]. The number of nearest neighbors k is set to 12.</p><p>We used five CP modules in the architecture. Two CP modules are in res 3 groups with spatial resolution of 28×28   and the rest three are in res 4 groups with spatial resolution 14×14. We only listed the case of using 12 frames as input. For 24-or 48-frame input, all "12" in the second column of <ref type="table" target="#tab_9">Table 7</ref> should be replaced by 24 or 48.</p><formula xml:id="formula_10">res3 28 × 28 × 8 3 × 3(×3), 128 3 × 3(×3), 128 × 2 3 × 3 × 3, 128 SMART 3 × 3 × 3, 128 × 2   3 × 3, 128 3 × 3, 128 NL block   × 2   3 × 3, 128 3 × 3, 128 CP module   × 2 res4 14 × 14 × 8 3 × 3(×3), 256 3 × 3(×3), 256 × 2 3 × 3 × 3, 256 SMART 3 × 3 × 3, 256 × 2   3 × 3, 256 3 × 3, 256 NL block   × 2   3 × 3, 256 3 × 3, 256 CP module   × 2 res5 7 × 7 × 8 3 × 3(×3), 512 3 × 3(×3), 512 × 2 3 × 3 × 3, 512 3 × 3 × 3, 512 × 2   3 × 3, 512 3 × 3, 512 NL block   × 2   3 × 3,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Per-class accuracy of Something-Something and Jester models</head><p>To understand the effect of CP module to the final performance, we provide the CPNet's per-class top-1 accuracy gain compared with the respective C2D baseline on Jester in <ref type="figure" target="#fig_7">Figure 8</ref> and Something-Something in <ref type="figure" target="#fig_9">Figure 10</ref>.</p><p>We can see that categories that strongly rely on motion (especially in long-range) in videos typically have large accuracy improvement after adding CP module. On the other hand, categories that doesn't require reasoning motion to classify have little or negative gain in accuracy. The results coincide with our intuition that CP module effectively captures dynamic content of videos.</p><p>On Jester dataset <ref type="bibr" target="#b29">[30]</ref>, the largest accuracy improvements are achieved in categories that involve long-range spatial motion such as "Sliding Two Fingers Up", or longrange temporal relation such as "Stop Sign". At the same time, categories that don't even need multiple frames to  classify, such as "Thump Up" or "Thumb Down", have the smallest accuracy gain. On Something-Something dataset <ref type="bibr" target="#b11">[12]</ref>, the largest accuracy improvements are achieved in categories that involve long-range spatial motion such as "Moving away from something with your camera", or long-range temporal relation such as "Lifting up one end of something without letting it drop down". At the same time, categories that don't even need multiple frames to classify, such as "Showing a photo of something to the camera", have the smallest or negative accuracy gain .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Model Run Time</head><p>In this section, we provide time complexity results of our model. Our CP module can be very efficient in term of computation and memory, for both training and inference.</p><p>During training, NL Net <ref type="bibr" target="#b32">[33]</ref> computes a T HW ×T HW matrix followed by a row-wise softmax. The whole process is differentiable and all the intermediate values have to be stored for computing gradients during back propagation, which causes huge overhead in memory and computation. Unlike NL Net, our CP module's computation of a T HW × T HW matrix results in k integers used for indexing, which is non-differentiable. Thus CPNet doesn't compute gradients or store the intermediate values of the T HW × T HW matrix, a huge saving compared to NL Net and all other works involving global attention.</p><p>During inference, our CPNet is also efficient. We evaluate the inference time complexity of the CPNet model used in Jester v1 experiment. The spatial size is 112 × 112. and cuDNN. The model performances with various batch sizes and frame lengths are illustrated in <ref type="figure" target="#fig_8">Figure 9</ref>. With batch size of 1, CPNet can reach processsing speed of 10.1 videos/s for frame length of 8 and 3.9 videos/s for frame length of 32. The number of videos that can be processed in a given time also increases as batch size increases. We point out that there exist other more efficient implementations of CP module. In the main paper, we only presented the approach of the finding per-point k-NN in a point cloud via computing a pairwise feature distance matrix of size T HW × T HW followed by a row-wise arg top k, which has time complexity of O((T HW ) 2 · (C + k)). This is the most convenient way to implement in deep learning frameworks such as Tensorflow. However, when deployed on inference platforms, per-point k-NN can be computed by much more efficient approaches with geometric data structures such as k-d tree <ref type="bibr" target="#b0">[1]</ref> or Bounding Volume Hierarchy (BVH) <ref type="bibr" target="#b4">[5]</ref> in C dimensional space. The time complexity will then be O((T HW ) log(T HW ) · (C + k)), which includes both the construction and traversal of such tree data structures. Accelerating k-d tree or BVH on various platforms is an ongoing research problem in computer systems &amp; architectures community and is not the focus of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. More Visualizations</head><p>In this section, we provide more visualizations on examples from Kinetics <ref type="bibr" target="#b17">[18]</ref> in <ref type="figure" target="#fig_10">Figure 11</ref>, Something-Something <ref type="bibr" target="#b11">[12]</ref> in <ref type="figure" target="#fig_0">Figure 12</ref> and Jester <ref type="bibr" target="#b29">[30]</ref> in <ref type="figure" target="#fig_13">Figure 13</ref>. They further show CP module's ability to propose reasonable correspondences and robustness to errors in correspondence proposal.</p><p>Despite what has been shown in the main paper, we also notice some negative examples. For example, in <ref type="figure" target="#fig_10">Figure  11</ref>(a), when proposing correspondences of the boy's left ice skate, CP module incorrectly proposed the a girl's left ice skate due to the two ice skates' visual features being too similar. CP module also didn't completely overwhelm this wrong proposal after max pooling. However, we notice that this wrong proposal is weak in the output signal: it only activates 3 out of 64 channels during max pooling which is acceptable. We point out that such "error" could also be fixed in later stages of the network or even be beneficial for applications that require reasoning relations between similar but different objects.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>CP module architecture. Gray boxes denote tensors, white boxes denote operators and orange boxes denote neural networks with trainable weights. The dashed box represents the Correspondence Embedding layer, whose architecture is illustrated in detail inFigure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>An "up" example in our toy dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>How semantic feature map changes? We show in Figure 5 the heatmap of change in L 1 distance of the semantic feature map for each frame after going through CP module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>A video clip with label "playing basketball" from Kinetics validation set. A video clip with label "Rolling something on a flat surface" from Something-Something v2 validation set. A video clip with label "Thumb Up" from Jester v1 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization on our final models. The starting points of arrows are located at feature i0. Arrows point to the k proposed correspondences (k = 8) of feature i0. Proposed correspondences whose indices are in A i 0 (defined in Equation(2)) are pointed by red arrows otherwise by blue arrows. Feature changes after going through CP module are shown in heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>CP module inserted into a residual group of ResNet-18 backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>CP module inserted into a residual group of ResNet-101 backbone.of the main paper, our CPNet outperforms NL Net and ART-Net in terms of validation accuracy with fewer parameters, showing its superiority.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Per-class top-1 accuracy gain in percentage on Jester v1 dataset due to CP module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>The model backbone is ResNet-34. The computing platform is an NVIDIA GTX 1080 Ti GPU with Tensorflow Model run time (solid line) and number of video sequences per second (dashed line) of CPNet with ResNet-34 backbone and spatial size 112 × 112.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Per-class top-1 accuracy gain in percentage on Something-Something v2 dataset due to CP module. A video clip with label "ice skating" from Kinetics validation set. A video clip with label "riding a bike" from Kinetics validation set. A video clip with label "driving tractor" from Kinetics validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Additional Visualization on our final models on Kinetics dataset. Approach is the same as the main paper. A video clip with label "Turning something upside down" from Something-Something v2 validation set. A video clip with label "Picking something up" from Something-Something v2 validation set. A video clip with label "Moving something down" from Something-Something v2 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>A video clip with label "Dropping something next to something" from Something-Something v2 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Additional Visualization on our final models on Something-Something v2 dataset. Approach is the same as the main paper. A video clip with label "Drumming Fingers" from Jester v1 validation set. A video clip with label "Shaking Hand" from Jester v1 validation set. A video clip with label "Stop Sign" from Jester v1 validation set. A video clip with label "Pushing Two Fingers Away" from Jester v1 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Additional Visualization on our final models on Jester v1 dataset. Approach is the same as the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Architectures for toy experiment</figDesc><table><row><cell>layer</cell><cell>I3D NL Net [33]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Architectures used in Kinetics experiments inTable 3(d).</figDesc><table><row><cell>layer</cell><cell>output size</cell><cell>C2D baseline</cell><cell>CPNet (Ours) 6 CP modules</cell></row><row><cell cols="2">conv 1 56 × 56 × 8</cell><cell>7 × 7, 64, stride 2, 2(, 1)</cell><cell>7 × 7, 64 stride 2, 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Kinetics datasets results for ablations and comparison with other prior works. The top-1/top-5 accuracies are shown. Ablation on CP module's k values used in training and testing time. 59.9/82.3 59.2/81.6 56.6/79.4 52.5/76.1 49.0/72.6 44.6/58.5 k = 2 59.1/81.8 60.2/82.5 59.6/81.8 56.9/80.1 53.0/77.1 48.9/73.5 k = 4 59.0/81.2 60.2/82.4 60.5/82.6 59.0/81.7 55.3/79.2 49.2/73.5 k = 8 53.4/76.3 56.8/79.5 59.6/81.9 60.7/82.8 59.7/82.1 57.0/80.3 k = 16 51.3/75.1 53.8/77.3 56.8/79.7 59.8/82.1 60.6/82.8 59.2/81.8 k = 32 52.6/76.6 53.8/77.7 55.5/79.1 58.2/80.8 60.0/82.2 60.4/82.4</figDesc><table><row><cell cols="3">(a) number of CP modules model top-1 top-5</cell><cell>(b) top-1/top-5</cell><cell></cell><cell></cell><cell></cell><cell>test</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C2D</cell><cell>56.9</cell><cell>79.5</cell><cell>accuracy</cell><cell>k = 1</cell><cell cols="2">k = 2</cell><cell>k = 4</cell><cell>k = 8</cell><cell cols="2">k = 16</cell><cell>k = 32</cell></row><row><cell>1 CP</cell><cell>60.3</cell><cell>82.4</cell><cell>k = 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 CPs</cell><cell>60.4</cell><cell>82.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 CPs 6 CPs</cell><cell>61.0 61.1</cell><cell>83.1 83.1</cell><cell>train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(c) CP module positions</cell><cell cols="9">(d) Kinetics validation accuracy of architectures in Table 2. Clip length is 8 frames.</cell></row><row><cell cols="3">model top-1 top-5</cell><cell>frame rate</cell><cell cols="4">1/12 of original frame rate</cell><cell cols="4">1/4 of original frame rate</cell></row><row><cell>C2D</cell><cell>56.9</cell><cell>79.5</cell><cell>val configuration</cell><cell cols="8">1-clip, 1 crop 25-clip, 10 crops 1-clip, 1 crop 25-clip, 10 crops</cell></row><row><cell>res3</cell><cell>60.4</cell><cell>82.4</cell><cell>accuracy</cell><cell cols="3">top-1 top-5 top-1</cell><cell>top-5</cell><cell cols="3">top-1 top-5 top-1</cell><cell>top-5</cell></row><row><cell>res4</cell><cell>60.8</cell><cell>82.8</cell><cell>C2D</cell><cell>56.9</cell><cell>79.5</cell><cell>61.3</cell><cell>83.6</cell><cell>54.1</cell><cell>77.4</cell><cell>60.8</cell><cell>83.3</cell></row><row><cell>res5</cell><cell>59.2</cell><cell>81.6</cell><cell>C3D [28]</cell><cell>58.3</cell><cell>80.7</cell><cell>64.4</cell><cell>85.8</cell><cell>55.0</cell><cell>78.5</cell><cell>63.3</cell><cell>85.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NL C2D Net [33]</cell><cell>58.6</cell><cell>81.3</cell><cell>63.3</cell><cell>85.1</cell><cell>55.3</cell><cell>78.6</cell><cell>62.1</cell><cell>84.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ARTNet [32]</cell><cell>59.1</cell><cell>81.1</cell><cell>65.1</cell><cell>86.1</cell><cell>56.1</cell><cell>78.7</cell><cell>64.2</cell><cell>85.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CPNet (Ours)</cell><cell>61.1</cell><cell>83.1</cell><cell>66.3</cell><cell>87.1</cell><cell>57.2</cell><cell>80.8</cell><cell>64.9</cell><cell>86.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>model</cell><cell cols="3">params (M) top-1 top-5</cell></row><row><cell>I3D Inception [3]</cell><cell>25.0</cell><cell>72.1</cell><cell>90.3</cell></row><row><cell>Inception-ResNet-v2 [2]</cell><cell>50.9</cell><cell>73.0</cell><cell>90.9</cell></row><row><cell>NL C2D ResNet-101 [33]</cell><cell>48.2</cell><cell>75.1</cell><cell>91.7</cell></row><row><cell>CPNet C2D ResNet-101 (ours)</cell><cell>42.1</cell><cell>75.3</cell><cell>92.4</cell></row></table><note>Large RGB-only models on Kinetics validation accuracy. Clip length for NL Net and our CPNet is 32 frames.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>TwentyBN datasets results. Our CPNet outperforms all published results, with fewer number of parameters.</figDesc><table><row><cell cols="5">(a) Something-Something v2 Results</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Jester v1 Results</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>params (M)</cell><cell>top-1</cell><cell>val</cell><cell>top-5</cell><cell>top-1</cell><cell>test</cell><cell>top-5</cell><cell>model</cell><cell>params (M)</cell><cell>val</cell><cell>test</cell></row><row><cell>Goyal et al. [12]</cell><cell>22.2</cell><cell cols="6">51.33 80.46 50.76 80.77</cell><cell>BesNet [11]</cell><cell>37.8</cell><cell>-</cell><cell>94.23</cell></row><row><cell>MultiScale TRN [39]</cell><cell>22.8</cell><cell cols="6">48.80 77.64 50.85 79.33</cell><cell>MultiScale TRN [39]</cell><cell>22.8</cell><cell cols="2">95.31 94.78</cell></row><row><cell>Two-stream TRN [39]</cell><cell>46.4</cell><cell cols="6">55.52 83.06 56.24 83.15</cell><cell>TPRN [35]</cell><cell>22.0</cell><cell cols="2">95.40 95.34</cell></row><row><cell>C2D Res18 baseline</cell><cell>10.7</cell><cell cols="3">35.24 64.49</cell><cell>-</cell><cell></cell><cell>-</cell><cell>MFNet [20]</cell><cell>41.1</cell><cell cols="2">96.68 96.22</cell></row><row><cell>C2D Res34 baseline</cell><cell>20.3</cell><cell cols="3">39.64 69.61</cell><cell>-</cell><cell></cell><cell>-</cell><cell>MFF [19]</cell><cell>43.4</cell><cell cols="2">96.33 96.28</cell></row><row><cell>CPNet Res18, 5 CP (ours)</cell><cell>11.3</cell><cell cols="6">54.08 82.10 53.31 81.00</cell><cell>C2D Res34 baseline</cell><cell>20.3</cell><cell>84.73</cell><cell>-</cell></row><row><cell>CPNet Res34, 5 CP (ours)</cell><cell>21.0</cell><cell cols="6">57.65 83.95 57.57 84.26</cell><cell>CPNet Res34, 5 CP (ours)</cell><cell>21.0</cell><cell cols="2">96.70 96.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>(d)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Complete Architectures used in Kinetics dataset comparison experiments.</figDesc><table><row><cell>layer</cell><cell>output size</cell><cell>C2D (C3D)</cell><cell></cell><cell>ARTNet [32]</cell><cell></cell><cell cols="2">NL C2D Net 6 NL blocks [33]</cell><cell cols="2">CPNet (Ours) 6 CP modules</cell></row><row><cell cols="2">conv1 56 × 56 × 8</cell><cell cols="2">7 × 7(×3), 64, stride 2, 2(, 1)</cell><cell cols="2">SMART 7 × 7 × 3, 64, stride 2, 2, 1</cell><cell cols="2">7 × 7, 64, stride 2, 2</cell><cell cols="2">7 × 7, 64, stride 2, 2</cell></row><row><cell>res2</cell><cell>56 × 56 × 8</cell><cell>3 × 3(×3), 64 3 × 3(×3), 64</cell><cell>× 2</cell><cell>3 × 3 × 3, 64 SMART 3 × 3 × 3, 64</cell><cell>× 2</cell><cell>3 × 3, 64 3 × 3, 64</cell><cell>× 2</cell><cell>3 × 3, 64 3 × 3, 64</cell><cell>× 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>CPNet Architectures used in Kinetics large model experiments.</figDesc><table><row><cell>layer</cell><cell>output size</cell><cell cols="4">CPNet, 5 CP modules</cell></row><row><cell cols="2">conv1 56 × 56 × 8</cell><cell></cell><cell></cell><cell cols="2">7 × 7, 64, stride 2, 2</cell></row><row><cell></cell><cell></cell><cell cols="2"></cell><cell>1 × 1, 64</cell><cell></cell></row><row><cell>res2</cell><cell>56 × 56 × 8</cell><cell cols="2"></cell><cell>3 × 3, 64</cell><cell> × 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 × 1, 256</cell></row><row><cell></cell><cell></cell><cell cols="2"></cell><cell cols="2">1 × 1, 128 </cell></row><row><cell></cell><cell></cell><cell cols="2"></cell><cell cols="2">3 × 3, 128  × 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 × 1, 512</cell></row><row><cell>res3</cell><cell>28 × 28 × 8</cell><cell cols="2"></cell><cell cols="2">1 × 1, 128</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">  </cell><cell cols="2">3 × 3, 128 1 × 1, 512</cell><cell>   × 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">CP module</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1 × 1, 256</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">3 × 3, 256</cell><cell> × 20</cell></row><row><cell></cell><cell></cell><cell cols="4">1 × 1, 1024</cell></row><row><cell>res4</cell><cell>14 × 14 × 8</cell><cell></cell><cell></cell><cell cols="2">1 × 1, 256</cell><cell></cell></row><row><cell></cell><cell></cell><cell>  </cell><cell cols="3">3 × 3, 256 1 × 1, 1024    × 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">CP module</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 × 1, 512</cell><cell></cell></row><row><cell>res5</cell><cell>7 × 7 × 8</cell><cell></cell><cell></cell><cell cols="2">3 × 3, 512</cell><cell> × 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">1 × 1, 2048</cell></row><row><cell></cell><cell>1 × 1 × 1</cell><cell cols="4">global average pooling, fc 400</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>CPNet Architectures used in Something-Something and Jester dataset experiments.</figDesc><table><row><cell>layer</cell><cell>output size</cell><cell cols="3">CPNet, 5 CP modules</cell></row><row><cell cols="2">conv1 56 × 56 × 12</cell><cell></cell><cell cols="2">7 × 7, 64, stride 2, 2</cell></row><row><cell>res2</cell><cell>56 × 56 × 12</cell><cell></cell><cell>3 × 3, 64 3 × 3, 64</cell><cell>× 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3 × 3, 128 3 × 3, 128</cell><cell>× 2</cell></row><row><cell>res3</cell><cell>28 × 28 × 12</cell><cell></cell><cell>3 × 3, 128</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>3 × 3, 128</cell><cell> × 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CP module</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3 × 3, 256 3 × 3, 256</cell><cell>× 3</cell></row><row><cell>res4</cell><cell>14 × 14 × 12</cell><cell></cell><cell>3 × 3, 256</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>3 × 3, 256</cell><cell> × 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CP module</cell></row><row><cell>res5</cell><cell>7 × 7 × 12</cell><cell></cell><cell>3 × 3, 512 3 × 3, 512</cell><cell>× 3</cell></row><row><cell></cell><cell>1 × 1 × 1</cell><cell cols="3">global average pooling, fc 174 or fc 27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Putting something that cannot actually stand upright upright on the table, so it falls on its side Putting something on the edge of something so it is not supported and falls down Putting something that can't roll onto a slanted surface, so it stays where it is Lifting something up completely, then letting it drop down Holding something next to something Pretending to put something on a surface Tilting something with something on it slightly so it doesn't fall down Pretending to pick something up Putting something similar to other things that are already on the table Pretending to turn something upside down Trying to pour something into something, but missing so it spills next to it Tearing something just a little bit Pulling two ends of something so that it separates into two pieces Plugging something into something but pulling it right out as you remove your hand Moving something up Moving something down Pretending to throw something Putting something that can't roll onto a slanted surface, so it slides down Putting something in front of something Moving something closer to something Moving something away from something Spinning something so it continues spinning Moving something and something closer to each other Moving something away from the camera Turning the camera downwards while filming something Turning the camera left while filming something Approaching something with your camera Letting something roll up a slanted surface, so it rolls back down Moving something towards the camera Turning the camera right while filming something Lifting up one end of something without letting it drop down Moving away from something with your camera</figDesc><table><row><cell>Dropping something behind something</cell></row><row><cell>Something colliding with something and both are being deflected</cell></row><row><cell>Putting something into something</cell></row><row><cell>Pouring something into something until it overflows</cell></row><row><cell>Poking a hole into something soft</cell></row><row><cell>Turning the camera upwards while filming something</cell></row><row><cell>Folding something</cell></row><row><cell>Pulling something from right to left</cell></row><row><cell>Uncovering something</cell></row><row><cell>Showing something next to something</cell></row><row><cell>Pushing something from right to left</cell></row><row><cell>Pulling something from behind of something</cell></row><row><cell>Taking something out of something</cell></row><row><cell>Pretending to put something into something</cell></row><row><cell>Closing something</cell></row><row><cell>Moving something and something away from each other</cell></row><row><cell>Pouring something out of something</cell></row><row><cell>Pulling something from left to right</cell></row><row><cell>Tilting something with something on it until it falls off</cell></row><row><cell>Pretending to put something next to something</cell></row><row><cell>Pretending to put something behind something</cell></row><row><cell>Dropping something into something</cell></row><row><cell>Putting something onto something</cell></row><row><cell>Putting something next to something</cell></row><row><cell>Spilling something behind something</cell></row><row><cell>Opening something</cell></row><row><cell>Covering something with something</cell></row><row><cell>Lifting something up completely without letting it drop down</cell></row><row><cell>Taking one of many similar things on the table</cell></row><row><cell>Dropping something onto something</cell></row><row><cell>Pretending to scoop something up with something</cell></row><row><cell>Sprinkling something onto something</cell></row><row><cell>Dropping something in front of something</cell></row><row><cell>Holding something over something</cell></row><row><cell>Dropping something next to something</cell></row><row><cell>Bending something so that it deforms</cell></row><row><cell>Picking something up</cell></row><row><cell>Rolling something on a flat surface</cell></row><row><cell>Pushing something so that it slightly moves</cell></row><row><cell>Pushing something from left to right</cell></row><row><cell>Putting something underneath something</cell></row><row><cell>Something falling like a feather or paper</cell></row><row><cell>Pulling two ends of something so that it gets stretched</cell></row><row><cell>Something colliding with something and both come to a halt</cell></row><row><cell>Putting something, something and something on the table</cell></row><row><cell>Showing something behind something</cell></row><row><cell>Hitting something with something</cell></row><row><cell>Holding something</cell></row><row><cell>Unfolding something</cell></row><row><cell>Poking something so lightly that it doesn't or almost doesn't move</cell></row><row><cell>Burying something in something</cell></row><row><cell>Piling something up</cell></row><row><cell>Poking a stack of something so the stack collapses</cell></row><row><cell>Removing something, revealing something behind</cell></row><row><cell>Pretending to put something underneath something</cell></row><row><cell>Pushing something so that it almost falls off but doesn't</cell></row><row><cell>Spreading something onto something</cell></row><row><cell>Putting something upright on the table</cell></row><row><cell>Pretending to pour something out of something, but something is empty</cell></row><row><cell>Pretending to open something without actually opening it</cell></row><row><cell>Squeezing something</cell></row><row><cell>Trying but failing to attach something to something because it doesn't stick</cell></row><row><cell>Stuffing something into something</cell></row><row><cell>Pushing something so that it falls off the table</cell></row><row><cell>Tipping something over</cell></row><row><cell>Throwing something against something</cell></row><row><cell>Attaching something to something</cell></row><row><cell>Pretending to poke something</cell></row><row><cell>Spilling something next to something</cell></row><row><cell>Pretending to close something without actually closing it</cell></row><row><cell>Pushing something with something</cell></row><row><cell>Throwing something</cell></row><row><cell>Laying something on the table on its side, not upright</cell></row><row><cell>Pretending to take something out of something</cell></row><row><cell>Putting something on a surface</cell></row><row><cell>Putting something and something on the table</cell></row><row><cell>Pretending or failing to wipe something off of something</cell></row><row><cell>Twisting something</cell></row><row><cell>Holding something behind something</cell></row><row><cell>Pretending to put something onto something</cell></row><row><cell>Wiping something off of something</cell></row><row><cell>Moving something across a surface until it falls down</cell></row><row><cell>Showing that something is empty</cell></row><row><cell>Lifting something with something on it</cell></row><row><cell>Lifting a surface with something on it until it starts sliding down</cell></row><row><cell>Spilling something onto something</cell></row><row><cell>Pretending to sprinkle air onto something</cell></row><row><cell>Holding something in front of something</cell></row><row><cell>Putting something onto something else that cannot support it so it falls down</cell></row><row><cell>Turning something upside down</cell></row><row><cell>Putting something behind something</cell></row><row><cell>Showing something on top of something</cell></row><row><cell>Something falling like a rock</cell></row><row><cell>Pouring something onto something</cell></row><row><cell>Moving something across a surface without it falling down</cell></row><row><cell>Moving something and something so they pass each other</cell></row><row><cell>Pretending to be tearing something that is not tearable</cell></row><row><cell>Tipping something with something in it over, so something in it falls out</cell></row><row><cell>Pretending to squeeze something</cell></row><row><cell>Scooping something up with something</cell></row><row><cell>Bending something until it breaks</cell></row><row><cell>Poking a hole into some substance</cell></row><row><cell>Touching (without moving) part of something</cell></row><row><cell>Letting something roll along a flat surface</cell></row><row><cell>Moving part of something</cell></row><row><cell>Pulling something onto something</cell></row><row><cell>Putting something on a flat surface without letting it roll</cell></row><row><cell>Putting number of something onto something</cell></row><row><cell>Throwing something in the air and catching it</cell></row><row><cell>Poking something so that it falls over</cell></row><row><cell>Trying to bend something unbendable so nothing happens</cell></row><row><cell>Stacking number of something</cell></row><row><cell>Pretending or trying and failing to twist something</cell></row><row><cell>Tearing something into two pieces</cell></row><row><cell>Pushing something so it spins</cell></row><row><cell>Lifting a surface with something on it but not enough for it to slide down</cell></row><row><cell>Failing to put something into something because something does not fit</cell></row><row><cell>Pushing something onto something</cell></row><row><cell>Showing that something is inside something</cell></row><row><cell>Spinning something that quickly stops spinning</cell></row><row><cell>Throwing something in the air and letting it fall</cell></row><row><cell>Letting something roll down a slanted surface</cell></row><row><cell>Poking something so that it spins around</cell></row><row><cell>Pretending to take something from somewhere</cell></row><row><cell>Pushing something off of something</cell></row><row><cell>Something being deflected from something</cell></row><row><cell>Taking something from somewhere</cell></row><row><cell>Plugging something into something</cell></row><row><cell>Pouring something into something</cell></row><row><cell>Lifting up one end of something, then letting it drop down</cell></row><row><cell>Pretending to spread air onto something</cell></row><row><cell>Pulling something out of something</cell></row><row><cell>Showing something to the camera</cell></row><row><cell>Digging something out of something</cell></row><row><cell>Twisting (wringing) something wet until water comes out</cell></row><row><cell>Showing a photo of something to the camera</cell></row><row><cell>Throwing something onto a surface</cell></row><row><cell>Poking something so it slightly moves</cell></row><row><cell>Putting something onto a slanted surface but it doesn't glide down</cell></row><row><cell>Poking a stack of something without the stack collapsing</cell></row><row><cell>Pulling two ends of something but nothing happens</cell></row><row><cell>Moving something and something so they collide with each other</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">There are space for accuracy improvement when using 48 frames.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head><p>A. Overview</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<idno>1975. 12</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-theshelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Christoph Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical geometric models for visible surface algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Random Dilation Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gölge</surname></persName>
		</author>
		<ptr target="http://www.erogol.com/random-dilation-networks-action-recognition-videos" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense. CoRR, abs/1706.04261</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fründ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motion fused frames: Data level fusion strategy for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kopuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Flownet3d: Learning scene flow in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Twentybn</surname></persName>
		</author>
		<ptr target="https://20bn.com/datasets/jester.2,5" />
		<title level="m">The 20BN-jester Dataset V1</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Žbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Appearance-andrelation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal pyramid relation network for video-based gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
