<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Perturbations in Encoder-Decoders for Fast Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
							<email>sho.takase@nlp.c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
							<email>shun.kiyono@riken.jp</email>
							<affiliation key="aff1">
								<orgName type="department">RIKEN / Tohoku University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Perturbations in Encoder-Decoders for Fast Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling <ref type="bibr" target="#b1">(Bengio et al., 2015)</ref> and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time.</p><p>Experimental results show that the simple techniques such as word dropout <ref type="bibr" target="#b11">(Gal and Ghahramani, 2016)</ref> and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster. Our code is publicly available at https://github.com/takase/rethink_perturbations. 1  We assume that we use on-demand instances having 8 V100 GPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in neural encoder-decoders have driven tremendous success for sequenceto-sequence problems including machine translation , summarization <ref type="bibr" target="#b27">(Rush et al., 2015)</ref>, and grammatical error correction (GEC) <ref type="bibr" target="#b13">(Ji et al., 2017)</ref>. Since neural models can be too powerful, previous studies have proposed various regularization methods to avoid over-fitting.</p><p>To regularize neural models, we often apply a perturbation <ref type="bibr" target="#b12">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b18">Miyato et al., 2017)</ref>, which is a small difference from a correct input. During the training process, we force the model to output the correct labels for both perturbed inputs and unmodified inputs. In sequenceto-sequence problems, existing studies regard the following as perturbed inputs: (1) sequences containing tokens replaced from correct ones <ref type="bibr" target="#b1">(Bengio et al., 2015;</ref><ref type="bibr" target="#b6">Cheng et al., 2019)</ref>, (2) embeddings injected small differences <ref type="bibr" target="#b28">(Sato et al., 2019)</ref>. For example, <ref type="bibr" target="#b1">Bengio et al. (2015)</ref> proposed the scheduled sampling that samples a token from the output probability distribution of a decoder and uses it as a perturbed input for the decoder. <ref type="bibr" target="#b28">Sato et al. (2019)</ref> applied an adversarial perturbation, which significantly increases the loss value of a model, to the embedding spaces of neural encoder-decoders.</p><p>Those studies reported that their methods are effective to construct robust encoder-decoders. However, their methods are much slower than the training without using such perturbations because they require at least one forward computation to obtain the perturbation. In fact, we need to run the decoder the same times as the required number of perturbations in the scheduled sampling <ref type="bibr" target="#b1">(Bengio et al., 2015)</ref>. For adversarial perturbations <ref type="bibr" target="#b28">(Sato et al., 2019)</ref>, we have to compute the backpropagation in addition to forward computation because we use gradients to obtain perturbations.</p><p>Those properties seriously affect the training budget. For example, it costs approximately 1,800 USD for each run when we train Transformer (big) with adversarial perturbations <ref type="bibr" target="#b28">(Sato et al., 2019)</ref> on the widely used WMT English-German training set in AWS EC2 1 . Most studies conduct multiple runs for the hyper-parameter search and/or model ensemble to achieve better performance <ref type="bibr">(Barrault et al., 2019)</ref>, which incurs a tremendous amount of training budget for using such perturbations. <ref type="bibr" target="#b33">Strubell et al. (2019)</ref> and <ref type="bibr">Schwartz et al. (2019)</ref> indicated that recent neural approaches increase computational costs substantially, and they encouraged exploring a cost-efficient method. For instance, <ref type="bibr" target="#b16">Li et al. (2020)</ref> explored a training strategy to obtain the best model in a given training time. However, previous studies have paid little attention to the costs of computing perturbations.</p><p>Thus, we rethink a time efficient perturbation method. In other words, we address the question whether perturbations proposed by recent studies as effective methods are time efficient. We compare several perturbation methods for neural encoderdecoders in terms of computational time. We introduce light computation methods such as word dropout <ref type="bibr" target="#b11">(Gal and Ghahramani, 2016)</ref> and using randomly sampled tokens as perturbed inputs. These methods are sometimes regarded as baseline methods <ref type="bibr" target="#b1">(Bengio et al., 2015)</ref>, but experiments on translation datasets indicate that these simple methods surprisingly achieve comparable scores to those of previous effective perturbations <ref type="bibr" target="#b1">(Bengio et al., 2015;</ref><ref type="bibr" target="#b28">Sato et al., 2019)</ref> in a shorter training time. Moreover, we indicate that these simple methods are also effective for other sequence-to-sequence problems: GEC and summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definition of Encoder-Decoder</head><p>In this paper, we address sequence-to-sequence problems such as machine translation with neural encoder-decoders, and herein we provide a definition of encoder-decoders.</p><p>In sequence-to-sequence problems, neural encoder-decoders generate a sequence corresponding to an input sequence. Let x 1:I and y 1:J be input and output token sequences whose lengths are I and J, respectively: x 1:I = x 1 , ..., x I and y 1:J = y 1 , ..., y J . Neural encoder-decoders compute the following conditional probability:</p><formula xml:id="formula_0">p(Y |X) = J+1 j=1 p(y j |y 0:j−1 , X),<label>(1)</label></formula><p>where y 0 and y J+1 are special tokens representing beginning-of-sentence (BOS) and end-of-sentence (EOS) respectively, X = x 1:I , and Y = y 1:J+1 . In the training phase, we optimize the parameters θ to minimize the negative log-likelihood in the training data. Let D be the training data consisting of a set of pairs of X n and Y n :</p><formula xml:id="formula_1">D = {(X n , Y n )} |D| n=1</formula><p>. We minimize the following loss function:</p><formula xml:id="formula_2">L(θ) = − 1 |D| (X,Y )∈D log p(Y |X; θ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Definition of Perturbations</head><p>This section briefly describes perturbations used in this study. This study focuses on three types of perturbations: word replacement, word dropout, and adversarial perturbations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct input tokens</head><p>Word replacement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word dropout</head><p>Adversarial perturbation</p><formula xml:id="formula_3">x' 1 b x1 e(x' 1 ) r x1 x 2 x' 2 b x2 e(x' 2 ) r x2 … y 0 y' 0 b y0 e(y' 0 ) r y0 y 1 y' 1 b y1 e(y' 1 ) r y1</formula><p>… Decoder y 1 y 2 … <ref type="figure">Figure 1</ref>: Overview of perturbations used in this study. We can combine perturbations as shown in this figure because each type of perturbation is orthogonal.</p><p>perturbations used in this study. As shown in this figure, we can use all types of perturbations in the same time because perturbations are orthogonal to each other. In fact, we combine word replacement with word dropout in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Replacement: REP</head><p>For any approach that uses a sampled token instead of a correct token, such as the scheduled sampling <ref type="bibr" target="#b1">(Bengio et al., 2015)</ref>, we refer to this as a word replacement approach. In this approach, we construct a new sequence whose tokens are randomly replaced with sampled tokens. For the construction from the sequence X, we samplex i from a distribution Q x i and use it for the new sequence X with the probability α:</p><formula xml:id="formula_4">x i ∼ Q x i ,<label>(3)</label></formula><formula xml:id="formula_5">x i = x i with probability α x i with probability 1 − α.<label>(4)</label></formula><p>We construct Y from the sequence Y in the same manner. <ref type="bibr" target="#b1">Bengio et al. (2015)</ref> used a curriculum learning strategy to adjust α, and thus proposed several functions to decrease α based on the training step. Their strategy uses correct tokens frequently at the beginning of training, whereas it favors sampled tokens frequently at the end of training. We also adjust α with their use of the inverse sigmoid decay:</p><formula xml:id="formula_6">α t = max q, k k + exp( t k )<label>(5)</label></formula><p>where q and k are hyper-parameters. In short, α t decreases to q from 1, depending on the training step t. We use α t as α at t.</p><p>For Q x i , we prepare three types of distributions: conditional probability, uniform, and similarity.</p><p>Conditional Probability: REP(SS) <ref type="bibr" target="#b1">Bengio et al. (2015)</ref> proposed the scheduled sampling which uses predicted tokens during training to address the gap between training and inference. Formally, the scheduled sampling uses the following conditional probability as Q y i :</p><formula xml:id="formula_7">p(ŷ i |y 0:i−1 , X).<label>(6)</label></formula><p>Since the scheduled sampling is the method to compute the perturbation for the decoder side only, it uses the correct sequence as the input of the encoder side. In other words, the scheduled sampling does not provide any function for Q x i . The original scheduled sampling repeats the decoding for each of the tokens on the decoder side, and thus, requires computational time in proportion to the length of the decoder-side input sequence. To address this issue, <ref type="bibr" target="#b9">Duckworth et al. (2019)</ref> proposed a more time efficient method: parallel scheduled sampling which computes output probability distributions corresponding to each position simultaneously. In this study, we use parallel scheduled sampling instead of the original method.</p><p>Uniform: REP(UNI) The scheduled sampling is slow even if we use parallel scheduled sampling because it requires decoding at least once to compute Equation (6). Thus, we introduce two faster methods to explore effective perturbations from the perspective of computational time. In uniform, we use the uniform distributions on each vocabulary as Q x i and Q y i , respectively. For example, we randomly pick up a token from the source-side vocabulary and use the token asx i in Equation (4) to construct the source-side perturbed input. This method is used as the baseline in the previous study <ref type="bibr" target="#b1">(Bengio et al., 2015)</ref>.</p><p>Similarity: REP(SIM) We also explore more sophisticated way than the uniform distribution. We assume that the conditional probability of Equation (6) assigns high probabilities to tokens that are similar to the correct input token. Based on this assumption, we construct a distribution that enables us to sample similar tokens frequently. Let V x be the source-side vocabulary, E x ∈ R |Vx|×dx be the d x dimensional embedding matrix, and e(x i ) be the function returning the embedding of x i . We use the following probability distribution as Q x i :</p><formula xml:id="formula_8">softmax(E x e(x i )),<label>(7)</label></formula><p>where softmax(.) is the softmax function. Thus, Equation <ref type="formula" target="#formula_8">(7)</ref> assigns high probabilities to tokens whose embeddings are similar to e(x i ). In other words, Equation <ref type="formula" target="#formula_8">(7)</ref> is the similarity against x i without considering any context. We compute the probability distribution for the target side by using e(y i ) in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Dropout: WDROP</head><p>We apply the word dropout technique to compute the perturbed input. Word dropout randomly uses the zero vector instead of the embedding e(x i ) for the input token x i <ref type="bibr" target="#b11">(Gal and Ghahramani, 2016)</ref>:</p><formula xml:id="formula_9">b x i ∼ Bernoulli(β), (8) WDrop(x i , b x i ) = b x i e(x i ),<label>(9)</label></formula><p>where Bernoulli(β) returns 1 with the probability β and 0 otherwise. Thus, WDrop(x i , b x i ) returns e(x i ) with the probability β and the zero vector otherwise. We apply Equation <ref type="formula" target="#formula_9">(9)</ref> to each token in the input sequence. Then, we use the results as the perturbed input. <ref type="bibr" target="#b18">Miyato et al. (2017)</ref> proposed a method to compute adversarial perturbations in the embedding space. Their method adds adversarial perturbations to input embeddings instead of replacing correct input tokens with others. <ref type="bibr" target="#b28">Sato et al. (2019)</ref> applied this approach to neural encoder-decoders and reported its effectiveness. Thus, this study follows the methods used in <ref type="bibr" target="#b28">Sato et al. (2019)</ref>. The method seeks the adversarial perturbation, which seriously damages the loss value, based on the gradient of the loss function L(θ). Then, we add the adversarial perturbation to the input token embedding. Let r x i ∈ R dx be the adversarial perturbation vector for the input token x i . We obtain the perturbed input embedding e (x i ) with the following equations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial Perturbation: ADV</head><formula xml:id="formula_10">e (x i ) = e(x i ) + r x i ,<label>(10)</label></formula><formula xml:id="formula_11">r x i = c x i ||c x i || ,<label>(11)</label></formula><formula xml:id="formula_12">c x i = ∇ e(x i ) L(θ),<label>(12)</label></formula><p>where is a hyper-parameter to control the norm of the adversarial perturbation. We apply the above equations to all tokens in the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>In the training using word replacement and/or word dropout perturbations, we search the parameters predicting the correct output sequence from the perturbed input. For example, in the word replacement approach, we minimize the following negative loglikelihood:</p><formula xml:id="formula_13">L (θ) = − 1 |D| D log p(Y |X , Y ; θ), = − 1 |D| D J+1 j=1 log p(y j |y 0:j−1 , X ; θ).<label>(13)</label></formula><p>Virtual Adversarial Training When we use adversarial perturbations, we train parameters of the neural encoder-decoder to minimize both Equation (2) and a loss function A(θ) composed of perturbed inputs:</p><formula xml:id="formula_14">J (θ) = L(θ) + λA(θ),<label>(14)</label></formula><p>where λ is a hyper-parameter to control the balance of two loss functions. This calculation seems to be reasonably time efficient because adversarial perturbations require computing Equation <ref type="formula" target="#formula_2">(2)</ref>. <ref type="bibr" target="#b28">Sato et al. (2019)</ref> used the virtual adversarial training originally proposed in <ref type="bibr" target="#b19">Miyato et al. (2016)</ref> as a loss function for perturbed inputs. In the virtual adversarial training, we regard the output probability distributions given the correct input sequence as positive examples:</p><formula xml:id="formula_15">A(θ) = 1 |D| D KL (p(·|X; θ)||p(·|X, r; θ)) ,<label>(15)</label></formula><p>where r represents a concatenated vector of adversarial perturbations for each input token, and KL(·||·) denotes the Kullback-Leibler divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on Machine Translation</head><p>To obtain findings on sequence-to-sequence problems, we conduct experiments on various situations: different numbers of training data and multiple tasks. We mainly focus on translation datasets because machine translation is a typical sequenceto-sequence problem. We regard the widely used WMT English-German dataset as a standard setting. In addition, we vary the number of training data in machine translation: high resource in Section 4.2 and low resource in Section 4.3.  on other sequence-to-sequence problems: grammatical error correction (GEC) in Section 5 and summarization in Appendix A to confirm whether the findings from machine translation are applicable to other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Standard Setting</head><p>Datasets We used the WMT 2016 English-German training set, which contains 4.5M sentence pairs, in the same as <ref type="bibr" target="#b23">Ott et al. (2018)</ref>, and followed their pre-processing. We used newstest2013 as a validation set, and newstest2010-2012, and 2014-2016 as test sets. We measured case-sensitive detokenized BLEU with SacreBLEU (Post, 2018) 2 .</p><p>Methods We used Transformer <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> as a base neural encoder-decoder model because it is known as a strong neural encoderdecoder model. We used two parameter sizes: base and big settings in <ref type="bibr" target="#b41">Vaswani et al. (2017)</ref>. We applied perturbations described in Section 3 for comparison. For parallel scheduled sampling <ref type="bibr" target="#b9">(Duckworth et al., 2019)</ref>, we can compute output probability distributions multiple times but we used the first decoding result only because it is the fastest approach. We set q = 0.9, k = 1000, and β = 0.9. For ADV, we used the same hyperparameters as in <ref type="bibr" target="#b28">Sato et al. (2019)</ref>. Our implementation is based on fairseq 3 <ref type="bibr" target="#b22">(Ott et al., 2019)</ref>. We trained each model for a total of 50,000 steps.</p><p>Preliminary: To which sides do we apply perturbations? As described, perturbations based on REP(SS) can be applied to the decoder side only. <ref type="bibr" target="#b28">Sato et al. (2019)</ref>   REP(SIM), and WDROP to the encoder side, decoder side, and both as preliminary experiments. <ref type="table" target="#tab_3">Table 2</ref> shows BLEU scores on newstest2010-2016 and averaged scores when we varied the position of the perturbations. In this table, we indicate better scores than the original Transformer <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> (w/o perturbation) in bold. This table shows that it is better to apply word replacement (REP(UNI) and REP(SIM)) to the decoder side in Transformer (base). For WDROP, applying the encoder side is slightly better than other positions in Transformer (base). In contrast, applying perturbations to both sides achieved the best averaged BLEU scores for all methods in Transformer (big). These results imply that it is better to apply to word replacement and/or word dropout to both encoder and decoder sides if we prepare enough parameters for neural encoder-decoders. Based on these results, we select methods to compare against scheduled sampling (REP(SS)) and adversarial perturbations (ADV).</p><p>Table 2 also shows the results when we combined each word replacement with word dropout (REP(UNI)+WDROP and REP(SIM)+WDROP). REP(SIM)+WDROP slightly outperformed the separated settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare each perturbation in view of computational time. <ref type="table" target="#tab_5">Table 3</ref> shows BLEU scores of each method and computational speeds 4 based on Transformer (base) without any perturbations, i.e., larger is faster. In this table, we indicate the best score of each column for Transformer (base) and (big) settings in bold. This table indicates that Transformer without perturbations achieved a comparable score to previous studies <ref type="bibr" target="#b41">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b23">Ott et al., 2018)</ref> on newstest2014 in base and big settings. Thus, we consider that our trained Transformer models (w/o perturbation) can be regarded as strong baselines. This table shows that ADV achieved the best averaged score in Transformer (base), but this method required twice as much training time as the original Transformer (base). In contrast, REP(SIM) and WDROP achieved comparable scores to ADV although they slightly affected the computational time. REP(UNI) also achieved a slightly better averaged score than the original Transformer (base).</p><p>In the Transformer (big) setting, all perturbations surpassed the performance of w/o perturbation in the averaged score. REP(SS) and ADV improved the performance, but other methods outperformed these two methods with a small training time. Moreover, REP(UNI) and REP(SIM)+WDROP achieved the best averaged score. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the negative log-likelihood <ref type="formula" target="#formula_0">2010</ref>    values and BLEU scores on the validation set for each training time when we applied each perturbation to Transformer (big). In addition, <ref type="figure" target="#fig_1">Figure 2</ref> (c) shows the time required to achieve the BLEU score of Transformer w/o perturbation on the validation set (26.60, as described in <ref type="table" target="#tab_5">Table 3</ref>). These figures show that ADV requires twice as much time or more relative to other methods to achieve performance comparable to others. In NLL curves, REP(UNI), REP(SIM), and WDROP achieved better values than those of Transformer w/o perturbation in the early stage. In addition, WDROP was the fastest to achieve better NLL value. <ref type="figure" target="#fig_1">Figure 2</ref> (c) indicates that REP(UNI), REP(SIM), and WDROP achieved 26.60 BLEU score with smaller training time than that of Transformer w/o perturbation.</p><p>These results indicate that we can quickly improve the performance of Transformer with REP(UNI), REP(SIM), and WDROP. In particu-lar, when we prepare a large number of parameters for Transformer in machine translation, it is better to use these methods (and their combinations) as perturbations. We conduct more experiments to investigate whether these methods are also superior in other configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">High Resource</head><p>We compare each perturbation in the case where we have a large amount of training data.</p><p>Datasets We add synthetic parallel data generated from the German monolingual corpus using back-translation <ref type="bibr" target="#b29">(Sennrich et al., 2016a)</ref> to the training data used in Section 4.1. The origin of the German monolingual corpus is NewsCrawl 2015-2018 5 . We randomly sampled 5M sentences from each NewsCrawl corpus, and thus, obtained 20M sentences in total. We back-translated the corpus <ref type="formula" target="#formula_0">2010</ref>   with the German-English translation model, which is identical to Transformer (big) (w/o perturbation) used in Section 4.1 except for the direction of translation. Finally, we prepended a special token BT to the beginning of the source (English) side of the synthetic data following <ref type="bibr" target="#b5">(Caswell et al., 2019)</ref>. In addition, we upsampled the original bitext to adjust the ratio of the original and synthetic bitexts to 1:1.</p><p>Methods In this setting, we increase the parameter size of Transformer from the (big) setting to take advantage of large training data. Specifically, we increased the internal layer size of the FFN part from 4096 to 8192, and used 8 layers for both the encoder and decoder. The other hyper-parameters are same as in Section 4.1.</p><p>Results <ref type="table" target="#tab_7">Table 4</ref> shows BLEU scores of each method when we used a large amount of training data. This table indicates that all perturbations outperformed Transformer w/o perturbation in all test sets. Moreover, the fast methods REP(UNI), REP(SIM), WDROP, and their combinations achieved the same or better averaged scores than REP(SS) and ADV. Thus, these methods are not only fast but also significantly improve the performance of Transformer. In particular, since <ref type="table" target="#tab_5">Table  3</ref> shows that REP(UNI) and WDROP barely have any negative effect on the computational time, we consider them as superior methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Low Resource</head><p>Datasets We also conduct an experiment on a low resource setting. We used IWSLT 2014 German-English training set which contains 160k sentence pairs. We followed the preprocessing described in fairseq 6 <ref type="bibr" target="#b22">(Ott et al., 2019)</ref>. We used dev2010, 2012, and tst2010-2012 as a test set.</p><p>Methods In this setting, we reduced the parameter size of Transformer from the <ref type="formula">(base)</ref>   2048 to 1024. We used the same values for other hyper-parameters as in Section 4.1.</p><p>Results <ref type="table" target="#tab_9">Table 5</ref> shows BLEU scores of each method on the low resource setting. We trained three models with different random seeds for each method, and reported the averaged scores. In this table, we also report the results of REP(UNI), REP(SIM), WDROP, and their combinations trained with twice the number of updates (below ×2 training steps). This table shows that all perturbations also improved the performance from Transformer w/o perturbation. In contrast to <ref type="table" target="#tab_5">Tables 3 and  4</ref>, ADV achieved the top score when each model was trained with the same number of updates. However, as reported in Section 4.1, ADV requires twice or more as long as other perturbations for training. Thus, when we train Transformer with other perturbations with twice the number of updates, the training time is almost equal. In the comparison of (almost) equal training time, WDROP achieved a comparable score to ADV. Moreover, REP(UNI)+WDROP and REP(SIM)+WDROP 7 outperformed ADV. Thus, in this low resource setting, REP(UNI)+WDROP and REP(SIM)+WDROP are slightly better than ADV in computational time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Perturbed Inputs</head><p>Recent studies have used perturbations, especially adversarial perturbations, to improve the robustness of encoder-decoders <ref type="bibr" target="#b28">(Sato et al., 2019;</ref><ref type="bibr" target="#b6">Cheng et al., 2019;</ref>. In particular, <ref type="bibr" target="#b6">Cheng et al. (2019)</ref> analyzed the robustness of models trained with their adversarial perturbations over perturbed inputs. Following them, we also investigate the robustness of our trained Transformer (big) models.</p><p>We constructed perturbed inputs by replacing words in source sentences based on pre-defined ratio. If the ratio is 0.0, we use the original source sentences. In contrast, if the ratio is 1.0, we use the completely different sentences as source sentences. We set the ratio 0.01, 0.05, and 0.10. In this process, we replaced a randomly selected word with a word sampled from vocabulary based on uniform distribution. We applied this procedure to source sentences in newstest2010-2016. <ref type="table" target="#tab_11">Table 6</ref> shows averaged BLEU scores 8 of each method on perturbed newstest2010-2016. These BLEU scores are calculated against the original reference sentences. This table indicates that all perturbations improved the robustness of the Transformer (big) because their BLEU scores are better than one in the setting w/o perturbation. In comparison among perturbations, REP(SIM) (and REP(SIM)+WDROP) achieved significantly better scores than others on perturbed inputs. We emphasize that REP(SIM) surpassed ADV even though ADV is originally proposed to improve the robustness of models. This result implies that REP(SIM) is effective to construct robust models as well as to improve the performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on GEC</head><p>Datasets Following <ref type="bibr" target="#b14">Kiyono et al. (2020)</ref>, we used a publicly available dataset from the BEA shared task <ref type="bibr">(Bryant et al., 2019)</ref>. This dataset contains training, validation, and test splits. We also used the CoNLL-2014 test set (CoNLL) <ref type="bibr" target="#b21">(Ng et al., 2014)</ref> as an additional test set. We report F 0.5 score measured by the ERRANT scorer <ref type="bibr" target="#b4">(Bryant et al., 2017;</ref><ref type="bibr" target="#b10">Felice et al., 2016)</ref> for the BEA dataset and M 2 scorer <ref type="bibr" target="#b7">(Dahlmeier and Ng, 2012)</ref> for CoNLL.</p><p>Methods We used the same settings as <ref type="bibr" target="#b14">Kiyono et al. (2020)</ref>. Specifically, we trained Transformer (big) model w/o perturbation on the same parallel pseudo data provided by <ref type="bibr" target="#b14">Kiyono et al. (2020)</ref>, and then fine-tuned the model with perturbations. The hyper-parameters for perturbations are the same as those described in Section 4.1.</p><p>Results <ref type="table" target="#tab_13">Table 7</ref> shows the results of each method. This table reports the averaged score of five models trained with different random seeds. Moreover, we present the scores of <ref type="bibr" target="#b14">Kiyono et al. (2020)</ref>; our "w/o perturbation" model is a rerun of their work, that is, the experimental settings are identical 9 . <ref type="table" target="#tab_13">Table 7</ref> shows that all perturbations improved the scores except for REP(UNI) and REP(SIM) in the BEA test set (Test). Similar to the machine translation results, the simple methods WDROP, REP(UNI)+WDROP, and REP(SIM)+WDROP achieved comparable scores to ADV. Thus, these faster methods are also effective for the GEC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Word Replacement The naive training method of neural encoder-decoders has a discrepancy between training and inference; we use the correct tokens as inputs of the decoder in the training phase but use the token predicted at the previous time step as an input of the decoder in the inference phase. To address this discrepancy, <ref type="bibr" target="#b1">Bengio et al. (2015)</ref> proposed the scheduled sampling that stochastically uses the token sampled from the output probability distribution of the decoder as an input instead of the correct token.  modified the sampling method to improve the performance. In addition, <ref type="bibr" target="#b9">Duckworth et al. (2019)</ref> refined the algorithm to be suited to Transformer <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>. Their method is faster than the original scheduled sampling but slower and slightly worse than more simple replacement methods such as REP(UNI) and REP(SIM) in our experiments. <ref type="bibr" target="#b43">Xie et al. (2017)</ref> and <ref type="bibr" target="#b15">Kobayashi (2018)</ref> used the unigram language model and neural language model respectively to sample tokens for word replacement. In this study, we ignored contexts to simplify the sampling process, and indicated that such simple methods are effective for sequence-to-sequence problems.</p><p>Word Dropout <ref type="bibr" target="#b11">Gal and Ghahramani (2016)</ref> applied word dropout to a neural language model and it is a common technique in language modeling <ref type="bibr" target="#b17">(Merity et al., 2018;</ref><ref type="bibr" target="#b44">Yang et al., 2018;</ref><ref type="bibr" target="#b40">Takase et al., 2018)</ref>. <ref type="bibr" target="#b31">Sennrich and Zhang (2019)</ref> reported that word dropout is also effective for low resource machine translation. However, word dropout has not been commonly used in the existing sequenceto-sequence systems. Experiments in this study show that word dropout is not only fast but also contributes to improvement of scores in various sequence-to-sequence problems.</p><p>Adversarial Perturbations Adversarial perturbations were first discussed in the field of image processing <ref type="bibr" target="#b37">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b12">Goodfellow et al., 2015)</ref>. In the NLP field, <ref type="bibr" target="#b18">Miyato et al. (2017)</ref> applied adversarial perturbations to an embedding space and reported its effectiveness on text classification tasks. In sequence-to-sequence problems,  and <ref type="bibr" target="#b28">Sato et al. (2019)</ref> applied adversarial perturbations to embedding spaces in neural encoder-decoders. Moreover, <ref type="bibr" target="#b28">Sato et al. (2019)</ref> used virtual adversarial training <ref type="bibr" target="#b19">(Miyato et al., 2016)</ref> in training of their neural encoderdecoders. <ref type="bibr" target="#b6">Cheng et al. (2019)</ref> computed tokenlevel adversarial perturbations in machine translation. In other words, they introduced the strategy of adversarial perturbations into word replacement.</p><p>Their method is also effective but requires more computational time than  and <ref type="bibr" target="#b28">Sato et al. (2019)</ref> because it runs language models to obtain candidate tokens for perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We compared perturbations for neural encoderdecoders in view of computational time. Experimental results show that simple techniques such as word dropout <ref type="bibr" target="#b11">(Gal and Ghahramani, 2016)</ref> and random replacement of input tokens achieved comparable scores to sophisticated perturbations: the scheduled sampling <ref type="bibr" target="#b1">(Bengio et al., 2015)</ref> and adversarial perturbations <ref type="bibr" target="#b28">(Sato et al., 2019)</ref>, even though those simple methods are faster. In the low resource setting in machine translation, adversarial perturbations achieved high BLEU score but those simple methods also achieved comparable scores to the adversarial perturbations when we spent almost the same time for training. For the robustness of trained models, REP(SIM) is superior to others. This study indicates that simple methods are sufficiently effective, and thus, we encourage using such simple perturbations as a first step. In addition, we hope for researchers of perturbations to use the simple perturbations as baselines to make the usefulness of their proposed method clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiments on Summarization</head><p>We conduct experiments on two summarization datasets: Annotated English Gigaword <ref type="bibr" target="#b20">(Napoles et al., 2012;</ref><ref type="bibr" target="#b27">Rush et al., 2015)</ref> and DUC 2004 task 1 <ref type="bibr" target="#b24">(Over et al., 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Annotated English Gigaword</head><p>Datasets We used sentence-summary pairs extracted from Annotated English Gigaword <ref type="bibr" target="#b20">(Napoles et al., 2012;</ref><ref type="bibr" target="#b27">Rush et al., 2015)</ref> as the summarization dataset. This dataset contains 3.8M sentence-summary pairs as the training set and 1951 pairs as the test set. We extracted 3K pairs from the original validation set, which contains 190K pairs, for our validation set.</p><p>In summarization, most recent studies used large scale corpora to pre-train their neural encoderdecoder <ref type="bibr" target="#b8">(Dong et al., 2019;</ref><ref type="bibr" target="#b32">Song et al., 2019;</ref><ref type="bibr" target="#b26">Qi et al., 2020)</ref>. Thus, we also augmented the training data. We extracted the first sentence and headline of a news article in REALNEWS <ref type="bibr" target="#b45">(Zellers et al., 2019)</ref> and News Crawl <ref type="bibr">(Barrault et al., 2019)</ref> as sentence-summary pairs. In total, we used 17.1M sentence-summary pairs as our training data.</p><p>We used BPE <ref type="bibr" target="#b30">(Sennrich et al., 2016b)</ref> to construct a vocabulary set. We set the number of BPE merge operations at 32K and shared the vocabulary between both the encoder and decoder sides.</p><p>Methods We followed the configuration in Section 4.2 because it seems suitable for a large amount of training data. We used the same perturbations and hyper-parameters as in Section 4.2.</p><p>Results <ref type="table" target="#tab_15">Table 8</ref> shows the ROUGE F 1 scores of each method and scores reported in recent studies <ref type="bibr" target="#b8">(Dong et al., 2019;</ref><ref type="bibr" target="#b32">Song et al., 2019;</ref><ref type="bibr" target="#b26">Qi et al., 2020)</ref> In this experiment, we cannot report the result of ADV because the loss value of ADV exploded during training. We tried several random seeds for ADV, but all models failed to converge. Since we need a huge amount of budget to search more suitable hyper-parameters for ADV in this summarization dataset, we consider that it is impractical to report the result of ADV. <ref type="table" target="#tab_15">Table 8</ref> indicates that all perturbations improved the ROUGE score. In addition, REP(UNI), REP(SIM), WDROP, and their combinations outperformed the scheduled sampling. Thus, these fast methods are also superior perturbations in the summarization task. Moreover, REP(UNI) and  WDROP outperformed the current top score <ref type="bibr" target="#b26">(Qi et al., 2020)</ref> in ROUGE-1, L and ROUGE-2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DUC 2004 Task 1</head><p>Datasets We used sentence-summary pairs extracted from Annotated English Gigaword <ref type="bibr" target="#b20">(Napoles et al., 2012;</ref><ref type="bibr" target="#b27">Rush et al., 2015)</ref> and News Crawl <ref type="bibr">(Barrault et al., 2019)</ref> as our training dataset, which contains 10.1M pairs. Following recent studies <ref type="bibr" target="#b39">(Takase and Okazaki, 2019;</ref><ref type="bibr" target="#b38">Takase and Kobayashi, 2020)</ref>, we used BPE to construct a vocabulary set for the encoder side and characters as vocabulary for the decoder side. We set the number of BPE merge operations at 16K. For the test set, we used the DUC 2004 task 1 dataset <ref type="bibr" target="#b24">(Over et al., 2007)</ref> which contains 500 source sentences and four kinds of manually constructed reference summaries. We truncated characters over 75 bytes in each generated summary based on the official configuration.</p><p>Methods We used the Transformer (big) setting in this experiment. In addition, we introduced the output length control method proposed by <ref type="bibr" target="#b39">Takase and Okazaki (2019)</ref>. We used the same perturbations and hyper-parameters as in Section 4.1.</p><p>Results <ref type="table" target="#tab_15">Table 8</ref> shows recall-based ROUGE scores of each method and scores reported in recent studies <ref type="bibr" target="#b27">(Rush et al., 2015;</ref><ref type="bibr" target="#b36">Suzuki and Nagata, 2017;</ref><ref type="bibr" target="#b39">Takase and Okazaki, 2019;</ref><ref type="bibr" target="#b38">Takase and Kobayashi, 2020)</ref>. We also cannot report the result of ADV for the same reason as described in Appendix A.1.</p><p>This  <ref type="formula" target="#formula_2">(2020)</ref> -32.57 11.63 28.24 <ref type="table">Table 9</ref>: Recall-based ROUGE-1, 2, and L (R-1, R-2, and R-L respectively) on DUC 2004 task 1 test set. The lower part represents the scores reported by recent studies.</p><p>formed the current top score in ROUGE-1, 2, and L. Moreover, WDROP achieved better ROUGE-1 and L scores than the current top score. In contrast, REP(UNI) slightly harmed the performance in this configuration. These results indicate that WDROP and REP(SIM) are also effective for summarization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Positions <ref type="formula" target="#formula_0">2010</ref>     <ref type="table" target="#tab_0">Table 12</ref>: BLEU scores when we inject perturbations to a source sentence with 0.10.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Negative log-likelihood (NLL) values, BLEU scores of each method, and time to achieve BLEU score of Transformer w/o perturbation on validation set (newstest2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Setting</cell><cell cols="2">Genuine Synthetic</cell></row><row><cell>Standard</cell><cell>4.5M</cell><cell>-</cell></row><row><cell>High Resource</cell><cell>4.5M</cell><cell>20.0M</cell></row><row><cell>Low Resource</cell><cell>160K</cell><cell>-</cell></row></table><note>summarizes the number of training data in each configuration. Moreover, we conduct experiments</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Sizes of training datasets on our machine translation experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>reported their method was the most effective when they applied their ADV to both encoder and decoder sides. However, we do not have evidence for suitable sides in applying other perturbations. Thus, we applied REP(UNI), 2010 22.06 22.43 26.11 27.13 29.70 34.40 26.59 REP(UNI) enc 24.26 21.95 22.33 25.76 26.70 29.08 34.61 26.38 dec 24.27 21.99 22.29 26.31 27.28 29.74 34.42 26.61 both 24.30 22.20 22.43 26.06 26.82 29.42 34.13 26.48 REP(SIM) enc 24.12 22.02 22.14 26.21 27.01 29.33 34.56 26.48 dec 24.32 21.96 22.55 26.36 27.23 29.86 34.33 26.66 both 23.94 21.85 22.29 25.84 26.61 29.50 34.20 26.32 WDROP enc 24.31 22.12 22.45 26.20 27.09 29.95 34.58 26.67 dec 23.96 22.08 22.22 26.36 27.08 29.91 33.98 26.51 both 24.33 22.14 22.35 26.10 26.82 29.51 34.51</figDesc><table><row><cell></cell><cell></cell><cell>2011</cell><cell>2012</cell><cell>2013</cell><cell>2014</cell><cell>2015</cell><cell>2016</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell cols="3">Transformer (base)</cell><cell></cell><cell></cell></row><row><cell>w/o perturbation</cell><cell>-</cell><cell cols="6">24.27 26.54</cell></row><row><cell></cell><cell></cell><cell cols="2">Transformer (big)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o perturbation</cell><cell>-</cell><cell cols="6">24.22 22.11 22.69 26.60 28.46 30.50 33.58</cell><cell>26.88</cell></row><row><cell></cell><cell>enc</cell><cell cols="6">24.79 22.49 23.10 27.07 28.39 30.52 34.51</cell><cell>27.27</cell></row><row><cell>REP(UNI)</cell><cell>dec</cell><cell cols="6">24.33 22.34 22.63 26.93 28.22 30.36 33.41</cell><cell>26.89</cell></row><row><cell></cell><cell>both</cell><cell cols="6">24.75 22.68 23.32 27.01 28.89 31.38 34.94</cell><cell>27.57</cell></row><row><cell></cell><cell>enc</cell><cell cols="6">24.68 22.91 23.13 27.03 28.25 30.81 34.40</cell><cell>27.32</cell></row><row><cell>REP(SIM)</cell><cell>dec</cell><cell cols="6">24.51 22.22 22.83 26.46 28.64 30.68 33.58</cell><cell>26.99</cell></row><row><cell></cell><cell>both</cell><cell cols="6">24.77 22.50 23.10 26.91 28.98 31.03 34.29</cell><cell>27.37</cell></row><row><cell></cell><cell>enc</cell><cell cols="6">24.60 22.32 23.27 27.07 28.40 31.00 34.61</cell><cell>27.32</cell></row><row><cell>WDROP</cell><cell>dec</cell><cell cols="6">24.53 22.33 22.75 27.00 28.56 30.58 33.20</cell><cell>26.99</cell></row><row><cell></cell><cell>both</cell><cell cols="6">24.92 22.71 23.40 27.11 28.73 30.99 34.80</cell><cell>27.52</cell></row><row><cell>REP(UNI)+WDROP</cell><cell>both</cell><cell cols="6">24.82 22.82 23.38 27.30 28.56 30.65 35.02</cell><cell>27.51</cell></row><row><cell>REP(SIM)+WDROP</cell><cell>both</cell><cell cols="6">24.83 22.95 23.40 27.23 28.65 30.88 35.05</cell><cell>27.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>BLEU scores on newstest2010-2016 and averaged scores. Bolds are better scores than w/o perturbations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>22.06 22.43 26.11 27.13 29.70 34.40 26.59 ×1.00 REP(UNI) dec 24.27 21.99 22.29 26.31 27.28 29.74 34.42 26.61 ×0.99 REP(SIM) dec 24.32 21.96 22.55 26.36 27.23 29.86 34.33 26.66 ×0.95 WDROP enc 24.31 22.12 22.45 26.20 27.09 29.95 34.58 26.67 ×1.00 REP(SS) dec 24.18 22.03 22.38 26.04 27.15 29.77 34.24 26.54</figDesc><table><row><cell></cell><cell></cell><cell>2011</cell><cell>2012</cell><cell>2013</cell><cell>2014</cell><cell>2015</cell><cell>2016</cell><cell>Average Speed</cell></row><row><cell></cell><cell></cell><cell cols="3">Transformer (base)</cell><cell></cell><cell></cell></row><row><cell>w/o perturbation</cell><cell>-</cell><cell cols="6">24.27 ×0.88</cell></row><row><cell>ADV</cell><cell>both</cell><cell cols="6">24.34 22.19 22.58 26.19 27.10 29.78 34.89</cell><cell>26.72</cell><cell>×0.44</cell></row><row><cell></cell><cell></cell><cell cols="3">Transformer (big)</cell><cell></cell><cell></cell></row><row><cell>w/o perturbation</cell><cell>-</cell><cell cols="6">24.22 22.11 22.69 26.60 28.46 30.50 33.58</cell><cell>26.88</cell><cell>×0.60</cell></row><row><cell>REP(UNI)</cell><cell>both</cell><cell cols="6">24.75 22.68 23.32 27.01 28.89 31.38 34.94</cell><cell>27.57</cell><cell>×0.60</cell></row><row><cell>REP(SIM)</cell><cell>both</cell><cell cols="6">24.77 22.50 23.10 26.91 28.98 31.03 34.29</cell><cell>27.37</cell><cell>×0.55</cell></row><row><cell>WDROP</cell><cell>both</cell><cell cols="6">24.92 22.71 23.40 27.11 28.73 30.99 34.80</cell><cell>27.52</cell><cell>×0.60</cell></row><row><cell>REP(UNI)+WDROP</cell><cell>both</cell><cell cols="6">24.82 22.82 23.38 27.30 28.56 30.65 35.02</cell><cell>27.51</cell><cell>×0.60</cell></row><row><cell>REP(SIM)+WDROP</cell><cell>both</cell><cell cols="6">24.83 22.95 23.40 27.23 28.65 30.88 35.05</cell><cell>27.57</cell><cell>×0.55</cell></row><row><cell>REP(SS)</cell><cell>dec</cell><cell cols="6">24.44 21.97 22.74 26.77 28.44 30.83 33.71</cell><cell>26.99</cell><cell>×0.52</cell></row><row><cell>ADV</cell><cell>both</cell><cell cols="6">24.71 22.60 23.23 26.98 28.97 30.49 34.40</cell><cell>27.34</cell><cell>×0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>BLEU scores on newstest2010-2016, averaged scores, and computational speeds based on Transformer (base) w/o perturbation. Scores in bold denote the best result for each set for Transformer (base) and (big) settings.</figDesc><table><row><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>23.79 25.01 28.43 32.06 33.28 37.40 29.43 WDROP both 26.65 24.34 25.18 28.66 32.25 33.75 37.65 29.78 REP(UNI)+WDROP both 26.45 24.07 25.09 28.72 32.21 33.42 37.68 29.66 REP(SIM)+WDROP both 26.55 24.20 25.19 28.55 31.92 33.64 37.96</figDesc><table><row><cell></cell><cell></cell><cell>2011</cell><cell>2012</cell><cell>2013</cell><cell>2014</cell><cell>2015</cell><cell>2016</cell><cell>Average</cell></row><row><cell>w/o perturbation</cell><cell>-</cell><cell cols="6">25.63 23.62 24.54 28.39 31.50 32.96 36.47</cell><cell>29.02</cell></row><row><cell>REP(UNI)</cell><cell>both</cell><cell cols="6">26.36 24.18 25.14 28.54 32.35 33.80 37.73</cell><cell>29.73</cell></row><row><cell>REP(SIM)</cell><cell>both</cell><cell cols="6">26.04 29.72</cell></row><row><cell>REP(SS)</cell><cell>dec</cell><cell cols="6">25.81 23.64 24.73 28.46 31.84 33.29 36.59</cell><cell>29.19</cell></row><row><cell>ADV</cell><cell>both</cell><cell cols="6">25.79 24.07 24.92 28.64 32.04 33.35 37.20</cell><cell>29.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>BLEU scores of each method trained with a large amount of data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>BLEU scores in the low resource setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>27.57 26.58 23.14 18.93 REP(SIM) 27.37 26.95 25.34 23.13 WDROP 27.52 26.48 22.84 18.56 REP(UNI)+WDROP 27.51 26.55 23.18 19.05 REP(SIM)+WDROP 27.57 27.15 25.60 23.58</figDesc><table><row><cell>Method</cell><cell>0.00</cell><cell>0.01</cell><cell>0.05</cell><cell>0.10</cell></row><row><cell>w/o perturbation</cell><cell cols="4">26.88 25.81 21.94 17.80</cell></row><row><cell>REP(UNI)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>REP(SS)</cell><cell cols="4">26.99 25.89 22.08 17.93</cell></row><row><cell>ADV</cell><cell cols="4">27.34 26.32 22.43 18.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Averaged BLEU scores on newstest2010-2016</cell></row><row><cell>when we inject perturbations to a source sentence with</cell></row><row><cell>each ratio.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>F 0.5 scores of each method. The row of Kiyono et al. (2020) represents the reported scores of the model trained with the same configuration.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: F 1 values of ROUGE-1, 2, and L (R-1, R-2,</cell></row><row><cell>and R-L respectively) on the test set extracted from An-</cell></row><row><cell>notated English Gigaword. The lower part represents</cell></row><row><cell>the scores reported by recent studies.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>table indicates that REP(SIM), WDROP, and their combination improved the ROUGE scores. In particular, REP(SIM)+WDROP outper-</figDesc><table><row><cell>Method</cell><cell>Position</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>w/o perturbation</cell><cell>-</cell><cell cols="3">32.80 11.55 28.26</cell></row><row><cell>REP(UNI)</cell><cell>both</cell><cell cols="3">32.56 11.48 28.21</cell></row><row><cell>REP(SIM)</cell><cell>both</cell><cell cols="3">32.80 11.55 28.28</cell></row><row><cell>WDROP</cell><cell>both</cell><cell cols="3">33.06 11.45 28.51</cell></row><row><cell>REP(UNI)+WDROP</cell><cell>both</cell><cell cols="3">32.15 11.58 28.01</cell></row><row><cell>REP(SIM)+WDROP</cell><cell>both</cell><cell cols="3">32.80 11.73 28.46</cell></row><row><cell>REP(SS)</cell><cell>dec</cell><cell cols="3">32.83 11.41 28.14</cell></row><row><cell>Rush et al. (2015)</cell><cell>-</cell><cell>28.18</cell><cell cols="2">8.49 23.81</cell></row><row><cell>Suzuki and Nagata (2017)</cell><cell>-</cell><cell cols="3">32.28 10.54 27.80</cell></row><row><cell>Takase and Okazaki (2019)</cell><cell>-</cell><cell cols="3">32.29 11.49 28.03</cell></row><row><cell>Takase and Kobayashi</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>2011 2012 2013 2014 2015 2016 Average w/o perturbation -23.37 21.23 21.89 25.56 26.97 29.34 32.32</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>25.81</cell></row><row><cell>REP(UNI)</cell><cell>both</cell><cell>23.88 21.85 22.48 26.23 27.81 30.21 33.61</cell><cell>26.58</cell></row><row><cell>REP(SIM)</cell><cell>both</cell><cell>24.51 22.24 22.82 26.53 28.44 30.43 33.68</cell><cell>26.95</cell></row><row><cell>WDROP</cell><cell>both</cell><cell>24.01 21.90 22.48 26.24 27.60 29.71 33.44</cell><cell>26.48</cell></row><row><cell>REP(UNI)+WDROP</cell><cell>both</cell><cell>23.85 22.03 22.69 26.63 27.50 29.56 33.60</cell><cell>26.55</cell></row><row><cell>REP(SIM)+WDROP</cell><cell>both</cell><cell>24.47 22.61 23.15 26.88 28.24 30.14 34.53</cell><cell>27.15</cell></row><row><cell>REP(SS)</cell><cell>dec</cell><cell>23.49 21.18 21.82 25.79 27.17 29.39 32.38</cell><cell>25.89</cell></row><row><cell>ADV</cell><cell>both</cell><cell>23.94 21.70 22.46 25.99 27.71 29.28 33.13</cell><cell>26.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>BLEU scores when we inject perturbations to a source sentence with 0.01.</figDesc><table><row><cell>Method</cell><cell cols="3">Positions 2010 2011 2012 2013 2014 2015 2016 Average</cell></row><row><cell>w/o perturbation</cell><cell>-</cell><cell>19.78 18.51 18.70 21.58 22.74 24.81 27.45</cell><cell>21.94</cell></row><row><cell>REP(UNI)</cell><cell>both</cell><cell>21.02 19.38 19.67 22.76 23.85 26.08 29.24</cell><cell>23.14</cell></row><row><cell>REP(SIM)</cell><cell>both</cell><cell>23.13 21.13 21.60 24.98 26.69 28.38 31.49</cell><cell>25.34</cell></row><row><cell>WDROP</cell><cell>both</cell><cell>20.94 19.24 19.44 22.41 23.67 25.42 28.74</cell><cell>22.84</cell></row><row><cell>REP(UNI)+WDROP</cell><cell>both</cell><cell>20.99 19.64 19.93 22.89 23.77 25.64 29.40</cell><cell>23.18</cell></row><row><cell>REP(SIM)+WDROP</cell><cell>both</cell><cell>23.20 21.55 21.87 25.53 26.50 28.49 32.05</cell><cell>25.60</cell></row><row><cell>REP(SS)</cell><cell>dec</cell><cell>20.06 18.58 18.90 21.92 23.01 24.59 27.51</cell><cell>22.08</cell></row><row><cell>ADV</cell><cell>both</cell><cell>20.45 18.91 19.10 22.02 23.50 24.97 28.05</cell><cell>22.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11 :</head><label>11</label><figDesc>BLEU scores when we inject perturbations to a source sentence with 0.05.</figDesc><table><row><cell>Method</cell><cell cols="3">Positions 2010 2011 2012 2013 2014 2015 2016 Average</cell></row><row><cell>w/o perturbation</cell><cell>-</cell><cell>16.21 15.03 15.31 17.82 17.76 19.91 22.57</cell><cell>17.80</cell></row><row><cell>REP(UNI)</cell><cell>both</cell><cell>17.24 15.84 16.39 18.62 19.30 21.45 23.64</cell><cell>18.93</cell></row><row><cell>REP(SIM)</cell><cell>both</cell><cell>21.15 19.18 19.79 22.95 23.91 26.19 28.73</cell><cell>23.13</cell></row><row><cell>WDROP</cell><cell>both</cell><cell>16.79 15.54 16.06 18.35 18.68 20.57 23.96</cell><cell>18.56</cell></row><row><cell>REP(UNI)+WDROP</cell><cell>both</cell><cell>17.53 16.00 16.41 18.95 19.40 21.03 24.01</cell><cell>19.05</cell></row><row><cell>REP(SIM)+WDROP</cell><cell>both</cell><cell>21.58 19.86 20.10 23.50 24.22 26.27 29.55</cell><cell>23.58</cell></row><row><cell>REP(SS)</cell><cell>dec</cell><cell>16.31 15.21 15.18 18.01 18.11 20.00 22.69</cell><cell>17.93</cell></row><row><cell>ADV</cell><cell>both</cell><cell>16.47 15.24 15.50 18.01 18.07 19.84 23.44</cell><cell>18.08</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As reported in<ref type="bibr" target="#b23">Ott et al. (2018)</ref>, the BLEU score from SacreBLEU is often lower than the score from multi-bleu.perl but SacreBLEU is suitable for scoring WMT datasets<ref type="bibr" target="#b25">(Post, 2018)</ref>.3 https://github.com/pytorch/fairseq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We regard processed tokens per second as the computational speed of each method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">data.statmt.org/news-crawl/de/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">In the low resource setting, we applied only WDROP to an encoder side for REP(UNI)+WDROP and REP(SIM)+WDROP because the configuration achieved better performance than applying both perturbations to both sides.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For more details, Tables 10, 11, and 12 in Appendix show BLEU scores on each perturbed input.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">The scores of w/o perturbation are slightly worse than<ref type="bibr" target="#b14">Kiyono et al. (2020)</ref>. We consider that this is due to randomness in the training procedure.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Motoki Sato for sharing his code with us to compare adversarial perturbations. We thank Jun Suzuki and Sosuke Kobayashi for their valuable comments. We thank anonymous reviewers for their useful suggestions. This work was supported by JSPS KAKENHI Grant Number JP18K18119 and JST ACT-X Grant Number JPMJAX200I. The first author is supported by Microsoft Research Asia (MSRA) Collaborative Research Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (WMT19)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation (WMT)</title>
		<meeting>the Fourth Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Advances in Neural Information Processing Systems 28 (NIPS)</title>
		<imprint>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Andersen, and Ted Briscoe. 2019. The BEA-2019 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<biblScope unit="page" from="52" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="793" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tagged back-translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation (WMT)</title>
		<meeting>the Fourth Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust neural machine translation with doubly adversarial inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4324" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Better Evaluation for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32 (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Parallel scheduled sampling. CoRR, abs/1906.04331</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic Extraction of Learner Errors in ESL Sentences Using Linguistically Enhanced Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A nested attention neural hybrid model for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Massive exploration of pseudo data for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2134" to="2145" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Train large, then compress: Rethinking model size for efficient training and inference of transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11432" to="11442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularizing and Optimizing LSTM Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributional smoothing with virtual adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annotated Gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 Shared Task on Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<title level="m">Duc in context. Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2401" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective adversarial regularization for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoki</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="204" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Revisiting lowresource neural machine translation: A case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th</title>
		<meeting>the 57th</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cutting-off redundant repeating generations for neural abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="291" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
		<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">All word embeddings from one embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33 (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3775" to="3785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Positional encoding to control output sequence length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3999" to="4004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Direct output connection for a high-rank language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4599" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving neural language modeling via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6555" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Data noising as smoothing in neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32 (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9054" to="9065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bridging the gap between training and inference for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
