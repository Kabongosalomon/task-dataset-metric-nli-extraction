<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Learned Confidence for Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong</forename><forename type="middle">Min</forename><surname>Kye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae</forename><forename type="middle">Beom</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoirin</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
							<email>sjhwang82@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AITRICS</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Learned Confidence for Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transductive inference is an effective means of tackling the data deficiency problem in few-shot learning settings. A popular transductive inference technique for few-shot metric-based approaches, is to update the prototype of each class with the mean of the most confident query examples, or confidence-weighted average of all the query samples. However, a caveat here is that the model confidence may be unreliable, which may lead to incorrect predictions. To tackle this issue, we propose to meta-learn the confidence for each query sample, to assign optimal weights to unlabeled queries such that they improve the model's transductive inference performance on unseen tasks. We achieve this by meta-learning an input-adaptive distance metric over a task distribution under various model and data perturbations, which will enforce consistency on the model predictions under diverse uncertainties for unseen tasks. Moreover, we additionally suggest a regularization which explicitly enforces the consistency on the predictions across the different dimensions of a high-dimensional embedding vector. We validate our few-shot learning model with meta-learned confidence on four benchmark datasets, on which it largely outperforms strong recent baselines and obtains new state-of-the-art results. Further application on semi-supervised few-shot learning tasks also yields significant performance improvements over the baselines. The source code of our algorithm is available at https://github.com/seongmin-kye/MCT.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot learning, the problem of learning under data scarcity, is an important challenge in deep learning as large number of training instances may not be available in many real-world settings. While the recent advances in meta-learning made it possible to obtain impressive performance on few-shot learning tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>, it still remains challenging in cases where we are given very little information (e.g. one-shot learning). Some of the metric-based meta-learning approaches tackle this problem using transductive learning or semi-supervised learning, by leveraging the structure of the unlabeled instances at the inference time <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>. Popular approach for these problem includes leveraging nearest neighbor graph for propagating labels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40]</ref>, or using predicted soft or hard labels on unlabeled samples to update the class prototype <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>. However, all these transductive or semi-supervised inference approaches are fundamentally limited by the intrinsic unreliability of the labels predicted on the unseen samples.</p><p>In this work, we aim to tackle this problem by proposing a novel confidence-based transductive inference scheme for metric-based meta-learning models. Specifically, we first propose to meta-learn the distance metric (or metric) to assign different confidence scores to each query (or test) instance for each class, such that the updated prototypes obtained by confidence-weighted averaging of the queries improve classification of the query samples. This is done by learning a metric length-scale term for each individual instance or a pair of instances. However, the confidence prediction on the test instances for unseen task should be inevitably unreliable, since the samples come from an unknown distribution. To account for such uncertainties of prediction on an unseen task, we further propose (a) ProtoNets (+4.44%) (b) Instance-wise metric (+8.89%) (c) MCT (+15.56%) <ref type="figure" target="#fig_1">Figure 1</ref>: Transductive inference with confidence scores. We visualize t-SNE embeddings on a 3-way 1-shot task, where each color stands for different class. The numbers show the accuracy increase after transduction for this task. The transparency shows the confidence scores for red class. to generate various model and data perturbations, such as random dropping of residual blocks and random augmentations. This randomness helps the model better learn the confidence measure by considering various uncertainties for an unseen task (see <ref type="figure" target="#fig_1">Figure 1</ref>), and also allows us to take an ensemble over the confidence measures under random perturbations at test time. We refer to this transductive inference using meta-learned input-adaptive confidence under various perturbations as Meta-Confidence Transduction (MCT).</p><p>To further enhance the reliability of the output confidence, we introduce additional regularizations to enforce consistency among the transformed samples in the embedding space. Specifically, we compose episodes with differently augmented support and query set and train the model to enforce the distribution of these two sets to be close to each other. Moreover, we also enforce consistency among the dimension-wise classification of the high-dimensional embedding vectors, such that their predictions are coherent.</p><p>We validate our transductive inference scheme for metric-based meta-learning models on four benchmark datasets against existing transductive approaches, which shows that the models using meta-learned confidence significantly outperform existing transductive inference methods, and obtain new state-of-the-art results. We further verify the generality of our MCT on semi-supervised learning tasks, where we assign confidence scores to unlabeled data. The results show that MCT outperforms relevant baselines by large margins, which shows the efficacy of our method. Further ablation studies show that both meta-learning of the input-adaptive distance metric and various perturbations are crucial in the success of our method in assigning correct confidence to each test sample.</p><p>Our main contributions are as follows:</p><p>• We propose to meta-learn an input-adaptive distance metric, which allows to output an accurate and reliable confidence for an unseen test samples that can directly improve upon the transductive inference performance. • To further enhance the reliability of the learned confidence, we introduce various types of model and data perturbations during meta-learning, such that the meta-learned confidence can better account for uncertainties at unseen tasks. • We suggest consistency regularizations across different perturbations and predictions for each embedding dimension, which improves the consistency of the embeddings. • We validate our model on four benchmark datasets for few-shot classification and achieve new state-of-the-art results, largely outperforming all baselines. Further experimental validation of our model on semi-supervised few-shot learning also verifies its efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Distance-based meta-learning for few-shot classification The goal of few-shot classification is to correctly classify query set examples given only a handful of support set examples. Due to its limited amount of data, each task-specific classifier should resort to the meta-knowledge accumulated from the previous tasks, which is referred to as meta-learning <ref type="bibr" target="#b33">[34]</ref>. Meta-learning of few-shot classification can roughly be divided into several categories such as optimization-based method <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref>, distance-based approaches <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>, class or task-wise network modulation with amortization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>, or some combination of those approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref>. We use a distance-based approach in this work, which allows us to directly compare distance between examples on a metric space. For example, Matching Networks <ref type="bibr" target="#b37">[38]</ref> use cosine distance, whereas Prototypical Networks <ref type="bibr" target="#b28">[29]</ref> use euclidean distance with each class prototype set to the mean of support embeddings.</p><p>Transductive learning Since few-shot classification is intrinsically challenging, we may assume that we can access other unlabeled query examples, which is called transductive learning <ref type="bibr" target="#b35">[36]</ref>. Here we name a few recent works. TPN <ref type="bibr" target="#b19">[20]</ref> constructs a nearest-neighbor graph and propagate labels to pseudo-label the unlabeled query examples. EGNN <ref type="bibr" target="#b12">[13]</ref> similarly constructs a nearest-neighbor graph, but utilizes both edge and node features in the update steps. On the other hand, Hou et al. <ref type="bibr" target="#b10">[11]</ref> tries to update class prototypes by picking top-k confident queries with their own criteria. Our approach also updates class prototypes for each transduction step, but makes use of all the query examples instead of a small subset of k examples. Semi-supervised learning In the few-shot classification, semi-supervised learning can access additional large amount of unlabeled data. Ren et al. <ref type="bibr" target="#b24">[25]</ref> proposed several variants of soft k-means method in prototypical networks <ref type="bibr" target="#b28">[29]</ref>, where soft label is predicted confidence of unlabeled sample. Li et al. <ref type="bibr" target="#b17">[18]</ref> proposed the self-training method with pseudo labeling module based on gradient descent approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. Basically, if an unlabeled query set is used for few-shot classification instead of an additional unlabeled set, it becomes transductive learning, and vice versa. Our approach has connection to soft k-means method of Ren et al. <ref type="bibr" target="#b24">[25]</ref>, but we predict the confidence with input-adaptive distance metric and use meta-learned confidence under various perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Few-shot Classification</head><p>We start by introducing notations. In the conventional C-way N -shot classification, we first sample C classes randomly from the entire set of classes, and then sample N and M examples from each class for the support set and query set, respectively. We define this sampling distribution as p(τ ). As a result, we have a support set</p><formula xml:id="formula_0">S = {(x i , y i )} C×N i=1 and query set Q = {(x i ,ỹ i )} C×M i=1 , where y,ỹ ∈ {1, .</formula><p>. . , C} are the class labels. If some portion of the support set is unlabeled, then the problem becomes semi-supervised learning. The convention for the evaluation of few-shot classification models is to use N ∈ {1, 5} (i.e. 1or 5-shot) and M = 15.</p><p>The goal of few-shot classification is to correctly classify query examples in Q given the support set S. Since S includes only a few examples for each class, conventional learning algorithms will mostly fail due to overfitting (e.g. consider 1-shot classification). Thus, most existing approaches tackle this problem by meta-learning over a task distribution p(τ ), such that the later tasks can benefit from the knowledge obtained over the previous training episodes.</p><p>One of the most popular and successful approaches for few-shot classification is the metric-based approach, in which we aim to learn an embedding function f θ (x) ∈ R l that maps an input x to a latent embedding z in an l-dimensional metric space (which is usually the penultimate layer of a convolutional network). Support set and query examples are then mapped into this space, such that we can measure the distance between class prototypes and query embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transductive Inference with Soft k-means</head><p>We now describe and discuss transductive inference using the confidence scores of query examples computed by soft k-means algorithm <ref type="bibr" target="#b24">[25]</ref>. Suppose that we are given an episode consisting of support set S and query set Q. We also define S c as the set of support examples in class c and Q x = {x 1 , . . . ,x C×M } as the set of all query instances. Starting from prototypical networks <ref type="bibr" target="#b28">[29]</ref>, we first compute the initial prototype P</p><formula xml:id="formula_1">(0) c = 1 |Sc| x∈Sc f θ (x) for each class c = 1, . . . , C.</formula><p>Then, for each step t = 1, . . . , T , and for each query examplex ∈ Q x , we compute its confidence score, which denote the probability of it belonging to each class c, as follows:</p><formula xml:id="formula_2">q (t−1) c (x) = exp(−d(f θ (x), P (t−1) c )) C c =1 exp(−d(f θ (x), P (t−1) c ))<label>(1)</label></formula><p>where d(·, ·) is Euclidean distance and P (t−1) denotes t − 1 steps updated prototype. We then update the prototypes of class c based on the confidence scores (or soft labels) q</p><formula xml:id="formula_3">(t−1) c (x) for allx ∈ Q x : P (t) c = x∈Sc 1 · f θ (x) + x∈Q x q (t−1) c (x) · f θ (x) x∈Sc 1 + x∈Q x q (t−1) c (x)<label>(2)</label></formula><p>which is the weighted average that we previously mentioned. Note that the confidence of the support examples is always 1, since their class labels are observed. We repeat the process until t = 1, . . . , T .  Questions However, confidence-based transduction, such as soft k-means, leads to a couple of new questions, which is the focus of this work: 1) Is using the confidence of the model indeed helpful in transductive inference? 2) Can we trust the model confidence that is output from the few-shot task?</p><p>4 Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Meta-Confidence Transduction</head><p>In order to address the first question, we propose to Meta-Confidence Transduction (MCT). As shown in the method overview in <ref type="figure" target="#fig_0">Figure 2</ref>, we meta-learn the distance metric by learning an input-dependent temperature scaling for confidence, using the various perturbations on confidence in training.</p><p>Meta-learning confidence with input-adaptive distance metric We first propose to meta-learn the input-adaptive metric by performing transductive inference during training with query instances, to obtain a metric that yield performance improvements when performing transductive inference using it. Specifically, we meta-learn the distance metric d φ in Eq. <ref type="formula" target="#formula_4">(3)</ref>, which we define as Euclidean distance with normalization and instance-wise metric scaling g I φ , or pair-wise metric scaling g P φ :</p><formula xml:id="formula_4">d I φ (a 1 , a 2 ) = a 1 / a 1 2 g I φ (a 1 ) − a 2 / a 2 2 g I φ (a 2 ) 2 2 , d P φ (a 1 , a 2 ) = a 1 / a 1 2 g P φ (a 1 , a 2 ) − a 2 / a 2 2 g P φ (a 1 , a 2 ) 2 2<label>(3)</label></formula><p>for all a 1 , a 2 ∈ R l . Note that the normalization allows the confidence to be mainly determined by metric scaling. In order to obtain the optimal scaling function g φ ∈ {g I φ , g P φ } for transduction, we first compute the query likelihoods after T transduction steps, and then optimize φ, the parameter of the scaling function g φ by minimizing the following instance-wise loss for d φ ∈ {d I φ , d P φ }:</p><formula xml:id="formula_5">L τ I (θ, φ) = 1 |Q| (x,ỹ)∈Q − log p(ỹ|x, S; θ, φ) (4) = 1 |Q| (x,ỹ)∈Q d φ (f θ (x), P (T ) c ) + C c =1 exp(−d φ (f θ (x), P (T ) c )) .<label>(5)</label></formula><p>As for g φ , we simply use a CNN with fully-connected layers which takes either the feature map of an instance or the concatenated feature map of a pair of instances as an input. We set the number of transduction steps to T = 1 for training to minimize the computational cost, but use T = 10 for test.</p><p>Model and data perturbations The model confidence from few-shot tasks is intrinsically unreliable due to the data scarcity problems, even if the model has been meta-learned over similar tasks. One way to output more reliable and consistent confidence scores is to enforce the model to output consistent predictions while perturbing either the model or the data. In this work, we consider the following two sources of perturbations:</p><p>• Model perturbation: We consider two confidence scores, one from the full network (fullpath) and the other from a sub-network generated by dropping a block (drop-path) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> from the full network. • Data perturbation: We also consider two confidence scores, one from the original image and the other from horizontally flipped image. Require: Full-path embedding function f θ and block-dropped embedding function f D θ . Require: Flip augmentation Aug(·) and define f A θ as f θ (Aug(·)).</p><formula xml:id="formula_6">1: h θ ← Sample from {f θ , f D θ , f A θ , f A,D θ } Select a confidence space 2: for c ∈ {1, . . . , C} do 3: P c ← 1 |Sc| x∈Sc h θ (x).</formula><p>Compute prototype on confidence space</p><formula xml:id="formula_7">4: for c ∈ {1, . . . , C} do 5: qc(x) ← exp −d(h θ (x), P c ) C c =1 exp −d(h θ (x), P c )</formula><p>for allx ∈ Qx Compute confidence score 6:</p><formula xml:id="formula_8">Pc ← x∈Sc 1 · f θ (x) + x∈Q x qc(x) · f θ (x) x∈Sc 1 + x∈Q x qc(x)</formula><p>Compute prototype on full-path space</p><formula xml:id="formula_9">7: J ← 0 Initialize loss 8: for (x,ỹ) ∈ Q do 9: J ← J + 1 |Qx| d(f θ (x), Pỹ) + log c exp −d(f θ (x), P c ) Update loss</formula><p>By jointly considering these two sources of perturbations, we can have a total of four (2 × 2) scenarios (or sources) of possible transductive inferences. As shown in Algorithm 1, at training time, we randomly select a source of confidence and simulate a single transduction step. However at test time, we perform transductive inference for all scenarios using the ensemble confidence obtained from all perturbed sources. This process is done T times to get the final confidence scores. By doing so, we can enforce the model to consistently perform well under various transduction scenarios with different perturbations, leading to better performance due to the ensemble effect of meta-learned confidences (see the Section C of the appendix for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Consistency Regularization</head><p>In order to address the second question, we suggest consistency regularization for data and embedding. The quality of the confidence scores can be improved with consistency regularization. In semisupervised learning, consistency regularization is one of the most popular techniques as it allows for unlabeled examples to output consistent predictions under various perturbations, thereby improving the quality of the confidence scores <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Consistency over data perturbation We also propose to enforce the model to output consistent predictions under various perturbations of the query examples. The idea is that, even though we perturb the query examples by large amount, a good and discriminative embedding space for transductive inference should be able to correctly classify them. Specifically, we apply only the horizontal flipping and shifting to the examples in the support set (weak augmentation), whereas we apply horizontal flipping, shifting, RandAugment <ref type="bibr" target="#b2">[3]</ref> and CutOut <ref type="bibr" target="#b4">[5]</ref> to the examples in the query set (strong augmentation), and perform classification with those augmentations. This approach is related to FixMatch <ref type="bibr" target="#b29">[30]</ref> algorithm for semi-supervised learning, but we apply various augmentations to disjoint sets rather than to the same instance, which allows to achieve the same effect as a regularization without an explicit consistency loss.</p><p>Consistency over dimensions of embedding space Dense classification (DC) <ref type="bibr" target="#b18">[19]</ref> achieves successful performance improvement in few-shot classification. However, they apply spatial pooling to feature maps, in order to make embeddings at testing. This causes unnecessary bottlenecks, making it difficult to completely use the learned spatial information. To alleviate this problem, we reinterpret DC as a regularizer on the high dimensional embedding being learned. In other words, we do not apply spatial pooling at both training and testing, and then use flattened feature map as the embedding for each instance. We found that computing the distance with densely matching the spatial embeddings improves performance, without any additional parameters. When training with DC, we additionally compute dimension-wise loss L τ D , the average classification loss for each dimension of embedding (e.g. 64-way classification for miniImageNet). Hence, final learning objective is</p><formula xml:id="formula_10">L = E p(τ ) [λL τ I + L τ D ],</formula><p>where L τ I is the instance-wise loss in Eq. 5 and λ is the balancing factor.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Dataset We validate our method on four popular benchmark datasets for few-shot classification. 1) miniImageNet, 2) tieredImageNet, 3) CIFAR-FS, and 4) FC100. Please see the Section A.1 of the appendix regarding the detailed information for each of the datasets.</p><p>Experimental setting Here we mention a few important experimental settings of our model. During training, we apply the weight decay of 0.0005, and unless otherwise indicated, apply the augmentations proposed in Section 4.2 by default. When the image size is 32 × 32, we apply maxpooling only to the second and the fourth layer to increase the dimensionality of the final embedding space. For our full models, we evaluate the expectation over task distribution p(τ ) via Monte-Carlo (MC) approximation with a single sample during training to obtain the learning objective, where we set λ = 0.5 which we found with a validation set. More details (e.g. learning rate scheduling, detailed network architectures and settings for semi-supervised experiment) can be found in the Section A and B of the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>Inductive inference We first examine the results of inductive inference. We define Meta-Confidence Induction (MCI) as an our proposed metric with consistency regularizations only. The top rows of <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> show the accuracy of MCI and the existing inductive inference methods for few-shot classification. Our model achieves new state-of-the-art results for inductive inferecne models on all four benchmark datasets with significant margins. This performance gain is coming from both the consistency regularization over the data perturbation and on the dimensions of the embeddings. We analyze each component in detail in the following sections. <ref type="table">Table 4</ref>: Semi-supervised few-shot classification performance. We consider 5-way classification on miniIm-ageNet ('mini') and tieredImageNet ('tiered'). The baseline results are drawn from <ref type="bibr" target="#b17">[18]</ref>. All results are based on pre-trained ResNet-12 with full dataset in conventional supervised manner. "w/D" means that unlabeled set includes 3 distracting classes, which does not overlap the label space of the support set <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>.   Transductive inference The bottom rows of <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> show the results of transductive inference with the baselines and our full model, Meta-Confidence Transduction (MCT), which performs transductive inference with the meta-learned confidence. We again achieve new state-of-the-art results on all the datasets, with particularly good performance on one-shot classification. For fair comparsion against TPN <ref type="bibr" target="#b19">[20]</ref> and EGNN <ref type="bibr" target="#b12">[13]</ref> that use different backbone networks, we further perform an additional experiments using shallow backbone networks in <ref type="table" target="#tab_5">Table 3</ref>. Again, our model largely outperforms all baselines. Note that we use MCT without model perturbation (block drop) since ConvNet-64 and ConvNet-256 do not have skip connections.</p><formula xml:id="formula_11">Model</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised inference</head><p>We also perform experiments on semi-supervised classification in <ref type="table">Table 4</ref> to further validate the effectiveness and generality of our MCT. We follow the same experimental setting described in Li et al. <ref type="bibr" target="#b17">[18]</ref>. In the semi-supervised setting, instead of computing the confidence scores of query examples, we compute the confidence scores of unlabeled support examples in order to update the class prototype. Again, our MCT largely outperforms all the baselines including the recent LST model. The results demonstrate the effectiveness of our consistency regularizations and the distance metric scaling for correctly assigning confidence scores to unlabeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies</head><p>We next perform ablation studies of our model on miniImageNet dataset to identify from where the performance improvements come from. We use prototypical networks (PN) with ResNet-12 backbone networks for these experiments, including only a single component of MCT at a time.  Effect of the distance metrics We first study the effect of the distance metric in <ref type="table" target="#tab_4">Table 5</ref>. The performance in the transductive inference columns correspond to each of the models with the transductive inference with naive soft k-means algorithm <ref type="bibr" target="#b24">[25]</ref> without model and data perturbations. We see that the PN with metric scaling underperforms the plain PN with Euclidean distance. On the other hand, the proposed instance-wise and pair-wise metric significatly outperform both distances in both inductive and transductive inference settings, demonstrating the effectiveness of our input-dependent metric scaling methods over globally shared metric scaling. In <ref type="figure" target="#fig_3">Figure 3</ref>, we observe that instance-wise metric scaling assigns various scales to different inputs, whereas the pair-wise metric scaling assigns low values between the samples from the same class and high values between samples from different classes.  Effect of the model / data perturbation In <ref type="table" target="#tab_7">Table 6</ref>, We analyze the contribution of each type of uncertainty to the reliability of confidence. We observe that the performance of transductive inference improves as we add in each type of uncertainties. We use negative log-likelihood (NLL) as the quality measure for the confidence scores: the lower the NLL, the closer the confidence scores to the target label. We observe that both types of uncertainties are helpful in improving the reliability of the output confidence.  "w" and "s" denote "weak" and "strong", respectively. Detailed results with confidence interval can be found in Section D of the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the perturbation of query examples</head><p>We next analyze the effect of our augmentation strategy. We see from <ref type="table">Table 4</ref> that applying weak augmentations (horizontal flipping and shifting) to the examples in the support set while applying strong augmentations (horizontal flipping, shifting, RandAugment <ref type="bibr" target="#b2">[3]</ref> and CutOut <ref type="bibr" target="#b4">[5]</ref>) to the examples in the query set (weak-strong pair, i.e. w/s) outperforms other possible combinations of weak and strong augmentations. This result is reasonable since the class prototypes should remain as a stable target for stable training, while for query examples it may be beneficial to make them as diverse as possible, for the meta-learned confidence to account for various uncertainties with transductive inference.  Effect of the across-dimension consistency Lastly, we compare the effect of our consistency regularization across embedding dimensions with existing dense classification (DC) methods. In <ref type="table" target="#tab_9">Table 7</ref>, we see that embedding without global average pooling (GAP) outperforms the model with GAP, demonstrating the effectiveness of dense matching of spatial features that gets rid of unnecessary bottlenecks. Also, unlike the existing DC which train pixel-wise classifiers during training and instance-wisely predict at test time, our method has a consistent framework as it has an additional instance-wise loss term (Eq. (5)) that is used both at training and test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Using unlabeled data for few-shot learning, either test instances themselves (transductive) or others (semi-supervised) could help with predictions. Yet, they should be assigned correct confidence scores for optimal performance gains. In this work, we proposed to tackle them by meta-learning confidence scores, such that the prototypes updated with meta-learend scores optimize for the transductive inference performance. Specifically, we proposed to meta-learn the parameter of the length-scaling function, such that the proper distance metric for the confidence scores can be automatically determined. We also consider model and data-level uncertainties for unseen examples, for more robust confidence estimation. Moreover, to enhance the quality of confidence scores, we suggest a consistency regularization for data and embedding, which allows for consistent prediction under various perturbations. We experimentally validate our transductive inference model on four benchmark datasets and obtain state-of-the-art performances on both transductive and semi-supervised few-shot classification tasks. Further ablation studies confirm the effectiveness of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In real world scenarios, we may not have large amount of labeled data to train an accurate and reliable model on the target task, but should nevertheless obtain desired level of accuracy. To learn an accurate prediction model under such data-scarce scenarios, we may further exploit the unlabeled data given at test time (transductive inference), or extra unlabeled data during training (semi-supervised learning). Our model is especially helpful when using such unlabeled data to aid the learning with scarce data, as it is able to output accurate confidence scores for the unlabeled examples such that they help with the transductive inference or semi-supervised learning. Such low-resource learning can greatly reduce either the training time (for transductive inference) or human labeling cost (for semi-supervised learning) since we only need a few training data points to train a classifier that obtains the desirable level of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Datasets</head><p>We validate our method on four benckmark datasets for few-shot classification.</p><p>1) miniImageNet. This dataset <ref type="bibr" target="#b37">[38]</ref> consists of a subset of 100 classes sampled from the ImageNet dataset <ref type="bibr" target="#b26">[27]</ref>. Each class has 600 images, resized to 84 × 84 pixels. We use the split of 64/16/20 for training/validation/test.</p><p>2) tieredImageNet. This dataset <ref type="bibr" target="#b24">[25]</ref> is another subset of ImageNet, that consists of 779, 165 images of 84 × 84 pixels collected from 608 classes. The task is to generalize the few-shot classifier over 34 different superclasses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Network architectures</head><p>We consider ResNet-12 backbone and conventional 4-block convolutional networks with 64-64-64-64 (ConvNet-64) or 64-96-128-256 (ConvNet-256) channels for each layer. We implement the metric scaling function as a single convolutional block followed by two fully-connected layers (FC-layers).</p><p>The convolutional block consists of 3x3 convolution, batch normalization, ReLU activation and 2x2 max pooling. The first FC-layer is followed by batch normalization and ReLU activation, whereas the last FC-layer followed by sigmoid function to ensure non-negativity. Finally, in order to balance the effect of the scaling and normalized distance on confidence, we apply scaling (exp(α)) and shifting (exp(β)) to the output of the sigmoid function, where α and β are initialized to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameters</head><p>We apply dropout to each layer with the ratio of 0.1. We use SGD optimizer with the Nesterov momentum of 0.9 and set the weight decay to 0.0005. Following Snell et al. <ref type="bibr" target="#b28">[29]</ref>, we use higher way (15-way) classification for training and 5-way for test. The number of query examples for each class is set to 8 for training and 15 for test. For miniImageNet, CIFAR-FS and FC100, we set the initial learning rate to 0.1 and cut it to 0.006 and 0.0012 at 25, 000 and 35, 000 episodes, respectively. For tieredImageNet, we set the initial learning rate to 0.1 and decay it by a factor of 10 at every 20, 000 episode until convergence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Importance of model and data perturbations</head><p>To further investigate the effect of the model and data perturbations on confidence, we report more results of MCI and MCT in <ref type="table" target="#tab_12">Table 8</ref>. MCI is the model that is trained with inductive manner, whereas MCT is transductively trained with model and data perturbations. We see that the MCT outperforms MCI in both inductive and transductive settings. It means that meta-learned confidence with perturbations allows us to obtain more reliable confidence, which is further helpful for transduction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Detailed Result for Augmentation Strategy</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview. (a) To capture data uncertainty, we randomly apply horizontal flip augmentation to the whole data in episode. (b) Along with data uncertainty, we randomly drop the last residual block to capture the model uncertainty. (c) In order to efficiently train the confidence under these perturbations, we meta-learn the input-adaptive distance metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Meta-learning confidence with model and data perturbation. Require: The set of support examples Sc, for each class c ∈ {1, . . . , C}. Require: The set of all query examples (x,ỹ) ∈ Q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Histogram of metric scale, on a miniImageNet 5-way 5-shot task. σ corresponds to g φ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Test accuracy with various augmentation pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Average classification performance over 1000 randomly generated episodes, with 95% confidence intervals. We consider 5-way classification on all the datasets. * denotes it is reported from<ref type="bibr" target="#b39">[40]</ref>.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Backbone</cell><cell cols="2">miniImageNet 1-shot 5-shot</cell><cell cols="2">tieredImageNet 1-shot 5-shot</cell></row><row><cell></cell><cell>MTL  *  [31]</cell><cell>ResNet-12</cell><cell cols="4">61.20±1.80 75.53±0.80 65.62±1.80 80.61±0.90</cell></row><row><cell></cell><cell>TapNet [41]</cell><cell>ResNet-12</cell><cell cols="4">61.65±0.15 76.36±0.10 63.08±0.15 80.26±0.12</cell></row><row><cell></cell><cell>TADAM [22]</cell><cell>ResNet-12</cell><cell cols="4">58.56±0.39 76.65±0.38 62.13±0.31 81.92±0.30</cell></row><row><cell>Inductive</cell><cell>MetaOpt-SVM [15] Dense [19]</cell><cell>ResNet-12 ResNet-12</cell><cell cols="4">62.64±0.61 78.63±0.46 65.99±0.72 81.56±0.53 61.26±0.20 79.01±0.13 --</cell></row><row><cell></cell><cell>CAN [11]</cell><cell>ResNet-12</cell><cell cols="4">63.85±0.48 79.44±0.34 69.89±0.51 84.23±0.37</cell></row><row><cell></cell><cell>MCI (Pair)</cell><cell>ResNet-12</cell><cell cols="4">64.49±0.64 81.63±0.44 68.41±0.73 84.60±0.50</cell></row><row><cell></cell><cell>MCI (Instance)</cell><cell>ResNet-12</cell><cell cols="4">65.34±0.63 82.15±0.45 69.66±0.72 85.29±0.49</cell></row><row><cell></cell><cell>TPN [20]</cell><cell>ConvNet-64</cell><cell cols="4">55.51±0.86 69.86±0.65 59.91±0.94 73.30±0.75</cell></row><row><cell></cell><cell>EGNN  *  [13]</cell><cell cols="5">ConvNet-256 59.63±0.52 76.34±0.48 63.52±0.52 80.24±0.49</cell></row><row><cell></cell><cell>TEAM [23]</cell><cell>ResNet-18</cell><cell>60.07</cell><cell>75.90</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MAML+SCA [1]</cell><cell>DenseNet</cell><cell cols="2">62.86±0.79 77.46±1.18</cell><cell>-</cell><cell>-</cell></row><row><cell>Transductive</cell><cell>Fine-tuning [6] SIB [12]</cell><cell>WRN-28-10 WRN-28-10</cell><cell cols="4">65.73±0.68 78.40±0.52 73.34±0.71 85.50±0.50 70.0±0.6 79.2±0.4 --</cell></row><row><cell></cell><cell>CAN + Top-k [11]</cell><cell>ResNet-12</cell><cell cols="4">67.19±0.55 80.64±0.35 73.21±0.58 84.93±0.38</cell></row><row><cell></cell><cell>DPGN [40]</cell><cell>ResNet-12</cell><cell cols="4">67.77±0.32 84.60±0.43 72.45±0.51 87.24±0.39</cell></row><row><cell></cell><cell>MCT (Pair)</cell><cell>ResNet-12</cell><cell cols="4">76.16±0.89 85.22±0.42 80.68±0.89 86.63±0.89</cell></row><row><cell></cell><cell>MCT (Instance)</cell><cell>ResNet-12</cell><cell cols="4">78.55±0.86 86.03±0.42 82.32±0.81 87.36±0.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average classification performance on CIFAR-FS and FC100.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Backbone</cell><cell cols="2">CIFAR-FS 1-shot 5-shot</cell><cell>1-shot</cell><cell>FC100</cell><cell>5-shot</cell></row><row><cell></cell><cell>TADAM [22]</cell><cell>ResNet-12</cell><cell>-</cell><cell>-</cell><cell cols="2">40.1±0.4</cell><cell>56.1±0.4</cell></row><row><cell></cell><cell>MetaOpt-SVM [15]</cell><cell>ResNet-12</cell><cell cols="4">72.00±0.70 84.20±0.50 41.10±0.60 55.50±0.60</cell></row><row><cell>Inductive</cell><cell>Dense [19]</cell><cell>ResNet-12</cell><cell>-</cell><cell>-</cell><cell cols="2">42.04±0.17 57.05±0.16</cell></row><row><cell></cell><cell>MCI (Pair)</cell><cell>ResNet-12</cell><cell cols="4">76.23±0.72 88.39±0.44 43.01±0.58 59.67±0.56</cell></row><row><cell></cell><cell>MCI (Instance)</cell><cell>ResNet-12</cell><cell cols="4">77.84±0.64 89.11±0.45 44.69±0.60 60.33±0.59</cell></row><row><cell></cell><cell>Fine-tuning [6]</cell><cell cols="5">WRN-28-10 76.58±0.68 85.79±0.50 43.16±0.59 57.57±0.55</cell></row><row><cell></cell><cell>SIB [12]</cell><cell>WRN-28-10</cell><cell>80.0±0.6</cell><cell>85.3±0.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Transductive</cell><cell>DPGN [40]</cell><cell>ResNet-12</cell><cell cols="2">77.90±0.50 90.20±0.40</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MCT (Pair)</cell><cell>ResNet-12</cell><cell cols="4">87.28±0.70 90.50±0.43 51.27±0.80 62.59±0.60</cell></row><row><cell></cell><cell>MCT (Instance)</cell><cell>ResNet-12</cell><cell cols="4">85.61±0.69 90.03±0.46 51.16±0.88 63.28±0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>8±0.7 84.4±0.5 76.9±0.7 86.3±0.5 69.6±0.7 81.3±0.5 74.5±0.7 84.0±0.5</figDesc><table><row><cell></cell><cell cols="2">mini</cell><cell cols="2">tiered</cell><cell cols="2">mini w/D</cell><cell cols="2">tiered w/D</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Masked Soft k-Means [25]</cell><cell>62.1</cell><cell>73.6</cell><cell>68.6</cell><cell>81.0</cell><cell>61.0</cell><cell>72.0</cell><cell>66.9</cell><cell>80.2</cell></row><row><cell>TPN [20]</cell><cell>62.7</cell><cell>74.2</cell><cell>72.1</cell><cell>83.3</cell><cell>61.3</cell><cell>72.4</cell><cell>71.5</cell><cell>82.7</cell></row><row><cell>LST [17]</cell><cell>70.1</cell><cell>78.7</cell><cell>77.7</cell><cell>85.2</cell><cell>64.1</cell><cell>77.4</cell><cell>73.5</cell><cell>83.4</cell></row><row><cell>MCT (Instance)</cell><cell>73.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Average classification performance over 1000 randomly generated episodes, with 95% confidence intervals. d(·, ·) denotes Euclidean distance. s ∈ R is a learnable parameter initialized to 7.5, following<ref type="bibr" target="#b21">[22]</ref>.</figDesc><table><row><cell>Model</cell><cell>Distance Metric</cell><cell cols="2">Inductive 1-shot 5-shot</cell><cell>Transductive 1-shot 5-shot</cell></row><row><cell>Prototypical Networks (PN) [29]</cell><cell>d(a1, a2)</cell><cell cols="3">57.36±0.66 75.59±0.51 68.58±0.92 78.71±0.53</cell></row><row><cell>PN + metric scaling [22]</cell><cell>s · d(a1, a2)</cell><cell cols="3">55.43±0.67 74.52±0.49 68.34±0.87 78.57±0.51</cell></row><row><cell>PN + Instance-wise metric (Eq. 3)</cell><cell>d I φ (a1, a2)</cell><cell cols="3">61.08±0.66 77.26±0.46 70.34±0.87 79.54±0.54</cell></row><row><cell>PN + Pair-wise metric (Eq. 3)</cell><cell>d P φ (a1, a2)</cell><cell cols="3">61.81±0.58 77.67±0.50 71.95±0.81 81.06±0.51</cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell>Backbone</cell><cell>miniImageNet 1-shot 5-shot</cell></row><row><cell></cell><cell></cell><cell>TPN [20]</cell><cell>ConvNet-64</cell><cell>55.51±0.86 69.86±0.65</cell></row><row><cell></cell><cell></cell><cell>MCT (Instance)</cell><cell>ConvNet-64</cell><cell>63.53±0.91 75.15±0.56</cell></row><row><cell></cell><cell></cell><cell>EGNN [13]</cell><cell cols="2">ConvNet-256 59.63±0.52 76.34±0.48</cell></row><row><cell></cell><cell></cell><cell cols="3">MCT (Instance) ConvNet-256 70.10±0.87 80.56±0.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison with other transductive models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Test NLL vs. performance of transductive inference with pair-wise distance metric. NLL is computed just before taking the initial transductive step.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>The inductive inference performance with various dimension-wise classification methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Thus the entire dataset is split into 20/6/8 superclasses for training/validation/test, where each superclass contains 351, 97, and 160 low-level classes, respectively.3) CIFAR-FS. This dataset<ref type="bibr" target="#b1">[2]</ref> is a variant of CIFAR-100 dataset used for few-shot classification, which contains 100 classes that describe general object categories. For each class, there are 600 images of 32 × 32 pixels. The dataset is split into 64/16/20 classes for training/validation/test.4) FC100. This is another few-shot classification dataset<ref type="bibr" target="#b21">[22]</ref> compiled by reorganizing the CIFAR-100 dataset. The task for this dataset is to generalize across 20 superclasses, as done with the tieredImageNet dataset. The superclasses are divided into 12/4/4 classes for training/validation/test, each of which contains 60/20/20 low-level classes, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>49±0.64 81.63±0.44 75.07±0.89 84.09±0.47 MCT (Pair) 64.89±0.63 82.48±0.42 76.16±0.89 85.22±0.42 MCI (Instance) 65.34±0.63 82.15±0.45 76.21±0.82 84.49±0.46 MCT (Instance) 66.47±0.63 83.29±0.42 78.55±0.86 86.03±0.42</figDesc><table><row><cell>Model</cell><cell>Inductive 1-shot 5-shot</cell><cell>Transductive 1-shot 5-shot</cell></row><row><cell>MCI (Pair)</cell><cell>64.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Effect of model and data perturbations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Result of augmentation strategy on pair-wise distance metric. 21±0.61 75.99±0.52 64.43±0.62 78.49±0.50 strong strong 60.93±0.65 76.59±0.45 69.55±0.71 79.15±0.52 strong weak 56.75±0.64 74.32±0.50 66.12±0.60 75.71±0.52 weak strong 61.81±0.58 77.67±0.50 71.95±0.81 81.06±0.51</figDesc><table><row><cell cols="2">Support Query</cell><cell cols="2">Inductive</cell><cell cols="2">Transductive</cell></row><row><cell>Aug</cell><cell>Aug</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>weak</cell><cell>weak</cell><cell>58.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Result of augmentation strategy on instance-wise distance metric. 90±0.61 74.76±0.47 67.25±0.89 76.68±0.55 strong strong 59.74±0.62 75.59±0.49 69.01±0.93 78.99±0.53 strong weak 58.09±0.64 74.03±0.48 67.94±0.88 77.24±0.55 weak strong 61.08±0.66 77.26±0.46 69.45±0.86 79.54±0.54</figDesc><table><row><cell cols="2">Support Query</cell><cell cols="2">Inductive</cell><cell cols="2">Transductive</cell></row><row><cell>Aug</cell><cell>Aug</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>weak</cell><cell>weak</cell><cell>58.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Settings for Semi-Supervised Few-shot Classification</head><p>We split both miniImageNet and tieredImageNet into labeled and unlabeled sets, following previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. Before we train the model with semi-supervised learning, we pre-train the model with conventional supervised manner (e.g. 64-way classification for miniImageNet). At the training phase, we additionally use 15 instances for each class. At test phase, we use 30 and 50 unlabeled instances for each class on 1-shot and 5-shot task, respectively, following Li et al. <ref type="bibr" target="#b17">[18]</ref>. For fair comparison with masked soft k-means of Ren et al. <ref type="bibr" target="#b24">[25]</ref>, we use single update step with unlabeled set for both training and testing.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Detailed Explanation of Meta-Confidence Transduction C.1 Design choices</head><p>Model perturbation To generate the model uncertainty, we drop the last residual block in residual network <ref type="bibr">(ResNet)</ref>. As discussed in Veit et al. <ref type="bibr" target="#b36">[37]</ref>, dropping single upper block in ResNet doesn't significantly affect model performance. Furthermore, we empirically found that block drop allows us to obtain the model with less dependency rather than dropout.</p><p>Data perturbation There can be various choices of augmentation method to perturb the data. However, we found that large transformation from the raw image can degrade classification accuracy at inference, causing large information loss on few data regime. Thus, we choose horizontal flip augmentation, which can perturb the data without losing information, to obtain perturbed confidences in training and testing consistently.</p><p>Optimization The reason we optimize only a single full-path is as follows. First, since we randomly apply horizontal flipping to the whole data in each episode, perturbed spaces with flipped images are optimized through the sequence of episodes. Secondly, as drop-path is one of the ensemble path of full-path, it is jointly optimized with full-path <ref type="bibr" target="#b36">[37]</ref>.  for c = 1, . . . , C do <ref type="bibr">6:</ref> for h θ in F do 7:</p><p>for allx ∈ Q x Compute local confidence <ref type="bibr" target="#b7">8</ref>:</p><p>Obtain ensemble confidence score <ref type="bibr">9:</ref> for h θ in F do 10:</p><p>Update class c prototype for each space</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Transductive inference</head><p>As shown in Algorithm 2, we update the class prototypes by considering various types of uncertainties. Given an episode consisting of raw images, we generate another episode by flipping the original images. First, prototypes of full-path and drop-path are obtained by averaging embedding of support set. By using these prototypes, we compute the confidence scores for each space and class, respectively. With the ensemble confidence score obtained from various spaces and queries, we update prototypes of each space. Then, we repeatedly update the prototype T times by using an averaged confidence. Finally, q (T ) (x) is used for inference. The size of circles shows the confidence score for the red class. Every figure is visualized by same task. conf denotes confidence. In each row, we show the transduction with local confidence and the transduction with ensemble confidence, where local confidence is derived from each space. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E Qualitative Analysis</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by self-critique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9936" to="9946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical data augmentation with no separate search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A two-stage approach to few-shot learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4005" to="4016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Empirical bayes transductive meta-learning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10276" to="10286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10276" to="10286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9258" to="9267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3603" to="3612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7957" to="7968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICJV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<title level="m">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to Learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<title level="m">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BlockDrop: Dynamic Inference Paths in Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dpgn: Distribution propagation graph network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06549</idno>
		<title level="m">Tapnet: Neural network augmented with task-adaptive projection for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shiarli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
