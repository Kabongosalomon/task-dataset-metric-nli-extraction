<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gating Revisited: Deep Multi-layer RNNs That Can Be Trained</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><forename type="middle">Ozgur</forename><surname>Turkoglu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">EcoVision Lab -Photogrammetry and Remote Sensing</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>D&amp;apos;aronco</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">EcoVision Lab -Photogrammetry and Remote Sensing</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Dirk</forename><surname>Wegner</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">EcoVision Lab -Photogrammetry and Remote Sensing</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Computational Science</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">EcoVision Lab -Photogrammetry and Remote Sensing</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gating Revisited: Deep Multi-layer RNNs That Can Be Trained</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Recurrent neural network</term>
					<term>Deep RNN</term>
					<term>Multi-layer RNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new STAckable Recurrent cell (STAR) for recurrent neural networks (RNNs), which has fewer parameters than widely used LSTM <ref type="bibr" target="#b15">[16]</ref> and GRU [10] while being more robust against vanishing or exploding gradients. Stacking recurrent units into deep architectures suffers from two major limitations: (i) many recurrent cells (e.g., LSTMs) are costly in terms of parameters and computation resources; and (ii) deep RNNs are prone to vanishing or exploding gradients during training. We investigate the training of multi-layer RNNs and examine the magnitude of the gradients as they propagate through the network in the "vertical" direction. We show that, depending on the structure of the basic recurrent unit, the gradients are systematically attenuated or amplified. Based on our analysis we design a new type of gated cell that better preserves gradient magnitude. We validate our design on a large number of sequence modelling tasks and demonstrate that the proposed STAR cell allows to build and train deeper recurrent architectures, ultimately leading to improved performance while being computationally more efficient. Index Terms-Recurrent neural network, Deep RNN, Multi-layer RNN. ! Konrad Schindler (M'05-SM'12) received the Diplomingenieur (M.Tech.) degree from Vienna</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recurrent Neural Networks (RNN) have established themselves as a powerful tool for modelling sequential data. They have led to significant progress for a variety of applications, notably language processing and speech recognition <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p><p>The basic building block of an RNN is a computational unit (or cell) that combines two inputs: the data of the current time step in the sequence and the unit's own output from the previous time step. While RNNs can in principle handle sequences of arbitrary and varying length, they are (in their basic form) challenged by long-term dependencies, since learning those would require the propagation of gradients over many time steps. To alleviate this limitation, gated architectures have been proposed, most prominently Long Short-Term Memory (LSTM) cells <ref type="bibr" target="#b15">[16]</ref> and Gated Recurrent Units (GRU) <ref type="bibr" target="#b9">[10]</ref>. They use gating mechanisms to store and propagate information over longer time intervals, thus mitigating the vanishing gradient problem.</p><p>In general, abstract features are often represented better by deeper architectures <ref type="bibr" target="#b4">[5]</ref>. In the same way that multiple hidden layers can be stacked in traditional feed-forward networks, multiple recurrent cells can also be stacked on top of each other, i.e., the output (or the hidden state) of the lower cell is connected to the input of the nexthigher cell, allowing for different dynamics. For instance one might expect low-level cues to vary more with lighting, whereas high-level representations might exhibit objectspecific variations over time. Several works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b48">[49]</ref> have shown the ability of deeper recurrent architectures to extract more complex features from the input and make better predictions. However, such architectures are usually composed of just two or three layers because training deeper recurrent architectures still presents an open problem. More specifically, deep RNNs suffer from two main shortcomings: (i) they are difficult to train because of gradient instability, i.e., the gradient either explodes or vanishes during training; and (ii) the large number of parameters contained in each single cell makes deep architectures extremely resourceintensive. Both issues restrict the practical use of deep RNNs and particularly their usage for image-like input data, which generally requires multiple convolutional layers to extract discriminative, abstract representations. Our work aims to address these weaknesses by designing a recurrent cell that, on the one hand, requires fewer parameters and, on the other hand, allows for stable gradient back-propagation during training; thus allowing for deeper architectures.</p><p>Contributions We present a detailed, theoretical analysis of how the gradient magnitude changes as it propagates through a cell in a deep RNN lattice. Our analysis offers a different perspective compared to existing literature about RNN gradients, as it focuses on the gradient flow across layers in depth direction, rather than the recurrent flow across time. We show that the two dimensions behave differently, i.e., the ability to preserve gradients in time direction does not necessarily mean that they are preserved across layers, too. We believe that the analysis in this paper contributes a further, complementary step towards a full understanding of gradient propagation in deep RNNs.</p><p>We leverage our analysis to design a new, lightweight gated cell, termed the STAckale Recurrent (STAR) unit. The STAR cell better preserves the gradient magnitude in the deep RNN lattice, while at the same time using fewer parameters than existing gated cells like LSTM <ref type="bibr" target="#b15">[16]</ref> and GRU <ref type="bibr" target="#b9">[10]</ref>.</p><p>We compare deep recurrent architectures built from different cells in an extensive set of experiments with several arXiv:1911.11033v4 [cs.CV] 6 Mar 2021 popular datasets. The results confirm our analysis: training very deep recurrent nets fails with most conventional units, whereas the proposed STAR unit allows for significantly deeper architectures. Moreover, our experiments show that the proposed cell outperforms alternative designs on several different tasks and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Vanishing or exploding gradients during training are a long-standing problem of recurrent (and other) neural networks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Perhaps the most effective measure to address them so far has been to introduce gating mechanisms in the RNN structure, as first proposed by <ref type="bibr" target="#b15">[16]</ref> in the form of the LSTM (long short-term memory), and later by other architectures such as gated recurrent units <ref type="bibr" target="#b9">[10]</ref>.</p><p>Importantly, RNN training needs proper initialisation. In <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref> it has been shown that initialising the weight matrices with identity and orthogonal matrices can be useful to stabilise the training. This idea is further develop in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[45]</ref>, where authors impose the orthogonality throughout the entire training to keep the amplification factor of the weight matrices close to unity, leading to a more stable gradient flow. Unfortunately, it has been shown <ref type="bibr" target="#b42">[43]</ref> that such hard orthogonality constraints hurt the representation power of the model and in some cases even destabilise the optimisation.</p><p>Another line of work has studied ways to mitigate the vanishing gradient problem by introducing additional (skip) connections across time and/or layers. Authors in <ref type="bibr" target="#b6">[7]</ref> have shown that skipping state updates in RNNs shrinks the effective computation graph and thereby helps to learn longer-range dependencies. Other works such as <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref> introduce a residual connection between LSTM layers; however, the performance improvements are limited. In <ref type="bibr" target="#b10">[11]</ref> the authors propose a gated feedback RNN that extends the stacked RNN architecture with extra connections. An obvious disadvantage of such an architecture are the extra computations and memory costs of the additional connections. Moreover, the authors only report results for rather shallow networks up to 3 layers.</p><p>Many of the aforementioned works propose new RNN architectures by leveraging a gradient propagation analysis. However all of these studies, as well as other studies which specifically aim at modelling accurately gradient propagation in RNNs <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b27">[28]</ref>, overlook the propagation of the gradient along the "vertical" depth dimension. In this work we will employ similar gradient analysis techniques, but focus on the depth dimension of the network.</p><p>Despite the described efforts, it remains challenging to train deep RNNs. In <ref type="bibr" target="#b48">[49]</ref> authors propose to combine LSTMs and highway networks <ref type="bibr" target="#b35">[36]</ref> to form Recurrent Highway Networks (RHN) and train deeper architectures. RHN are popular and perform well on language modelling tasks, but they are still prone to exploding gradients, as illustrated in our experiments. Another solution to alleviate gradient instability in deep RNNs was recently proposed in <ref type="bibr" target="#b24">[25]</ref>. The work suggests the use of a restricted RNN called IndRNN where all interactions are removed between neurons in the hidden state of a layer. This idea combined with the usage of batch normalization appears to greatly stabilize the gradient propagation through layers at the cost of a much lower representation power per layer. Such feature hinders IndRNN ability to achieve high performance for complex problems such as satellite image sequence classification or other computer vision tasks. In these tasks it is very important to merge information from neighboring pixels to increase the receptive field of the network so that the model has the ability to represent long-range spatial dependencies. Since IndRNN has no interaction between neurons it is difficult to achieve good spatio-temporal modeling effectively.</p><p>To process image sequence data, computer vision systems often rely on Convolutional LSTMs <ref type="bibr" target="#b45">[46]</ref>. But while very deep CNNs are very effective and now standard <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b34">[35]</ref>, stacks made of more than a few convLSTMs do not train well. Moreover, the computational cost increase rather quickly due to the large numbers of parameters in each LSTM cell. In practice, shallow versions are preferred, for instance <ref type="bibr" target="#b25">[26]</ref> use a single layer for action recognition, and <ref type="bibr" target="#b47">[48]</ref> use two layers to recognise hand gestures (combined with a deeper feature extractor without recursion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND AND PROBLEM STATEMENT</head><p>In this section we revisit the mathematics of RNNs with particular emphasis on the gradient propagation. We will then leverage this analysis to design a more stable recurrent cell, which is described in Sec. 4. A RNN cell is a non-linear transformation that maps the input signal x t at time t and the hidden state of the previous time step t−1 to the current hidden state h t :</p><formula xml:id="formula_0">h t = f (x t , h t−1 , W ) ,<label>(1)</label></formula><p>with W the trainable parameters of the cell. The input sequences have an overall length of T , which can vary. It depends on the task whether the final state h T , the complete sequence of states {h t }, or a single sequence label (typically defined as the average 1 T t h t ) are the desired target prediction for which loss L is computed. Learning amounts to fitting W to minimise the loss, usually with stochastic gradient descent.</p><p>When stacking multiple RNN cells on top of each other, the hidden state of the lower level l − 1 is passed on as input to the next-higher level l ( <ref type="figure" target="#fig_0">Fig. 1</ref>). In mathematical terms this corresponds to the recurrence relation</p><formula xml:id="formula_1">h l t = f (h l−1 t , h l t−1 , w) .<label>(2)</label></formula><p>Temporal unfolding leads to a two-dimensional lattice with depth L and length T ( <ref type="figure" target="#fig_0">Fig. 1</ref>), the forward pass runs from left to right and from bottom to top. Gradients flow in opposite direction: at each cell the gradient w.r.t. the loss arrives at the output gate and is used to compute the gradient w.r.t. (i) the weights, (ii) the input, and (iii) the previous hidden state. The latter two gradients are then propagated through the respective gates to the preceding cells in time and depth. In the following, we investigate how the magnitude of these gradients changes across the lattice. The analysis, backed up by numerical simulations, shows that common RNN cells are biased towards attenuating or amplifying the gradients and thus prone to destabilising the training of deep recurrent networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gradient Magnitudes</head><p>The gradient w.r.t. the trainable weights at a single cell in the lattice is</p><formula xml:id="formula_2">g w = ∂h l t ∂w g h l t ,<label>(3)</label></formula><p>where ∂h l t ∂w denotes the Jacobian matrix and g h l t is a column vector containing the partial derivatives of the loss w.r.t. the cell's output (hidden) state. From the equation, it becomes apparent that the Jacobian acts as a "gain matrix" on the gradients, and should on average preserve their magnitude to prevent them from vanishing or exploding. We obtain the recurrence for propagation by expanding the gradient</p><formula xml:id="formula_3">g h l t g h l t = ∂h l+1 t ∂h l t g h l+1 t + ∂h l t+1 ∂h l t g h l t+1 = J l+1 t g h l+1 t + H l t+1 g h l t+1 ,<label>(4)</label></formula><p>with J l t the Jacobian w.r.t. the input and H l t the Jacobian w.r.t. the hidden state. Ideally we would like the gradient magnitude g h l t 2 to remain stable for arbitrary l and t. Characterising that magnitude completely is difficult because correlations may exist between g h l+1 t and g h l t+1 for instance, due to weight sharing. Nonetheless, it is evident that the two Jacobians J l+1 t and H l t+1 play a fundamental role: if their singular values are small, they will attenuate the gradients and cause them to vanish sooner or later. If their singular values are large, they will amplify the gradients and make them explode. <ref type="bibr" target="#b0">1</ref> In the following, we analyse the behaviour of the two matrices for two widely used RNN cells. We first consider the most simple RNN cell, hereinafter called Vanilla RNN (vRNN). Its recurrence equation reads</p><formula xml:id="formula_4">h l t = tanh(W x h l−1 t + W h h l t−1 + b)<label>(5)</label></formula><p>from which we get the two Jacobians</p><formula xml:id="formula_5">J l t = D tanh(Wxh l−1 t +W h h l t−1 +b) W x<label>(6)</label></formula><formula xml:id="formula_6">H l t = D tanh(Wxh l−1 t +W h h l t−1 +b) W h<label>(7)</label></formula><p>where D x denotes a diagonal matrix with the elements of vector x as diagonal entries. Ideally, we would like to know the expected values of the two matrices' singular values. Unfortunately, there is no easy way to derive a closedform analytical expressions for them, but we can compute them for a fixed, representative point. The most natural and illustrative choice is to set h l−1 t = h l t−1 = 0 because (i) in practice, RNNs' initial hidden states are set to h l 0 = 0 (like in our experiments), and (ii) it is a stable and attracting fixed point so if the hidden state is perturbed around this point, it tends to return to its initial point. Note that this does not mean that the hidden state of the network is always equal to zero. The goal of the assumption is only simply to fix the value of the hidden state, in order to analyse the gradient propagation. We further choose weight matrices W h and W x with average singular value equal to one and b = 0 (different popular initialisation strategies, such as orthogonal and identity matrices, are aligned with this assumption). Moreover, according to <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, in the limit of a very wide network the parameters tend to stay close to their initial values, as a result the assumptions made are still legitimate during training (see the Appendix for empirical evidence). Since the derivative tanh (0) = 1, the average singular values of all matrices in Eq. (7) are equal to 1 in this configuration.</p><p>We expect to obtain a gradient g h l t with a larger magnitude by combining the contributions of g h l+1 t and g h l t+1 . To obtain a more precise estimate of the resulting gradient we should take into account the correlation between the two terms. However, if we examine two extreme cases (i) there is no or very small correlation between two gradient contributions, and (ii) they are highly (positively) correlated. The scaling factors of vRNN for the gradient are 1.414 and 2. respectively. Therefore, regardless of the correlation between the two terms, the gradient of vRNN is systematically growing while it propagates back in time and through layers. A deep network made of vRNN cells with orthogonal or identity initialisation can thus be expected to suffer, especially in the initial training phase, from exploding gradients as we move towards shallower layers and further back in time.</p><p>To validate this assumption, we set up a toy example of a deep vRNN and compute the average gradient magnitude w.r.t. the network parameters for each cell in the unfolded network. For the numerical simulation we initialise all the 1. A subtle point is that sometimes large gradients are the precursor of vanishing gradients, if the associated large parameter updates cause the non-linearities to saturate.</p><formula xml:id="formula_7">J l t = D tanh(c l t ) D (o l t ) Wxo + D tanh(c l t ) D o l t (D c l t−1 D (f l t ) W xf + D z l t D (i l t ) W xi + D i l t D (z l t ) Wxz)<label>(8)</label></formula><formula xml:id="formula_8">H l t = D tanh(c l t ) D (o l t ) W ho + D tanh(c l t ) D o l t (D c l t−1 D (f l t ) W hf + D z l t D (i l t ) W xi + D i l t D (z l t ) W hz )<label>(9)</label></formula><p>hidden states and biases to 0, and chose random orthogonal matrices for the weights. Input sequences are generated with the random process x t = αx t−1 + (1 − α)z, where z ∼ N (0, 1) and the correlation factor α = 0.5 (the choice of the correlation factor does not seem to qualitatively affect the results). <ref type="figure" target="#fig_1">Figure 2</ref> depicts average gradient magnitudes over 10K runs with different weight initialisations and input sequences. As expected, the magnitude grows rapidly towards the earlier and shallower part of the network. We perform a similar analysis for the classical LSTM cell <ref type="bibr" target="#b15">[16]</ref>. The recurrent equations of the LSTM cell are the following:</p><formula xml:id="formula_9">i l t = σ(W xi h l−1 t + W hi h l t−1 + b i )<label>(10)</label></formula><formula xml:id="formula_10">f l t = σ(W xf h l−1 t + W hf h l t−1 + b f ) (11) o l t = σ(W xo h l−1 t + W ho h l t−1 + b o )<label>(12)</label></formula><formula xml:id="formula_11">z l t = tanh(W xz h l−1 t + W hz h l t−1 + b z )<label>(13)</label></formula><formula xml:id="formula_12">c l t = f l t • c l t−1 + i l t • z l t (14) h l t = o l t • tanh(c l t ),<label>(15)</label></formula><p>where i, f , o are the input, forget, and output gate activations, respectively, c is the cell state. The expressions of the Jacobians are reported in Eqs. <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9)</ref> where D x again denotes a diagonal matrix with the elements of vector x as diagonal entries. The equations are slightly more complicated, but are still amenable to the same type of analysis. We again choose the same exemplary conditions as for the vRNN above, i.e., hidden states and biases equal to zero and orthogonal weight matrices. By substituting the numerical values in the aforementioned equations, we can see that the sigmoid function causes the expected singular value of the two Jacobians to drop to 0.25. Contrary to the vRNN cell, we expect that even the two Jacobians combined will produce an attenuation factor well below 1 (considering the same two extreme cases, i.e., uncorrelated and highly correlated, the value is 0.354 and 0.5, respectively) such that the gradient magnitude will decline and eventually vanish. We point out that LSTM cells have a second hidden state, the so-called "cell state". The cell state only propagates along the time dimension and not across layers, which makes the overall effect of the corresponding gradients more difficult to analyse. However, for the same reason one would, in a first approximation, expect that the cell state mainly influences the gradients in the time direction, but cannot help the flow through the layers. Again the numerical simulation results support our hypothesis as can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>. The LSTM gradients propagate relatively well backward through time, but vanish quickly towards shallower layers.</p><p>In summary, the gradient propagation behaves differently in time and depth directions. When considering the latter we need to take into consideration the gradient of the output w.r.t. the input state, too, and not exclusively consider the gradient w.r.t. the previous hidden state. Moreover, we need to take into account that the output of each cell is connected to two cells rather then one adjacent cell. Note that this analysis is valid both when the loss is computed only using the final state T , and when all states are used ( <ref type="figure" target="#fig_1">Fig. 2</ref>). In the latter case, we simply need to sum the contribution of all the separate losses. Usually, parameters are shared among different times t in RNNs, but not among different layers. If parameters are shared among different time steps, gradients accumulate row-wise ( <ref type="figure" target="#fig_1">Fig. 2)</ref> increasing the gradient magnitude w.r.t. the parameters. This, however, is not true in the vertical direction as weights are not shared. As a consequence, it is particularly important to ensure that the gradient magnitude is preserved between adjacent layers. the tanh non-linearity to the hidden state and remove the cell state altogether. As a final modification, we also remove the input gate i l t from the architecture and couple it with the forget gate. We observed in detailed simulations that the input gate harms performance of deeper networks. This finding is consistent with the theory: for an LSTM cell with only the output gate removed, the Jacobians H l t , J t l will on average have singular values 1, respectively 0.5 (under the same conditions of Sec. 3). This suggests exploding gradients, which we indeed observe in numerical simulations. Moreover, signal propagation is less stable: state values can easily saturate if the two gates that control flow into the memory go out of sync. The gate structure of RHN <ref type="bibr" target="#b48">[49]</ref> is similar to that configuration, and does empirically suffer from exploding, then vanishing, gradient ( <ref type="figure" target="#fig_3">Fig. 4b</ref>).</p><p>More formally, our proposed STAR cell in the l-th layer takes the input h l−1 t (in the first layer, x t ) at time t and nonlinearly projects it to the space where the hidden vector h l lives, equation <ref type="bibr" target="#b15">16</ref>. Furthermore, the previous hidden state and the new input are combined into the gating variable k l t (equation 17). k l t is our analogue of the forget gate and controls how information from the previous hidden state and the new input are combined into a new hidden state. The complete dynamics of the STAR unit is given by the expressions</p><formula xml:id="formula_13">z l t = tanh(W z h l−1 t + b z )<label>(16)</label></formula><formula xml:id="formula_14">k l t = σ(W x h l−1 t + W h h l t−1 + b k )<label>(17)</label></formula><formula xml:id="formula_15">h l t = tanh (1 − k l t ) • h l t−1 + k l t • z l t .<label>(18)</label></formula><p>The Jacobian matrices for the STAR cell can be computed similarly to how it is done for the vRNN and LSTM (see the Appendix). In this case each of the two Jacobians has average singular values equal to 0.5. In the same two extreme cases previously considered, the scaling factor for the gradient becomes 0.707 and 1, respectively. Even if the gradient decays in the first case (worst case scenario, no correlation between two gradient contributions), it does so more slowly compared to LSTM. In the second case, the gradient can propagate without decaying or amplifying which is the ideal scenario. Empirically we have observed that, for an arbitrary STAR cell in the grid, these two terms are highly positively correlated, leading, ultimately, to a gradient scaling factor close to one. We repeat the same numerical simulations as above for the STAR cell, and find that it indeed maintains healthy gradient magnitudes throughout most of the deep RNN <ref type="figure" target="#fig_1">(Fig. 2</ref>). Finally, we point out that our proposed STAR architecture requires significantly less memory than most alternative designs. For the same input and hidden state size, STAR has a 50%, respectively and 60% smaller memory footprint than GRU and LSTM. In the next section, we experimentally validate on real datasets that deep RNNs built from STAR units can be trained to a significantly greater depth while performing on par or better than stateof-the-art despite having fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate the performance of several well-known RNN cells as well as that of the proposed STAR cell on different sequence modelling tasks with ten different datasets: sequential versions of MNIST <ref type="bibr" target="#b22">[23]</ref>, the adding <ref type="bibr" target="#b15">[16]</ref>, and copy memory <ref type="bibr" target="#b5">[6]</ref> problems, music modeling <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b28">[29]</ref>, character-level language modeling <ref type="bibr" target="#b26">[27]</ref>, which are a common testbeds for recurrent networks; three different remote sensing datasets, where time series of intensities observed in satellite images shall be classified into different agricultural crops <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>; and Jester <ref type="bibr" target="#b0">[1]</ref> for hand gesture recognition. We use convolutional layers for gesture recognition and pixel-wise crop classification, whereas we employ conventional fully connected layers for the other tasks. The recurrent units we compare include the vRNN, the LSTM, the LSTM with only a forget gate <ref type="bibr" target="#b39">[40]</ref>, the GRU, the RHN <ref type="bibr" target="#b48">[49]</ref>, IndRNN <ref type="bibr" target="#b24">[25]</ref>, temporal convolution network (TCN) <ref type="bibr" target="#b3">[4]</ref>, transformer <ref type="bibr" target="#b40">[41]</ref>, and the proposed STAR. The experimental protocol is similar for all tasks: For each model variant, we train multiple versions with different depth (number of layers) and the best performing one is picked. Classification performance is measured by the rate of correct predictions (top-1 accuracy) for classification tasks, bits per character for character-level language modeling task and negative loglikelihood (NLL) for the rest of the tasks. Throughout the different experiments, we use orthogonal initialisation for weight matrices of RNNs. Training and network details for each experiment can be found in the Appendix. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pixel-by-pixel MNIST</head><p>We flatten all 28×28 grey-scale images of handwritten digits of the MNIST dataset <ref type="bibr" target="#b22">[23]</ref> into 784×1 vectors, and the 784 values are sequentially presented to the RNN. The models' task is to predict the digit after having seen all pixels. The second task, pMNIST, is more challenging. Before flattening the images pixels are shuffled with a fixed random permutation, turning correlations between spatially close pixels into non-local long-range dependencies. As a consequence, the model needs to remember dependencies between distant parts of the sequence to classify the digit correctly. <ref type="figure" target="#fig_3">Fig. 4a</ref> shows the average gradient norms per layer at the start of training for 12-layer networks built from different RNN cells. Propagation through the network increases the gradients for the vRNN and shrinks them for the LSTM. As the optimisation proceeds, we find that STAR and IndRNN remain stable, whereas all other units see a rapid decline of the gradients already within the first epoch, except for RHN, where the gradients explode, see <ref type="figure" target="#fig_3">Fig. 4b</ref>. Consequently, STAR and IndRNN are the only units for which a 12-layer model can be trained, as also confirmed by the evolution of the training loss, <ref type="figure" target="#fig_3">Fig. 4c</ref>.</p><p>IndRNN's gradient propagation through layers is also stable even though not as good as STAR's. However, In-dRNN strongly relies on Batch Normalization (BN) <ref type="bibr" target="#b16">[17]</ref> for stable gradient propagation through layers while STAR does not require BN. If we remove the BNs between consecutive layers in IndRNN (denoted IndRNN w/o BN), its gradient propagation through layers and iterations becomes very unstable (see <ref type="figure" target="#fig_3">Fig. 4a and 4b)</ref>. Indeed, IndRNN cannot be trained in those cases. It does not only fail for deeper, 12-layer setups applied to sequential MNIST, but also for shallower designs. Apart from increasing the computation overhead, general drawbacks of IndRNN's dependency on BNs are: (i) slow convergence during training and (ii) poor performance during inference if batch size is small (see the Appendix for further quantitative analysis). <ref type="figure" target="#fig_4">Fig. 5</ref> confirms that stacking into deeper architectures does benefit RNNs (except for vRNN); but it increases the risk of a catastrophic training failure. STAR is significantly more robust in that respect and can be trained up to &gt;20 layers. On the comparatively easy and saturated MNIST data, the performance is comparable to a successfully trained LSTM (at depth 2-8 layers, LSTM training sometimes catastrophically fails; the displayed accuracies are averaged only over successful training runs).</p><p>In 1 we show that our STAR cell mostly outperforms existing methods. As STAR is specifically designed to improve gradient propagation in the vertical direction, we conduct one additional experiment with a hybrid architecture: we use LSTM with a forget gate (which achieves good performance on the MNIST dataset in the one layer case) as first layer of the network and we stack seven layers of STAR cells on top. Such a design increases the capacity of the first layer without endangering gradient propagation. This further improves accuracy for both MNIST and pMNIST, leading to on par performance across both tasks with the best stateof-the-art methods BN-LSTM <ref type="bibr" target="#b11">[12]</ref> and IndRNN <ref type="bibr" target="#b24">[25]</ref>. Both methods employ Batch Normalization <ref type="bibr" target="#b16">[17]</ref> inside the cells to improve the performance wrt to the simpler form of LSTM and IndRNN. We tested a version of the STAR cell which used BN and also in this case the modification lead to some performance improvements. This modification, however, is rather general and independent of the cell architecture, as it can be added to most of the other existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Adding Problem / Copy Memory</head><p>The adding problem <ref type="bibr" target="#b15">[16]</ref> and the copy memory <ref type="bibr" target="#b5">[6]</ref> are common benchmarks to evaluate whether a network is able to learn long-term memory. In the adding problem, two sequences of length T are taken as input: the first sequence consists of independent samples in range (0, 1), while the second sequence is a binary vector with two entries set to 1 and the rest 0. The goal is to sum the two entries of the first sequence indicated by the two entries of 1 in the second sequence. In copy memory task, the input sequence is of length T + 20. The first 10 values in the sequence are chosen randomly among the digits {1, ..., 8}, the sequence is then followed by T zeros, the last 11 entries are filled with the digit 9 (the first 9 is a delimiter). The goal is to generate an output of the same length that is zero everywhere except for the last 10 values after the delimiter, where the model is expected to repeat the 10 values encountered at the beginning of the input sequence. We perform experiments with two different sequence lengths, T = 200 and T = 1000, using different RNNs with the same number of parameters (70K). The results are shown in <ref type="figure" target="#fig_6">Fig. 6, 7</ref>. The vRNN is unable to perform long-term memorization, whereas LSTM has issue with longer sequences (T = 1000). In contrast, both STAR and GRU, can learn long-term memory even  <ref type="bibr" target="#b21">[22]</ref> 97.0% 82.0% 100 uRNN <ref type="bibr" target="#b2">[3]</ref> 95.1% 91.4% 512 FC uRNN <ref type="bibr" target="#b44">[45]</ref> 96.9% 94.1% 512 Soft ortho <ref type="bibr" target="#b42">[43]</ref> 94.1% 91.4% 128 AntisymRNN <ref type="bibr" target="#b7">[8]</ref> 98.8% 93.1% 128 IndRNN <ref type="bibr" target="#b24">[25]</ref> 99.0% 96.0% 128 BN-LSTM <ref type="bibr" target="#b11">[12]</ref> 99.0% 95.4% 100 sTANH-RNN <ref type="bibr" target="#b46">[47]</ref> 98 when the sequences are very long. An advantage of STAR in this case is its faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">TUM &amp; BreizhCrops Time Series Classification</head><p>We evaluate model performance on a more realistic sequence modelling problem, where the aim is to classify agricultural crop types using sequences of satellite images. In this case, time-series modelling captures phenological evidence, i.e. different crops have different growing patterns over the season. For the TUM dataset, the input is a time series of 26 multi-spectral Sentinel-2A satellite images with a ground resolution of 10 m collected over a 102 km x 42 km area north of Munich, Germany between December 2015 and August 2016 <ref type="bibr" target="#b30">[31]</ref>. We use patches of 3×3 pixels recorded in 6 spectral channels and flattened into 54×1 vectors as input. For the BreizhCrop dataset, the input is a time series of 45 multi-spectral Sentinel-2A satellite images with a ground resolution of 10 m collected from 580k field parcels in the Region of Brittany, France of the season 2017. The input is 4 spectral channels (R, G, B, NIR) <ref type="bibr" target="#b32">[33]</ref>. In the first task, only TUM dataset is used. The vectors are sequentially presented to the RNN model, which outputs a prediction at every time step (note that for this task the correct answer can sometimes be "cloud", "snow", "cloud shadow" or "water", which are easier to recognise than many crops). STAR outperforms all baselines, and it is again more suitable for stacking into deep architectures <ref type="figure" target="#fig_7">(Fig. 8</ref>). In the second task, both datasets are used. The goal is a singlestep prediction i.e., the model predicts a crop type after the entire sequence is presented. STAR significantly outperforms all the baselines including TCN and the recently proposed method, IndRNN <ref type="bibr" target="#b24">[25]</ref> (Tab. 2). Note that IndRNN also aims to build deep multi-layer RNNs. The performance gain is stronger in the BreizhCrop datasets. This is probably because the sequence is longer and the depth of the network helps to capture more complex dependencies in the data.     investigate the ability of RNNs to represent music <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b36">[37]</ref>. The performance on both tasks is measured in terms of perframe negative log-likelihood (NLL) on a test set. We follow the exact same experimental setup described in <ref type="bibr" target="#b36">[37]</ref>. STAR works better than all tested RNN baselines, and performs on par with TCN (see Tab. 3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Music Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Character-level Language Modeling</head><p>For this task we used the PennTreebank (PTB) <ref type="bibr" target="#b26">[27]</ref>. When used as a character-level language corpus, PTB contains 5,059K characters for training, 396K for validation, and 446K for testing, with an alphabet size of 50. The goal is to predict the next character given the preceding context. We follow the exact same experimental setup as <ref type="bibr" target="#b3">[4]</ref>. The performance is measured in terms of bits per character (BPC, i.e. average cross entropy over the alphabet) on the test set. On this task STAR outperforms all baselines, including Transformer and TCN (see Tab. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Hand-gesture recognition from video</head><p>We also evaluate STAR on sequences of images, using convolutional layers. We analyse performance of STAR versus state-of-the-art on gesture recognition from video and pixelwise crop classification. The 20BN-Jester dataset V1 <ref type="bibr" target="#b0">[1]</ref> is a large collection of densely-labelled short video clips, where each clip contains a predefined hand gesture performed by a worker in front of a laptop camera or webcam. In total, the dataset includes 148'094 RGB video files of 27 types of gestures (see <ref type="figure" target="#fig_0">Fig. 11</ref>). The task is to classify which gesture is seen in a video. 32 consecutive frames of size 112×112 pixels are sequentially presented to the convolutional RNN.   At the end, the model again predicts a gesture class via an averaging layer over all time steps. The outcome for convolutional RNNs is coherent with the previous results, see <ref type="figure" target="#fig_7">Fig. 8b</ref>, Tab. 5. Going deeper improves the performance of all four tested convRNNs. The improvement is strongest for convolutional STAR, and the best performance is reached with a deep model (12 layers). In summary, the results confirm both our intuitions that depth is particularly useful for convolutional RNNs, and that STAR is more suitable for deeper architectures, where it achieves higher performance with better memory efficiency. We note that in the shallow 1-2 layer setting the conventional LSTM performs a slightly better than the three others, likely due to its larger capacity. Lastly, we conduct the same additional experiment with the hybrid architecture as we do for MNIST tasks. We stack seven layers of STAR on top of one layer of LSTM. This further improves the results and achieves 92.7% accuracy (compared this to eight LSTM layers, which achieve only 91.8% accuracy with about twice as many parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">TUM image series pixel-wise classification</head><p>In another experiment with convolutional RNNs, we classify crops pixel-wise (and thus use convolutional layers) using a dataset <ref type="bibr" target="#b31">[32]</ref> (TUM) containing Sentinel-2A optical satellite image sequences (RGB and NIR at 10 m ground sampling distance) accompanied by ground-truth land cover maps. Each satellite image sequence contains 30 images of size 48 × 48 px collected in 2016 within a 102 km × 42 km region north of Munich, Germany (see <ref type="figure" target="#fig_0">Fig. 12</ref>). We compare pixel-wise classification accuracy for a network with a fixed depth of four layers and for four different basic recurrent cells LSTM, LSTM with only a forget gate, GRU, and the proposed STAR cell (Tab. 6). Moreover   <ref type="figure">Fig. 9</ref>: Accuracy versus number of model parameters for the gesture recognition task (Jester).</p><p>we include the performance obtained in <ref type="bibr" target="#b31">[32]</ref> using a bidirectional convolutional GRU with a single layer. Our STAR cell outperforms all other methods (Tab. 6) while requiring less memory and being computationally less costly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Computational Resources and Training Time</head><p>Last, we compare to widely used recurrent units LSTM and GRU in terms of parameter efficiency and training time for the convolutional version used in gesture recognition. We plot performance versus number of parameters ( <ref type="figure">Fig. 9</ref>) STAR outperforms LSTM and performs on par with GRU, but requires only half the number of parameters. We plot accuracy on the validation dataset versus training time for different recurrent units for the gesture recognition task in <ref type="figure" target="#fig_0">Fig. 10</ref>. STAR does not only require significantly less parameters but can also be trained much faster: the validation accuracy on the dataset after 8 hours is comparable to the best validation achieved by the LSTM and the GRU after 20 hours of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have proposed STAR, a novel stackable recurrent cell type that it is specifically designed to be employed in deep recurrent architectures. A theoretical analysis and associated numerical simulations indicated that widely used standard RNN cells like LSTM and GRU do not preserve gradient magnitudes in the "vertical" direction during backpropagation. As the depth of the network grows, the risk of either exploding or vanishing gradients increases. We leveraged this analysis to design a novel cell that better preserves the gradient magnitude between two adjacent layers, is better suited for deep architectures, and requires fewer parameters than other widely used recurrent units. An extensive experimental evaluation on several publicly available datasets confirms that STAR units can be stacked into deeper architectures and in many cases performs better than state-of-theart architectures. We see two main directions for future work. On the one hand, it would be worthwhile to develop a more formal and thorough mathematical analysis of the gradient flow, and perhaps even derive rigorous bounds for specific cell types, that could, in turn, inform the network design. On the other hand, it appears promising to investigate whether the analysis of the gradient flows could serve as a basis for better initialisation schemes to compensate the systematic influences of the cells structure, e.g., gating functions, in the training of deep RNNs.  Jan Dirk Wegner is associate professor at University of Zurich and head of the EcoVision Lab at ETH Zurich. Jan was PostDoc (2012-2016) and senior scientist (2017-2020) in the Photogrammetry and Remote Sensing group at ETH Zurich after completing his PhD (with distinction) at Leibniz Universität Hannover in 2011. He was granted multiple awards, among others an ETH Postdoctoral fellowship and the science award of the German Geodetic Commission. Jan was selected for the WEF Young Scientist Class 2020 as one of the 25 best researchers world-wide under the age of 40 committed to integrating scientific knowledge into society for the public good. Jan is chair of the ISPRS II/WG 6 "Large-scale machine learning for geospatial data analysis" and organizer of the CVPR EarthVision workshops.</p><p>In this section, we extend the numerical simulations of the gradient propagation in the unfolded recurrent neural network to two further cell architectures, namely the GRU <ref type="bibr" target="#b9">[10]</ref> and the LSTM with only forget gate for the synthetic dataset (Sec. A.2.1); and for the real dataset, MNIST (Sec. A.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Synthetic Dataset</head><p>The setup of the numerical simulations is the same as the one described in Section 3. As can be seen from <ref type="figure" target="#fig_0">Fig. 13</ref> the GRU and the LSTM with only forget gate mitigate the attenuation of gradients to some degree. However, we observe that the corresponding standard deviations are much higher, i.e., the gradient norm greatly varies across different runs, see <ref type="figure" target="#fig_0">Fig. 14.</ref> We found that the gradients within a single run oscillate a lot more, for both LSTMw/f and GRU, and make training unstable which is undesirable. Moreover, the gradient magnitudes evolve very differently for different initial values, meaning that the training is less robust against fluctuations of the random initialisation.</p><p>final output loss </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 MNIST Dataset</head><p>In this section, we perform the same numerical analysis conducted before but using MNIST as input data. The goal is to verify whether during the first epoch the gradient propagation behaves in the same way as for the synthetic dataset. First, in <ref type="figure" target="#fig_0">Fig 17 and Fig 18,</ref> we plot the evolution of the Hilbert-Schmidt norm (also called Frobenius norm) normalized by the square root of the hidden state size and the average hidden state value, respectively. The experiments are conducted using the proposed STAR method with MNIST as input data, the figures show the evolution of the norms and hidden states for the different layers of the recurrent network. The plots show the validity of our assumptions, during the initial training phase and with orthogonal matrix initialization the norm of the matrices is close to one, which translates to singular values close to one. The mean values of the hidden states are instead close to zero, which is consistent with the analysis conducted in Section 3. We reiterate that the plot shows how the average value of the hidden state remains close to 0 during training. This does not say that the hidden state is always ≡ 0. Additionally, we show the gradient propagation in the two-dimensional lattice, as done in <ref type="figure" target="#fig_0">Fig. 13</ref>, for different cell types with MNIST as input data. We create 12-by-784 latices with twelve layers RNNs. RNN weights are initialized the same way in the real experiments except the forget bias of the LSTM which is set to one (popular initialization scheme for the LSTM) due to numerical instability with the chrono method <ref type="bibr" target="#b38">[39]</ref>.</p><p>In <ref type="figure" target="#fig_0">Fig. 15</ref> we can see that cells show similar behavior for the MNIST dataset. Even though on average STAR and GRU signal propagation looks fine, gradients within a single run oscillate a lot more for GRU (see <ref type="figure" target="#fig_0">Fig. 16</ref>) as seen in the previous numerical simulation (see <ref type="figure" target="#fig_0">Fig. 14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Training details</head><p>We provide more details about training procedures for the experimental analysis in the main paper in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Pixel-by-pixel MNIST</head><p>Following <ref type="bibr" target="#b38">[39]</ref>, chrono initialisation is applied for the bias term of k, b k . The basic idea is that k should not be too large; such that the memory h can be retained over longer time intervals. The same initialisation is used for the input and forget bias of the LSTM and the RHN and for the forget bias of the LSTMw/f and the GRU. For the final prediction, a feedforward layer with softmax activation converts the hidden state to a class label. The numbers of hidden units in the RNN layers are set to 128. All networks are trained for 100 epochs with batch size 100, using the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with learning rate 0.001, β 1 = 0.9 and β 2 = 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 TUM time series classification</head><p>We use the same training procedure as described in the previous section for pixel-by-pixel MNIST. Again, a feedforward layer is appended to the RNN output to obtain a prediction. The numbers of hidden units in the RNN layers is set to 128. All networks are trained for 30 epochs with batch size 500, using Adam <ref type="bibr" target="#b19">[20]</ref> with learning rate 0.001, β 1 = 0.9 and β 2 = 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 BreizhCrop time series classification</head><p>A feedforward layer is appended to the RNN output to obtain a prediction. The numbers of hidden units in the RNN layers is set to 128. All networks are trained for 30 epochs with batch size 1024, using Adam <ref type="bibr" target="#b19">[20]</ref> with learning rate 0.001 and β 1 = 0.9 and β 2 = 0.999. The learning rate scheduler of <ref type="bibr" target="#b40">[41]</ref> is used with 10 warm-up steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 Adding problem / Copy memory</head><p>Following <ref type="bibr" target="#b38">[39]</ref>, chrono initialisation is applied for the bias term of k, b k . The same initialisation is used for the input and forget bias of the LSTM and for the forget bias of the GRU. The number of hidden units is set to 128 for STAR and LSTM, 150 for GRU and 256 for vRNN. 2-layer STAR is used to have same number of parameters. Networks are trained using Adam <ref type="bibr" target="#b19">[20]</ref> with learning rate 0.001, β 1 = 0.9 and β 2 = 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.5 Music modeling</head><p>We follow the exact same experimental setup described in <ref type="bibr" target="#b36">[37]</ref>. Baseline results are taken from <ref type="bibr" target="#b36">[37]</ref>. The input sequence length is set to 200. STAR is trained for 500 iterations with batch size 1, using RMSProp. Dropout with keep probability 0.8 is applied. Other hyper-parameters (number of layer, momentum etc.) are searched as described in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.6 Character-level language modeling</head><p>We follow the exact same experimental setup described in <ref type="bibr" target="#b3">[4]</ref>. Results for vRNN, LSTM, GRU and TCN are directly taken from <ref type="bibr" target="#b3">[4]</ref>. The input sequence length is set to 400. The number of hidden units is set to 410 for STAR; therefore, the total number of parameters for 6layers STAR makes 3M. STAR is trained for 50 epochs with batch size 32, using Adam <ref type="bibr" target="#b19">[20]</ref> with learning rate 0.001, β 1 = 0.9 and β 2 = 0.999. The learning rate is decayed when the validation performance is no longer improved. Gradient clipping with 1 is applied. For IndRNN, the input sequence length is set to 50 because it performs poorly if set to 400. We set the number of hidden units to 660; therefore, the total number of parameters for 6layers IndRNN is 3M and we train it for 100 epochs. Note that we took the IndRNN implementation from https://github.com/Sunnydreamrain/IndRNN pytorch.</p><p>A.3.7 Hand-gesture recognition from video All convolutional kernels are of size 3×3. Each convolutional RNN layer has 64 filters. A shallow CNN is used to convert the hidden state to a label, with 4 layers that have filter depths 128, 128, 256 and 256, respectively. All models are trained with stochastic gradient descent (SGD) with momentum (β = 0.9). The batch size is set to 8, the learning rate starts at 0.001 and decays polynomially to 0.000001 over a total of 30 epochs. L2-regularisation with weight 0.00005 is applied to all parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.8 TUM image series pixel-wise classification</head><p>All convolutional kernels are of size 3×3. Each convolutional RNN layer has 32 filters. A shallow CNN is used to convert the hidden state to a label, with 2 layers that have filter depths 64. All models are fitted with Adam <ref type="bibr" target="#b19">[20]</ref>. The batch size is set to 1, the learning rate starts at 0.001 and decays polynomially to 0.000001 over a total of 25 epochs.    Test-set performance indRNN (6 layers), batch size=128 indRNN (6 layers), batch size=2 STAR (4 layers), batch size=128 STAR (4 layers), batch size=2 <ref type="figure" target="#fig_0">Fig. 19</ref>: Performance comparison for different batch sizes on the sequential MNIST task. If using a batch size of 128, both STAR and IndRNN converge to a solution; IndRNN is faster at the beginning but STAR eventually achieves better performance. IndRNN becomes very slow to train for a batch size of 2 (64x more steps per epoch) and it cannot achieve the same test performance as with the standard batch size (128). In contrast, STAR does not encounter these problems and clearly performs superior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(a) General structure of an unfolded deep RNN (b) Detail of the gradient backpropagation in the two dimensional lattice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Mean value of gradient magnitude with respect to the parameters for different RNN units. top row: loss L(h L T ) only on final prediction. bottom row: loss L(h L 1 . . . h L T ) over all time steps. As the gradients flow back through time and layers, for a network of vanilla RNN units they get amplified; for LSTM units they get attenuated; whereas the proposed STAR unit approximately preserves their magnitude. See the Appendix for the results with the real data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>RNN cell structures: STAR, GRU and LSTM, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>training loss versus iteration, 1 st epoch Gradient magnitudes of pix-by-pix MNIST. (a) Mean gradient norm per layer at the start of training. (b) Evolution of gradient norm during 1 st training epoch. (c) Loss during 1 st epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Accuracy results for pixel-by-pixel MNIST tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>JSB Chorales [ 2 ]Fig. 6 :</head><label>26</label><figDesc>is a polyphonic music dataset consisting of the entire corpus of 382 four-part harmonized chorales Performance comparison for adding problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Performance comparison for copy memory task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Time series classification. (a) Crop classes. (b) Hand gestures (convolutional RNNs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Test accuracy versus training time for the gesture recognition task (Jester), 4 layers networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Example frames of the Jester dataset. Columns show 1 st , 8 th , 16 th , 24 th , 32 nd frames, respectively. First row: Sliding two fingers right. Second row: Sliding two fingers down. Third row: Zooming in with two fingers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>Example satellite images of the TUM dataset. Each row shows randomly sampled images (in order, only R, G and B channels) from a satellite image time-series. The last column shows the ground-truth where different colors correspond to different crop types. Mehmet Ozgur Turkoglu received his BSc degrees in both electrical engineering and physics from Bogazici University in 2016. He studied a master's in electrical engineering with a specialization in computer vision at the University of Twente. He is a PhD candidate in the EcoVision group at ETH Zürich since 2018. His research interests include computer vision, deep learning and their applications to remote sensing data. He is particularly interested in deep sequence modeling of time-series data. Stefano D'Aronco received his BS and MS degrees in electronic engineering from the Università degli studi di Udine, in 2010 and 2013 respectively. He then joined the Signal Processing Laboratory (LTS4) in 2014 as a PhD student under the supervision of Prof. Pascal Frossard. He received his PhD in Electrical Engineering fromÉcole Polytechnique Fédérale de Lausanne in 2018. He is Postdoctoral researcher in the EcoVision group at ETH Zürich since 2018. His research interests include several machine learning topics, such as Bayesian inference method and deep learning, with particular emphasis on applications related to remote sensing an environmental monitoring.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Mean gradient magnitude w.r.t. the parameters for LSTM with only forget gate, GRU, and the proposed STAR cell. top row: loss L(h L T ) only on final prediction. bottom row: loss L(h L 1 . . . h L T ) over all time steps. Mean-normalised standard deviation of gradient magnitude for LSTM with only forget gate, GRU, and the proposed STAR cell. top row: loss L(h L T ) only on final prediction. bottom row: loss L(h L 1 . . . h L T ) over all time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 :Fig. 16 :</head><label>1516</label><figDesc>Mean gradient magnitude w.r.t. the parameters for vRNN, LSTM, GRU, and the proposed STAR cell for MNIST dataset. Loss L(h L T ) only on final prediction. Gradient magnitude comparison within a single run for MNIST dataset. top two rows: GRU samples. bottom two rows: STAR samples. Samples are randomly picked.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 :Fig. 18 :</head><label>1718</label><figDesc>(c) matrix norm, ||Wx|| versus iteration, 1 st epoch Weight matrix norms of pix-by-pix MNIST during 1 st epoch, the Hilbert-Schmidt norm, ||A mxm || = T r(AA T ), divided by √ m. Different curves correspond different layers. Mean hidden state vector, E t,n [h l ] of pix-by-pix MNIST during 1 st epoch. Different curves correspond different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Performance comparison for time series crop classification.</figDesc><table><row><cell></cell><cell cols="2">JSB Chorales</cell><cell cols="2">Piano-Midi</cell></row><row><cell>Method</cell><cell cols="4">NLL #params NLL #params</cell></row><row><cell>vRNN [37]</cell><cell>8.72</cell><cell>40k</cell><cell>7.65</cell><cell>140k</cell></row><row><cell>LSTM [37]</cell><cell>8.51</cell><cell>650k</cell><cell>7.84</cell><cell>480k</cell></row><row><cell>GRU [37]</cell><cell>8.53</cell><cell>640k</cell><cell>7.62</cell><cell>690k</cell></row><row><cell>diagRNN [37]</cell><cell>8.14</cell><cell>420k</cell><cell>7.48</cell><cell>360k</cell></row><row><cell>TCN (2 layers) [4]</cell><cell>8.10</cell><cell>300k</cell><cell>-</cell><cell>-</cell></row><row><cell>STAR (2 layers)</cell><cell>8.13</cell><cell>360k</cell><cell>7.40</cell><cell>480k</cell></row><row><cell>STAR (4 layers)</cell><cell>8.09</cell><cell>830k</cell><cell>-</cell><cell>-</cell></row></table><note>by J. S. Bach. Each input is a sequence of chord elements. Each element is an 88-bit binary code that corresponds to the 88 keys of a piano, with 1 indicating a key pressed at a given time. Piano-Midi [29] is a classical piano MIDI archive that consists of 130 pieces by various composers. These datasets have been used in several previous works to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Performance comparison for music task. The performance is measured in terms of negative log-likelihood (NLL).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Performance comparison for PennTreebank</cell></row><row><cell cols="3">character-level language modeling. The performance is mea-</cell></row><row><cell cols="3">sured in terms of bits per character (BPC). *We run this</cell></row><row><cell cols="3">experiment as designed in [4]'s experimental setup with a</cell></row><row><cell cols="3">limited number of parameters to allow for a fair comparison.</cell></row><row><cell cols="3">Note that [25] reports a better result, but uses many more</cell></row><row><cell>model parameters.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Accuracy #params</cell></row><row><cell>convLSTM (8 layers)</cell><cell>91.8 %</cell><cell>2.2M</cell></row><row><cell>convLSTM w/f (8 layers)</cell><cell>92.0 %</cell><cell>1.1M</cell></row><row><cell>convGRU (12 layers)</cell><cell>92.5 %</cell><cell>2.5M</cell></row><row><cell>convSTAR (8 layers)</cell><cell>92.3 %</cell><cell>0.8M</cell></row><row><cell>convSTAR (12 layers)</cell><cell>92.5 %</cell><cell>1.2M</cell></row><row><cell>convLSTM convSTAR (8 layers)</cell><cell>92.7 %</cell><cell>0.9M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table /><note>Performance comparison for the gesture recogni- tion task (Jester).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>Performance comparison for TUM pixel-wise image classification task.</figDesc><table><row><cell></cell><cell>convLSTM</cell><cell cols="2">convLSTM w/f</cell><cell>convGRU</cell><cell></cell><cell>convSTAR</cell></row><row><cell></cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accuracy</cell><cell>0.88</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.84</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">#params (M)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>||W h || versus iteration, 1 st epoch</figDesc><table><row><cell></cell><cell>1.50 1.75 2.00</cell><cell></cell><cell>W_z_layer_2 W_z_layer_3 W_z_layer_4 W_z_layer_5</cell><cell cols="2">W_z_layer_6 W_z_layer_7 W_z_layer_8 W_z_layer_9</cell><cell cols="2">W_z_layer_10 W_z_layer_11 W_z_layer_12</cell></row><row><cell>weight norm</cell><cell>0.75 1.00 1.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>100</cell><cell>200 iteration</cell><cell>300</cell><cell>400</cell><cell>500</cell></row><row><cell></cell><cell></cell><cell cols="6">(a) matrix norm, ||Wz|| versus iteration, 1 st epoch</cell></row><row><cell></cell><cell>1.50 1.75 2.00</cell><cell></cell><cell>W_h_layer_2 W_h_layer_3 W_h_layer_4 W_h_layer_5</cell><cell cols="2">W_h_layer_6 W_h_layer_7 W_h_layer_8 W_h_layer_9</cell><cell cols="2">W_h_layer_10 W_h_layer_11 W_h_layer_12</cell></row><row><cell>weight norm</cell><cell>0.75 1.00 1.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>100</cell><cell>200 iteration</cell><cell>300</cell><cell>400</cell><cell>500</cell></row><row><cell>weight norm</cell><cell>0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00</cell><cell cols="2">100 (b) matrix norm, 0 W_x_layer_2 W_x_layer_3 W_x_layer_4 W_x_layer_5</cell><cell cols="2">200 iteration W_x_layer_6 300 W_x_layer_7 W_x_layer_8 W_x_layer_9</cell><cell cols="2">400 W_x_layer_10 500 W_x_layer_11 W_x_layer_12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">THE STAR UNITBuilding upon the previous analysis, we introduce a novel RNN cell designed to avoid vanishing or exploding gradients while reducing the number of parameters. We start from the Jacobian matrix of the LSTM cell and investigate what design features are responsible for such low singular values. We see in Eq. (9) that every multiplication with tanh non-linearity (D tanh(.) ), gating functions (D σ(.) ), and with their derivatives can only ever decrease the singular values of W , since all those terms are always &lt;1. The effect is particularly pronounced for the sigmoid and its derivative, |σ (·)| ≤ 0.25 and E[|σ(x)|] = 0.5 for zeromean, symmetric distribution of x. In particular, the output gate o l t is a sigmoid and plays a major role in shrinking the overall gradients, as it multiplicatively affects all parts of both Jacobians. As a first measure, we thus propose to remove the output gate, which leads to h l t and c l t carrying the same information (the hidden state becomes an elementwise non-linear transformation of the cell state). To avoid this duplication and further simplify the design, we transfer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. Code and trained models (in Tensorflow), as well as code for the simulations (in PyTorch), are available online: https://github.com/0zgur0/STAR Network.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the Swiss Federal Office for Agriculture (FOAG) for partially funding this Research project through the Deep-Field Project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A.1 RNN Cells Dynamics</head><p>In the following, we provide more detailed insights about the updating rules of the tested cell types. Vanilla RNN update rule:</p><p>LSTM update rule:</p><p>LSTM with only forget gate, update rule:</p><p>GRU update rule:</p><p>STAR Jacobians:</p><p>Convolutional STAR: We briefly describe the convolutional version of our proposed cell. The main difference is matrix multiplications now become convolutional operations. The dynamics of the convSTAR cell is given in the following equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Further Numerical Gradient Propagation Analysis</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://20bn.com/datasets/jester" />
		<title level="m">The 20bn-jester dataset v1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Harmonising chorales by probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moray</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Foundations and trends® in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNN</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Skip RNN: Learning to skip state updates in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<title level="m">Anti-symmetricRNN: A dynamical system view on recurrent neural networks. In ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamical isometry and a mean field theory of rnns: Gating enables signal propagation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent orthogonal networks and long-memory tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Untersuchungen zu Dynamischen Neuronalen Netzen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">91</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma Thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Residual LSTM: Design of a deep recurrent architecture for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwon</forename><surname>Lee</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VideoLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient orthogonal parametrisation of recurrent neural networks using householder reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakaria</forename><surname>Mhammedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hellicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashfaqur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A discriminative model for polyphonic piano transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exploring the depths of recurrent neural networks with stochastic residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabeek</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal vegetation modelling using long short-term memory networks for crop identification from medium-resolution multi-spectral satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-temporal land cover classification with sequential recurrent encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS International Journal of Geo-Information</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Breizhcrops: A satellite time series dataset for crop type identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Lefèvre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Diagonal rnns in symbolic music modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Can recurrent neural networks warp time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of the forget gate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos</forename><surname>Van Der Westhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04849</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">On orthogonality and learning recurrent networks with long term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang. R-Transformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05572</idno>
		<title level="m">Recurrent neural network enhanced transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Architectural complexity measures of recurrent neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention in convolutional LSTM for gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>Syed Afaq Ali Shah, and Mohammed Bennamoun</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
