<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Online Video Object Segmentation with Motion Property Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhuo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Online Video Object Segmentation with Motion Property Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Unsupervised video object segmentation</term>
					<term>salient motion</term>
					<term>object proposals</term>
					<term>video understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised video object segmentation aims to automatically segment moving objects over an unconstrained video without any user annotation. So far, only few unsupervised online methods have been reported in literature and their performance is still far from satisfactory, because the complementary information from future frames cannot be processed under online setting. To solve this challenging problem, in this paper, we propose a novel Unsupervised Online Video Object Segmentation (UOVOS) framework by construing the motion property to mean moving in concurrence with a generic object for segmented regions. By incorporating salient motion detection and object proposal, a pixel-wise fusion strategy is developed to effectively remove detection noise such as dynamic background and stationary objects. Furthermore, by leveraging the obtained segmentation from immediately preceding frames, a forward propagation algorithm is employed to deal with unreliable motion detection and object proposals. Experimental results on several benchmark datasets demonstrate the efficacy of the proposed method. Compared to the state-of-the-art unsupervised online segmentation algorithms, the proposed method achieves an absolute gain of 6.2%. Moreover, our method achieves better performance than the best unsupervised offline algorithm on the DAVIS-2016 benchmark dataset. Our code is available on the project website: https://github.com/visiontao/uovos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Online Video Object Segmentation with Motion Property Understanding</head><p>Tao Zhuo, Zhiyong Cheng, Peng Zhang, Yongkang Wong, Member, IEEE and Mohan Kankanhalli, Fellow, IEEE Abstract-Unsupervised video object segmentation aims to automatically segment moving objects over an unconstrained video without any user annotation. So far, only few unsupervised online methods have been reported in literature and their performance is still far from satisfactory, because the complementary information from future frames cannot be processed under online setting. To solve this challenging problem, in this paper, we propose a novel Unsupervised Online Video Object Segmentation (UOVOS) framework by construing the motion property to mean moving in concurrence with a generic object for segmented regions. By incorporating salient motion detection and object proposal, a pixel-wise fusion strategy is developed to effectively remove detection noise such as dynamic background and stationary objects. Furthermore, by leveraging the obtained segmentation from immediately preceding frames, a forward propagation algorithm is employed to deal with unreliable motion detection and object proposals. Experimental results on several benchmark datasets demonstrate the efficacy of the proposed method. Compared to the state-of-the-art unsupervised online segmentation algorithms, the proposed method achieves an absolute gain of 6.2%. Moreover, our method achieves better performance than the best unsupervised offline algorithm on the DAVIS-2016 benchmark dataset. Our code is available on the project website: https://github.com/visiontao/uovos. Index Terms-Unsupervised video object segmentation, salient motion, object proposals, video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T He task of Video Object Segmentation (VOS) is to separate objects (foreground) from the background. This is important for the wide range of video understanding applications, such as video surveillance, unmanned vehicle navigation and action recognition. Traditionally, most approaches in VOS mainly focused on background modeling in stationary camera scenarios. Recently, this focus has been shifted from stationary camera to freely moving camera environment <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Due to the complex video content (e.g. object deformation, background clutter and occlusion) and the dynamic nature of moving background caused by camera motion, moving object segmentation under the moving camera environment is still a challenging problem.</p><p>Depending on whether the object mask is manually annotated or not, existing VOS algorithms can be broadly categorized into semi-supervised approach or unsupervised approach. Generally, the former <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref> aims to segment specific objects based on the user annotation (often the first frame of a video). In contrast, the latter <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref> aims to automatically segment moving objects without any user annotation on the given video. In this paper, we mainly focus on the unsupervised VOS task.</p><p>The popular unsupervised VOS methods often focus on clustering the long-term trajectories of pixels <ref type="bibr" target="#b14">[14]</ref>, superpixels <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref> or object proposals <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref> across the entire video, and the pixels with consistent trajectories are clustered as foreground. This long-term trajectory-based strategy often requires the entire video sequence upfront to obtain good results. Thus, it must operate in an offline manner with the following problems.</p><p>1) The targeted moving object must appear in most frames of the given video <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[19]</ref>, otherwise it will probably be classified as background.</p><p>2) The requirement of the entire video implies that the offline methods cannot segment moving objects in a frame-by-frame manner. Therefore, it is impractical for video streaming applications (e.g. video surveillance). 3) Due to the large memory requirement, this strategy also becomes infeasible for analyzing a long video sequence.</p><p>In order to overcome the limitations of offline approaches, the development of unsupervised online VOS frameworks has attracted more attention. Wang et al. <ref type="bibr" target="#b20">[20]</ref> combined current frame with several forward-backward neighboring frames to generate short-term trajectories. Based on the spatio-temporal saliency map generated by optical flow field and salient object detection, moving objects are automatically segmented. However, since a moving object is not always salient in some videos, the spatio-temporal saliency map cannot produce good segmentation results in that case. Different from the online strategy with short-term trajectories, some researchers adopted another tracking-based unsupervised online framework for VOS. Briefly, by automatically initializing the target object on a few frames with different motion cues, an online tracking method is then used to propagate the initialized object regions to subsequent frames, as in <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>. However, the segmentation results are subject to the quality of the initialized object regions. Besides, these methods suffer from arXiv:1810.03783v2 [cs.CV] 6 Aug 2019 video frame optical flow salient motion map objectness mask salient motion mask fused mask <ref type="figure">Fig. 1</ref>. Two examples of the moving object segmentation with salient motion detection and object proposals. Salient motion map denotes the moving probability of each pixel; salient motion mask represents the extracted moving regions; objectness mask is the detected generic object regions; fused mask is our motion segmentation result. Based on our fusion method, moving background (e.g. moving water) and stationary objects can be effectively removed.</p><p>error accumulation <ref type="bibr" target="#b5">[6]</ref> when the tracking initialized object regions to the subsequent frames.</p><p>Recently, deep learning based methods have been deployed to automatically segment moving objects with motion cues. For example, Tokmakov et al. <ref type="bibr" target="#b6">[7]</ref> adopted an end-to-end framework on the optical flow field for motion segmentation, followed by an object proposals model <ref type="bibr" target="#b25">[25]</ref> to extract the candidate objects. Jain et al. <ref type="bibr" target="#b26">[26]</ref> proposed a two-stream fully convolutional network to combine the object proposals and motion for segmenting generic objects in videos. Unlike traditional methods, deep learning based approaches require a large amount of well-annotated data for training. In addition, when the object movements and video scenarios are very different from the training data, their performance may degrade substantially.</p><p>Based on the above analysis, although much progress has been made by existing methods, developing accurate unsupervised online VOS algorithms remains a challenging problem. In this paper, motivated by the moving object definition in which a segmented region should be moving and indicate a generic object, which we call motion property, we propose a novel fully Unsupervised Online VOS (UOVOS) framework for more accurate moving object segmentation. To extract the regions that satisfy both moving object properties (i.e. moving and generic object), we propose a novel motion segmentation method that segments moving objects between two video frames with salient motion detection and object proposals. Specifically, the salient motion detection method is used to extract moving regions (denoted as salient motion mask) on the optical flow; and the object proposals method is applied to detect the generic object regions (denoted as objectness mask) on each frame. However, neither the salient motion mask or objectness mask alone can accurately detect regions with both "moving" and "generic objects" properties. Therefore, we propose a pixel-level fusion method to operate on the intersection of the detected regions by the salient motion map and objectness map. As shown in <ref type="figure">Figure 1</ref>, by fusing the salient motion detection result and object proposals, the moving background regions and static objects can be effectively removed by our method. Unlike the existing deep learning methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[26]</ref> that learn the motion segmentation model from a large number of well annotated data, our method does not require any additional training data as it is able to directly employ a pretrained object proposals model <ref type="bibr" target="#b27">[27]</ref> without fine-tuning.</p><p>In addition, due to complex video scenarios, salient motion detection and object proposals in individual frame are not always reliable. With the observation that the video content in neighboring frames often share consistent motion dynamic, we propose a forward propagation refinement method to predict more accurate moving and generic object regions. By propagating the results of several previous frames to the current frame, a more accurate segmentation result is estimated with the refined salient motion mask and objectness mask.</p><p>Finally, to produce accurate object boundaries, we adopt a CRF model <ref type="bibr" target="#b28">[28]</ref> for further segmentation refinement. Based on the proposed motion segmentation and forward propagation refinement, our method is able to automatically segment moving objects in an online manner. To demonstrate the effectiveness of our proposed approach, we conduct evaluation on the DAVIS-2016 <ref type="bibr" target="#b4">[5]</ref>, SegTrack-v2 <ref type="bibr" target="#b21">[21]</ref> and FBMS-59 <ref type="bibr" target="#b29">[29]</ref> benchmark datasets. Experimental results show the effectiveness and competitive accuracy of our method. Besides, compared to the state-of-the-art methods, our method significantly outperforms the unsupervised online algorithms by 6.2%, and even achieves better performance than the best unsupervised offline on the DAVIS-2016 dataset.</p><p>In summary, our main contributions are as follows. 1) We propose a novel Unsupervised Online Video Object Segmentation (UOVOS) framework, which utilizes from motion property. In particular, we design a pixel-wise fusion method for the salient motion detection and object proposals, which can effectively remove moving background and stationary object noise. 2) To deal with unreliable salient motion and object proposals in complex videos, we propose a forward propagation method by leveraging the segmentation mask from previous frames for mask refinement. 3) We conduct comprehensive experiments on three benchmark datasets. The experimental results show that our method significantly outperforms the state-of-the-art methods by a large margin of 6.2%. The remainder of the paper is organized as follows. The related work is reviewed in Section II. Section III elaborates on our approach. Section IV discusses the experiments and results. Section V concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Semi-supervised VOS</head><p>Semi-supervised VOS methods aim to segment specific objects in videos based on the user annotation on some video frames (often the first frame of the video). Recent semisupervised methods <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref> often assume that the object mask is known in the first frame, followed by a tracking method to segment it in the subsequent frames. In order to alleviate the drift problem <ref type="bibr" target="#b34">[34]</ref> in tracking stage, Fan et al. <ref type="bibr" target="#b10">[10]</ref> annotated the object mask in a few frames, and adopted a local mask transfer method to propagate the source annotation to terminal images in both forward and backward directions. Recently, many deep learning based approaches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b36">[36]</ref> have been developed for semisupervised VOS, making significant progress. For example, RGMP method <ref type="bibr" target="#b36">[36]</ref> proposes a hybrid model that fuses the mask detection and propagation in an encoder-decoder network. It can leverage the temporal information from the previous frame and the annotated object mask in the first frame for current frame processing. Benefiting from the effective network architecture design, accurate results can be obtained for both single object and multi-object segmentation by the semi-supervise methods. However, due to the requirement of object annotation in videos, semi-supervised approaches are not feasible in and scalable for some applications, such as video surveillance systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised VOS</head><p>Unsupervised VOS algorithms aim to automatically segment moving objects without any user annotation. Compared to semi-supervised methods, unsupervised algorithms cannot segment a specific object due to motion ambiguity between different instances and dynamic background. The early methods <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b37">[37]</ref> are often based on geometric scene modeling <ref type="bibr" target="#b38">[38]</ref>, where the geometric model fitting error is used to classify the foreground/background label of the corresponding pixels. Sheikh et al. <ref type="bibr" target="#b13">[13]</ref> adopted a homography model to distinguish foreground/background trajectories, but they assume an affine model over a more accurate perspective camera model. For more accurate scene modeling, Jung et al. <ref type="bibr" target="#b37">[37]</ref> used multiple fundamental matrices to describe each moving object and segment the moving objects with epipolar geometry constraint. Unfortunately, this method is only valid for rigid objects and scenarios. For semantic video processing, some unsupervised methods adopted robust PCA method <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref> for moving foreground estimation. Later, long-term trajectorybased strategy <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref> becomes a common method in unsupervised VOS. Depending on the analytic levels, the long-term trajectories are often generated on pixels <ref type="bibr" target="#b14">[14]</ref>, superpixels <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref> or object proposals <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, in which pixels with consistent trajectories are clustered as foreground and others are background. In order to obtain the accurate segmentation results, the long-term trajectory-based methods often take the entire video sequence as input, and thus they cannot segment moving objects in an online manner. In this paper, we mainly extract the moving objects by fusing the salient motion segmentation and object proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Motion Segmentation</head><p>In an online VOS framework, motion segmentation between two adjacent frames is the key to segmenting moving objects frame-by-frame. Since the early geometry-based methods are sensitive to the selected model (i.e. 2D homography or 3D fundamental matrix) <ref type="bibr" target="#b38">[38]</ref>, recent methods try to distinguish foreground/background with different motion cues. Papazoglou and Ferrari <ref type="bibr" target="#b0">[1]</ref> first detected the motion boundaries based on the magnitude of optical flow field's gradient, and then used the filled binary motion boundaries to represent the moving regions. However, this method is very sensitive to motion boundary extraction, ignoring object information. In order to remove camera translation and rotation, Bideau et al. <ref type="bibr" target="#b24">[24]</ref> utilized the angle and magnitude of optical flow to maximize the information about how objects are moving differently. This method requires the focal length of camera to estimate its rotation and translation. However, given an arbitrary video sequence, the focal length of camera is often unknown. Inspired by salient object detection methods on static images <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>, salient motion detection methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b47">[47]</ref> have been applied on optical flow field for moving object segmentation, where pixels with high motion contrast are classified as foreground. Due to the lack of object information, it cannot handle moving background (e.g. moving water) that do not indicate a generic object.</p><p>Recently, deep learning based methods have been widely applied in VOS. For example, Tokmakov et al. <ref type="bibr" target="#b6">[7]</ref> proposed an end-to-end CNN-based framework to automatically learn motion patterns from optical flow field, followed by an object proposals model and CRF model for segmentation refinement. To fuse the motion and appearance information in a unified framework, Jain et al. <ref type="bibr" target="#b26">[26]</ref> designed a two-stream CNN, where the appearance stream is used to detect object regions while the other motion stream is used to find moving regions. In contrast to previous methods, we propose a new motion segmentation method with motion property in this paper. Specifically, since a segmented region should be moving and indicate a generic object, we apply off-the-shelf salient motion detection model <ref type="bibr" target="#b44">[44]</ref> and object proposal model <ref type="bibr" target="#b27">[27]</ref> for accurate motion segmentation. Unlike other end-to-end deep learning based motion segmentation methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[26]</ref> that require a large number of training samples to learn motion patterns, our method directly uses a pretrained object proposal model without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Semantic Segmentation with Object Proposals</head><p>A comprehensive review on the topic of object proposal is out of the scope of this paper. Here, we only focus on the most related and recent works. The purpose of semantic segmentation <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref> is to identify a set of generic objects in a given image with segmented regions. To generate object proposals, Krähenbühl et al. <ref type="bibr" target="#b48">[48]</ref> identified a set of automatically placed seed superpixels to hit all objects in the given image, then the foreground and background masks are generated by computing geodesic distance transform on these seeds. Finally, critical level sets is applied on the geodesic distance transform to discovering objects. Recently, with the success of deep learning in object detection, DeepMask <ref type="bibr" target="#b49">[49]</ref> learns to propose object segment candidates with Fast R-CNN <ref type="bibr" target="#b50">[50]</ref>. He et al. <ref type="bibr" target="#b27">[27]</ref> proposed Mask R-CNN framework for simultaneous instance-level recognition and segmentation. By incorporating a mask branch for segmentation, Mask R-CNN extended Faster R-CNN <ref type="bibr" target="#b51">[51]</ref> and achieved good segmentation results. In this paper, we directly use a pretrained Mask R-CNN model to generate objectness map without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><formula xml:id="formula_0">Let L t i ∈ {0, 1} represent the foreground (denoted by 1) or background (denoted by 0) label of i-th pixel I t i in t-th video frame I t .</formula><p>Given an input video stream I = {I 1 , . . . , I T }, our goal is to predict a set of binary foreground/background masks L = {L 1 , . . . , L T } in a fully unsupervised and online manner.</p><p>In contrast to existing methods, our method is based on the motion property which requires the segmented region in VOS to be moving and indicate a generic object. We propose a new moving object segmentation framework by referring to the salient motion detection and object proposal methods. More specifically, for each frame 1 , a salient motion detection method is applied to detect moving regions (i.e. salient motion mask) and an object proposal method is used to detect generic objects (i.e. objectness mask). Then, the detected results of this two methods are fused with our proposed fusion method (Section III-B). The results by the salient motion detection and object proposal methods are not always reliable, especially for complex video scenes. To alleviate this problem, we propose a forward propagation refinement method to improve the segmentation results (Section III-C). In addition, a CRF model is applied to further refine the results (Section III-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion Segmentation</head><p>In the following, salient motion mask represents moving regions while objectness mask denotes generic objects. As mentioned, our motion segmentation is an effective fusion of salient motion segmentation and object proposal techniques. In the next section, we will introduce two techniques in sequence, followed by the proposed fusion method.</p><p>1) Salient motion mask: Motion reveals how foreground pixels move differently than their surrounding background ones. Thus, it is very useful for moving regions extraction. Unlike static camera environments studied in traditional background subtraction problems, foreground pixel displacements and camera movements are often unknown under freely moving camera environments.</p><p>In this work, we employ saliency detection <ref type="bibr" target="#b44">[44]</ref> on optical flow to separate the moving regions from static background. This method computes global motion contrast of each pixel in a frame showing good performance for motion segmentation tasks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b47">[47]</ref>. Specifically, let F t = {F t 1 , F t 2 , . . . , F t N } be the backward optical flow field between two frames I t and</p><formula xml:id="formula_1">I t−1 , where each element F t i = [u t i , v t i ]</formula><p>is the optical flow vector of <ref type="bibr" target="#b0">1</ref> Notice that our method processes a given video in a frame-by-frame manner, which means the future frames are not processed.</p><p>optical flow salient motion map salient motion mask pixel I t i in horizontal and vertical directions, N is the total number of the frame pixels. LetS t be the salient motion map on optical flow field F t , the global motion contrastS t i of each pixel I t i is computed as:</p><formula xml:id="formula_2">S t i (F t ) = ∀F t j ∈F t d(F t i , F t j )<label>(1)</label></formula><p>whereS t i ∈ [0, 1] and d(·) is a distance metric <ref type="bibr" target="#b43">[43]</ref>. For the sake of efficiency, we use the Minimum Barrier Distance (MBD) transform <ref type="bibr" target="#b44">[44]</ref> to detect salient motion.</p><p>Given an unconstrained video sequence, the object movements and camera motion are unknown. In order to detect moving regions under various motion contrasts, we utilize an adaptive threshold method <ref type="bibr" target="#b52">[52]</ref> to extract the salient motion map. Then, pixels with high motion contrast are classified as foreground and the rest is background pixels. Let φ be the binary splitting function of our adaptive threshold method, the salient motion mask S t is computed as:</p><formula xml:id="formula_3">S t = φ(S t )<label>(2)</label></formula><p>where each element S t i ∈ {0, 1} denotes the binary foreground/background label of pixel I t i . Different from moving object segmentation, salient motion mask only represents the moving regions. Without any prior information about the object, moving background (e.g. wave) may be classified as moving object (see <ref type="figure" target="#fig_1">Figure 2</ref>). Therefore, we incorporate object proposals to detect generic object.</p><p>2) Objectness mask: As mentioned, salient motion segmentation method cannot differentiate moving objects from moving background. Therefore, an object proposal technique is applied to extract generic objects. Based on the success of deep learning in object detection, Mask R-CNN <ref type="bibr" target="#b27">[27]</ref> extends Faster R-CNN algorithm <ref type="bibr" target="#b51">[51]</ref> by adding a branch for predicting segmentation masks on each region of interest, and achieves the state-of-the-art detection and segmentation performance in static images. In this work, we use the pretrained Mask R-CNN <ref type="bibr" target="#b27">[27]</ref> model in VOS to remove the moving background regions.</p><p>In order to obtain an objectness mask O t with high recall, we set a low object confidence threshold (0.5 in our experiments) to extract the generic object regions. Based on the binary objectness mask from Mask R-CNN, multiple segmented object regions can be obtained. Since the object region of interest also requires to satisfy the "moving" property, we directly use the binary objectness mask for fusion without any further processing, as illustrated in <ref type="figure">Figure 1</ref>. Though the object proposal model is not reliable enough in some complex video scenes, with false positive detections and missing objects shown in <ref type="figure" target="#fig_2">Figure 3</ref>, it still provides useful object information about the scenes.</p><p>It is worth mentioning that we directly use the Mask R-CNN model pretrained on MS-COCO dataset <ref type="bibr" target="#b53">[53]</ref> without any further fine-tuning in our implementation. In spite of that, it produces promising segmentation results on two benchmark datasets (see Section IV-E). This demonstrates the potential of our method, since it is very different from many existing methods (such as <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b32">[32]</ref>) which require careful fine-tuning of the pretrained model for better results.</p><p>3) Mask fusion: As mentioned, the goal of motion segmentation is to detect moving objects. By computing the intersection region of the salient motion and the objectness mask, both moving and generic object properties can be satisfied. In the following, we describe our mask fusion method.</p><p>In practice, directly extracting the intersection region may result in inaccurate segmentation. For example, as shown in <ref type="figure">Figure 4</ref>, when a part of the object moves in non-rigid objects, the segmentation results are incomplete to cover the whole object region. To alleviate such problems, we first dilate the salient motion mask to produce moving regions with higher segmentation recall, and then use the dilated moving regions for mask fusion. Although some background regions may possibly be incorporated by the dilation operation, our experiments show that it can be effectively removed by fusing it with the objectness mask.</p><p>Let S t be the salient motion mask on optical flow field F t , O t be the objectness mask on current frame I t , D denote the image dilation function and r represent the dilated radius. Then our fused segmentation mask P t of frame I t is computed by fusing the binary mask S t and O t as:</p><formula xml:id="formula_4">P t = D(S t , r) ∩ O t<label>(3)</label></formula><p>where each element P t i ∈ {0, 1} denotes the binary foreground/background label of each pixel I t i , operator ∩ indicates the pixel-wise multiplication on the D(S, r) t and O t . Our experiments on two benchmark datasets show that salient motion detection and object proposals are complementary to each other in VOS (see Section IV-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Forward Propagation Refinement</head><p>In some complex video scenarios, it is difficult to obtain reliable salient motion detection and object proposals results on each frame (see <ref type="figure" target="#fig_1">Figure 2, 3 and 4)</ref>. Note that the video content in neighboring frames often share consistent motion dynamic. <ref type="figure">Fig. 4</ref>. Examples of incomplete salient motion segmentation when parts of the object move non-rigidly. In the first image, the "head" and "two legs" of the bear are not completely moving; the second image demonstrates that only "right leg" and part of "left leg" of the man are moving; and the third image indicates that some "legs" remain stationary. In other words, the content of the current frame is similar to the previous one. Therefore, we propose a forward propagation refinement method, which leverages the segmentation masks of previous frames for temporal mask consistency, and thus obtain more robust and accurate segmentation. Let P t denote the segmentation mask of t-th frame without forward propagation refinement (namely, obtained by Eqn. 3); M t denotes the segmentation mask of t-th frame with the refinement method. For frame I t , suppose we consider the segmentation masks of previous n frames, i.e. {M t−n , . . . , M t−1 }, which are propagated to the current frame (based on the pixelwise tracking with optical flow) as {M t−n , . . . ,M t−1 } for segmentation refinement.</p><p>The refined salient motion mapS t of current processing frame is recomputed with the original salient motion mapS t (obtained from Eqn. 1) and propagated masks {M t−n , . . . ,M t−1 } as:</p><formula xml:id="formula_5">S t = θS t + (1 − θ) n τ =1M t−τ<label>(4)</label></formula><p>where θ ∈ (0, 1) is a weight to balance these two components. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, the unreliable salient motion segmentation can be improved by forward propagating a set of previous segmentation masks.</p><p>As the examples in <ref type="figure">Figure 1</ref> show, given an arbitrary video sequence, the accuracy and robustness of both the motion and objectness components cannot be known in advance.</p><p>optical flow video frame objectness mask salient motion mask motion segmentation refined segmentation <ref type="figure">Fig. 6</ref>. Examples of the forward propagation refinement. The first row (video bear in DAVIS-2016 dataset <ref type="bibr" target="#b4">[5]</ref>) shows the improvement for inaccurate salient motion segmentation; and the second row (video frog in SegTrack-v2 dataset <ref type="bibr" target="#b21">[21]</ref>) presents the improvement for unreliable object proposals, respectively.</p><p>Therefore, for the sake of simplicity, we use the same weight θ to improve the objectness maskŌ t of current frame via:</p><formula xml:id="formula_6">O t = θO t + (1 − θ) n τ =1M t−τ<label>(5)</label></formula><p>Similar to the motion segmentation between two frames, the improved segmentation mask M t is obtained by fusing the refined masksS t andŌ t as:</p><formula xml:id="formula_7">M t = D(φ(S t ), r) ∩ φ(Ō t )<label>(6)</label></formula><p>where M 2 = P 2 indicates the initial motion segmentation between the first and second video frames. Compared to the individually extracted motion segmentation between two frames, by propagating previous segmentations to the current frame, our method is able to improve both the unreliable salient motion segmentation and object proposals. As shown in <ref type="figure">Figure 6</ref>, based on the forward propagation refinement, the segmentation results are improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CRF Refinement</head><p>Notice that the segmentation based on the motion cannot detect the object boundary very accurately in some cases <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>. It may therefore degrade the results of our method even worse than the proposed forward propagation refinement (denoted by M t ). To alleviate this problem, the standard CRF model <ref type="bibr" target="#b28">[28]</ref> can be applied to our framework to further improving the final segmentation result, denoted as L t . Based on the step-by-step processing strategy, we initialize the final segmentation label L t with the binary mask M t .</p><p>For segmentation optimization, we formulate our moving object segmentation task as a binary classification problem, where the pixel labels are computed by constructing a graph G = V, E . Here, V denotes a set of vertices that correspond to the image pixels and E represents edges that connect the four neighboring pixels. The goal is to estimate optimal foreground/background label L t = {L t 1 , L t 2 , ..., L t N } as:</p><formula xml:id="formula_8">L t = arg min L t E(L t )<label>(7)</label></formula><p>where L t i ∈ {0, 1} is the label of each pixel and 0 denotes background. The energy function for labeling L t of all pixels is defined as: where</p><formula xml:id="formula_9">E(L t ) = i∈V U t i (L t i ) + λ (i,j)∈E W t ij (L t i , L t j )<label>(8)</label></formula><formula xml:id="formula_10">U t i (L t i ) is the appearance based unary term. W t ij (L t i , L t j )</formula><p>is the pairwise term for spatial smoothness purpose. λ &gt; 0 controls the relative effect of the two terms. The unary term U t i (L t i ) models the deviations from the initially estimated foreground/background appearance in RGB color space. Let C t f be the total cost of assigning background to foreground and C t b be the total cost of assigning foreground to background. U t i (L t i ) is formulated as:</p><formula xml:id="formula_11">U t i (L t i ) = (1 − L t i )C t f + L t i C t b<label>(9)</label></formula><p>Taking account of the color Gaussian Mixture Model (GMM), the unary term is computed by a mixture of Gaussian probability distribution with k c = 5 components as in <ref type="bibr" target="#b54">[54]</ref>. The pairwise term W t ij (L t i , L t j ) is used to ensure that neighboring pixels are assigned with the same label, which is computed by an exponential function as:</p><formula xml:id="formula_12">W t ij (L t i , L t j ) = (L t i − L t j ) 2 exp(−β||I t i − I t j || 2 )<label>(10)</label></formula><p>where β &gt; 0 is a constant parameter, I t i and I t j are the intensity values of 4 neighboring pixels in frame I t . Then an efficient max-flow algorithm is applied to find the optimal labeling with minimal energy <ref type="bibr" target="#b54">[54]</ref>.</p><p>The refined segmentations are shown in the second row of <ref type="figure" target="#fig_4">Figure 7</ref>, which improves the initial results (first row of <ref type="figure" target="#fig_4">Figure  7</ref>) on the object boundaries. Finally, our entire approach is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>In this section, we first describe the implementation details, followed by the introduction of experimental datasets. Next, we detail the baselines and evaluation metrics, and finally report and analyze the experimental results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Inspired by salient object detection on static images, previous works often applied salient object detection on optical flow field as salient motion detection, which has been demonstrated to be effective in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b26">[26]</ref>. In this work, we adopt an efficient salient object detection method MBD <ref type="bibr" target="#b44">[44]</ref> on SIFT flow <ref type="bibr" target="#b55">[55]</ref> to detect the moving regions. The objectness mask is detected by Mask R-CNN, which is the state-of-the-art method. In our implementation, we used the trained Mask R-CNN model (based on MS-COCO dataset) without any fine-tuning. We adopted the CRF model in <ref type="bibr" target="#b28">[28]</ref> for final segmentation refinement. It is worth mentioning that, for all the above models, we used the provided default parameters of these approaches without any fine-tuning, and we achieve the state-of-the-art performance ( as shown in Section IV-E).</p><p>Without additional specification hereafter, the reported results are based on the following parameter settings: for object proposals, the confidence threshold of object detection is set to 0.5 and the radius for image dilation operator is 6. The Otsu's method <ref type="bibr" target="#b52">[52]</ref> is used for adaptive threshold segmentation. The number of adaptive thresholds is set to 3 for salient motion segmentation and 2 for multi-frame object mask in our experiments. Besides, the number of previous frames for forward propagation refinement method is set to 2 (i.e. n = 2 in Section III-C).</p><p>Our method is mainly implemented in MATLAB and evaluated on a desktop with 1.7GHz Intel Xeon CPU and 32GB RAM. Given an image of resolution 480 × 854 pixels, the average processing time of the key components is shown in <ref type="table" target="#tab_1">Table I</ref>. From the table, we can see that the main computational cost of our approach lies in the optical flow estimation component, while the other components are very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets and Evaluation Metrics</head><p>To test the performance of our method, we evaluate it on two densely annotated (ground-truth masks on all video frames) VOS benchmark datasets: a high resolution DAVIS-2016 dataset <ref type="bibr" target="#b4">[5]</ref> and a low resolution SegTrack-v2 dataset <ref type="bibr" target="#b21">[21]</ref>. Beside, we report the performance of the proposed method on a sparsely annotated (ground-truth masks on a few video frames only) benchmark dataset FBMS-59 <ref type="bibr" target="#b29">[29]</ref>. 1) Datasets: The DAVIS-2016 dataset <ref type="bibr" target="#b4">[5]</ref> is currently the most challenging VOS benchmark, which contains 50 high resolution video sequences of diverse object categories and 3455 densely annotated pixel-wise ground-truth. Videos in this dataset are unconstrained and the challenging problems include appearance change, dynamic background, fast-motion, motion blur and occlusion.</p><p>SegTrack-v2 dataset <ref type="bibr" target="#b21">[21]</ref> is a widely used benchmark for VOS, which consists of 14 low resolution videos with a total of 1066 frames. The ground-truth of this dataset is also pixelwise annotated. The main challenges in SegTrack-v2 dataset include drastic appearance change, complex background, occlusion, abrupt motion and multiple moving objects. Similar to previous methods <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b26">[26]</ref>, we treated multiple objects with individual ground-truth as a single foreground for evaluation.</p><p>FBMS-59 dataset is composed of 59 videos, in which 29 are used for training and 30 for evaluation. Similar to the previous work <ref type="bibr" target="#b19">[19]</ref>, we report the performance of our method on 30 test videos for comparison. Besides, since the FBMS-59 dataset contains multiple moving objects, we also convert them to a single foreground.</p><p>2) Evaluation metrics: For quantitative analysis, the standard evaluation metrics: region similarity J , contour accuracy F and temporal stability T are adopted. Region similarity J is defined as the mean Intersection-over-Union (mIoU) of the estimated segmentation and the ground-truth mask. F measures the accuracy of the contours and T measures the temporal stability of the segmentation results in VOS. More description about the evaluation metrics can be found in <ref type="bibr" target="#b4">[5]</ref>. For performance comparison between the proposed segmentation and the state-of-the-art approaches, we utilized the provided codes and parameter configurations from the benchmark website 2 . Since mIoU denotes the region similarity between the segmentation result and ground-truth, we mainly analyze the performance of each algorithm with mIoU metric as in previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>To demonstrate the influence of each component in the proposed method, we reported the performance of different video frame optical flow salient motion map objectness mask salient motion mask fused mask <ref type="figure">Fig. 8</ref>. Examples of failed mask fusion on SegTrack-v2 dataset <ref type="bibr" target="#b21">[21]</ref>. Due to the low video resolution and cluttered background, Mask-RCNN <ref type="bibr" target="#b27">[27]</ref> failed to detect accurate object proposals with pretrained model. The video in first row and second row is birdfall and worm, respectively. modalities fusion on two densely annotated datasets DAVIS-2016 <ref type="bibr" target="#b4">[5]</ref> and SegTrack-v2 <ref type="bibr" target="#b21">[21]</ref>. To demonstrate the robustness and effectiveness of each component, we set θ = 0.9 and n = 2. Besides, all parameters in our method are kept same on these two datasets for performance evaluation. For ease of presentation, we denote the key component of our approach as follows.</p><p>• S: salient motion segmentation on optical flow field. • O: object proposals on current video frame. • P: forward propagation refinement with several previous segmentations. • C: coarse-to-fine segmentation with CRF.</p><p>Based on these components, the improvements of each additional component are reported in <ref type="table" target="#tab_1">Table II</ref>. Next, we detailedly analyze the effectiveness of each component in our approach.</p><p>1) Effectiveness of the mask fusion.: As a reminder, the mask fusion is to remove some potential segmentation noise, such as moving background and static objects. The moving regions and object regions are detected by the salient motion detection method and object proposal method, respectively.</p><p>As shown in <ref type="table" target="#tab_1">Table II</ref>, on DAVIS-2016 dataset, the mIoU of salient motion detection S is 57.1%, which denotes the accuracy of moving region segmentation. Similarly, the performance of object proposals O is 57.1%, which denotes the accuracy of object region segmentation. Based on our pixelwise fusion method, moving background regions and static objects can be effectively removed. Compared to the salient motion detection component S, the mIoU after fusion (S+O, 69.6%) is significantly improved by an absolute gain of 12.5%.</p><p>Similarly, on SegTrack-v2 dataset, the mIoU of salient motion detection component S is 47.3% and the object proposals O is 54.0%. Based on the proposed mask fusion method, the fused results S+O (55.3%) have achieved an absolute gain of 8.0% compared to S (47.3%).</p><p>Because the videos in SegTrack-v2 dataset are of lowresolution, the semantic object segmentation results of the object proposals model pretrained on MS-COCO dataset <ref type="bibr" target="#b53">[53]</ref> are not very good on some videos, and thus the improvement is not as high as in DAVIS-2016 dataset. As show in <ref type="figure">Figure 8</ref>, the moving objects are accurately extracted by salient motion segmentation. However, due to low video resolution and cluttered background in some complex scenes, Mask-RCNN <ref type="bibr" target="#b27">[27]</ref> failed to provide accurate generic object regions with the direct use of pretrained model. Therefore, it is expected that the performance of our method can be further improved by fine-tuning the object proposal model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Effectiveness of the forward propagation refinement:</head><p>In order to handle the unreliable salient motion detection and object proposals in individual video frame, we propose a forward propagation refinement method to improve the segmentation accuracy. As shown in <ref type="table" target="#tab_1">Table II</ref>, compared to the motion segmentation S+O between two video frames, the forward propagation refinement (S+O+P) can achieve absolute gain of 5.0% and 6.2% on DAVIS-2016 dataset and SegTrack-v2 dataset, respectively. Although the object movements and video quality are very different in these two datasets, the proposed method is still robust for both conditions.</p><p>3) Effectiveness of the CRF refinement: We also applied a CRF model for result refinement, and the segmentation accuracy can be further improved. As shown in <ref type="table" target="#tab_1">Table II</ref>, we achieve absolute gain of 2.6% and 2.8% on DAVIS-2016 dataset and SegTrack-v2 dataset, respectively. From the above results and analysis, we can see that each component of our model is very useful and can indeed improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Influence of Key Parameters</head><p>In this section, we analyze the influence of key parameters in our approach, including the accumulation weight θ and frame number of n for forward propagation refinement on two densely annotated DAVIS-2016 and SegTrack-v2 datasets.</p><p>1) Weight θ: θ is the weight that decide the contribution of the previous frames' segmentation results affect the current frame segmentation. When θ = 1.0, it denotes that no information from the previous frame propagates to the current one. As shown in the left of <ref type="figure">Figure 9</ref>, when the value of θ decreases from 1.0 to 0.7, the performance increases first and then slightly decreases on both datasets. The best performance is achieved by θ = 0.85 and 0.75 for DAVIS-2016 dataset and SegTrack-v2 dataset, respectively. Notice that the smaller the value of θ, the more information (segmentation results) from the previous frames are propagated to the current frame. Therefore, when θ becomes too small, the information from previous frames becomes dominating and thus deteriorates the performance <ref type="bibr" target="#b2">3</ref> . In our experiments, when θ is reduced to 0.7, the performance on both datasets is still better than θ = 1.0. This demonstrates that the proposed component (i.e. forward propagation refinement) is quite robust and can improve the performance within a wide range of θ.</p><p>2) Frame number n: Another key parameter is the number of previous frames, which decides how many previous segmentation masks are used for forward propagation refinement. n = 0 denotes the motion segmentation between two adjacent video frames. We analyze the forward propagation refinement with various n values and θ set to 0.9. the performance is shown in the right of <ref type="figure">Figure 9</ref>. From the results, we can see that the performance can be improved when n ∈ [1, 7] on both datasets. The larger the n is, the more previous frames are considered. When n is very large (e.g. n = 100), it means the information of frames which are far from the current frame (i.e. the 100-th frame before this frame) is also considered, which may lead to noisy information <ref type="bibr" target="#b3">4</ref> . In particular, the proposed method achieves the best performance of 74.63% when n = 2 on DAVIS-2016 dataset and 62.11% when n = 1 on SegTrack-v2 dataset. Because SegTrack-v2 dataset is having lower image resolution, the object proposals in SegTrack-v2 dataset is not as reliable as in DAVIS-2016 dataset, which is why the performance variation in DAVIS-2016 dataset is smoother, as shown in <ref type="figure">Figure 9</ref>.</p><p>E. Comparison to the State-of-the-art Methods 1) Baselines: We compared our method with several stateof-the-art unsupervised moving object segmentation methods to verify the effectiveness of our method. Based on whether they operate in offline or online manner, we group these competitors into two categories.</p><p>Unsupervised offline methods: To achieve good segmentation performance, offline methods often require the entire video sequence to generate long-term trajectories, and the  <ref type="figure">Fig. 9</ref>. Performance analysis with different weight θ and frame number n for forward propagation refinement on DAVIS-2016 and SegTrack-v2 dataset. θ = 1 and n = 0 denote the motion segmentation between two video frames, respectively. It can be seen that the proposed forward propagation refinement can improve the accuracy within a wide range of the parameters. Thus, it is not sensitive to these two parameters in both two datasets. moving objects are identified by motion or objectness cues. Based on the provided results of DAVIS-2016 dataset, the compared baselines include: ARP <ref type="bibr" target="#b19">[19]</ref>, FST <ref type="bibr" target="#b0">[1]</ref>, NLC <ref type="bibr" target="#b16">[16]</ref>, MSG <ref type="bibr" target="#b56">[56]</ref>, KEY <ref type="bibr" target="#b57">[57]</ref> and TRC <ref type="bibr" target="#b58">[58]</ref>, STP <ref type="bibr" target="#b59">[59]</ref> and ACO <ref type="bibr" target="#b60">[60]</ref>. Unsupervised online methods: Instead of generating longterm trajectories on the entire video sequence, online methods are able to segment the moving objects in a frame-by-frame manner. The compared baselines include: FSEG <ref type="bibr" target="#b26">[26]</ref>, LMP <ref type="bibr" target="#b6">[7]</ref>, CVOS <ref type="bibr" target="#b22">[22]</ref>, SAL <ref type="bibr" target="#b20">[20]</ref> and SFM <ref type="bibr" target="#b61">[61]</ref>. To be specific, FSEG <ref type="bibr" target="#b26">[26]</ref> and LMP <ref type="bibr" target="#b6">[7]</ref> are deep learning based methods which attempt to learn the moving patterns from optical flow field. FSEG <ref type="bibr" target="#b26">[26]</ref> fuses the appearance and motion in a two-stream fully convolutional neural network, where the appearancestream is used to extract the candidate object regions while the motion-stream is used to produce the moving foreground. LMP <ref type="bibr" target="#b6">[7]</ref> is also a fully convolutional network, which is learned from synthetic videos with ground-truth optical flow and motion segmentation. Based on the coarse motion segmentation, LMP adopts object proposals and CRF to refine the initial result. CVOS <ref type="bibr" target="#b22">[22]</ref> automatically segments moving objects with several frames, then a tracking strategy is used to propagate the initialized mask to subsequent frames. SAL <ref type="bibr" target="#b20">[20]</ref> is based on spatio-temporal saliency detection and performs VOS on multiple frames for online processing. SFM <ref type="bibr" target="#b61">[61]</ref> is a salient motion detection method that operates between two adjacent frames.</p><p>2) Quantitative Analysis: To demonstrate the performance of our approach, we compare it with several unsupervised methods on DAVIS-2016 <ref type="bibr" target="#b4">[5]</ref> dataset and SegTrack-v2 <ref type="bibr" target="#b21">[21]</ref> dataset. The quantitative comparison results on DAVIS-2016 dataset and SegTrack-v2 dataset are shown in <ref type="table" target="#tab_1">Table III and  Table IV</ref>. In addition, the compared algorithms and results on SegTrack-v2 dataset are obtained from a recent work <ref type="bibr" target="#b26">[26]</ref>. Similar to <ref type="bibr" target="#b26">[26]</ref>, we mainly analyze our method on the larger DAVIS-2016 dataset.</p><p>Performance on DAVIS-2016: Based on the optimum parameters from Section IV-D, we report the UOVOS results with parameter setting: θ = 0.85 and n = 5 on DAVIS-2016 dataset. <ref type="table" target="#tab_1">Table III</ref> shows the performance of our method with the region similarity J and contour accuracy F. It can be seen that our method achieves the best performance among all of the compared algorithms, including the best offline method </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure Offline Online</head><p>ARP <ref type="bibr" target="#b19">[19]</ref> FST <ref type="bibr" target="#b0">[1]</ref> NLC <ref type="bibr" target="#b16">[16]</ref> MSG <ref type="bibr" target="#b56">[56]</ref> KEY <ref type="bibr" target="#b57">[57]</ref> TRC <ref type="bibr" target="#b58">[58]</ref> FSEG <ref type="bibr" target="#b26">[26]</ref> LMP <ref type="bibr" target="#b6">[7]</ref> CVOS <ref type="bibr" target="#b22">[22]</ref> SAL <ref type="bibr" target="#b20">[20]</ref> SFM <ref type="bibr">[</ref> ARP. Especially, our approach obtains significant improvement in recall of both region similarity J (93.6%) and contour accuracy F (87.7%), which can achieve absolute gain of 4.4% and 4.9% respectively when compared to the best offline method ARP. Moreover, the decay of J and F, and temporal stability T of our method are also better than ARP. Because our method is an online one, we mainly analyze the comparisons with the state-of-the-art online methods. FSEG and LMP adopt an end-to-end deep learning framework for motion segmentation between two adjacent frames, and both of them fuse the optical flow field and object proposals for moving object segmentation. In contrast, our method is based on salient motion detection and object proposals, and thus it does not require further training on a large number of wellannotated data. Besides, since the video content often continuously changes, we use the important temporal connection of the video content for mask propagation among frames. As shown in <ref type="table" target="#tab_1">Table III</ref>, our method significantly outperforms the compared ones by a large margin. Specifically, our method outperforms FSEG by 6.2% and LMP by 8.1% on J metric. The online method CVOS is very sensitive to the object initialization and it suffers the drift problem when tracking the initialized object mask. As shown in <ref type="table" target="#tab_1">Table III</ref>, due to the unreliable online segmentation strategy, the accuracy of CVOS is only 51.4%. Another online approach SAL uses spatiotemporal saliency detection method to extract moving object regions. However, as the moving object is not always salient in some videos, and thus their segmentation result (42.6%) is also not good enough. SFM is a salient motion detection method, because it has not considered the object information and temporal connection of the video content, its segmentation result (53.2%) is also not very good.</p><p>Performance on SegTrack-v2: To demonstrate the performance of our method on the low-resolution dataset, we report the comparison results of our method with several available ones. Compared to the high-resolution DAVIS-2016 dataset, it is more difficult to predict accurate object regions with pretrained object proposals model on SegTrack-v2 dataset, as illustrated in <ref type="figure">Figure 8</ref>. NLC achieves the best performance on this dataset. However, it is an offline method based on non-local consensus voting of short-term and long-term motion saliency. Compared to the online method, our approach achieves better performance in most videos, as shown in <ref type="table" target="#tab_1">Table  IV</ref>.  Performance on FBMS-59: To further demonstrate the effectiveness of our method, we report the mIoU on the FBMS-59 test set. The results presented in <ref type="table" target="#tab_6">Table V</ref> are obtained from ARP. Without parameters fine-tuning, UOVOS still reports the best performance among the compared algorithms. Moreover, our algorithm achieves an absolute gain of +4.1 when compared to the offline method ARP.</p><p>3) Qualitative Evaluation: To qualitatively evaluate our method, we compare our method with several unsupervised offline and online methods on some challenging cases, including multiple moving objects, heavy occlusion, dynamic background, fast motion and motion blur, and non-planar scene. For performance comparison, we compare our method with the offline method (i.e. NLC <ref type="bibr" target="#b16">[16]</ref>), automatic initialization and tracking strategy based method CVOS <ref type="bibr" target="#b22">[22]</ref>, and two deep learning based methods (i.e. FSEG <ref type="bibr" target="#b26">[26]</ref> and LMP <ref type="bibr" target="#b6">[7]</ref>). The segmentation results on the above scenarios are illustrated in non-planar scene <ref type="figure">Fig. 10</ref>. Comparison on several challenging cases, which include multiple moving objects, heavy occlusion, dynamic background, fast motion and motion blur, and non-planar motion. The compared algorithms are NLC <ref type="bibr" target="#b16">[16]</ref> (offline), CVOS <ref type="bibr" target="#b22">[22]</ref>, FSEG <ref type="bibr" target="#b26">[26]</ref> and LMP <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure">Figure 10</ref>. We analyze the results of each scenario as follows. Multiple moving objects: An unconstrained video often contains multiple moving objects and the proposed UOVOS is able to segment them automatically. Similar to FSEG and LMP, for videos with multiple moving objects, we treat them as a single foreground. As shown in the first row of <ref type="figure">Figure 10</ref>, our method is able to segment the two moving objects in this video. For the offline method NLC, the moving person is classified as background which may be due to the small region size of this person. CVOS cannot automatically initialize the moving person, and thus failed to segment both of moving objects. The appearance stream of FSEG is not reliable to extract the object regions in this frame and failed to segment the moving person. Based on accurate motion segmentation and object proposals, LMP and UOVOS are able to successfully segment both objects. More results on multiple moving objects segmentation are reported in <ref type="table" target="#tab_1">Table IV</ref>, such as the bmx, drift, monkeydog and penguin videos.</p><p>Heavy occlusion: Occlusion is a very challenging problem in VOS, which can cause disconnected for long-term trajectories generation and drift problem for tracking. As shown in the second row of <ref type="figure">Figure 10</ref>, due to the disconnection trajectories caused by heavy occlusion, some background regions are classified as foreground by NLC. In addition, the segmentation is incomplete to cover the whole bus. CVOS uses an automatic object initialization and tracking strategy, and thus it suffers from the drift problem from tracking. The segmentation result of CVOS is also incomplete. LMP is learned on ground-truth of optical flow and motion segmentation of specific dataset and thus the performance of LMP is stable, such as the result shown in this frame. FSEG can achieve better performance by fusing object proposals and motion segmentation in a unified framework and our method is slightly better than FSEG.</p><p>Dynamic background: Dynamic background regions are difficult to remove without prior knowledge about the object. As shown in the third row of <ref type="figure">Figure 10</ref>, NLC and CVOS cannot get an accurate segmentation in this video. LMP failed to segment the moving object in this video. Because LMP adopts an end-to-end framework that learns the motion pattern from ground-truth optical flow and binary motion segmentation on the rigid scenes. Thus, it is difficult to obtain accurate results when the motion is caused by non-rigid background (such as waving water). Based on salient motion detection and robust object proposals, our approach achieves good segmentation results.</p><p>Fast motion and motion blur: When a object moves fast, it leads to unreliable optical flow estimation and motion blur. As shown in the fourth row of <ref type="figure">Figure 10</ref>, due to the fast car motion, the computed optical flow field is not accurate enough to indicate the moving car's region. Therefore, the segmentation result of NLC is incomplete and CVOS contains too many background regions. Similar to the dynamic background condition, LMP cannot obtain good segmentation when the computed optical flow field is not reliable. Based on the proposed robust forward propagation refinement, our method achieves better performance than FSEG in this frame.</p><p>Non-planar scene: Because of the nature of projecting a 3D world to a 2D plane (optical flow field), it is difficult to distinguish the moving foreground from static background when the scene is non-planar. As shown in the last row of <ref type="figure">Figure 10</ref>, due to the lack of prior knowledge about the object, the segmented foreground masks computed by NLC and CVOS are very different from each other, and both methods fail to obtain reliable segmentation results. With the help of robust object proposals, our method is able to obtain performance as good as that of FSEG and LMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we presented a new framework for the unsupervised online VOS problem. Motivated by two key properties of moving objects, namely "moving" and "generic", we propose to apply salient motion detection and object proposals techniques for this challenging problem. Moreover, we designed a pixel-level fusion method and a forward propagation refinement strategy to improve the segmentation performance. Comprehensive experiments performed on three benchmark datasets demonstrates the effectiveness of our method. Without fine-tuning the pre-trained Mask R-CNN model, our method can outperform existing state-of-the-art methods by a large margin. Besides, we indetail analyzed the results and showed how the proposed method deals with some challenging scenarios.</p><p>This work explores the potential of combining the salient motion detection and object proposal techniques for VOS. We hope that it can motivate more unsupervised online VOS studies on this new framework in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received July 20, 2018; revised January 10, 2019, March 30, 2019 and May 17, 2019; accepted July 9, 2019. This research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative. This research is also supported by National Natural Science Foundation of China 61571362 and Natural Science Basic Research Plan in Shaanxi Province of China (Program No. 2018JM6015). (Corresponding author: Tao Zhuo) T. Zhuo, Y. Wong and M. Kankanhalli are with the School of Computing, National University of Singapore, Singapore (email: {zhuotao,yongkang.wong}@nus.edu.sg and mohan@comp.nus.edu.sg). Z. Cheng is with the Qilu University of Technology (Shandong Academy of Sciences), Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Artificial Intelligence Institute, China, (email: jason.zy.cheng@gmail.com). P. Zhang is with the School of Computer Science, Northwestern Polytechnical University, China (email: zh0036ng@nwpu.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Examples of inaccurate salient motion mask with moving background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of unreliable objectness masks. The first two images show that some background regions are wrongly classified as generic objects, whereas the third image shows a missing object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>An illustration of the forward propagation refinement. The unreliable motion segmentation between two frames can be refined by propagating a set of previous segmentation masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Examples of segmentation refinement with CRF model. The first row shows the coarse segmentations whereas the second row shows the refined segmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>The proposed UOVOS framework Input: Video stream I = {I 1 , · · · , I T }, pretrained object proposals model O, image dilation radius r, the number of used previous video frames n, accumulation weight θ Output: binary segmentation masks L = {L 1 , · · · , L T } for t = 2:T do Optical flow field F t ← I t and I t−1 , SIFT Flow [55] Salient motion mapS t ← F t , MBD saliency [44] Salient motion mask S t ←S t , Eqn. 2 Objectness mask O t ← O and I t , Mask R-CNN [27] Motion segmentation P t ← r, S t and O t , Eqn. 3 if t &gt; n then Refined salient motion mapS t ← θ,S t and propagated masks {M t−n , · · · ,M t−1 }, Eqn. 4 Refined objectness mapŌ t ← θ, O t and propagated masks {M t−n , · · · ,M t−1 }, Eqn. 5 Refined segmentation M t ← r,S t andŌ t , Eqn. 6 CRF refinement L t ← M t and I t , CRF model [28] end end</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>RUN TIME OF EACH COMPONENT.</figDesc><table><row><cell>Component</cell><cell>Runtime (s)</cell></row><row><cell>Optical flow</cell><cell>8.0</cell></row><row><cell>Salient motion detection</cell><cell>0.01</cell></row><row><cell>Object proposals</cell><cell>0.3</cell></row><row><cell>Forward propagation refinement</cell><cell>0.05</cell></row><row><cell>CRF refinement</cell><cell>1.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY OF OUR METHOD WITH MIOU METRIC. THE IMPROVEMENT OF S+O IS COMPARED TO THE SALIENT MOTION SEGMENTATION S.</figDesc><table><row><cell>Fused components</cell><cell>DAVIS-2016</cell><cell>SegTrack-v2</cell></row><row><cell>S</cell><cell>57.1</cell><cell>47.3</cell></row><row><cell>O</cell><cell>57.1</cell><cell>54.0</cell></row><row><cell>S + O</cell><cell>69.6 (+12.5)</cell><cell>55.3 (+8.0)</cell></row><row><cell>S + O + P</cell><cell>74.6 ( +5.0)</cell><cell>61.5 (+6.2)</cell></row><row><cell>S + O + P + C</cell><cell>77.2 ( +2.6)</cell><cell>64.3 (+2.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III OVERALL</head><label>III</label><figDesc>RESULTS OF region similarity (J ), contour accuracy (F ) AND temporal stability (T ) ON DAVIS-2016 TRAINVAL DATASET. THE BEST RESULTS ARE MARKED IN Bold Font. OUR METHOD ACHIEVES SIGNIFICANT IMPROVEMENTS ON THE MEAN AND RECALL WITH BOTH J AND F METRICS. BESIDES, THE PROPOSED METHOD OUTPERFORMS THE UNSUPERVISED OFFLINE APPROACH ARP [19] ON ALL METRICS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV VIDEO</head><label>IV</label><figDesc>OBJECT SEGMENTATION RESULTS ON SEGTRACK-V2 DATASET WITH MIOU METRIC. AS REPORTED IN FSEG [26], WE SHOW THE RESULTS OF ALL 14 VIDEOS. THE RESULTS OF NLC ARE THE MIOU OVER 12 VIDEOS AS IN THEIR PAPER [16]. OUR METHOD OUTPERFORMS SEVERAL STATE-OF-ART METHODS, WHICH INCLUDE THE TWO-STREAM DEEP LEARNING BASED APPROACH FSEG [26].</figDesc><table><row><cell>Video</cell><cell></cell><cell>Offline</cell><cell></cell><cell cols="2">Online</cell></row><row><cell></cell><cell>FST [1]</cell><cell>KEY [57]</cell><cell>NLC [16]</cell><cell>FSEG [26]</cell><cell>UOVOS</cell></row><row><cell>birdfall</cell><cell>17.5</cell><cell>49.0</cell><cell>74.0</cell><cell>38.0</cell><cell>13.9</cell></row><row><cell>bird of paradise</cell><cell>81.8</cell><cell>92.2</cell><cell>-</cell><cell>69.9</cell><cell>79.7</cell></row><row><cell>bmx</cell><cell>67.0</cell><cell>63.0</cell><cell>79.0</cell><cell>59.1</cell><cell>62.4</cell></row><row><cell>cheetah</cell><cell>28.0</cell><cell>28.1</cell><cell>69.0</cell><cell>59.6</cell><cell>56.5</cell></row><row><cell>drift</cell><cell>60.5</cell><cell>46.9</cell><cell>86.0</cell><cell>87.6</cell><cell>84.3</cell></row><row><cell>frog</cell><cell>54.1</cell><cell>0.0</cell><cell>83.0</cell><cell>57.0</cell><cell>63.7</cell></row><row><cell>girl</cell><cell>54.9</cell><cell>87.7</cell><cell>91.0</cell><cell>66.7</cell><cell>76.6</cell></row><row><cell>hummingbird</cell><cell>52.0</cell><cell>60.2</cell><cell>75.0</cell><cell>65.2</cell><cell>64.5</cell></row><row><cell>monkey</cell><cell>65.0</cell><cell>79.0</cell><cell>71.0</cell><cell>80.5</cell><cell>87.4</cell></row><row><cell>monkeydog</cell><cell>61.7</cell><cell>39.6</cell><cell>78.0</cell><cell>32.8</cell><cell>51.4</cell></row><row><cell>parachute</cell><cell>76.3</cell><cell>96.3</cell><cell>94.0</cell><cell>51.6</cell><cell>88.4</cell></row><row><cell>penguin</cell><cell>18.3</cell><cell>9.3</cell><cell>-</cell><cell>71.3</cell><cell>50.9</cell></row><row><cell>soldier</cell><cell>39.8</cell><cell>66.6</cell><cell>83.0</cell><cell>69.8</cell><cell>83.2</cell></row><row><cell>worm</cell><cell>72.8</cell><cell>84.4</cell><cell>81.0</cell><cell>50.6</cell><cell>37.9</cell></row><row><cell>Average</cell><cell>53.5</cell><cell>57.3</cell><cell>80</cell><cell>61.4</cell><cell>64.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V VIDEO</head><label>V</label><figDesc>OBJECT SEGMENTATION RESULTS ON FBMS-59 TEST SET WITH</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MIOU METRIC.</cell><cell></cell><cell></cell></row><row><cell>FST [1]</cell><cell>STP [59]</cell><cell>NLC [16]</cell><cell>ACO [60]</cell><cell>ARP [19]</cell><cell>UOVOS</cell></row><row><cell>55.5</cell><cell>47.3</cell><cell>44.5</cell><cell>54.2</cell><cell>59.8</cell><cell>63.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://graphics.ethz.ch/ ∼ perazzif/davis/code.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The extreme case is when θ set to 0, which denotes that the information from previous frames overwrite the current frame and mislead the result.<ref type="bibr" target="#b3">4</ref> It is highly possible to introduce noisy information as the frame which is far from the current frame may contain very different content.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast appearance modeling for automatic primary video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="503" to="515" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised multiclass video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4321" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmentation in weakly labeled videos via a semantic ranking and optical warping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4025" to="4037" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3386" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance embedding transfer to unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6526" to="6535" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jump-Cut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH ASIA</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maerki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="743" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Background subtraction for freely moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1219" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="614" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="933" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, ser</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8692</biblScope>
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4083" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3442" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soattoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4268" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-occlusion and disocclusion in causal video object segmentation supplementary material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">It&apos;s moving! A probabilistic model for causal motion segmentation in moving camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>-M. Pia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bideau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, ser</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="433" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, ser</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">FusionSeg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3664" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">JOTS: Joint Online Tracking and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2229" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7415" to="7424" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust visual tracking revisited: From correlation filter to template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2777" to="2790" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rigid motion segmentation using randomized voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1210" to="1217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Moving object detection by detecting contiguous outliers in the low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Block-sparse RPCA for salient motion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1975" to="1987" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An approach to streaming video segmentation with sub-optimal low-rank decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1947" to="1960" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Double-constrained RPCA based on saliency maps for foreground detection in automated maritime surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-H</forename><surname>Zahzah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AVSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Minimum barrier salient object detection at 80 FPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1404" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Perceptual multichannel visual feature fusion for scene categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">429</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring visual and motion saliency for automatic video object extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Lien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2600" to="2610" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, ser</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="725" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1990" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context,&quot; in ECCV, ser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">GrabCut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SIFT flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1583" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1846" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos via alternate convex optimization of foreground and background distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="696" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<title level="m">Tao Zhuo is currently a Research Fellow at the School of Computing, National University of Singapore. He received the M.E. and PhD</title>
		<meeting><address><addrLine>Xian, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology from Northwestern Polytechnical University</orgName>
		</respStmt>
	</monogr>
	<note>respectively. His research interests include image/video processing, computer vision and machine learning</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">He received the Ph.D degree in computer science from Singapore Management University in 2016, and then worked as a Research Fellow in National University of Singapore. His research interests mainly focus on large-scale multimedia content analysis and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Www</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tkde</forename><surname>Ijcai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tcyb</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He has served as the PC member for several top conferences such as MM, MMM etc., and the regular reviewer for journals including TKDE, TIP, TMM etc</title>
		<imprint/>
		<respStmt>
			<orgName>Zhiyong Cheng is currently a Professor with Shandong Artificial Intelligence Institute, Qilu University of Technology (Shandong Academy of Sciences</orgName>
		</respStmt>
	</monogr>
	<note>His work has been published in a set of top forums, including ACM SIGIR, MM</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">His current research interests include object detection and tracking, computer vision and pattern recognition. He has published more than 80 high ranked international conference and journal papers and also has served as the technical committee in many international conferences and journals</title>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Singapore</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Peng Zhang is currently a full Professor in School of Computer Science, Northwestern Polytechnical University, China. He received the B.E. degree from the Xian Jiaotong University</orgName>
		</respStmt>
	</monogr>
	<note>He received his PhD from Nanyang Technological University</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">He obtained his BEng from the University of Adelaide and PhD from the University of Queensland. He has worked as a graduate researcher at NICTA&apos;s Queensland laboratory from</title>
	</analytic>
	<monogr>
		<title level="m">His current research interests are in the areas of Image/Video Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>National University of Singapore</orgName>
		</respStmt>
	</monogr>
	<note>Machine Learning, and Social Scene Analysis</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">He is the Director of N-CRiPT and also the Dean, School of Computing at NUS. Mohan obtained his BTech from IIT Kharagpur and MS &amp; PhD from the Rensselaer Polytechnic Institute</title>
	</analytic>
	<monogr>
		<title level="m">His current research interests are in Multimedia Computing, Multimedia Security &amp; Privacy, Image/Video Processing and Social Media Analysis. He is active in the Multimedia Research Community</title>
		<imprint/>
		<respStmt>
			<orgName>Mohan Kankanhalli is the Provost&apos;s Chair Professor at the Department of Computer Science of the National University of Singapore</orgName>
		</respStmt>
	</monogr>
	<note>and is on the editorial boards of several journals. Mohan is a Fellow of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
