<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang1@sydney.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. UDA is of particular significance since no extra effort is devoted to annotating target domain samples. However, the different data distributions in the two domains, or domain shift/discrepancy, inevitably compromise the UDA performance. Although there has been a progress in matching the marginal distributions between two domains, the classifier favors the source domain features and makes incorrect predictions on the target domain due to category-agnostic feature alignment. In this paper, we propose a novel category anchor-guided (CAG) UDA model for semantic segmentation, which explicitly enforces category-aware feature alignment to learn shared discriminative features and classifiers simultaneously. First, the category-wise centroids of the source domain features are used as guided anchors to identify the active features in the target domain and also assign them pseudo-labels. Then, we leverage an anchor-based pixel-level distance loss and a discriminative loss to drive the intra-category features closer and the inter-category features further apart, respectively. Finally, we devise a stagewise training mechanism to reduce the error accumulation and adapt the proposed model progressively. Experiments on both the GTA5→Cityscapes and SYNTHIA→Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the state-of-the-art methods. The code is available at https://github.com/RogerZhangzz/CAG_UDA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a classical computer vision task that refers to assigning pixel-wise category labels to a given image to facilitate downstream applications such as autonomous driving, video surveillance, and image editing. The recent progress in semantic segmentation has been dominated by deep neural networks trained on large datasets. Despite their success, annotating labels at the pixel level is prohibitively expensive and time-consuming, e.g., about 90 minutes for a single image in the Cityscapes dataset <ref type="bibr" target="#b7">[8]</ref>. One economical alternative is to exploit computer graphics techniques to simulate a virtual 3D environment and automatically generate images and labels, e.g., GTA5 <ref type="bibr" target="#b30">[31]</ref> and SYNTHIA <ref type="bibr" target="#b31">[32]</ref>. Although synthetic images have similar appearances to real images, there still exist subtle differences in textures, layouts, colors, and illumination conditions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>, which result in different data distributions, or domain discrepancy. Consequently, the performance of a certain model trained on synthetic datasets degrades drastically when applied to realistic scenes. To address this issue, one promising approach is domain adaptation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b12">13]</ref> to reduce the domain shift and learn a shared discriminative model for both domains. In this paper, we tackle the more challenging unsupervised domain adaptation (UDA) situation, where no labels are available in the target domain during training.</p><p>Previous methods have tried to learn domain-invariant representations by matching the distributions between source and target domains at the appearance level <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>, feature level <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13]</ref>, or output level <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26]</ref>. However, even though matching the global marginal distributions can bring the two domains closer, e.g., reaching a lower maximum mean discrepancy (MMD) <ref type="bibr" target="#b24">[25]</ref> or a saddle point in the minimax game via adversarial learning <ref type="bibr" target="#b12">[13]</ref>, it does not guarantee that samples from different categories in the target domain are properly separated, hence compromising the generalization ability. To tackle this issue, one could instead consider category-aware feature alignment by matching the local joint distributions of features and categories <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>. Other approaches adopt the idea of self-training by generating pseudo-labels for samples in the target domain and providing extra supervision to the classifier <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3]</ref>. Together with supervision from the source domain, this enforces the network to simultaneously learn domain-invariant discriminative feature representations and shared decision boundaries through back-propagation. The ideas of minimizing the entropy (uncertainty) of the output <ref type="bibr" target="#b38">[39]</ref> or discrepancies between the outputs of two classifiers (voters) <ref type="bibr" target="#b25">[26]</ref> have also been exploited to implicitly enforce category-level alignment.</p><p>Although category-level alignment and self-training methods have produced some promising results, there are still some outstanding issues that need to be addressed to further improve the adaptation performance. For example, error-prone pseudo-labels will mislead the classifier and accumulate errors. Meanwhile, implicit category-level alignment may be affected by category imbalance. To deal with these issues and take advantage of both approaches, here we propose a novel idea of category anchors, which facilitate both category-wise feature alignment and self-training. It is motivated by the observation that features from the same category tend to be clustered together. Moreover, the centroids of source domain features in each category can serve as explicit anchors to guide adaptation.</p><p>Specifically, we propose a novel category anchor-guided unsupervised domain adaptation model (CAG-UDA) for semantic segmentation. This model explicitly enforces category-wise feature alignment to learn shared feature representations and classifiers for both domains simultaneously. First, the centroids of category-wise features in the source domain are used as anchors to identify the active features in the target domain. Then, we assign pseudo-labels to these active features according to the category of the closest anchor. Lastly, two loss functions are proposed: the first is a pixel-level distance loss between the guiding anchors and active features, which pushes them closer and explicitly minimizes the intra-category feature variance; the other is a pixel-level discriminative loss to supervise the classifier and maximize the inter-category feature variance. To reduce the error accumulation of incorrect pseudo-labels, we propose a stagewise training mechanism to adapt the model progressively.</p><p>The main contributions of this paper can be summarized as follows. First, we propose a novel category anchor idea to tackle the challenging UDA problem in semantic segmentation. Second, we propose a simple yet effective category anchor-based method to identify active features in the target domain, further enabling category-wise feature alignment. Finally, the proposed CAG-UDA model achieves new state-of-the-art performance in both GTA5→Cityscapes and SYNTHIA→Cityscapes scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many recent advances in computer vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b4">5]</ref> have been based on deep neural networks trained on large-scale labeled datasets such as ImageNet <ref type="bibr" target="#b8">[9]</ref>, Pascal VOC <ref type="bibr" target="#b9">[10]</ref>, MS COCO <ref type="bibr" target="#b21">[22]</ref>, and Cityscapes <ref type="bibr" target="#b7">[8]</ref>. However, a domain shift between training data and testing data impairs model performance <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. To overcome this issue, a variety of domain adaptation methods for classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref>, detection <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16]</ref>, and segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b46">47]</ref> have been proposed. In this paper, we focus on the challenging semantic segmentation problem. The current mainstream approaches include style transfer <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>, feature alignment <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>, and self-training <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b20">21]</ref>. As our work is most related to the latter two approaches, we briefly review and discuss their characteristics.</p><p>Feature distribution alignment: Previous methods that match the global marginal distributions between two domains <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref> do not distinguish local category-wise feature distribution shifts. Consequently, error-prone predictions are made for misaligned features with shared decision bound-aries. In contrast to these methods, we propose a category-wise feature alignment method to explicitly reduce category-level mismatches and learn discriminative domain-invariant features. The idea of category-level feature alignment was also exploited in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref> for semantic segmentation. Luo et al. proposed a weighted adversarial learning method to align the category-level feature distributions implicitly <ref type="bibr" target="#b25">[26]</ref>. Saito et al. tried to align the feature distributions and learn discriminative domain-invariant features by utilizing task-specific classifiers as a discriminator <ref type="bibr" target="#b32">[33]</ref>. In contrast to the implicit feature alignment in the aforementioned methods, we propose a novel category anchor-guided method, which directly aligns category-wise features in both domains.</p><p>Pseudo-label assignment: Assigning pseudo-labels to target domain samples based on the trained classifiers helps adapt the feature extractor and classifier to the target domain. Zou et al. <ref type="bibr" target="#b46">[47]</ref> proposed an iterative self-training UDA model by alternatively generating pseudo-labels and retraining the model. They also dealt with the category imbalance issue by controlling the proportion of selected pseudo-labels in each category <ref type="bibr" target="#b46">[47]</ref>. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed a bidirectional learning domain adaptation model that alternately trains the image translation model and the self-supervised segmentation adaptation model. In contrast to these methods, where pseudo-labels were determined according to the predicted category probability, we propose a category anchor-based method to generate trustable pseudo-labels. Compared with selected samples that have been "correctly" classified with high confidence, our selected samples are not determined by the decision boundaries so are more informative for the classifier to further adapt to the target domain.</p><p>The idea of assigning pseudo-labels based on category centers has also been utilized in domain adaptation for classification, e.g., category centroids in <ref type="bibr" target="#b40">[41]</ref>, prototypes in <ref type="bibr" target="#b2">[3]</ref>, and cluster centers in <ref type="bibr" target="#b18">[19]</ref>. The former two methods minimize the distance loss against category centroids, while the third minimizes contrastive domain discrepancies. Our method differs from these methods in several ways. First, we tackle the more challenging task of image semantic segmentation rather than image classification, where dense pixel-wise labels need to be predicted as not just single labels for entire images. Second, we fix the category centroids (hence called category anchors) instead of updating them at each iteration. On one hand, the mini-batch size used for segmentation (e.g., 1) in this paper is much smaller than that used for classification. On the other hand, pixels are spatially coherent in an image, so the category centroids calculated at each iteration will be biased and unreliable due to the dominance of homogeneous features. Third, the pseudo-labels of target domain samples are determined by their distance against the category centroids from the source domain instead of the target domain. This is reasonable since: 1) the source domain category centroids are calculated from all training samples based on ground-truth labels, which are reliable; 2) driving the target domain features towards the source domain category centroids can effectively reduce the domain discrepancy. Fourth, together with the category anchor-based distance loss, we also add the segmentation loss based on the pseudo-labeled target samples to learn discriminative feature representations and adapt the decision boundaries simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A category anchor-guided UDA model for semantic segmentation 3.1 Problem Formulation</head><p>Supervised semantic segmentation: A semantic segmentation model M can be formulated as a mapping function from the image domain X to the output label domain Y :</p><formula xml:id="formula_0">M : X → Y,<label>(1)</label></formula><p>which predicts a pixel-wise category labelŷ close to the ground-truth annotation y ∈ Y for a given image x ∈ X. Usually, the segmentation model M is trained in a supervised manner by minimizing the difference between the predictionŷ and its ground-truth y for every training sample x. The cross-entropy (CE) loss is widely used as a measurement, which is defined as:</p><formula xml:id="formula_1">LCE = − N i=1 H×W j=1 C c=1 yijclog (pijc) ,<label>(2)</label></formula><p>where N is the number of training images, H and W denote the image size, j is the pixel index, C is the number of categories, c is the category index, y ijc ∈ {0, 1} is the one-hot vector representation of the ground-truth label, i.e., ∀i, j, c y ijc = 1, and p ijc is the predicted category probability by M .</p><p>UDA for semantic segmentation: Generally, a segmentation model trained on a source domain X s has a limited generalization capability to a target domain X t , when the distributions between X s and X t are different, i.e., there is a domain shift/discrepancy. Several unsupervised domain adaptation models have been proposed, which can be formulated as the following mapping function:</p><formula xml:id="formula_2">M uda : Xs ∪ Xt → Ys ∪ Yt,<label>(3)</label></formula><p>where M uda is trained on the labeled training samples (X s , Y s ) in the source domain together with the training unlabeled samples X t in the target domain. Typically, the aforementioned CE loss and some domain-adaptation losses are used to align the distributions of both domains (e.g., p (X s ) and p (Y s )) and to learn domain-invariant discriminative feature representations.</p><p>Model components: The main semantic segmentation approaches have been based on fully convolutional neural networks (CNNs) since the seminal work in <ref type="bibr" target="#b23">[24]</ref>. Usually, a DCNN-based model has two parts: an encoder Enc and a decoder Dec, where the encoder maps the input image into a low-dimensional feature space and then the decoder decodes it to the label space. The decoder can be further divided into a feature transformation net f D and a classifier Cls, where Cls denotes the last classification layer and f D denotes the remaining part in Dec. Typical encoders are the classification networks pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, e.g., VGGNet <ref type="bibr" target="#b34">[35]</ref> and ResNet <ref type="bibr" target="#b11">[12]</ref>. The decoder consists of convolutional layers responsible for context modeling, multi-scale feature fusion, etc. UDA methods typically employ a segmentation model with carefully designed modules for domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>The network architecture of our proposed CAG-UDA model is shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>. The CAG-UDA model employs Deeplab v2 <ref type="bibr" target="#b3">[4]</ref> as the base segmentation model, where ResNet-101 is used as the encoder Enc and the ASPP module is used in the decoder Dec. To reduce the domain shift, we devise a category anchor-guided alignment module on the features from f D , consisting of category anchor construction (CAC), active target sample identification (ATI), and pseudo-label assignment (PLA) as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). The details are as follows.</p><p>Category anchor construction (CAC): Based on the observation that pixels in the same category cluster in the feature space, we propose to calculate the centroids of the features of each category in the source domain as a representative of the feature distribution, i.e., the mean. Considering that the features fed into the classifier directly relate to the decision boundaries, we choose the features from f D to calculate these centroids. Mathematically, this can be written as:</p><formula xml:id="formula_3">f s c = 1 |Λ s c | N i=1 H×W j y s ijc (fD (Enc (x s i )) |j) ,<label>(4)</label></formula><p>where Λ s c is the index set of all pixels on the training images in the source domain X s belonging to the c th category, i.e., Λ s c = {(i, j) |y s ijc = 1}, |Λ s c | denotes the number of pixels in Λ s c , i.e.,</p><formula xml:id="formula_4">|Λ s c | = N i=1</formula><p>H×W j</p><p>yijc, and f D (x s i ) | j is the feature vector at index j on the feature map f D (x s i ). It is noteworthy that we calculate the category centroids at the beginning of each training stage and then keep them fixed during training (we propose a stagewise training mechanism in Section 3.4.). Therefore, we call these centroids category anchors (CAs) in this paper, i.e., CA = {f s c , c = 1, ..., C}.</p><p>Active target sample identification (ATI): To align the category-wise feature distributions between two domains, we expect that the category centroids from the target domain get closer to the category anchors during training. However, on one hand, target sample labels are unavailable. On the other hand, the calculated centroids on target samples are very unstable at each iteration since the minibatch size is very small (i.e., 1) in this paper and image pixels are spatially coherent. To tackle these issues, we propose identifying active target samples and assigning them pseudo-labels for the subsequent feature alignment. The term "active target samples" refers to target samples near one category anchor and far from the other anchors, i.e., being activated by one specific category anchor. Mathematically, this can be formulated as follows. We first define the distance between a target feature f D (Enc (x t i )) | j and the c th category anchor as</p><formula xml:id="formula_5">d t ijc = f s c − fD Enc x t i |j 2 ,<label>(5)</label></formula><p>where · 2 is the L 2 norm of a vector. Then, we sort {d t ijc , c = 1, ..., C} in an ascending order and compare the shortest distance d t ijc * with the second shortest d t ijc . If their difference is larger than a predefined threshold d , we identify this target sample as active one, i.e.,</p><formula xml:id="formula_6">a t ij = 1, d t ijc − d t ijc * &gt; d , 0, otherwise,<label>(6)</label></formula><p>where a t ij denotes the active state of the target feature f D (Enc (x t i )) | j . Like the category anchors, we calculate the active states at the beginning of each training stage and keep them fixed during subsequent stages. This is explained in Section 3.4, where we introduce a stagewise training mechanism.</p><p>Pseudo-label assignment (PLA): After we obtain the active state according to Eq. (6), a pseudo label c * can be assigned to x t i according to its closest category anchor f s c * with a reliable margin d :</p><formula xml:id="formula_7">y t ijc * = 1, if d t ijc * &lt; d t ijc − d , ∀c = c * .<label>(7)</label></formula><p>Due to the lack of the target domain labels, the classifier layer is biased to the source domain and does not generalize well to the target domain, as shown in <ref type="figure" target="#fig_0">Figure 1(c)</ref>. Consequently, some of the pseudo-labels from predicted probabilities may be error-prone. However, based on the observation of the intra-category clustering characteristics, the generated pseudo-labels via category anchors are independent of the biased classifier and are thus more reliable than those assigned by predicted category probabilities. Further, considering that high-probability samples have been "correctly" classified by the classifier layer with high confidence, these samples provide only weak supervision signals. In contrast, active samples are more informative for adapting the classifier to the target domain as the classifier layer may not predict these active samples with high probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Objective Functions</head><p>When training the CAG-UDA model, we leverage a CE loss L s CE as defined in Eq. <ref type="bibr" target="#b1">(2)</ref>. We also propose a category-wise distance loss L s dis on the source domain samples and two domain adaptation losses on the active target samples, i.e., a CE loss L t CE and a category-wise distance loss L t dis based on the pseudo-labels, to guide the adaptation process. These are defined as:</p><formula xml:id="formula_8">L s dis = N i=1 H×W j=1 C c=1 y s ijc f s c − fD (Enc (x s i )) |j 2 ,<label>(8)</label></formula><formula xml:id="formula_9">L t CE = − M i=1 H×W j=1 a t ij C c=1ŷ t ijc log p t ijc ,<label>(9)</label></formula><formula xml:id="formula_10">L t dis = M i=1 H×W j=1 a t ij C c=1ŷ t ijc f s c − fD Enc x t i |j 2 .<label>(10)</label></formula><p>Although only the active samples are directly driven towards the category anchors by L t dis , other inactive target samples within each category may also follow the active samples due to being clustered. Therefore, minimizing L t dis indeed reduces the intra-category variances in the target domain. Meanwhile, L t CE leverages the pseudo-labels to update the network weights together with the source domain CE loss, prompting the encoder, decoder, and classifier to adapt to the target domain and therefore reducing the intra-and inter-category variances simultaneously. The illustration is show in <ref type="figure" target="#fig_0">Figure 1(c)</ref>. To leverage the complementarity between the proposed category anchor-based PLA and category probability-based PLA in <ref type="bibr" target="#b46">[47]</ref>, we also identify active target samples based on the predicted category probability and add an extra CE loss L tP CE similar to Eq. (9).</p><formula xml:id="formula_11">L tP CE = − M i=1 H×W j=1 a tP ij C c=1ŷ tP ijc log p t ijc ,<label>(11)</label></formula><p>where a tP ij , y tP ijc refer to the probability-based active state and assigned pseudo-labels respectively. Then the final objective function is as follows:</p><formula xml:id="formula_12">L = L s CE + λ1 L s dis + L t dis + λ2 L t CE + L tP CE ,<label>(12)</label></formula><p>where λ 1 and λ 2 are loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Stagewise Training Procedure</head><p>We tried to train the CAG-UDA model in a single stage and update the pseudo-labels at each iteration. However, it is not stable because there are some error-prone pseudo-labels, which may produce incorrect supervision signals, lead to more erroneous pseudo-labels iteratively and trap the network to a local minimum with poor performance eventually, e.g. less than 30 mIoU. To address this issue, we propose a stagewise training mechanism as summarized in Algorithm 1. First, we pretrain the segmentation model on the source domain. Then, we leverage the global feature alignment method in <ref type="bibr" target="#b13">[14]</ref> to warm up the training process and obtain a well-initialized model. Next, we train the CAG-UDA model with the proposed losses for several stages. At the beginning of each stage, we calculate the CAs, identify the active target samples, and assign pseudo-labels to them. By using this stagewise delayed updating mechanism, we avoid updating the pseudo-labels at each iteration and reduce the error accumulation. Hence, L t dis and L t CE serve as two regularizations on the network.  for n ← 1 to L do 8: M k ← M k−1 11: end for 12: Prediction: (Ŷ s ,Ŷ t ) ← (X s , X t ) and M K .</p><formula xml:id="formula_13">ATI: {d t ijc }, {a t ij } ← M k−1 , (X s , Y s , X t ),</formula><formula xml:id="formula_14">SGD: training M k−1 on (X s , Y s , X t , {ŷ t ijc * }, {f s c },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets and evaluation metrics: Following <ref type="bibr" target="#b20">[21]</ref>, we evaluate the CAG-UDA model in two common scenarios, GTA5 <ref type="bibr" target="#b30">[31]</ref>→Cityscapes <ref type="bibr" target="#b7">[8]</ref> and SYNTHIA <ref type="bibr" target="#b31">[32]</ref>→Cityscapes <ref type="bibr" target="#b7">[8]</ref>. GTA5 contains  Cityscapes is divided into a training set, a validation set, and a testing set. The training set consists of 2,957 2048×1024-pixel images and the validation set contains 500 images at the same resolution. Following common practice, we report the results on the Cityscapes validation set, specifically, the category-wise intersection over union (IoU). Moreover, we also report the mean IoU (mIoU) of all 19 categories in the GTA5→Cityscapes scenario and the 16 common categories in the SYNTHIA→Cityscapes scenario. Some methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref> only reported mIoU for 13 common categories in the SYNTHIA→Cityscapes scenario, denoted as mIoU* in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details:</head><p>In our experiments, training images were randomly cropped to 1280×640 pixels after being randomly resized by ×1 ∼ ×1.5. Due to GPU memory limitations, the batch size was set to 1 and the weights of all batch normalization layers were frozen. In the warm-up phase, we used a CNN-based domain discriminator comprising 5 convolutional layers of kernel size 3×3, filter numbers [64, 128, 256, 512, 1], and stride 2. The first three convolutional layers are followed by a ReLU layer, while the fourth layer is followed by a leaky ReLU layer parameterized by 0.2. We used a CE loss and an adversarial loss to train the model for 20 epochs. The adversarial loss weights were set to 1e-2. In the stagewise training phase, we trained the CAG-UDA mode for 20 epochs with the SGD optimizer. The initial learning rate was 2.5e-4, which decayed by the poly policy with power 0.9. The weight decay, momentum, λ 1 , and λ 2 were set to 1e-4, 0.9, 0.3, and 0.7, respectively. d was set to 2.5. We also assigned pseudo-labels based on predicted category probabilities, and the threshold P 0 was set to 0.95. Experiments were conducted on a TITAN Tesla V100 GPU with PyTorch implementation. Code will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Quantitative results: The results of the GTA5→Cityscapes scenario are presented in <ref type="table" target="#tab_1">Table 1</ref> with the best results highlighted in bold. All the models adopted ResNet-101 as a backbone network for fair comparison. Overall, our CAG-UDA model strikingly outperforms all other models with a 50.2 mIoU, surpassing the model trained on the source domain by a significant gain of 16.1. Compared with CLAN <ref type="bibr" target="#b25">[26]</ref> and DISE <ref type="bibr" target="#b1">[2]</ref>, which implicitly align category-level features, our model achieves an extra gain of 4.5 and outperforms them on fence, traffic sign, rider, train, and bike by large margins. This is due to the proposed category anchor-guided alignment method, which explicitly uses category centroids as representatives of feature distributions, reducing the side effect of category imbalance. Like <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b12">13]</ref>, BLF in <ref type="bibr" target="#b20">[21]</ref> also involves a style-transfer module but combines it with self-training in a bidirectional learning framework. It achieved the second-best mIoU of 48.5. BLF achieves better results than the CAG-UDA model on stuff categories such as road, building, wall, terrace, and sky but is inferior to the CAG-UDA model for small objects. This is because BLF includes a  style-transfer module that benefits from the texture clues in the stuff categories and assigns reliable pseudo-labels accordingly. In contrast, CAG-UDA uses a category-anchor guided method that can tackle the category imbalance and generate more informative pseudo-labels, leading to better results on more categories.</p><p>We also present the result on the testing set of the Cityscapes dataset in <ref type="table" target="#tab_2">Table 2</ref>. The CAG-UDA model reaches 51.7 mIoU, proving the good generalization of our method.</p><p>Results in the SYNTHIA→Cityscapes scenario are listed in <ref type="table" target="#tab_3">Table 3</ref>. Same as the previous work, we report the performance of the CAG-UDA model in two mIoU metrics: 13 categories (mIoU*) and 16 categories (mIoU) for fair comparisons. Since the domain shift is much larger than the above scenario, the performance is slightly worse. The CAG-UDA model still achieves better results than all previous SOTA methods, including CLAN, BLF, etc. Similar to the above discussions with the GTA5 dataset, the superiority of the CAG-UDA model remains in small objects like pole, sign, person, and bike.</p><p>Qualitative results: Some qualitative segmentation examples are given in <ref type="figure" target="#fig_3">Figure 2(a)</ref>. Training merely on the source domain dataset leads to a limited generalization ability, e.g., the road and person were incorrectly predicted as sidewalk and building in the first row. Benefited from the category anchor-guided adaptation, the proposed CAG-UDA model achieves better results, especially for small objects, e.g., pole, sign, and person. Besides, we also attribute it to the proposed CAs-based pseudo label assignment, which successfully activated small objects and assigned them trustable pseudolabels, as highlighted in red circles in <ref type="figure" target="#fig_3">Figure 2(b)</ref>. More results can be found in the supplement. Ablation studies: The ablation study results are listed in <ref type="table" target="#tab_4">Table 4</ref>. We add a superscript P to the symbols of losses to denote that the active target samples are identified by category probabilities as described in Section 3.3. Several models were trained by combining L t CE with different losses. As can be seen from the 2 nd and 3 rd rows, the proposed category anchor-guided PLA is more effective than the predicted category probability-based one. More detailed comparisons of different hyper-parameters can be found in the supplement. In addition, the CE loss is more effective than the distance loss. The results in the 4 th row demonstrate the complementarity between the CE loss and distance loss, as well as between the category anchor-based and probability-based PLA. We combine them as in Eq. (12) to train the CAG-UDA model and obtain a better result as listed in the bottom row. Finally, the stagewise trained CAG-UDA model obtains an mIoU of 50.2, outperforming the SOTA models. Besides, the CAG-UDA model has been trained for an extra stage, e.g., Stage 4. However, it is saturated at 50.2 mIoU with no improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Limitations</head><p>The proposed CAG-UDA model relies on reliable pseudo-labels to guarantee a correct supervision imposed on the network to be trained. To this end, we adopt a warm-up strategy to roughly align two domains together and increase the reliability of the generated pseudo-labels by the CAs, as described in Section 3.4. In contrast, we also conducted an experiment by removing the warm-up stage and observed a significant drop of 6.3 mIoU. Some techniques can also be used to obtain reliable pseudo-labels such as enforcing local smoothness on the probability map, utilizing a normalized threshold during assigning pseudo-labels, and reducing the appearance bias through a style transfer module. We leave it as the future work to build a stage-free and end-to-end CAG-UDA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel category anchor-guided (CAG) unsupervised domain adaptation (UDA) model for semantic segmentation. The CAG-UDA model successfully adapts the segmentation model to the target domain through category-wise feature alignment guided by category anchors. Specifically, we proposed a category anchor construction module, an active target sample identification module, and a pseudo-label assignment module. We utilized a distance loss and a CE loss based on the identified active target samples, which complementarily enhance the adaptation performance. We also proposed a stagewise training mechanism to reduce the error accumulation and adapt the CAG-UDA model progressively. The experiments on the GTA5 and SYNTHIA datasets demonstrate the superiority of the CAG-UDA model over representative methods on generalization to the Cityscapes dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of the proposed category anchor-guided UDA model for semantic segmentation. (a) The architecture of the proposed CAG-UDA model consists of an encoder, a feature transformer (f D ), and a classifier. The green part denotes the source domain flow while the orange parts represent the target domain flow. (b) The illustration of the process of active target sample identification and pseudo label assignment described in Section 3.2. (c) The illustration of the proposed category-wise feature alignment with the anchor-based pixel-level distance loss L dis and cross-entropy loss L CE described in Section 3.3. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Stagewise training the CAG-UDA model Input: training dataset: (X s , Y s , X t ), maximum stages: K, maximum iterations: L, distance threshold: d . Output: M K and (Ŷ s ,Ŷ t ). 1: Pretraining: M p 0 ← (X s , Y s ) according to [4]; 2: Warm-up: M 0 ← (X s , Y s ) and M p 0 according to [14]; 3: for k ← 1 to K do 4: CAC: {f s c } ← M k−1 and (X s , Y s ) according to Eq. (4); 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>{f s c } and d according to Eq. (5) and Eq. (6); 6: PLA: {ŷ t ijc * } ← {d t ijc }, d according to Eq. (7); 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>(a) Subjective evaluation of the CAG-UDA model on some images from the Cityscapes validation set. (b) Comparison between probability-based PLA and the proposed CAs-based PLA on an image from the Cityscapes training set. Best viewed in color and zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of the CAG-UDA model and SOTA methods ( GTA5→Cityscapes).<ref type="bibr" target="#b15">16</ref>.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6 AdaptSegNet[36] 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.2 50.2</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>terrace</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="21">Source only 75.8 Source only 69.9 22.3 75.6 15.8 20.1 18.8 28.2 17.1 75.6 8.00 73.5 55.0 2.9 66.9 34.4 30.8 0.00 18.4 0.00 33.3</cell></row><row><cell>DCAN[40]</cell><cell cols="20">85.0 30.8 81.3 25.8 21.2 22.2 25.4 26.6 83.4 36.7 76.2 58.9 24.9 80.7 29.5 42.9 2.50 26.9 11.6 41.7</cell></row><row><cell>Source only</cell><cell cols="20">75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6</cell></row><row><cell>CLAN[26]</cell><cell cols="20">87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2</cell></row><row><cell>AdvEnt[39]</cell><cell cols="20">89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.5</cell></row><row><cell>DISE[2]</cell><cell cols="20">91.5 47.5 82.5 31.3 25.6 33.0 33.7 25.8 82.7 28.8 82.7 62.4 30.8 85.2 27.7 34.5 6.4 25.2 24.4 45.4</cell></row><row><cell>Cycada[13, 21]</cell><cell cols="20">86.7 35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19.0 65.0 12.0 28.6 4.5 31.1 42.0 42.7</cell></row><row><cell>Source only</cell><cell cols="20">69.0 12.7 69.5 9.9 19.5 22.8 31.7 15.3 73.9 11.3 67.2 54.7 23.9 53.4 29.7 4.6 11.6 26.1 32.5 33.6</cell></row><row><cell>BLF[21]</cell><cell cols="20">91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5</cell></row><row><cell>Source only</cell><cell cols="20">69.8 25.4 74.7 11.3 18.3 24.2 35.6 23.3 72.0 14.4 65.3 58.7 29.0 53.1 14.3 19.2 7.9 15.1 16.3 34.1</cell></row><row><cell>CAG-UDA</cell><cell>90.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of the CAG-UDA model on the testing set ( GTA5→Cityscapes).</figDesc><table><row><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>terrace</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="20">CAG-UDA 93.2 57.0 85.6 35.7 25.1 37.5 30.8 45.3 87.1 50.1 89.4 62.7 40.8 87.8 18.0 32.4 34.5 34.4 35.4 51.7</cell></row><row><cell cols="20">24,966 1914×1052-pixel images and has the same 19 category annotations as Cityscapes. SYN-</cell></row><row><cell cols="20">THIA contains 9,400 1914×1052-pixel images and only has 16 common category annotations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of the CAG-UDA model and SOTA methods ( SYNTHIA→Cityscapes).</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vegetable</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motor</cell><cell>bike</cell><cell>mIoU mIoU*</cell></row><row><cell cols="4">AdaptSegNet[36] 79.2 37.2 78.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">9.9 10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3</cell><cell>-</cell><cell>45.9</cell></row><row><cell>CLAN[26]</cell><cell cols="3">81.3 37.0 80.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7</cell><cell>-</cell><cell>47.8</cell></row><row><cell>BLF[21]</cell><cell cols="3">86.0 46.7 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>-</cell><cell>51.4</cell></row><row><cell>CAG-UDA(13)</cell><cell cols="3">84.8 41.7 85.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">13.7 23.0 86.5 78.1 66.3 28.1 81.8 21.8 22.9 49.0</cell><cell>-</cell><cell>52.6</cell></row><row><cell>DCAN[40]</cell><cell cols="17">82.8 36.4 75.7 5.1 0.1 25.8 8.0 18.7 74.7 76.9 51.1 15.9 77.7 24.8 4.1 37.3 38.4</cell><cell>-</cell></row><row><cell>DISE[2]</cell><cell cols="7">91.7 53.5 77.1 2.5 0.2 27.1 6.2</cell><cell cols="10">7.6 78.4 81.2 55.8 19.2 82.3 30.3 17.1 34.3 41.5</cell><cell>-</cell></row><row><cell>AdvEnt[39]</cell><cell cols="7">85.6 42.2 79.7 8.7 0.4 25.9 5.4</cell><cell cols="10">8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0 41.2</cell><cell>-</cell></row><row><cell>CAG-UDA(16)</cell><cell cols="17">84.7 40.8 81.7 7.8 0.0 35.1 13.3 22.7 84.5 77.6 64.2 27.8 80.9 19.7 22.7 48.3 44.5</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of ablation study (GTA5→Cityscapes).<ref type="bibr" target="#b24">25</ref>.4 74.7 11.3 18.3 24.2 35.6 23.3 72.0 14.4 65.3 58.7 29.0 53.1 14.3 19.2 7.9 15.1 16.3 34.1 -Warm-up 88.4 45.2 82.0 30.1 22.0 35.4 36.7 23.7 82.7 27.6 70.8 51.4 26.9 81.5 14.5 25.0 21.4 13.0 7.9 41.4 7.3 +L tP CE 88.8 45.5 83.7 33.2 21.4 39.5 40.0 25.9 83.9 33.8 74.3 58.2 24.9 84.8 19.3 32.8 22.6 15.0 14.7 44.3 10.2 +L t CE 88.3 46.9 81.5 28.7 27.7 38.9 27.0 40.4 83.7 31.2 74.9 61.8 30.2 84.0 15.9 36.7 23.4 23.3 31.7 46.1 12.0 46.6 82.1 30.2 28.4 39.7 31.3 38.8 83.6 30.7 75.1 61.9 28.5 84.3 16.3 36.3 29.1 25.0 29.4 46.6 12.5 +L t CE + L tP CE 88.9 47.1 83.0 31.0 27.3 39.7 31.0 36.0 84.3 32.6 75.1 62.0 29.4 84.6 16.6 35.7 27.2 19.2 28.4 46.3 12.2 CAG-UDA (Stage 1) 88.8 47.5 83.6 31.7 29.1 39.7 34.4 35.6 84.4 33.0 76.8 62.1 28.2 84.5 17.2 35.2 32.0 25.8 27.6 47.2 13.1 CAG-UDA (Stage 2) 90.4 50.6 84.0 33.5 28.3 39.9 31.6 42.4 85.1 35.2 77.3 61.5 34.2 84.9 19.4 41.7 41.0 27.3 32.0 49.5 15.4 CAG-UDA (Stage 3) 90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.2 50.2 16.1</figDesc><table><row><cell></cell><cell>road</cell><cell>side.</cell><cell>buil.</cell><cell>wall</cell><cell>fenc.</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>terr.</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU gain</cell></row><row><cell cols="21">Source only 69.8 +L sP dis + L tP 89.4 40.1 81.8 31.0 22.6 39.9 41.2 23.2 83.0 28.3 68.5 54.5 23.8 85.7 21.5 25.6 0.7 13.9 8.5 dis +L s dis + L t dis 88.9 41.7 82.0 31.7 22.5 39.7 41.2 23.5 82.7 27.0 70.0 57.8 25.7 85.8 21.9 27.7 1.1 18.0 11.1 42.1 41.2</cell><cell>7.1 8.0</cell></row><row><cell>+L s dis + L t dis + L t CE</cell><cell>88.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the Australian Research Council Project FL-170100117 and the National Natural Science Foundation of China Project 61806062.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08585</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (Neurips)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cycada: Cycleconsistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5001" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stacked robust adaptively regularized autoregressions for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="561" to="574" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge transfer for spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="484" to="496" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00976</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (Neurips)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10620</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (Neurips)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09478</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint intermodal and intramodal label transfers for extremely rare or unseen classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1360" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (Neurips)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3752" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Virtual and real world adaptation for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="797" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12833</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5419" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast haze removal for nighttime image using maximum reflectance prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7418" to="7426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully point-wise convolutional neural network for modeling statistical regularities in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Famed-net: A fast and accurate multi-scale end-to-end dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="72" to="84" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2020" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
