<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Learning by Convex Combination of Semantic Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
							<email>norouzi@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Google, Inc</orgName>
								<orgName type="institution" key="instit2">Canada Mountain View</orgName>
								<address>
									<region>ON, CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
							<email>tmikolov@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Google, Inc</orgName>
								<orgName type="institution" key="instit2">Canada Mountain View</orgName>
								<address>
									<region>ON, CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
							<email>bengio@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Google, Inc</orgName>
								<orgName type="institution" key="instit2">Canada Mountain View</orgName>
								<address>
									<region>ON, CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
							<email>singer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Google, Inc</orgName>
								<orgName type="institution" key="instit2">Canada Mountain View</orgName>
								<address>
									<region>ON, CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
							<email>shlens@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Google, Inc</orgName>
								<orgName type="institution" key="instit2">Canada Mountain View</orgName>
								<address>
									<region>ON, CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
							<email>afrome@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Google, Inc</orgName>
								<orgName type="institution" key="instit2">Canada Mountain View</orgName>
								<address>
									<region>ON, CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
							<email>gcorrado@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Google, Inc</orgName>
								<orgName type="institution" key="instit2">Canada Mountain View</orgName>
								<address>
									<region>ON, CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Google, Inc</orgName>
								<orgName type="institution" key="instit2">Canada Mountain View</orgName>
								<address>
									<region>ON, CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Learning by Convex Combination of Semantic Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional n-way classification framing of image understanding, particularly in terms of the promise for zero-shot learning -the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing n-way image classifier and a semantic word embedding model, which contains the n class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The classic machine learning approach to object recognition presupposes the existence of a large labeled training dataset to optimize the free parameters of an image classifier. There have been continued efforts in collecting larger image corpora with a broader coverage of object categories (e.g., <ref type="bibr" target="#b2">[3]</ref>), thereby enabling image classification with many classes. While annotating more object categories in images can lead to a finer granularity of image classification, creating high quality fine grained image annotations is challenging, expensive, and time consuming. Moreover, as new visual entities emerge over time, the annotations should be revised, and the classifiers should be re-trained.</p><p>Motivated by the challenges facing standard machine learning framework for n-way classification, especially when n (the number of class labels) is large, several recent papers have proposed methods for mapping images into semantic embedding spaces <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. In doing so, it is hoped that by resorting to nearest neighbor search in the embedding space with respect to a set of label embedding vectors, one can address zero-shot learning -annotation of images with new labels corresponding to previously unseen object categories. While the common practice for image embedding is to learn a regression model from images into a semantic embedding space, it has been unclear whether there exists a more direct way to transform any probabilistic n-way classifier into an image embedding model, which can be used for zero-shot learning. In this work, we present a simple method for constructing an image embedding system by combining any existing probabilistic n-way image classifier with an existing word embedding model, which contains the n class labels in its vocabulary. We show that our simple method confers many of the advantages associated with more complex image embedding schemes.</p><p>Recently, zero-shot learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> has received a growing amount of attention <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>A key to zero-shot learning is the use of a set of semantic embedding vectors associated with the class labels. These semantic embedding vectors might be obtained from human-labeled object attributes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, or they might be learned from a text corpus in an unsupervised fashion, based on an independent natural language modeling task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref>. Regardless of the way the label embedding vectors are obtained, previous work casts zero-shot learning as a regression problem from the input space into the embedding space. In contrast, given a pre-trained standard classifier, our method maps images into the semantic embedding space via the convex combination of the class label embedding vectors. The values of a given classifier's predictive probabilities for different training labels are used to compute a weighted combination of the label embeddings in the semantic space. This provides a continuous embedding vector for each image, which is then used for extrapolating the pre-trained classifier's predictions beyond the training labels, into a set of test labels.</p><p>The effectiveness of our method called "convex combination of semantic embeddings" (ConSE) is evaluated on ImageNet zero-shot learning task. By employing a convolutional neural network <ref type="bibr" target="#b6">[7]</ref> trained only on 1000 object categories from ImageNet, the ConSE model is able to achieve 9.4% hit@1 and 24.7% hit@5 on 1600 unseen objects categories, which were omitted from the training dataset. When the number of test classes gets larger, and they get further from the training classes in the ImageNet category hierarchy, the zero-shot classification results get worse, as expected, but still our model outperforms a recent state-of-the-art model <ref type="bibr" target="#b5">[6]</ref> applied to the same task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous work</head><p>Zero-shot learning is closely related to one-shot learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref>, where the goal is to learn object classifiers based on a few labeled training exemplars. The key difference in zero-shot learning is that no training images are provided for a held-out set of test categories. Thus, zero-shot learning is more challenging, and the use of side information about the interactions between the class labels is more essential in this setting. Nevertheless, we expect that advances in zero-shot learning will benefit one-shot learning, and visual recognition in general, by providing better ways to incorporate prior knowledge about the relationships between the object categories.</p><p>A key component of zero-shot learning is the way a semantic space of class label embeddings is defined. In computer vision, there has been a body of work on the use of human-labeled visual attributes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> to help detecting unseen object categories. Binary attributes are most commonly used to encode presence and absence of a set of visual characteristics within instances of an object category. Some examples of these attributes include different types of materials, different colors, textures, and object parts. More recently, relative attributes <ref type="bibr" target="#b14">[15]</ref> are proposed to strengthen the attribute based representations. In attribute based approaches, each class label is represented by a vector of attributes, instead of the standard one-of-n encoding. And multiple classifiers are trained for predicting each object attribute. While this is closely related to our approach, the main issue with attribute-based classification is its lack of scalability to large-scale tasks. Annotating thousands of attributes for thousands of object classes is an ambiguous and challenging task in itself, and the applicability of supervised attributes to large-scale zero-shot learning is limited. There has been some recent work showing good zero-shot classification performance on visual recognition tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11]</ref>, but these methods also rely on the use of knowledge bases containing descriptive properties of object classes, and the WordNet hierarchy.</p><p>A more scalable approach to semantic embeddings of class labels builds upon the recent advances in unsupervised neural language modeling <ref type="bibr" target="#b1">[2]</ref>. In this approach, a set of multi-dimensional embedding vectors are learned for each word in a text corpus. The word embeddings are optimized to increase the predictability of each word given its context <ref type="bibr" target="#b11">[12]</ref>. Essentially, the words that cooccur in similar contexts, are mapped onto similar embedding vectors. Frome et al. <ref type="bibr" target="#b5">[6]</ref> and Socher et al. <ref type="bibr" target="#b17">[18]</ref> exploit such word embeddings to embed textual names of object class labels into a continuous semantic space. In this work, we also use the skip-gram model <ref type="bibr" target="#b11">[12]</ref> to learn the class label embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement</head><p>Assume that a labeled training dataset of images D 0 ≡ {(x i , y i )} m i=1 is given, where each image is represented by a p-dimensional feature vector, x i ∈ R p . For generality we denote X def = R p . There are n 0 distinct class labels available for training, i.e., y i ∈ Y 0 ≡ {1, . . . , n 0 }. In addition, a test dataset denoted D 1 ≡ {(x j , y j )} m j=1 is provided, where x j ∈ X as above, while y j ∈ Y 1 ≡ {n 0 + 1, . . . , n 0 + n 1 }. The test set contains n 1 distinct class labels, which are omitted from the training set. Let n = n 0 +n 1 denote the total number of labels in the training and test sets.</p><p>The goal of zero-shot learning is to train a classifier on the training set D 0 , which performs reasonably well on the unseen test set D 1 . Clearly, without any side information about the relationships between the labels in Y 0 and Y 1 , zero-shot learning is infeasible as Y 0 ∩ Y 1 = ∅. However, to mitigate zero-shot learning, one typically assumes that each class label y (1 ≤ y ≤ n) is associated with a semantic embedding vector s(y) ∈ S ≡ R q . The semantic embedding vectors are such that two labels y and y are similar if and only if their semantic embeddings s(y) and s(y ) are close in S, e.g., s(y), s(y ) S is large. Clearly, given an embedding of training and test class labels into a joint semantic space i.e., {s(y); y ∈ Y 0 ∪ Y 1 }, the training and test labels become related, and one can hope to learn from the training labels to predict the test labels.</p><p>Previous work (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>) has addressed zero-shot classification by learning a mapping from input features to semantic label embedding vectors using a multivariate regression model. Accordingly, during training instead of learning an n 0 -way classifier from inputs to training labels (X → Y 0 ), a regression model is learned from inputs to the semantic embedding space (X → S). A training dataset of inputs paired with semantic embeddings, i.e., {(x i , s(y i )); (x i , y i ) ∈ D 0 }, is constructed to train a regression function f : X → S that aims to map x i to s(y i ). Once f (·) is learned, it is applied to a test image x j to obtain f (x j ), and this continuous semantic embedding for x j is then compared with the test label embedding vectors, {s(y ); y ∈ Y 1 }, to find the most relevant test labels. Thus, instead of directly mapping from X → Y 1 , which seems impossible, zero-shot learning methods first learn a mapping X → S, and then a deterministic mapping such as k-nearest neighbor search in the semantic space is used to map a point in S to a ranked list of labels in Y 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ConSE: Convex combination of semantic embeddings 4.1 Model Description</head><p>In contrast to previous work which casts zero-shot learning as a regression problem from the input space to the semantic label embedding space, in this work, we do not explicitly learn a regression function f : X → S. Instead, we follow the classic machine learning approach, and learn a classifier from training inputs to training labels. To this end, a classifier p 0 is trained on D 0 to estimate the probability of an image x belonging to a class label y ∈ Y 0 , denoted p 0 (y | x), where n0 y=1 p 0 (y | x) = 1. Given p 0 , we propose a method to transfer the probabilistic predictions of the classifier beyond the training labels, to a set of test labels.</p><p>Let y 0 (x, 1) denote the most likely training label for an image x according to the classifier p 0 . Formally, we denote</p><formula xml:id="formula_0">y 0 (x, 1) ≡ argmax y∈Y0 p 0 (y | x) .<label>(1)</label></formula><p>Analogously, let y 0 (x, t) denote the t th most likely training label for x according to p 0 . In other words, p 0 ( y 0 (x, t) | x) is the t th largest value among {p 0 (y | x); y ∈ Y 0 }. Given the top T predictions of p 0 for an input x, our model deterministically predicts a semantic embedding vector f (x) for an input x, as the convex combination of the semantic embeddings {s( y 0 (x, t))} T t=1 weighted by their corresponding probabilities. More formally,</p><formula xml:id="formula_1">f (x) = 1 Z T t=1 p( y 0 (x, t) | x) · s( y 0 (x, t)) ,<label>(2)</label></formula><p>where Z is a normalization factor given by Z = T t=1 p( y 0 (x, t) | x), and T is a hyper-parameter controlling the maximum number of embedding vectors to be considered. If the classifier is very confident in its prediction of a label y for x, i.e., p 0 (y | x) ≈ 1, then f (x) ≈ s(y). However, if the classifier have doubts whether an image contains a "lion" or a "tiger", e.g., p 0 (lion | x) = 0.6 and p 0 (tiger | x) = 0.4, then our predicted semantic embedding, f (x) = 0.6 · s(lion) + 0.4 · s(tiger), will be something between lion and tiger in the semantic space. Even though "liger" (a hybrid cross between a lion and a tiger) might not be among the training labels, because it is likely that s(liger) ≈ 1 2 s(lion) + 1 2 s(tiger), then it is likely that f (x) ≈ s(liger). Given the predicted embedding of x in the semantic space, i.e., f (x), we perform zero-shot classification by finding the class labels with embeddings nearest to f (x) in the semantic space. The top prediction of our model for an image x from the test label set, denoted y 1 (x, 1), is given by</p><formula xml:id="formula_2">y 1 (x, 1) ≡ argmax y ∈Y1 cos(f (x), s(y )) ,<label>(3)</label></formula><p>where we use cosine similarity to rank the embedding vectors. Moreover, let y 1 (x, k) denote the k th most likely test label predicted for x. Then, y 1 (x, k) is defined as the label y ∈ Y 1 with the k th largest value of cosine similarity in {cos(f (x), s(y )); y ∈ Y 1 }. Note that previous work on zero-shot learning also uses a similar k-nearest neighbor procedure in the semantic space to perform label extrapolation. The key difference in our work is that we define the embedding prediction f (x) based on a standard classifier as in Eq. <ref type="formula" target="#formula_1">(2)</ref>, and not based on a learned regression model. For the specific choice of cosine similarity to measure closeness in the embedding space, the norm of f (x) does not matter, and we could drop the normalization factor (1/Z) in Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Difference with DeViSE</head><p>Our model is inspired by a technique recently proposed for image embedding, called "Deep Visual-Semantic Embedding" (DeViSE) <ref type="bibr" target="#b5">[6]</ref>. Both DeViSE and ConSE models benefit from the convolutional neural network classifier of Krizhevsky et al. <ref type="bibr" target="#b6">[7]</ref>, but there is an important difference in the way they employ the neural net. The DeViSE model replaces the last layer of the convolutional net, the Softmax layer, with a linear transformation layer. The new transformation layer is trained using a ranking objective to map training inputs close to continuous embedding vectors corresponding to correct labels. Subsequently, the lower layers of the convolutional neural network are fine-tuned using the ranking objective to produce better results. In contrast, the ConSE model keeps the Softmax layer of the convolutional net intact, and it does not train the neural network any further. Given a test image, the ConSE simply runs the convolutional classifier and considers the top T predictions of the model. Then, the convex combination of the corresponding T semantic embedding vectors in the semantic space (see Eq. <ref type="formula" target="#formula_1">(2)</ref>) is computed, which defines a deterministic transformation from the outputs of the Softmax classifier into the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare our approach, "convex combination of semantic embedding" (ConSE), with a stateof-the-art method called "Deep Visual-Semantic Embedding" (DeViSE) <ref type="bibr" target="#b5">[6]</ref> on the ImageNet dataset <ref type="bibr" target="#b2">[3]</ref>. Both of the ConSE and DeViSE models make use of the same skipgram text model <ref type="bibr" target="#b11">[12]</ref> to define the semantic label embedding space.    <ref type="figure" target="#fig_0">Fig. 1</ref> shows that the labels predicted by the ConSE(10) model are generally coherent and they include very few outliers. In contrast, the top 5 labels predicted by the DeViSE model include more outliers such as "flip-flop" predicted for a "Steller sea lion", "pipe" and "shaker" predicted for a "hamster", and "automatic rifle" predicted for a "farm machine".  <ref type="bibr" target="#b5">[6]</ref> and ConSE (T ) for T = 1, 10, 1000 on ImageNet zero-shot learning task. When testing the methods with the datasets indicated with (+1K), training labels are also included as potential labels within the nearest neighbor classifier, hence the number of candidate labels is 1000 more. In all cases, zero-shot classes did not occur in the training set, and none of the test images is annotated with any of the training labels.</p><p>The high level of annotation granularity in Imagenet, e.g., different types of sea lions, creates challenges for recognition systems which are based solely on visual cues. Using models such as ConSE and DeViSE, one can leverage the similarity between the class labels to expand the original predictions of the image classifiers to a list of similar labels, hence better retrieval rates can be achieved.</p><p>We report quantitative results in terms of two metrics: "flat" hit@k and "hierarchical" precision@k. Flat hit@k is the percentage of test images for which the model returns the one true label in its top k predictions. Hierarchical precision@k uses the ImageNet category hierarchy to penalize the predictions that are semantically far from the correct labels more than the predictions that are close. Hierarchical precision@k measures, on average, what fraction of the model's top k predictions are among the k most relevant labels for each test image, where the relevance of the labels is measure by their distance in the Imagenet category hierarchy. A more formal definition of hierarchical precision@k is included in the supplementary material of <ref type="bibr" target="#b5">[6]</ref>. Hierarchical precision@1 is always equivalent to flat hit@1.  <ref type="table" target="#tab_1">Table 1</ref>. For each dataset, we consider including and excluding the training labels within the label candidates used for k-nearest neighbor label ranking (i.e., Y 1 in Eq. <ref type="formula" target="#formula_2">(3)</ref>). None of the images in the test set are labeled as training labels, so including training labels in the label candidate set for ranking hurts the performance as finding the correct labels is harder in a larger set. Datasets that include training labels in their label candidate set are marked by "(+1K)". The results demonstrate <ref type="table" target="#tab_1">Test Label Set  Model  1  2  5  10</ref>    <ref type="bibr" target="#b5">[6]</ref> reported that the ranking loss used within the DeViSE significantly outperforms the the squared loss used in <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical precision@k</head><p>Not surprisingly, the performance of the models is best when training labels are excluded from the label candidate set. All of the models tend to predict training labels more often than test labels, especially at their first few predictions. For example, when training labels are included, the performance of ConSE(10) drops from 9.4% hit@1 to 0.3% on the 2-hops dataset. This suggests that a procedure better than vanilla k-nearest neighbor search needs to be employed in order to distinguish images that do not belong to the training labels. We note that the DeViSE has a slightly lower bias towards training labels as the performance drop after inclusion of training labels is slightly smaller than the performance drop in the ConSE model. <ref type="table" target="#tab_3">Table 2</ref> shows hierarchical precision@k results for the Softmax baseline, DeViSE, and ConSE(10) on the zero-shot learning task. The results are only reported for ConSE (10) because T = 10 seems to perform the best among T = 1, 10, 1000. The hierarchical metric also confirms that the ConSE improves upon the DeViSE for zero-shot learning. We did not compare against the Softmax baseline on the flat hit@k measure, because the Softmax model cannot predict any of the test labels. However, using the hierarchical metric, we can now compare with the Softmax baseline when the training labels are also included in the label candidate set (+1K). We find that the top k predictions of the ConSE outperform the Softmax baseline in hierarchical precision@k.</p><p>Even though the ConSE model is proposed for zero-shot learning, we assess how the ConSE compares with the DeViSE and the Softmax baseline on the standard classification task with the training 1000 labels, i.e., the training and test labels are the same. <ref type="table" target="#tab_4">Table 3</ref> and 4 show the flat hit@k and hierarchical precision@k rates on the 1000-class learning task. According to <ref type="table" target="#tab_4">Table 3</ref>, the ConSE(10) model improves upon the Softmax baseline in hierarchical precision at 5, 10, and 20, suggesting that the mistakes made by the ConSE model are on average more semantically consistent with the correct class labels, than the Softmax baseline. This improvement is due to the use of label embedding vectors learned from Wikipedia articles. However, on the 1000-class learning task, the ConSE(10) model underperforms the DeViSE model. We note that the DeViSE model is trained with respect to a k-nearest neighbor retrieval objective on the same specific set of 1000 labels, so its better performance on this task is expected.</p><p>Although the DeViSE model performs better than the ConSE on the original 1000-class learning task (   is not a one-to-one correspondence between the class labels and the word embedding vectors. Rather, because of the way the Imagenet synsets are defined, each class label is associated with several synonym terms, and hence several word embedding vectors. In the process of mapping the Softmax scores to an embedding vector, the ConSE model first averages the word vectors associated with each class label, and then linearly combine the average vectors according to the Softmax scores. However, when we rank the word vectors to find the k most likely class labels, we search over individual word vectors, without any averaging of the synonym words. Thus, the ConSE(1) might produce an average embedding which is not the closest vector to any of the word vectors corresponding to the original class label, and this results in a slight difference in the hit@1 scores for ConSE(1) and the Softmax baseline. While other alternatives exist for this part of the algorithm, we intentionally kept the ranking procedure exactly the same as the DeViSE model to perform a direct comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The ConSE approach to mapping images into a semantic embedding space is deceptively simple.</p><p>Treating classifier scores as weights in a convex combination of word vectors is perhaps the most direct method imaginable for recasting an n-way image classification system as image embedding system. Yet this method outperforms more elaborate joint training approaches both on zero-short learning and on performance metrics which weight errors based on semantic quality. The success of this method undoubtedly lays is its ability to leverage the strengths inherent in the state-of-the-art image classifier and the state-of-the-art text embedding system from which it was constructed.</p><p>While it draws from their strengths, we have no reason to believe that ConSE depends on the details the visual and text models from which it is constructed. In particular, though we used a deep convolutional network with a Softmax classifier to generate the weights for our linear combination, any visual object classification system which produces relative scores over a set of classes is compatible with the ConSE framework. Similarly, though we used semantic embedding vectors which were the side product of an unsupervised natural language processing task, the ConSE framework is applicable to other alternative representation of text in which similar concepts are nearby in vector space. The choice of the training corpus for the word embeddings affects the results too.</p><p>One feature of the ConSE model which we did not exploit in our experiments is its natural representation of confidence. The norm of the vector that ConSE assigns to an image is a implicit expression of the model's confidence in the embedding of that image. Label assignments about which the Softmax classifier is uncertain be given lower scores, which naturally reduces the magnitude of the ConSE linear combination, particularly if Softmax probabilities are used as weights without renormalization. Moreover, linear combinations of labels with disparate semantics under the text model will have a lower magnitude than linear combinations of the same number of closely related labels. These two effects combine such that ConSE only produces embeddings with an L2-norm near 1.0 for images which were either nearly completely unambiguous under the image model or which were assigned a small number of nearly synonymous text labels. We believe that this property could be fruitfully exploited in settings where confidence is a useful signal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Zero-shot test images from ImageNet, and their corresponding top 5 labels predicted by the Softmax Baseline<ref type="bibr" target="#b6">[7]</ref>, DeViSE<ref type="bibr" target="#b5">[6]</ref>, and ConSE(T = 10). The labels predicted by the Softmax baseline are the labels used for training, and the labels predicted by the other two models are not seen during training of the image classifiers. The correct labels are shown in blue. Examples are hand-picked to illustrate the cases that the ConSE(10) performs well, and a few failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>depicts some qualitative results. The first column shows the top 5 predictions of the convolutional net, referred to as the Softmax baseline<ref type="bibr" target="#b6">[7]</ref>. The second and third columns show the zero-shot predictions by the DeViSE and ConSE(10) models. The ConSE(10) model uses the top T = 10 predictions of the Softmax baseline to generate convex combination of embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The skipgram model was trained on 5.4 billion words from Wikipedia.org to construct 500 dimensional word embedding vectors. The embedding vectors are then normalized to have a unit norm. The convolutional neural network of<ref type="bibr" target="#b6">[7]</ref>, used in both ConSE and DeViSE, is trained on ImageNet 2012 1K set with 1000 training labels. Because the image classifier, and the label embedding vectors are identical in the ConSE and DeViSE models, we can perform a direct comparison between the two embedding techniques.We mirror the ImageNet zero-shot learning experiments of<ref type="bibr" target="#b5">[6]</ref>. Accordingly, we report the zero-shot generalization performance of the models on three test datasets with increasing degree of difficulty.</figDesc><table><row><cell>Test Image</cell><cell>Softmax Baseline [7]</cell><cell>DeViSE [6]</cell><cell>ConSE (10)</cell></row><row><cell></cell><cell>wig</cell><cell>water spaniel</cell><cell>business suit</cell></row><row><cell></cell><cell>fur coat</cell><cell>tea gown</cell><cell>dress, frock</cell></row><row><cell></cell><cell>Saluki, gazelle hound</cell><cell>bridal gown, wedding gown</cell><cell>hairpiece, false hair, postiche</cell></row><row><cell></cell><cell>Afghan hound, Afghan</cell><cell>spaniel</cell><cell>swimsuit, swimwear, bathing suit</cell></row><row><cell></cell><cell>stole</cell><cell>tights, leotards</cell><cell>kit, outfit</cell></row><row><cell></cell><cell>ostrich, Struthio camelus</cell><cell>heron</cell><cell>ratite, ratite bird, flightless bird</cell></row><row><cell></cell><cell>black stork, Ciconia nigra</cell><cell>owl, bird of Minerva, bird of night</cell><cell>peafowl, bird of Juno</cell></row><row><cell></cell><cell>vulture</cell><cell>hawk</cell><cell>common spoonbill</cell></row><row><cell></cell><cell>crane</cell><cell>bird of prey, raptor, raptorial bird</cell><cell>New World vulture, cathartid</cell></row><row><cell></cell><cell>peacock</cell><cell>finch</cell><cell>Greek partridge, rock partridge</cell></row><row><cell></cell><cell>sea lion</cell><cell>elephant</cell><cell>California sea lion</cell></row><row><cell></cell><cell>plane, carpenter's plane</cell><cell>turtle</cell><cell>Steller sea lion</cell></row><row><cell></cell><cell>cowboy boot</cell><cell>turtleneck, turtle, polo-neck</cell><cell>Australian sea lion</cell></row><row><cell></cell><cell>loggerhead, loggerhead turtle</cell><cell>flip-flop, thong</cell><cell>South American sea lion</cell></row><row><cell></cell><cell>goose</cell><cell>handcart, pushcart, cart, go-cart</cell><cell>eared seal</cell></row><row><cell></cell><cell>hamster</cell><cell>golden hamster, Syrian hamster</cell><cell>golden hamster, Syrian hamster</cell></row><row><cell></cell><cell>broccoli</cell><cell>rhesus, rhesus monkey</cell><cell>rodent, gnawer</cell></row><row><cell></cell><cell>Pomeranian</cell><cell>pipe</cell><cell>Eurasian hamster</cell></row><row><cell></cell><cell>capuchin, ringtail</cell><cell>shaker</cell><cell>rhesus, rhesus monkey</cell></row><row><cell></cell><cell>weasel</cell><cell>American mink, Mustela vison</cell><cell>rabbit, coney, cony</cell></row><row><cell></cell><cell>thresher, threshing machine</cell><cell>truck, motortruck</cell><cell>flatcar, flatbed, flat</cell></row><row><cell></cell><cell>tractor</cell><cell>skidder</cell><cell>truck, motortruck</cell></row><row><cell></cell><cell>harvester, reaper</cell><cell>tank car, tank</cell><cell>tracked vehicle</cell></row><row><cell></cell><cell>half track</cell><cell>automatic rifle, machine rifle</cell><cell>bulldozer, dozer</cell></row><row><cell></cell><cell>snowplow, snowplough</cell><cell>trailer, house trailer</cell><cell>wheeled vehicle</cell></row><row><cell>(farm machine)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Tibetan mastiff</cell><cell>kernel</cell><cell>dog, domestic dog</cell></row><row><cell></cell><cell>titi, titi monkey</cell><cell>littoral, litoral, littoral zone, sands</cell><cell>domestic cat, house cat</cell></row><row><cell></cell><cell>koala, koala bear, kangaroo bear</cell><cell>carillon</cell><cell>schnauzer</cell></row><row><cell></cell><cell>llama</cell><cell>Cabernet, Cabernet Sauvignon</cell><cell>Belgian sheepdog</cell></row><row><cell></cell><cell>chow, chow chow</cell><cell>poodle, poodle dog</cell><cell>domestic llama, Lama peruana</cell></row><row><cell>(alpaca, Lama pacos)</cell><cell></cell><cell></cell><cell></cell></row></table><note>The first test dataset, called "2-hops" includes labels from the 2011 21K set which are visually and semantically similar to the training labels in the ImageNet 2012 1K set. This dataset only includes labels within 2 tree hops of the ImageNet 2012 1K labels. A more difficult dataset including labels within 3 hops of the training labels is created in a similar way and referred to as "3-hops". Finally, a dataset of all the labels in the ImageNet 2011 21K set is created. The three test datasets respectively include 1, 589, 7, 860, and 20, 900 labels. These test datasets do not include any image labeled with any of the 1000 training labels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell># Candidate</cell><cell></cell><cell></cell><cell cols="3">Flat hit@k (%)</cell><cell></cell></row><row><cell>Test Label Set</cell><cell>Labels</cell><cell>Model</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell>DeViSE</cell><cell cols="5">6.0 10.0 18.1 26.4 36.4</cell></row><row><cell>2-hops</cell><cell>1, 589</cell><cell>ConSE(1) ConSE(10)</cell><cell cols="5">9.3 14.4 23.7 30.8 38.7 9.4 15.1 24.7 32.7 41.8</cell></row><row><cell></cell><cell></cell><cell cols="6">ConSE(1000) 9.2 14.8 24.1 32.1 41.1</cell></row><row><cell></cell><cell></cell><cell>DeViSE</cell><cell cols="2">0.8 2.7</cell><cell cols="3">7.9 14.2 22.7</cell></row><row><cell>2-hops (+1K)</cell><cell>1, 589 +1000</cell><cell>ConSE(1) ConSE(10)</cell><cell cols="5">0.2 7.1 17.2 24.0 31.8 0.3 6.2 17.0 24.9 33.5</cell></row><row><cell></cell><cell></cell><cell cols="6">ConSE(1000) 0.3 6.2 16.7 24.5 32.9</cell></row><row><cell></cell><cell></cell><cell>DeViSE</cell><cell cols="2">1.7 2.9</cell><cell>5.3</cell><cell cols="2">8.2 12.5</cell></row><row><cell>3-hops</cell><cell>7, 860</cell><cell>ConSE(1) ConSE(10)</cell><cell cols="2">2.6 4.2 2.7 4.4</cell><cell cols="3">7.3 10.8 14.8 7.8 11.5 16.1</cell></row><row><cell></cell><cell></cell><cell cols="3">ConSE(1000) 2.6 4.3</cell><cell cols="3">7.6 11.3 15.7</cell></row><row><cell></cell><cell></cell><cell>DeViSE</cell><cell cols="2">0.5 1.4</cell><cell>3.4</cell><cell>5.9</cell><cell>9.7</cell></row><row><cell>3-hops (+1K)</cell><cell>7, 860 +1000</cell><cell>ConSE(1) ConSE(10)</cell><cell cols="2">0.2 2.4 0.2 2.2</cell><cell>5.9 5.9</cell><cell cols="2">9.3 13.4 9.7 14.3</cell></row><row><cell></cell><cell></cell><cell cols="3">ConSE(1000) 0.2 2.2</cell><cell>5.8</cell><cell cols="2">9.5 14.0</cell></row><row><cell></cell><cell></cell><cell>DeViSE</cell><cell cols="2">0.8 1.4</cell><cell>2.5</cell><cell>3.9</cell><cell>6.0</cell></row><row><cell>ImageNet 2011 21K</cell><cell>20, 841</cell><cell>ConSE(1) ConSE(10)</cell><cell cols="2">1.3 2.1 1.4 2.2</cell><cell>3.6 3.9</cell><cell>5.4 5.8</cell><cell>7.6 8.3</cell></row><row><cell></cell><cell></cell><cell cols="3">ConSE(1000) 1.3 2.1</cell><cell>3.8</cell><cell>5.6</cell><cell>8.1</cell></row><row><cell></cell><cell></cell><cell>DeViSE</cell><cell cols="2">0.3 0.8</cell><cell>1.9</cell><cell>3.2</cell><cell>5.3</cell></row><row><cell>ImageNet 2011 21K (+1K)</cell><cell>20, 841 +1000</cell><cell>ConSE(1) ConSE(10)</cell><cell cols="2">0.1 1.2 0.2 1.2</cell><cell>3.0 3.0</cell><cell>4.8 5.0</cell><cell>7.0 7.5</cell></row><row><cell></cell><cell></cell><cell cols="3">ConSE(1000) 0.2 1.2</cell><cell>3.0</cell><cell>4.9</cell><cell>7.3</cell></row></table><note>Flat hit@k performance of DeViSE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>shows flat hit@k results for the DeViSE and three versions of the ConSE model. The ConSE model has a hyper-parameter T that controls the number of training labels used for the convex combination of semantic embeddings. We report the results for T = 1, 10, 1000 as ConSE (T ) in Table 1. Because there are only 1000 training labels, T is bounded by 1 ≤ T ≤ 1000. The results are reported on the three test datasets; the dataset difficulty increases from top to bottom in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and</cell></row><row><cell>ConSE(10) on ImageNet zero-shot learning task.</cell></row></table><note>that the ConSE model consistently outperforms the DeViSE on all of the datasets for all values of T . Among different versions of the ConSE, ConSE(10) performs the best. We do not directly compare against the method of Socher et al. [18], but Frome et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Hierarchical precision@k</cell></row></table><note>, 4), it does not generalize as well as the ConSE model to the unseen zero-shot learning categories (Table 1, 2). Based on this observation, we conclude that a better k-nearest neighbor</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Hierarchical precision@k performance of Softmax baseline<ref type="bibr" target="#b6">[7]</ref>, DeViSE<ref type="bibr" target="#b5">[6]</ref>, and ConSE on ImageNet original 1000-class learning task.</figDesc><table><row><cell>Flat hit@k (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Flat hit@k performance of Softmax baseline<ref type="bibr" target="#b6">[7]</ref>, DeViSE<ref type="bibr" target="#b5">[6]</ref>, and ConSE on ImageNet original 1000-class learning task. classification on the training labels, does not automatically translate into a better k-nearest neighbor classification on a zero-shot learning task. We believe that the DeViSE model suffers from a variant of overfitting, which is the model has learned a highly non-linear and complex embedding function for images. This complex embedding function is well suited for predicting the training label embeddings, but it does not generalize well to novel unseen label embedding vectors. In contrast, a simpler embedding model based on convex combination of semantic embeddings (ConSE) generalizes more reliably to unseen zero-shot classes, with little chance of overfitting. Implementation details. The ConSE(1) model takes the top-1 prediction of the convolutional net, and expands it to a list of labels based on the similarity of the label embedding vectors. To implement ConSE(1) efficiently, one can pre-compute a list of test labels for each training label, and simply predict the corresponding list based on the top prediction of the convolutional net. The top prediction of the ConSE(1) occasionally differs from the top prediction of the Softmax baseline due to a detail of our implementation. In the Imagenet experiments, following the setup of the DeViSE model, there</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cross-generalization: learning novel classes from a single example by feature replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database. CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CogSci</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<title level="m">Relative attributes. ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Evaluating knowledge transfer and zero-shot learning in a largescale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Evaluating knowledge transfer and zero-shot learning in a largescale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
