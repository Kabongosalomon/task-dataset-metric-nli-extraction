<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
						</author>
						<title level="a" type="main">The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human Pose Estimation</term>
					<term>Keypoint Detection</term>
					<term>Data Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Being a fundamental component in training and inference, data processing has not been systematically considered in human pose estimation community, to the best of our knowledge. In this paper, we focus on this problem and find that the devil of human pose estimation evolution is in the biased data processing. Specifically, by investigating the standard data processing in state-of-the-art approaches mainly including coordinate system transformation and keypoint format transformation (i.e., encoding and decoding), we find that the results obtained by common flipping strategy are unaligned with the original ones in inference. Moreover, there is a statistical error in some keypoint format transformation methods. Two problems couple together, significantly degrade the pose estimation performance and thus lay a trap for the research community. This trap has given bone to many suboptimal remedies, which are always unreported, confusing but influential. By causing failure in reproduction and unfair in comparison, the unreported remedies seriously impedes the technological development. To tackle this dilemma from the source, we propose Unbiased Data Processing (UDP) consist of two technique aspect for the two aforementioned problems respectively (i.e., unbiased coordinate system transformation and unbiased keypoint format transformation). Base on UDP, we wipe out the trap by giving out a deep insight of the existing biased data processing pipeline, whose origin, effects and some confusing remedies are thoroughly studied. Besides, as a model-agnostic approach and a superior solution, UDP successfully pushes the performance boundary of human pose estimation. For example on COCO test-dev set, UDP promotes top-down method HRNet-W32-256×192 by 1.7 AP (73.5 to 75.2) for free and promotes bottom-up methods HRNet-W32-512×512 by 2.7 AP with an acceleration of 6.1 times. The HRNet-W48-384×288 equipped with UDP achieves 76.5 AP and sets a new state-of-the-art for human pose estimation. As a meaningful milestone for pursuing high performance human pose estimation, UDP has been the key base of the winner in 2020 COCO Keypoint Detection Challenge. The code is public available for reference. 1 5 7.5 25 65 67 69 71 73 75 77 +1.7 +1.5 + 2.7 AP Inference Speed of IPS/PPS x 6.1 Baseline UDP HigherHRNet HRNet SimpleBaseline COCO test-dev CrowdPose test Top-down Bottom-up 63 Fig. 1: The improvement of performance on COCO test-dev set and CrowdPose test set when the proposed Unbiased Data Processing (UDP) is applied to the state-of-the-art methods. At no cost, UDP improves the APs of top-down methods SimpleBaseline [24] and HRNet [25] by a considerable margin. In bottom-up paradigm, UDP offers both accuracy improvement and inference acceleration to HRNet [25] and HigherHRNet [26]. that the widely used data processing pipelines in most state-ofthe-art human pose estimation systems [24], [25], [26], [27], [28], [29] are defective. The chief causes are two common problems: i)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>2D Human pose estimation has been extensively studied in computer vision literature and serves many complicated downstream visual understanding tasks such as 3D human pose estimation <ref type="bibr" target="#b1">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>, <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, human phasing <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>, health care <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, video surveillance <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref> and action recognition <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>. In this paper, we pay attention to the data processing aspect, considering it as a fundamental component. All visual recognition tasks are born with data processing, and in general share data processing methodology with each other like data augmentation and transformation between different coordinate systems. However, when compared with other tasks like classification <ref type="bibr" target="#b19">[19]</ref>, object detection <ref type="bibr" target="#b20">[20]</ref> and semantic segmentation <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, the performance of human pose estimation algorithms is much more sensitive to the methods used in data processing on account of the evaluation principle. In the evaluation of human pose estimation, the metrics are calculated based on the positional offset between ground truth annotations and predicted results <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b23">[23]</ref>, where small disturbance caused by data processing will affect the performance of pose estimators by a large margin.</p><p>Although it is of significant, to the best of our knowledge, data processing has not been systematically considered in human pose estimation community. When this topic is addressed, we find When flipping testing strategy is adopted, the results from flipped image are unaligned with those from the origin image. The bias derives from utilizing pixel for measuring the size of images when performing coordinate system transformation in resizing operation. ii) Defective keypoint format transformation (i.e., encoding and decoding) methods would lead to extra precision degradation. The two problems accumulatively degrade the performance of human pose estimators, lay a trap for the research community and subsequently has given born to many suboptimal remedies. The empirical remedies are always unreported but with huge impact on the performance like direct compensation in post processing <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, while the others are reported but at tremendous cost of latency like using higher network output resolution in HigherHRNet <ref type="bibr" target="#b26">[26]</ref>. It is worth noting that, by causing failure in reproduction and unfair in comparison, the unreported remedies will obstruct the development of the human pose estimation technologies.</p><p>In this paper, we offer a reasonable and free access to thoroughly solving the two aforementioned problems by proposing Unbiased Data Processing (UDP) system. Corresponding to the two aforementioned problems, UDP consists of two technical aspects: the unbiased coordinate system transformations and the unbiased keypoint format transformations. Aiming at the unbiased coordinate system transformations, we firstly propose to follow the principle of defining and analyzing this problem in continuous space. Then the concept of coordinate system transformation is defined based on this principle and the targets of unbias in this sub problem are introduced. Subsequently, the coordinate system transformations in different elementary operations (e.g., cropping, resizing, rotating and flipping) are formally designed, which finally compose the common coordinate system transformations used in training and testing process. With mathematical reasoning, we verify the unbiased property of the designed coordinate system transformation pipeline, and subsequently offer a deep insight of the existing biased coordinate system transformation pipeline, whose origin, effects and some confusing remedies are thoroughly studied. Analogously, the concept of unbiased keypoint format transformation is proposed, two unbiased keypoint format transformation methods are introduced and a typical biased example is analyzed thoroughly. As a result with UDP, the aforementioned trap can be remove and a higher as well as more reliable baseline can be achieved.</p><p>To showcase the effectiveness of the proposed method, we perform comprehensive experiments on the COCO Keypoint Detection benchmarks <ref type="bibr" target="#b20">[20]</ref>. As a model-agnostic approach and a superior solution, UDP successfully pushes the performance boundary of the human pose estimation problem as illustrated in <ref type="figure" target="#fig_7">Figure 1</ref>. Specifically, UDP boosts the performance of the methods in top-down paradigm without any extra latency. For example, UDP promotes the SimpleBaseline <ref type="bibr" target="#b24">[24]</ref> by 1.5 AP (70.2 to 71.7) and 1.0 AP (71.9 to 72.9) within ResNet50-256×192 and ResNet152-256×192 configurations, respectively. For HRNet <ref type="bibr" target="#b25">[25]</ref> within W32-256×192 and W48-256×192 configurations, UDP obtains gains by 1.7 AP (73.5 to 75.2) and 1.4 AP (74.3 to 75.7), respectively. The HRNet-W48-384×288 equipped with UDP achieves 76.5 AP (1.0 improvement) and sets a new state-of-the-art for top-down human pose estimation. Besides, in bottom-up paradigm, UDP simultaneously offers both accuracy improvement and latency reducing on the baselines. For HRNet-W32-512×512 configuration in HigherHRNet <ref type="bibr" target="#b26">[26]</ref>, UDP promotes its performance by 2.7 AP, and at the same time, offers an acceleration by 6.1 times. For HigherHRNet-W32-512×512 configuration, the promotion and acceleration are +0.8 AP and 2.6 times respectively. In addition, we also perform experiments on extra dataset CrowdPose <ref type="bibr" target="#b30">[30]</ref> to verify the generalization ability of UDP among different data distributions. Experimental results show that the performance of UDP in this dataset is in line with that on COCO dataset. Finally to verify the statement in methodology analysis, we measure the contribution of each element in UDP and the effect of the existing remedies in relative works with exhaustive ablation study. Based on the experiment results, we call for attention on the data processing aspect when designing or evaluating the future works. The code is public available for reference. This paper is built upon our conference paper <ref type="bibr" target="#b31">[31]</ref> and significantly extended in several aspects. First, we rearrange the methodology section for methodical stating, and explain it with more specific background introduction and more detailed mathematical reasoning. Second, we extend the coverage of UDP by applying it to methods in bottom-up paradigm and make great discovery and promotion on state-of-the-art method HigherHRNet <ref type="bibr" target="#b26">[26]</ref>. Third, we use extra dataset CrowdPose <ref type="bibr" target="#b30">[30]</ref> to verify the generalization ability of UDP. In COCO and LVIS 2020 competitions, UDP serves as the baseline for the winner UDP++ <ref type="bibr" target="#b32">[32]</ref>, which marks this work as a meaningful milestone for pursuing high performance human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In recent years, research community has witnessed a significant advance from single person <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref> to multi-person pose estimation <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b50">[50]</ref>, where the latter can be generally categorized into bottom-up <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b52">[51]</ref> and top-down <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b53">[52]</ref>, <ref type="bibr" target="#b54">[53]</ref> approaches.</p><p>Bottom-up methods start by detecting identity-free joints for all the persons in an input image and then group them into person instances. In this paradigm, both cost and efficient are considered, both the identity-free joint detection and grouping strategy are the main concerns. OpenPose <ref type="bibr" target="#b46">[46]</ref> builds a model that contains two branches to predict keypoint heatmaps and pairwise relationships (part affinity fields) between them, where the latter acts as the main cue in grouping process. MultiPoseNet <ref type="bibr" target="#b55">[54]</ref> simultaneously achieves human detection and pose estimation, and proposes PRN to group the keypoints by the bounding box of each people. Aiming at resolving the human pose estimation problem in crowd sense, Li et al. <ref type="bibr" target="#b30">[30]</ref> design a new model by combining joint-candidate single person pose estimation and global maximum joints association. Simultaneously, a new dataset named CrowdPose is collected specific for performance evaluation in crowd senses. Newell et al. <ref type="bibr" target="#b49">[49]</ref> use one network for both heatmap prediction and embedding study. Grouping is done by utilizing association embedding, which assigns each keypoint with a tag and groups keypoints based on the L2 distance between tag vectors. As a follower, Chen et al. <ref type="bibr" target="#b26">[26]</ref> replace the hourglass style networks in <ref type="bibr" target="#b49">[49]</ref> with the proposed HigherHRNet. By using higher output resolution, HigherHRNet improves the precision of the predictions by a large margin.</p><p>. https://github.com/HuangJunJie2017/UDP-Pose . https://cocodataset.org/workshop/coco-lvis-eccv-2020.html Top-down methods achieve multi-person pose estimation by the two-stages process, including obtaining person bounding boxes through a person detector like Faster R-CNN <ref type="bibr" target="#b56">[55]</ref> and predicting keypoint locations within these boxes. As single person pose estimation is performed with fixed scale patches, most state-of-theart performances on multi-person popular benchmarks COCO <ref type="bibr" target="#b20">[20]</ref> are achieved by top-down methods <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b31">[31]</ref>. Existing works with this paradigm pay more attention to the designing of network structure. Chen et al. <ref type="bibr" target="#b57">[56]</ref> propose Structure-aware Convolutional Network trained with Generative Adversarial Networks for human pose structure exploiting. Following ShuffleNet <ref type="bibr" target="#b58">[57]</ref> and SENet <ref type="bibr" target="#b59">[58]</ref>, Su et al. <ref type="bibr" target="#b60">[59]</ref> propose Channel Shuffle Module (CSM) and Spatial, Channel-wise Attention Residual Bottleneck (SCARB) specific for human pose estimation problem. CPN <ref type="bibr" target="#b27">[27]</ref> and MSPN <ref type="bibr" target="#b28">[28]</ref> are the leading methods on COCO keypoint challenge, adopting cascade network to refine the keypoints prediction. Sim-pleBasline <ref type="bibr" target="#b24">[24]</ref> adds a few deconvolutional layers to enlarge the resolution of output features. Thought simple, it has a competitive performance among existing works. HRNet <ref type="bibr" target="#b25">[25]</ref> maintains highresolution representations through the whole process, achieving state-of-the-art performance on public datasets. Mask R-CNN <ref type="bibr" target="#b54">[53]</ref> builds an end-to-end framework and achieves a good balance between performance and inference speed.</p><p>Data processing in human pose estimation mainly includes coordinate system transformation and keypoint format transformation. Coordinate system transformation means transforming the data (i.e., keypoint coordinates and image matrixes) between different coordinate systems when some operations are conducted like cropping, resizing, rotating and flipping. During this process, most state-of-the-art methods <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref> use pixel to measure the size of images when performing resizing operation, leading to unaligned results when using flipping strategy in inference. This bias degrades the accuracy by a large margin, lays a trap for research community and has given bone to some suboptimal remedies. The remedies are all empirical and always unreported. For example, without any explanation, SimpleBaseline <ref type="bibr" target="#b24">[24]</ref> HRNet <ref type="bibr" target="#b25">[25]</ref> and Darkpose <ref type="bibr" target="#b29">[29]</ref> empirically shift the result from flipped image by 1 pixel in network output coordinate system to suppress the predicting error. CPN <ref type="bibr" target="#b27">[27]</ref> and MSPN <ref type="bibr" target="#b28">[28]</ref> achieve similar effect by shifting the average result by 2 pixels in network input coordinate system. HigherHRNet <ref type="bibr" target="#b26">[26]</ref> proposes to use higher network output resolution and conducts the experiment with some unreported compensation for large superiority on the baseline. These remedies are effective and appealing, but being the recipe for disaster as they hinder the development of technology by causing failure in reproduction and unfair in comparison. In this paper, we propose unbiased coordinate system transformation to thoroughly solve this problem, which will not only boost the performance of the existing methods but also provide a more reliable baseline for future works. Keypoint format transformation (i.e., encoding and decoding) commonly denotes the transformation between joint coordinates and heatmaps, which is firstly proposed in <ref type="bibr" target="#b38">[38]</ref> and has been widely used in state-of-the-art methods <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b54">[53]</ref>. In training process, it encodes the annotated keypoint coordinate into a heatmap with Gaussian distribution. And in testing process, it decodes the network predicted heatmap back into keypoint coordinate. This pipeline shows superior performance when compared with directly predicting the keypoint coordinates <ref type="bibr" target="#b61">[60]</ref>, but is still imperfect on account of its defective design and inherent precision degradation. The combined classification and regression format based encoding-decoding paradigm in <ref type="bibr" target="#b47">[47]</ref> provides an mathematically error-free entrance to further promote the prediction accuracy. Analogously, Darkpose <ref type="bibr" target="#b29">[29]</ref> achieve unbiased keypoint format transformation by proposing a distribution-aware decoding method to match the encoding method use in <ref type="bibr" target="#b26">[26]</ref>. In this paper, we will introduce these two unbiased keypoint format transformation paradigm, verify their unbias property and show their superiority on the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UNBIASED DATA PROCESSING FOR HUMAN POSE ESTIMATION</head><p>In human pose estimation, data processing involves the transformation between different coordinate system and the transformation between different keypoint format. In the following, we will give details introduction of our unbiased data processing method in these two aspects respectively (i.e., unbiased coordinate system transformation and unbiased keypoint format transformation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unbiased Coordinate System Transformation</head><p>As it is new to this topic and quite ambiguous to community, for clarified and reasonable statement, the concept of unbiased coordinate system transformation is constructed from the base. We firstly propose the unified definition of data in continuous space. Then based on this definition, the concept of coordinate system transformation and the targets of unbias are introduced. We design the coordinate system transformation in some elementary operations (i.e., cropping, resizing, rotating and flipping) before we construct the common composite transformations between the coordinate systems involved in human pose estimation problem(i.e., source image coordinate system, network input coordinate system and network output coordinate system). Subsequently, we verify the unbias properties of the designed coordinate system transformation pipeline with mathematical reasoning. And at last to showcase how the defective coordinate system affect the research community, some biased data processing methods are analyzed, and the theory behind some reported techniques and unreported tricks used in state-of-the-arts are thoroughly studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">An Unified Definition of Data in Continuous Space.</head><p>The image matrixes and the target keypoint coordinates are the main data involved in human pose estimation problem. The images are stored and processed in a discrete format, but the keypoint coordinates are defined, processed and evaluated in continuous spaces. To avoid precision degradation in the coordinate system transformation pipeline, an unified paradigm is required for uniformly analyzing and dealing with different data in the coordinate system transformation problems.</p><p>To this end, we assume that there is a continuous image plane and consider each image matrix as a discrete sampling result on it, where each pixel in an image matrix is a specific sample point. Formally, in line with the definition of target keypoint coordinates in COCO dataset <ref type="bibr" target="#b20">[20]</ref>, we define the coordinate system O-XY as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> to describe the continuous image planes. The origin of the coordinate system is located at the most top-left pixel, the O-X direction is from left to right and the O-Y direction is from top to down. Besides, the distance between adjacent pixels is assumed to be equivalent and is defined as the unit length of the coordinate system. Then we have an image matrix as a sampling result of the image plane I, which is denoted as  h are the width and height of the image counted in unit lengths. And a set of target keypoints are also defined in the same image plane and denoted as {k = (x, y)}.</p><formula xml:id="formula_0">{I(p) = (r, g, b)|p = (x, y), x ∈ {0, 1, 2...w}, y ∈ {0, 1, 2...h}}. w and</formula><p>It is worth noting that the size of the sample points defined here is infinitely small and the size of the images' semantically meaningful area is calculated with the unit length. As a result, the image size (i.e., w for width and h for height) we discussed following is different from the resolution of the image matrix, which is widely used for defining the image size in common sense. Formally, the relationship between them is as follow:</p><formula xml:id="formula_1">w = w p − 1 h = h p − 1<label>(1)</label></formula><p>where w p and h p are the width and height of the image matrix counted in pixels. We use superscript p to discriminate the variables counted in pixel from those measured in unit length. </p><formula xml:id="formula_2">→ ( , ) ( , ) s→ −1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">The Concept of Coordinate System</head><p>Transformation.</p><p>The coordinate system transformation in human pose estimation can be generally formulated as the data description transformation from the source coordinate system into the destination coordinate system. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, we label the source coordinate system with subscript s as O s − X s Y s and the target coordinate</p><formula xml:id="formula_3">system as O d − X d Y d .</formula><p>Then the transformation of keypoint coordinates can be formulated as:</p><formula xml:id="formula_4">k d = T s→d k s (2)</formula><p>where T s→d is the coordinate system transformation matrix from the source coordinate system to the destination coordinate system. And the transformation of the contents in image matrix can be formulated as:</p><formula xml:id="formula_5">I d (p d ) = I s (T −1 s→d p d )<label>(3)</label></formula><p>where T −1 s→d is the inverse of T s→d . Equation 3 means that we make the image constant semantically aligned with the annotated keypoints in the destination coordinate system by setting the color of position p d the same as that in the source image at position T −1 s→d p d . The results of backtracking T −1 s→d p d are usually not integers, and thus, I s (T −1 s→d p d ) should be calculated by bilinear interpolation with the valid surrounding points (i.e., the purple points in <ref type="figure" target="#fig_2">Figure 3</ref>). As we only have a sampling result (i.e., the image matrix) of the image plane, interpolation is the optimal way to reduce the precision degradation in image transformation, but can not thoroughly remedy it. Thus, as the precision degradation of interpolation is irreversible and cumulative, we have a principle that the less interpolation done in the data processing pipeline is the better in designing coordinate system transformation pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">The Targets of Unbias.</head><p>Unbias is a target in coordinate system transformation designing, which contains two aspect: One is to keep the semantical alignment after performing transformations. Semantical alignment means that the positional relativeness between different data (i.e., images and keypoint positions) is unchanged (e.g., the annotated position of nose in destination space is still exactly located upon the nose in the image described in destination space). This is guaranteed by keeping the transformation matrix the same in both Equation 2 and Equation 3.</p><p>Another aspect is to make the predicting result exactly aligned with the ground truth under the assumption that the network has a perfect learning ability. In other words, we hope the network's learning ability to be the unique source of precision degradation, and there are no defects in the design of the coordinate transformation pipeline will cause precision degradation. In the following, we will detail our unbiased coordinate system transformation pipeline and prove its unbiased property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Coordinate System Transformation in Elementary Operations.</head><p>Coordinate system transformations in human pose estimation derives from some elementary operations like cropping, resizing, rotating and flipping.</p><p>Cropping, as illustrated in <ref type="figure">Figure 4</ref>, is conducted according to a specific Region of Interest (ROI) defined in the source coordinate system ROI = (bx s , by s , bw s , bh s ), where (bx s , by s ) denotes its center position and (bw s , bh s ) denotes its width and height. The destination coordinate system can be obtained by moving the origin of the source coordinate system to the upper left corner of the ROI. Thus, the transformation matrix should be designed as: Resizing, as illustrated in <ref type="figure">Figure 5</ref>, changes the sampling strategy only and keep the semantic constant of the image the same as the source. We make the four corner sample point semantically align with the source four corner sample point and let the other sample points evenly distributed among the area dividing by the four corners. Thus the only thing that changes is the unit length of the coordinate system and the transformation matrix should be designed as: Rotating, as illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>, is conducted according to a rotation center which is always set as the center of a specific ROI instead of the origin of the coordinate system. This design aims at keeping the center position of ROI unchanged (i.e., (bx s , by s ) = (bx d , by d )). For example, the ROI refers to the bounding boxes of human instances in top-down paradigm and the whole image in bottom-up paradigm. So, the transformation matrix should be designed as the combination of three elementary transformations: Flipping, as illustrated in <ref type="figure" target="#fig_5">Figure 7</ref>, generally takes x = w s /2 as the mirror and horizontally exchanges the images' content. So, the transformation matrix should be designed as:</p><formula xml:id="formula_6">T crop (ROI) =   1 0 −bx s + 0.5bw s 0 1 −by s + 0.5bh s 0 0 1   (4) = ( , , , ) ( , ) Crop</formula><formula xml:id="formula_7">T resize (w s , h s , w d , h d ) =   w d ws 0 0 0 h d hs 0 0 0 1   (5) Rotate = ( ,<label>, , ) ( , )</label></formula><formula xml:id="formula_8">T rot (θ, ROI) =T d2→d T d1→d2 T s→d1 =   1 0 bx s 0 −1 by s 0 0 1     cos θ sin θ 0 − sin θ cos θ 0 0 0 1     1 0 −bx s 0 −1 by s 0 0 1   =   cos θ − sin θ −bx s cos θ + by s sin θ + bx s sin θ cos θ −bx s sin θ − by s cos θ + by s 0 0 1   (6) Flip = /2</formula><formula xml:id="formula_9">T f lip (w s ) =   −1 0 w s 0 1 0 0 0 1   (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Common Coordinate System Transformation</head><p>In human pose estimation as illustrated in <ref type="figure" target="#fig_6">Figure 8</ref> During the training process, the data is firstly transformed from the source image coordinate systems into the network input coordinate systems according to a specific ROI (bx s , by s , bw s , bh s ) and a rotation angle θ. Some elementary operations are conducted orderly: </p><formula xml:id="formula_10">T f lip (w i ) =   −1 0 w i 0 1 0 0 0 1   T rot (θ, (0.5w i , 0.5h i , w i , h i )) =   cos θ − sin θ −0.5w i cos θ + 0.5h i sin θ + 0.5w i sin θ cos θ −0.5w i sin θ − 0.5h i cos θ + 0.5h i 0 0 1   T resize (bw s , bh s , w i , h i ) =</formula><formula xml:id="formula_11">O s -X s Y s , network input coordinate system O i -X i Y i and network output coordinate system O o -X o Y o .</formula><p>Then we have the combined transformation:</p><formula xml:id="formula_12">k i = T s→i,train k s I i (p i ) = I s (T −1 s→i,train p i ) T s→i,train = T f lip T rot T resize T crop (9) Equation 9</formula><p>integrates not only the necessary transformations likes cropping and resizing, but also the optional augmentations (i.e., T f lipping for random flipping, T rotating for random rotating, T cropping for half body and random cropping.) used in human pose estimator training. Cropping and Resizing are necessary, while flipping and rotating are optional. The image matrixes in network input space are set as the network input and we have the inference results in the network output space:</p><formula xml:id="formula_13">k o = N (I i )<label>(10)</label></formula><p>where N denotes the networks. The annotation is simultaneously transformed from the network input space into the network output space by a simple resizing operation:</p><formula xml:id="formula_14">k o = T i→o k i T i→o = T resize T resize (w i , h i , w o , h o ) =   wo wi 0 0 0 ho hi 0 0 0 1  <label>(11)</label></formula><p>And k o in the network output space serves as the supervision:</p><formula xml:id="formula_15">Loss = ||k o − k o ||<label>(12)</label></formula><p>The networks are optimized in the training process and as an ideal result, we have:</p><formula xml:id="formula_16">Loss = ||k o − k o || = 0 N (I i ) =k o = k o = T i→o k i<label>(13)</label></formula><p>which means that the network learns not only the reflection from image matrixes I i to keypoint positions k i , but also the reflection of the transformation T i→o defined in Equation <ref type="bibr" target="#b11">11</ref>.</p><p>In testing process, only the image matrixes are transformed from the source image coordinate systems into the network input coordinate system with the necessary elementary transformations, which should be in line with those in the training process: </p><formula xml:id="formula_17">I i (p i ) = I s (T −1 s→i,test p i ) T s→i,test = T resize T crop T resize (bw s , bh s , w i , h i ) =</formula><p>Then the network outputs in <ref type="figure" target="#fig_7">Equation 10</ref> are transformed back to the source image coordinate systems by inverse transformations:</p><formula xml:id="formula_19">k s = T o→sko T o→s = T crop T resize T resize (w o , h o , bw s , bh s ) =    bws wo 0 0 0 bhs ho 0 0 0 1    T crop (0.5w s − bx s + 0.5bw s , 0.5y s − by s + 0.5bh s , w s , h s ) =   1 0 bx s − 0.5bw s 0 1 by s − 0.5bh s 0 0 1  <label>(15)</label></formula><p>With Equation <ref type="bibr" target="#b13">13</ref> as assumption and taking <ref type="bibr">Equation 11</ref>, Equation 14, Equation <ref type="bibr" target="#b15">15</ref> into consideration, we have the following identical relation:</p><formula xml:id="formula_20">k s = T o→sko = T o→s T i→o k i = T o→s T i→o T s→i,test k s =   1 0 bx s − 0.5bw s 0 1 by s − 0.5bh s 0 0 1      bws wo 0 0 0 bhs ho 0 0 0 1      wo wi 0 0 0 ho hi 0 0 0 1     wi bws 0 0 0 hi bhs 0 0 0 1     1 0 −bx s + 0.5bw s 0 1 −by s + 0.5bh s 0 0 1   k s = k s<label>(16)</label></formula><p>This inference prove that the result in the source image space is exactly equal to the ground truth, which means that the data transformation pipeline designed above is unbiased and no systematic error would be involved. When flipping ensemble is used in testing process, the flipped image is obtained by performing flipping transformation in the network input space:</p><formula xml:id="formula_21">I i,f lip (p i,f lip ) = I i (T −1 i→(i,f lip) p i,f lip ) T i→(i,f lip) = T f lip (w i ) =   −1 0 w i 0 1 0 0 0 1  <label>(17)</label></formula><p>Then we have the network predictionk o,f lip = N (I i,f lip ) which is subsequently flipped back in the network output space:</p><formula xml:id="formula_22">k o = T (o,f lip)→oko,f lip T (o,f lip)→o = T f lip (w o ) =   −1 0 w o 0 1 0 0 0 1  <label>(18)</label></formula><p>with <ref type="bibr">Equation 13</ref> as assumption and taking Equation 11, Equation 17 and Equation <ref type="bibr" target="#b18">18</ref> into consideration, we have the following identical relation:</p><formula xml:id="formula_23">k o = T (o,f lip)→oko,f lip = T (o,f lip)→o T (i,f lip)→(o,f lip) k i,f lip = T (o,f lip)→o T (i,f lip)→(o,f lip) T i→(i,f lip) k i = T f lip (w o )T resize (w i , h i , w o , h o )T f lip (w i )k i =   −1 0 w o 0 1 0 0 0 1     wo wi 0 0 0 ho hi 0 0 0 1     −1 0 w i 0 1 0 0 0 1   k i =   wo wi 0 0 0 ho hi 0 0 0 1   k i = T i→o k i =k o<label>(19)</label></formula><p>This inference prove that, in the network output space, the results from flipped images are aligned with those from the origin images. By taking Equation <ref type="bibr" target="#b16">16</ref> into consideration, the results from flipped images in the source image space are also aligned with the ground truths and no systematic error would be involved. The establish of Equation <ref type="bibr" target="#b16">16</ref> and Equation <ref type="bibr" target="#b19">19</ref> guarantees the unbiased property in the coordinate system transformation pipeline. They will be used as the guideline for checking biased coordinate system transformation pipelines in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Diagnosis of the Biased Coordinate System Transformation</head><p>In most state-of-the-arts <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, the bias problem in coordinate system transformation pipeline derives from using resolution (w p s , h p s ) counted in pixels instead of size (w s , h s ) measured in unit length when performing resizing transformation. As a consequence, it changes Equation <ref type="bibr" target="#b16">16</ref> and Equation 19 into: </p><formula xml:id="formula_24">k s = T o→sko = T o→s T i→o k i = T o→s T i→o T s→i,test k s =   1 0 bx s − 0.</formula><formula xml:id="formula_25">k o = T (o,f lip)→oko,f lip = T (o,f lip)→o T (i,f lip)→(o,f lip) k i,f lip = T (o,f lip)→o T (i,f lip)→(o,f lip) T i→(i,f lip) k i = T f lip (w o )T resize (w p i , h p i , w p o , h p o )T f lip (w i )k i =   −1 0 w o 0 1 0 0 0 1      w p o w p i 0 0 0 h p o h p i 0 0 0 1      −1 0 w i 0 1 0 0 0 1   k i =    w p o w p i 0 w p o w p i − 1 0 h p o h p i 0 0 0 1    k i =    1 0 w p o w p i − 1 0 1 0 0 0 1       w p o w p i 0 0 0 h p o h p i 0 0 0 1    k i =   1 0 1−s s 0 1 0 0 0 1  k o<label>(21)</label></formula><p>where s = w p i /w p o is the stride factor for describing the size variation of network features. Here,k s is still equal to k s , indicating that the aforementioned modification will not change the unbiased property in coordinate system transformation pipeline T o→s T i→o T s→i,test and should have no effect on the precision of the predicted results. However when flipping ensemble is adopted in testing process,k o is not exactly aligned withk o , and there is </p><formula xml:id="formula_26">an offset of 1−s s in O o -X o direction.</formula><formula xml:id="formula_27">e(x) o = |x(k o,avg ) − x(k o )| = | 1 − s 2s | = 0.375| s=4 (23)</formula><p>wherek o is regarded as ground truth as it has been proved unbiased by <ref type="bibr">Equation 20</ref>. The magnitude of this predicting error is so large that the performance will be degraded by a considerable margin. In state-of-the-arts, there are some empirical remedies for this error, which can be classified into two categories: direct compensation or using higher resolution. As the error | 1−s 2s | has a fixed scale which is determined by the stride factor, direct compensation is effective, being the remedy in most state-of-the-arts top-down methods <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>. For example, SimpleBaseline <ref type="bibr" target="#b24">[24]</ref>, HRNet <ref type="bibr" target="#b25">[25]</ref> and DarkPose <ref type="bibr" target="#b29">[29]</ref>  </p><p>In this way, the final error can be reduced to</p><formula xml:id="formula_29">e(x) o = | 1 2s | = 0.125| s=4<label>(25)</label></formula><p>where e(x) o &lt; e(x) o when s &gt; 2, which makes sense in most existing top-down methods with a stride factor of 4 <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>. Intuitively, as a result of reasoning, an extra compensation for e(x) o in network output space can make the result of existing work more accurate. We will verify this in ablation study. Besides, when mapping e(x) o back to source image coordinate system (O s -X s Y s ) with Equation 15, we have:</p><formula xml:id="formula_30">e(x) s = | 1 2s × bw s w p o | = | bw s 2w p i |<label>(26)</label></formula><p>where bw s is fixed in inference process. Equation 26 means that higher network input resolution can help suppress the predicted error caused by e(x) s . In other words, the existing top-down methods benefit more from higher input resolution and suffer more accuracy loss from lower input resolution. Without shifting one pixel in network output space, we have:</p><formula xml:id="formula_31">e(x) s = | s − 1 2s × bw s w p o | = | bw s (s − 1) 2w p i |<label>(27)</label></formula><p>which means that both higher input resolution and higher output resolution can help suppress this error. And this contributes the most performance boosting in HigherHRnet <ref type="bibr" target="#b26">[26]</ref> who empirically proposes to use higher output resolution to pursue high precision at the cost of tremendous latency in both network inference and post processing. By contrast, unbiased data processing provides a free access to achieve similar performance improvement with a low output resolution. Besides of using higher output resolution, HigherHRNet uses another unreported operation that resizes the network output into a a resolution as high as the network input. This operation also coincidentally remedies the error caused by biased coordinate system transformation pipeline and benefit the performance of HigherHRNet structure at the cost of extra latency in post processing. Through ablation study, we will show this operation are gilding the lily when the coordinate system transformation pipeline is unbiased as it will involve extra error and change the distribution of the network output by performing an extra interpolation in resizing operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unbiased Keypoint Format Transformation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The Concept of Unbiased Keypoint Format Transformation.</head><p>As the coordinate of keypoint is not the superior format for convolutional network study, the intuitively more proper format of heatmap is proposed and quickly has been proved effective. The keypoint fromat transformation refers to the transformations between keypoint coordinates and heatmaps which is widely used in state-of-the-art methods. In common sense, encoding denotes the transformation from coordinate format into heatmap format, while decoding denotes the inverse transformation.</p><formula xml:id="formula_32">H = Encoding(k) k = Decoding(H)<label>(28)</label></formula><p>Target of Unbiased in keypoint format transformation designing is to avoid precision degeneration in the encoding and decoding transformation. As a formulated target, we should have:</p><formula xml:id="formula_33">k = Decoding(Encoding(k))<label>(29)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Unbiased Keypoint Format Transformation.</head><p>In this subsection, we will introduce two unbiased keypoint format transformation paradigm and simultaneously showcase their unbias property.</p><p>Combined classification and regression format is inspired by the works in object detection <ref type="bibr" target="#b56">[55]</ref> where anchors are used to predict bounding boxes, and first proposed in <ref type="bibr" target="#b47">[47]</ref>. We give details introduction here with some modifications. In training process, each annotated keypoint k = (m, n) is encoded through:</p><formula xml:id="formula_34">C(x, y, m, n) = 1 if (x − m) 2 + (y − n) 2 &lt; r 2 0 otherwise X (x, y, m, n) = m − x Y(x, y, m, n) = n − y<label>(30)</label></formula><p>where C is the classification heatmap act as the anchor in object detection for preliminarily locate the keypoint. r is a hyperparameter referring the radius of the area classified as positive.</p><p>Consist of offset vectors, X and Y are the regression heatmap for preserving the residual locating information. Then the loss is designed as: </p><p>where C in Loss reg defines the region of interesting, which means that we only need to learn the offset among the area where the classification label is true. The network is optimized in the training process and as a ideal result, we have:</p><formula xml:id="formula_36">C,X ,Ŷ = C, X , Y<label>(32)</label></formula><p>Then in testing processing, the prediction is decoding by:</p><formula xml:id="formula_37">k =k h + (X (k h ),Ŷ(k h )) k h = argmax(Ĉ)<label>(33)</label></formula><p>where the position of highest responsek h is located first and is subsequently updated by utilizing the predicted offsets. By taking Equation <ref type="bibr" target="#b30">30</ref> and Equation 32 into consideration, we have:</p><formula xml:id="formula_38">k =k h + (X (k h ),Ŷ(k h )) = (x(k h ), y(k h )) + (m − x(k h ), n − y(k h )) = (m, n) = k<label>(34)</label></formula><p>which means that no systematic error is involved in the keypoint format transformation pipeline and the unbiased target in Equation 29 is achieved. Classification format is widely used in most state-of-the-arts, where classification heatmap is used only with a gaussian-like distribution:</p><formula xml:id="formula_39">C(x, y, m, n) = exp(− (x − m) 2 + (y − n) 2 2δ 2 )<label>(35)</label></formula><p>The loss is designed as:</p><formula xml:id="formula_40">Loss = ||C −Ĉ|| C = N (I)<label>(36)</label></formula><p>The network is optimized in the training process and as a ideal result, we have:Ĉ = C (37)</p><p>In testing process, we introduce the decoding method DARK <ref type="bibr" target="#b29">[29]</ref> who decoding the classification heatmap into keypoint coordinates by searching the center of the gaussian distribution where the first derivative is equal to zero:</p><formula xml:id="formula_41">k =k h −Ĉ (k h ) −1Ĉ (k h ) k h = argmax(Ĉ)<label>(38)</label></formula><p>whereĈ andĈ are the first order derivative and second order derivative (i.e., Hessian) ofĈ. According to <ref type="bibr" target="#b29">[29]</ref>, the precision degradation caused by Taylor series approximation is negligible andk is theoretically close to k, which matches the purpose of unbias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Analysis of Biased Keypoint Format Transformation.</head><p>We take the keypoint format transformation method used in SimpleBaseline <ref type="bibr" target="#b24">[24]</ref>, HRNet <ref type="bibr" target="#b25">[25]</ref> and HigherHRNet <ref type="bibr" target="#b26">[26]</ref> as the example for studying the effect of biased data form transformation. keypoints are encoded into classification heatmap with gaussian distribution as Equation <ref type="bibr" target="#b35">35</ref>, but are decoded by the a suboptimal method:k </p><formula xml:id="formula_42">=k h + 0.25 * sign(Ĉ (k h )) k h = argmax(Ĉ) sign(x) = 1 if x &gt; 0 −1 otherwise</formula><formula xml:id="formula_43">E(|m s −m s |) = E(|m o −m o |) × bw s w o<label>(42)</label></formula><p>Considering error E(|m −m|) and E(|n −n|), the methods with biased data form transformation benefit from higher network output resolution. And this also contributes part of the performance boosting in HigherHRnet <ref type="bibr" target="#b26">[26]</ref>.  </p><p>where we use a approximation to simplified the following analysis. = 0.375, the biased decoding method contributes a variance which will have extra negative impact on the final performance. It is worth noting that, the actual errors are more complicated than that analyzed above, as the approximation in Equation 44 also has an impact on the errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Result on COCO dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Implementation Details</head><p>For top-down paradigm, we take SimpleBaseline <ref type="bibr" target="#b24">[24]</ref> and HRNet <ref type="bibr" target="#b25">[25]</ref> as baseline and use the official implementation . All training settings are preserved except for the data processing pipeline proposed in this paper. Unbiased keypoint format transformation in combined classification and regression format is used in this paradigm as default with hyper-parameters r = 0.0625 * w p o in Equation <ref type="bibr" target="#b30">30</ref>. Classification format is verified in ablation study with hyper-parameters δ = 2.0 in Equation <ref type="bibr" target="#b35">35</ref>. During inference, HTC <ref type="bibr" target="#b71">[70]</ref> detector is used to detect human instances. With multiscale test, the 80-class and person AP on COCO val set <ref type="bibr" target="#b20">[20]</ref> are 52.9 and 65.1, respectively. The results of HRNet <ref type="bibr" target="#b25">[25]</ref> and Sim-pleBaseline <ref type="bibr" target="#b24">[24]</ref> on COCO val set with this human detection are reproduced for fair comparison. The inference speed is tested on val set and measured in Person Per Second (PPS). The hardware environment mainly includes a single RTX 2080ti GPU and an Intel(R) Xeon(R) E5-2630-v4@2.20GHz CPU.</p><p>For bottom-up paradigm, we take HigherHRNet <ref type="bibr" target="#b26">[26]</ref> as baseline and both HRNet and HigherHRNet network structures are exploited. All training settings are preserved except for the data processing pipeline proposed in this paper. During inference, the operation of resizing the network output is removed and the decoding method is replaced with the unbiased one in Equation <ref type="bibr" target="#b38">38</ref>. Testing with single scale and multi-scale (i.e., [×2,×1,×0.5], where ×2 means that the input resolution is enlarged by factor 2 like 512×512 to 1024×1024) are reported respectively. The inference speed is measured in Image Per Second (IPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results of top-down paradigm on the val set.</head><p>We report the performance improvement when UDP is applied to SimpleBaseline <ref type="bibr" target="#b24">[24]</ref> and HRNet <ref type="bibr" target="#b25">[25]</ref> in <ref type="table" target="#tab_1">Table 1</ref>. Considering the series of SimpleBaseline, the promotions are +1.6 AP (71.3 to 72.9) for ResNet-50 backbone and +1.4 AP (72.9 to 74.3) for ResNet-152 backbone. For higher network input resolution, the promotions are +0.8 AP and +0.9 AP respectively. For HRNet family, the promotion is +1.2 AP (75.6 to 76.8) for HRNet-w32 backbone and +1.3 AP (75.9 to 77.2) for HRNet-w48 backbone. For higher network input resolution, the promotions are +1.1 AP and +0.7 AP respectively. We summarize some key characteristics of the results: i) improvements are consistent among different backbone types, which indicates that the learning ability of the network has little impact on the precision loss caused by the biased data processing pipeline. This indicates that more powerful network structures proposed in future work would not help solving the bias problem and UDP is the necessary solution. ii) improvements on methods with smaller network input resolution are more than that with larger network input resolution. This is in line with the analysis in methodology that larger network input size can help suppressing the error and models with smaller network input size suffer more precision degression. iii) No extra latency is involved in the proposed method, which means that UDP provides the aforementioned improvement at no cost. We take the most recent method HigherHRNet <ref type="bibr" target="#b26">[26]</ref> as the representative baseline with two network constructions HRNet and HigherHRNet. With biased data processing as reported in <ref type="bibr" target="#b26">[26]</ref>, HRNet-W32-512×512 configuration only scores 64.4 AP with an inference speed of 0.8 IPS and HigherHRNet-W32-512×512 configuration 67.1 AP with an inference speed of 1.1 IPS. By contrast with UDP, HRNet-W32-512×512 configuration scores 67.0 AP with an inference speed of 4.9 IPS which has 2.6 AP superiority and 6.1 times faster than the baseline. The performance of this configuration is even close to the baseline with HigherHRNet-W32-512×512 configuration, and still 4.5 times faster than it. HigherHRNet-W32-512×512-UDP configuration scores 67.8 AP with an inference speed of 2.9 IPS, which has 0.7 AP superiority and 2.6 times faster than the baseline configuration HigherHRNet-W32-512×512. At no cost, UDP offers both performance boosting and latency reducing. With UDP, we have a more reasonable performance difference between HRNet-W32-512×512 and HigherHRNet-W32-512×512 on COCO val set, which is +0.8 AP improvement at the cost of +70% extra latency in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.4</head><p>Results on the test-dev set. <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_7">Figure 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on CrowdPose dataset</head><p>We utilize the CrowdPose <ref type="bibr" target="#b30">[30]</ref> dataset to verify the generalization ability of UDP among different data distributions. HigherHRNet <ref type="bibr" target="#b26">[26]</ref> is used as baseline and the experimental configurations are set the same as those in COCO dataset. In line with <ref type="bibr" target="#b26">[26]</ref>, models are trained on train and val sets and tested on test set. We report the improvement of AP on <ref type="table" target="#tab_3">Table 3</ref>. According to the experimental results, UDP not only promotes the accuracy of all configurations, but also speeds up the inference by a large margin. This is in line with that in COCO dataset. The exceptional thing is that, when UDP is applied, HigherHRNet-W32-512×512 configuration (65.6 AP with 2.4 IPS inference speed) and HigherHRNet-W48-640×640 configuration (66.7 AP with 1.8 IPS inference speed) with higher output resolution doesn't show any superiority on HRNet-W32 configuration (66.1 AP with 4.5 IPS inference speed) and HRNet-W48-640×640 configuration (67.2 AP with 4.2 IPS inference speed). This puts doubt on the generalization of the techniques proposed in HigherHRNet <ref type="bibr" target="#b26">[26]</ref>. Thus we empirically argue that, by effecting the performance and misguided the researchers, the biased data processing pipeline has a negative effect on the technology development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study on Top-down Paradigm</head><p>In this subsection, we use HRNet-W32 backbone and 256 × 192 input size to perform ablation study on the techniques involved in . This verify the conjecture that using resolution counted in pixels instead of size measured in unit length when performing resizing transformation has no impact on the unbias property of the data processing pipeline. However, when FT is adopted, the performance of configuration C doesn't shows any improvement on configuration A, and instead, even drops by 1.2 AP from 74.5 AP to 73.3 AP. This showcase the tremendous negative effect of the error e(x) o reported in Equation <ref type="bibr" target="#b23">23</ref>. The trap caused by biased coordinate system transformation pipeline is so deep that producing great demand for remedies. By contrast with the proposed UCST, configuration D (75.7 AP) has 1.3 AP improvement on configuration B (74.4 AP). UCST is the prerequisite for performance improving with FT.</p><p>By performing an empirical compensation, configuration E with SNOOP scores 75.6 AP, which is close to the result in configuration D with UCST. This means that, by taking the unbiased configuration D as reference, 66.7% of error e(x) o suppressed by SNOOP has a dominating effect on the performance, and the remainder (i.e., e(x) o ) would have little impact on the performance (i.e., around 0.1 AP, 75.6→75.7). We subsequently perform EC in configuration F to verify this. According to the experimental  result, EC offers just 0.2 AP (75.6→75.8) improvement which is in line with the aforementioned inference. We empirically blame the ineffective of EC for the insensitive of the evaluation system, where the human pose are manually annotated with a certain variance. Proving by EC, the existence of residual error e(x) o indicates that the widely used unreported compensation (SNOOP) is a suboptimal remedy not only for its low interpretability, but also for its poorer accuracy.</p><p>With configuration E and I, we replace the encoding-decoding methods in configuration D with UKFT-CCRF and UKFT-CF, respectively. With UKFT, configuration E (76.8 AP) and I (76.8 AP) have similar improvement (+1.1 AP) upon baseline configuration D (75.7 AP), which indicates that the biased keypoint format transformation has a considerable impact on the performance. Beside, this also tells that the configuration (i.e., HRNet-W32 network structure with 256×192 input size and training settings in <ref type="bibr" target="#b25">[25]</ref>) used in this subsection has similar learning ability on the two unbiased format introduced in this paper. With configuration G where UCST is absent and only UKFT-CCRF is applied, the performance degrades by -2.3 AP to 74.5 AP. Both UCST and UKFT are important for accurate prediction and the defects in the data processing pipeline will have accumulative impact on the result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study on Bottom-up Paradigm</head><p>In this subsection, we study how Higher Network Output Resolution (HNOR), Unbiased Coordinate System Transformation (UCST), Unbiased Keypoint Format Transformation in Classification Form (UKFT-CF) and Resize the Network Output (RNO) affect the bottom-up method HightHRNet <ref type="bibr" target="#b26">[26]</ref>. Flipping Testing (FT) is used as default. Experimental settings and the corresponding performance on COCO val set are listed in <ref type="table" target="#tab_5">Table 5</ref>. With configuration B, We firstly remove the operation of RNO and apply UCST to the baseline configuration A (64.4 AP and 0.8 IPS). This offers a performance improvement of 1.5 AP and a speed up of 5.9 times to 65.9 AP with 4.9 IPS inference speed. By additionally applying UKFT-CF in configuration B, configuration C scores 67.0 AP with the same inference speed. Both UCST and UKFT are effective as in top-down paradigm.</p><p>The referenced configuration F with 67.1 AP and 1.1 IPS inference speed is the recommended settings in HigherHRNet <ref type="bibr" target="#b26">[26]</ref>. By constructing configuration E, we remove RNO from it to test the effect of this operation. And according to the result, RNO provides a negligible improvement of 0.2 AP at the high cost of 2.6 times latency in inference. With configuration H and I, we show that the proposed UCST and UKFT-CF incrementally promote the performance on configuration E by 0.4 AP to 67.3 AP and by additionally 0.5 AP to 67.8 AP, while the inference speed is maintained in 2.9 IPS. These improvements are relatively small when compared with that in configuration B and C. This is in line with the theory that HNOR helps suppress part of the systemic error hidden in data processing pipeline. When unbiased data processing is applying, the HigherHRNet-W32 backbone (i.e., configuration I) still has 0.8 AP superiority on HRNet-W32 (i.e., configuration C) but at the cost of extra 70% latency in inference.</p><p>Finally, with configuration D and J, we test the impact of Resize the RNO on the results with UDP. The performance variances are 0 AP with 6.1 times latency and -0.9 AP with 2.6 times latency respectively. RNO is unnecessary for bottomup paradigm when unbiased data processing is applied. The performance degradation in configuration D from C is derided from the distribution variation caused by the resizing operation. As this destroys the precondition of using UKFT-CF, where a gaussian distribution is strictly required <ref type="bibr" target="#b29">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, the common biased data processing for human pose estimation is quantitatively analysed. Interestingly, we find that the systematic errors in standard coordinate system transformation and keypoint format transformation couple together, significantly degrade the performance of human pose estimators in both topdown and bottom-up paradigms. A trap is laid for the research community and subsequently give born to many suboptimal remedies. This paper solves this problem by formulating a principled Unbiased Data Processing (UDP) strategy , which consists unbiased coordinate system transformation and unbiased keypoint format transformation. UDP not only pushes the performance boundary of human pose estimation, but also provides a reliable baseline for research community by wiping out the trap formulated in the defective data processing pipeline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of analyzing the coordinate system transformation problem in continuous space. O-XY denotes the coordinate system. An image matrix (the set of blue points) is regarded as a sampling result of the continuous image plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The illustration of the coordinate system transformation in human pose estimation problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :ResizeFig. 5 :</head><label>45</label><figDesc>The coordinate system transformation in cropping operation. The coordinate system transformation in resizing operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The coordinate system transformation in rotating operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>The coordinate system transformation in flipping operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>TFig. 8 :</head><label>8</label><figDesc>crop (bx s , by s , bw s , bh s ) =   1 0 −bx s + 0.5bw s 0 1 −by s + 0.5bh s The illustration of the common coordinate system transformation in human pose estimation problem. Three coordinate system are involved: source image coordinate system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>T 1 0</head><label>1</label><figDesc>crop (bx s , by s , bw s , bh s ) =  −bx s + 0.5bw s 0 1 −by s + 0.5bh s 0 0 1  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>5bw s 0 1 1 0 1 </head><label>111</label><figDesc>by s − 0.5bh s −bx s + 0.5bw s 0 1 −by s + 0.5bh s 0 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Takingk o as reference, 1−s s is the predicting error of resultk o in network output space. If we directly averagek o andk o as done in most existing works: k o,avg =k o +k o in O o -X o direction is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>empirically shift the result from flipped image by one pixel in O o -X o direction before performing the averaging operation to suppress this error: k o,avg =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Loss=</head><label></label><figDesc>Loss cls + Loss reg Loss cls = ||C −Ĉ|| Loss reg = C * ||X −X || + C * ||Y −Ŷ|| C,X ,Ŷ = N (I)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( 39 )</head><label>39</label><figDesc>According to the encoding in Equation<ref type="bibr" target="#b35">35</ref>, we haveargmax(Ĉ) = F loor(m) if m − F loor(m) &lt; 0.5 Ceil(m) otherwise sign(Ĉ (k h )) = 1 if m − F loor(m) &lt; 0.5 −1 otherwise(40)As an example, predicting coordinate in O − X direction has the distribution of:m = F loor(m) + 0.25 if m − F loor(m) &lt; 0.5 Ceil(m) − 0.25 otherwise(41)With the assumption that k is uniformly distributed in the image plane (i.e., both m − F loor(m) and n − F loor(n) are uniformly distributed in interval [0, 1)), the expected error in each direction is E(|m −m|) = E(|n −n|) = 1/8 = 0.125 unit length with a variance of V (|m −m|) = V (|n −n|) = 1/192 ≈ 0.0052.When mapping E(|m −m|) back to the source image coordinate system (O s -X s Y s ) with Equation 15, we have:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Error o e(x) = 1 2s in Equation 25 has an impact on the decoding result distribution. With a specific stride factor s = 4 and considering Equation 21, we have: As a result, the predicted heatmap from flipped image in O o -X o Y o is changed intoĈ o = C(x, y, m + 0.25, n), and the average heatmap distribution is changed into: C o,avg = C(x, y, m + 0.25, n) + C(x, y, m, n) 2 ≈ C(x, y, m + 0.125, n)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FF</head><label></label><figDesc>Finally, error o e(x) = 1 2s leads to a variation of the result distribution in Equation 39: loor(m) + 0.25 if m − F loor(m) &lt; 0.375 Ceil(m) − 0.25 if 0.375 ≤ m − F loor(m) &lt; 0.875 Ceil(m) + 0.25 otherwise (45) and the expected error in O o -X o direction is enlarged by just 1/32 unit length to E(|m−m|) = 5/32 ≈ 0.156 with a larger variance of V (|m −m|) = 37/3072 ≈ 0.012. Considering the error o e(x) = s−1 2s in Equation 23, the distribution of decoding result in Equation 39 will change into: loor(m) − 0.25 if m − F loor(m) &lt; 0.375 F loor(m) + 0.25 if 0.375 ≤ m − F loor(m) &lt; 0.875 Ceil(m) − 0.25 otherwise (46) and the expected error in O o -X o direction is enlarged by 1/4 unit length to E(|m −m|) = 3/8 = 0.375 with a larger variance of V (|m −m|) = 1/48 ≈ 0.0208. Compared with o e(x) = s−1 2s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>.</head><label></label><figDesc>https://github.com/leoxiaobin/deep-high-resolution-net.pytorch . https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation 4.1.3 Results of bottom-up paradigm on the val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>report the performance of UDP on COCO test-dev set. The results show similar improvement compared with val set, indicating the steady generalization property of UDP. Specifically, our approach promotes SimpleBaseline by 1.5 AP (70.2 to 71.7) and 1.0 AP (71.9 to 72.9) within ResNet50-256×192 and ResNet152-256×192 configurations, respectively. For HRNet within W32-256×192 and W48-256×192 configurations, UDP obtains gains by 1.7 AP (73.5 to 75.2) and 1.4 AP (74.3 to 75.7), respectively. The HRNet-W48-384×288 equipped with UDP achieves 76.5 AP and sets a new state-of-the-art for human pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, there are three coordinate systems are involved: source image coordinate systems denoted as O s -X s Y s with subscript s corresponding to the source image with a size of (w s , h s ), network input coordinate systems denoted as O i -X i Y i with subscript i corresponding to the network input with a size of (w i , h i ), and network output coordinate systems denoted as O o -X o Y o with subscript o corresponding to the network output with a size of (w o , h o ).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Performance of proposed UDP on COCO val set. IPS used in bottom-up paradigm denotes the inference speed of Image Per Second. PPS used in top-down paradigm denotes the inference speed of Person Per Second. † means unreported results in the original paper and trained with official implementation by us.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input size</cell><cell>IPS/PPS</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell></row><row><cell></cell><cell></cell><cell cols="2">Bottom-up methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HigherHRNet [26]</cell><cell>HRNet-W32</cell><cell cols="2">512 × 512 0.8</cell><cell>64.4</cell><cell>-</cell><cell>-</cell><cell>57.1</cell><cell>75.6</cell><cell>-</cell></row><row><cell>+UDP</cell><cell>HRNet-W32</cell><cell cols="3">512 × 512 4.9 (×6.1) 67.0 (+2.6)</cell><cell>86.2</cell><cell>72.0</cell><cell>60.7</cell><cell>76.7</cell><cell>71.6</cell></row><row><cell>HigherHRNet [26]</cell><cell cols="3">HigherHRNet-W32 512 × 512 1.1</cell><cell>67.1</cell><cell>86.2</cell><cell>73.0</cell><cell>61.5</cell><cell>76.1</cell><cell>718</cell></row><row><cell>+UDP</cell><cell cols="4">HigherHRNet-W32 512 × 512 2.9 (×2.6) 67.8 (+0.7)</cell><cell>86.2</cell><cell>72.9</cell><cell>62.2</cell><cell>76.4</cell><cell>72.4</cell></row><row><cell>HigherHRNet [26] †</cell><cell>HRNet-W48</cell><cell cols="2">640 × 640 0.6</cell><cell>67.9</cell><cell>86.7</cell><cell>74.4</cell><cell>62.5</cell><cell>76.2</cell><cell>73.0</cell></row><row><cell>+UDP</cell><cell>HRNet-W48</cell><cell cols="3">640 × 640 4.1 (×6.8) 68.9 (+1.0)</cell><cell>87.3</cell><cell>74.9</cell><cell>64.1</cell><cell>76.1</cell><cell>73.5</cell></row><row><cell>HigherHRNet [26]</cell><cell cols="3">HigherHRNet-W48 640 × 640 0.75</cell><cell>69.9</cell><cell>87.2</cell><cell>76.1</cell><cell>65.4</cell><cell>76.4</cell><cell>-</cell></row><row><cell>+UDP</cell><cell cols="4">HigherHRNet-W48 640 × 640 2.7 (×3.6) 69.9</cell><cell>87.3</cell><cell>76.2</cell><cell>65.9</cell><cell>76.2</cell><cell>74.4</cell></row><row><cell></cell><cell cols="6">Bottom-up methods with multi-scale ([×2,×1,×0.5]) test as in HigherHRNet [26]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UDP</cell><cell>HRNet-W32</cell><cell cols="2">512 × 512 -</cell><cell>70.4</cell><cell>88.2</cell><cell>75.8</cell><cell>65.3</cell><cell>77.6</cell><cell>74.7</cell></row><row><cell>HigherHRNet [26]</cell><cell cols="3">HigherHRNet-W32 512 × 512 -</cell><cell>69.9</cell><cell>87.1</cell><cell>76.0</cell><cell>65.3</cell><cell>77.0</cell><cell>-</cell></row><row><cell>+UDP</cell><cell cols="3">HigherHRNet-W32 512 × 512 -</cell><cell>70.2 (+0.3)</cell><cell>88.1</cell><cell>76.2</cell><cell>65.4</cell><cell>77.4</cell><cell>74.5</cell></row><row><cell>HigherHRNet [26] †</cell><cell>HRNet-W48</cell><cell cols="2">640 × 640 -</cell><cell>71.6</cell><cell>88.6</cell><cell>77.9</cell><cell>67.5</cell><cell>77.8</cell><cell>76.3</cell></row><row><cell>+UDP</cell><cell>HRNet-W48</cell><cell cols="2">640 × 640 -</cell><cell>71.3 (-0.3)</cell><cell>89.0</cell><cell>77.1</cell><cell>66.9</cell><cell>77.7</cell><cell>75.7</cell></row><row><cell>HigherHRNet [26]</cell><cell cols="3">HigherHRNet-W48 640 × 640 -</cell><cell>72.1</cell><cell>88.4</cell><cell>78.2</cell><cell>67.8</cell><cell>78.3</cell><cell>-</cell></row><row><cell>+UDP</cell><cell cols="3">HigherHRNet-W48 640 × 640 -</cell><cell>71.5 (-0.6)</cell><cell>88.3</cell><cell>77.3</cell><cell>67.9</cell><cell>77.2</cell><cell>75.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Top-down methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hourglass [40]</cell><cell>Hourglass</cell><cell cols="2">256 × 192 -</cell><cell>66.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CPN [27]</cell><cell>ResNet-50</cell><cell cols="2">256 × 192 -</cell><cell>69.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CPN [27]</cell><cell>ResNet-50</cell><cell cols="2">384 × 288 -</cell><cell>71.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MSPN [28]</cell><cell>MSPN</cell><cell cols="2">256 × 192 -</cell><cell>75.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SimpleBaseline [24] ResNet-50</cell><cell cols="2">256 × 192 23.0</cell><cell>71.3</cell><cell>89.9</cell><cell>78.9</cell><cell>68.3</cell><cell>77.4</cell><cell>76.9</cell></row><row><cell>+UDP</cell><cell>ResNet-50</cell><cell cols="2">256 × 192 23.0</cell><cell>72.9(+1.6)</cell><cell>90.0</cell><cell>80.2</cell><cell>69.7</cell><cell>79.3</cell><cell>78.2</cell></row><row><cell cols="2">SimpleBaseline [24] ResNet-152</cell><cell cols="2">256 × 192 11.5</cell><cell>72.9</cell><cell>90.6</cell><cell>80.8</cell><cell>69.9</cell><cell>79.0</cell><cell>78.3</cell></row><row><cell>+UDP</cell><cell>ResNet-152</cell><cell cols="2">256 × 192 11.5</cell><cell>74.3(+1.4)</cell><cell>90.9</cell><cell>81.6</cell><cell>71.2</cell><cell>80.6</cell><cell>79.6</cell></row><row><cell cols="2">SimpleBaseline [24] ResNet-50</cell><cell cols="2">384 × 288 20.3</cell><cell>73.2</cell><cell>90.7</cell><cell>79.9</cell><cell>69.4</cell><cell>80.1</cell><cell>78.2</cell></row><row><cell>+UDP</cell><cell>ResNet-50</cell><cell cols="2">384 × 288 20.3</cell><cell>74.0(+0.8)</cell><cell>90.3</cell><cell>80.0</cell><cell>70.2</cell><cell>81.0</cell><cell>79.0</cell></row><row><cell cols="2">SimpleBaseline [24] ResNet-152</cell><cell cols="2">384 × 288 11.1</cell><cell>75.3</cell><cell>91.0</cell><cell>82.3</cell><cell>71.9</cell><cell>82.0</cell><cell>80.4</cell></row><row><cell>+UDP</cell><cell>ResNet-152</cell><cell cols="2">384 × 288 11.1</cell><cell>76.2(+0.9)</cell><cell>90.8</cell><cell>83.0</cell><cell>72.8</cell><cell>82.9</cell><cell>81.2</cell></row><row><cell>HRNet [25]</cell><cell>HRNet-W32</cell><cell cols="2">256 × 192 6.9</cell><cell>75.6</cell><cell>91.9</cell><cell>83.0</cell><cell>72.2</cell><cell>81.6</cell><cell>80.5</cell></row><row><cell>+UDP</cell><cell>HRNet-W32</cell><cell cols="2">256 × 192 6.9</cell><cell>76.8(+1.2)</cell><cell>91.9</cell><cell>83.7</cell><cell>73.1</cell><cell>83.3</cell><cell>81.6</cell></row><row><cell>HRNet [25]</cell><cell>HRNet-W48</cell><cell cols="2">256 × 192 6.3</cell><cell>75.9</cell><cell>91.9</cell><cell>83.5</cell><cell>72.6</cell><cell>82.1</cell><cell>80.9</cell></row><row><cell>+UDP</cell><cell>HRNet-W48</cell><cell cols="2">256 × 192 6.3</cell><cell>77.2(+1.3)</cell><cell>91.8</cell><cell>83.7</cell><cell>73.8</cell><cell>83.7</cell><cell>82.0</cell></row><row><cell>HRNet [25]</cell><cell>HRNet-W32</cell><cell cols="2">384 × 288 6.2</cell><cell>76.7</cell><cell>91.9</cell><cell>83.6</cell><cell>73.2</cell><cell>83.2</cell><cell>81.6</cell></row><row><cell>+UDP</cell><cell>HRNet-W32</cell><cell cols="2">384 × 288 6.2</cell><cell>77.8(+1.1)</cell><cell>91.7</cell><cell>84.5</cell><cell>74.2</cell><cell>84.3</cell><cell>82.4</cell></row><row><cell>HRNet [25]</cell><cell>HRNet-W48</cell><cell cols="2">384 × 288 5.3</cell><cell>77.1</cell><cell>91.8</cell><cell>83.8</cell><cell>73.5</cell><cell>83.5</cell><cell>81.8</cell></row><row><cell>+UDP</cell><cell>HRNet-W48</cell><cell cols="2">384 × 288 5.3</cell><cell>77.8(+0.7)</cell><cell>92.0</cell><cell>84.3</cell><cell>74.2</cell><cell>84.5</cell><cell>82.5</cell></row><row><cell cols="4">3.2.4 Join Analysis of Biased Coordinate System Transfor-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">mation and Biased Keypoint Format Transformation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>The improvement of AP on COCO test-dev set when the proposed UDP is applied to state-of-the-art methods. † means unreported results in the original paper and trained with official implementation by us. Experimental settings and the corresponding performance on COCO val set are listed inTable 4.When FT is absent, configuration A and B have similar performances (74.5 AP and 74.4 AP) which is guaranteed by the establish of Equation<ref type="bibr" target="#b16">16</ref> and Equation 20</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input size</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell></row><row><cell></cell><cell></cell><cell>Bottom-up methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AE [49]</cell><cell>Hourglass [40]</cell><cell>512 × 512</cell><cell>56.6</cell><cell>81.8</cell><cell>61.8</cell><cell>49.8</cell><cell>67.0</cell><cell>-</cell></row><row><cell>G-RMI [47]</cell><cell>ResNet-101</cell><cell>353 × 257</cell><cell>64.9</cell><cell>85.5</cell><cell>71.3</cell><cell>62.3</cell><cell>70.0</cell><cell>69.7</cell></row><row><cell>PersonLab [51]</cell><cell>ResNet-152</cell><cell cols="2">1401 × 1401 66.5</cell><cell>88.0</cell><cell>72.6</cell><cell>62.4</cell><cell>72.3</cell><cell>-</cell></row><row><cell>PifPaf [61]</cell><cell>-</cell><cell>-</cell><cell>66.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HigherHRNet [26]</cell><cell>HRNet-W32</cell><cell>512 × 512</cell><cell>64.1</cell><cell>86.3</cell><cell>70.4</cell><cell>57.4</cell><cell>73.9</cell><cell>-</cell></row><row><cell>+UDP</cell><cell>HRNet-W32</cell><cell>512 × 512</cell><cell>66.8 (+2.7)</cell><cell>88.2</cell><cell>73.0</cell><cell>61.1</cell><cell>75.0</cell><cell>71.5</cell></row><row><cell>HigherHRNet [26]</cell><cell>HigherHRNet-W32</cell><cell>512 × 512</cell><cell>66.4</cell><cell>87.5</cell><cell>72.8</cell><cell>61.2</cell><cell>74.2</cell><cell>-</cell></row><row><cell>+UDP</cell><cell>HigherHRNet-W32</cell><cell>512 × 512</cell><cell>67.2 (+0.8)</cell><cell>88.1</cell><cell>73.6</cell><cell>62.0</cell><cell>74.3</cell><cell>72.0</cell></row><row><cell>HigherHRNet [26] †</cell><cell>HRNet-W48</cell><cell>640 × 640</cell><cell>67.4</cell><cell>88.6</cell><cell>74.2</cell><cell>62.6</cell><cell>74.3</cell><cell>72.8</cell></row><row><cell>+UDP</cell><cell>HRNet-W48</cell><cell>640 × 640</cell><cell>68.1 (+0.2)</cell><cell>88.3</cell><cell>74.6</cell><cell>63.9</cell><cell>74.1</cell><cell>73.1</cell></row><row><cell>HigherHRNet [26]</cell><cell>HigherHRNet-W48</cell><cell>640 × 640</cell><cell>68.4</cell><cell>88.2</cell><cell>75.1</cell><cell>64.4</cell><cell>74.2</cell><cell>-</cell></row><row><cell>+UDP</cell><cell>HigherHRNet-W48</cell><cell>640 × 640</cell><cell>68.6 (+0.2)</cell><cell>88.2</cell><cell>75.5</cell><cell>65.0</cell><cell>74.0</cell><cell>73.5</cell></row><row><cell></cell><cell cols="5">Bottom-up methods with multi-scale ([×2,×1,×0.5]) test as in HigherHRNet [26]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UDP</cell><cell>HRNet-W32</cell><cell>512 × 512</cell><cell>69.3</cell><cell>89.2</cell><cell>76.0</cell><cell>64.8</cell><cell>76.0</cell><cell>74.1</cell></row><row><cell>HigherHRNet [26] †</cell><cell>HRNet-W32</cell><cell>512 × 512</cell><cell>68.8</cell><cell>88.8</cell><cell>75.7</cell><cell>64.4</cell><cell>75.0</cell><cell>73.5</cell></row><row><cell>UDP</cell><cell>HigherHRNet-W32</cell><cell>512 × 512</cell><cell>69.1</cell><cell>89.1</cell><cell>75.8</cell><cell>64.4</cell><cell>75.5</cell><cell>73.8</cell></row><row><cell>HigherHRNet [26] †</cell><cell>HRNet-W48</cell><cell>640 × 640</cell><cell>70.4</cell><cell>89.7</cell><cell>77.4</cell><cell>66.4</cell><cell>75.7</cell><cell>75.2</cell></row><row><cell>+UDP</cell><cell>HRNet-W48</cell><cell>640 × 640</cell><cell>70.3</cell><cell>90.1</cell><cell>76.7</cell><cell>66.6</cell><cell>75.3</cell><cell>75.1</cell></row><row><cell>HigherHRNet [26]</cell><cell>HigherHRNet-W48</cell><cell>640 × 640</cell><cell>70.5</cell><cell>89.3</cell><cell>77.2</cell><cell>66.6</cell><cell>75.8</cell><cell>-</cell></row><row><cell>+UDP</cell><cell>HigherHRNet-W48</cell><cell>640 × 640</cell><cell>70.5</cell><cell>89.4</cell><cell>77.0</cell><cell>66.8</cell><cell>75.4</cell><cell>75.1</cell></row><row><cell></cell><cell></cell><cell>Top-down methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask-RCNN [53]</cell><cell>ResNet-50-FPN [62]</cell><cell>-</cell><cell>63.1</cell><cell>87.3</cell><cell>68.7</cell><cell>57.8</cell><cell>71.4</cell><cell>-</cell></row><row><cell cols="2">Integral Pose Regression [63] ResNet-101 [64]</cell><cell>256 × 256</cell><cell>67.8</cell><cell>88.2</cell><cell>74.8</cell><cell>63.9</cell><cell>74.0</cell><cell>-</cell></row><row><cell>SCN [56]</cell><cell>Hourglass [40]</cell><cell>-</cell><cell>70.5</cell><cell>88.0</cell><cell>76.9</cell><cell>66.0</cell><cell>77.0</cell><cell>-</cell></row><row><cell>CPN [27]</cell><cell>ResNet-Inception</cell><cell>384 × 288</cell><cell>72.1</cell><cell>91.4</cell><cell>80.0</cell><cell>68.7</cell><cell>77.2</cell><cell>78.5</cell></row><row><cell>RMPE [65]</cell><cell>PyraNet [66]</cell><cell>320 × 256</cell><cell>72.3</cell><cell>89.2</cell><cell>79.1</cell><cell>68.0</cell><cell>78.6</cell><cell>-</cell></row><row><cell>CFN [67]</cell><cell>-</cell><cell>-</cell><cell>72.6</cell><cell>86.1</cell><cell>69.7</cell><cell>78.3</cell><cell>64.1</cell><cell>-</cell></row><row><cell>CPN(ensemble) [27]</cell><cell>ResNet-Inception</cell><cell>384 × 288</cell><cell>73.0</cell><cell>91.7</cell><cell>80.9</cell><cell>69.5</cell><cell>78.1</cell><cell>79.0</cell></row><row><cell>Posefix [68]</cell><cell>ResNet-152</cell><cell>384 × 288</cell><cell>73.6</cell><cell>90.8</cell><cell>81.0</cell><cell>70.3</cell><cell>79.8</cell><cell>79.0</cell></row><row><cell>CSANet [69]</cell><cell>ResNet-152</cell><cell>384 × 288</cell><cell>74.5</cell><cell>91.7</cell><cell>82.1</cell><cell>71.2</cell><cell>80.2</cell><cell>80.7</cell></row><row><cell>MSPN [28]</cell><cell>MSPN [28]</cell><cell>384 × 288</cell><cell>76.1</cell><cell>93.4</cell><cell>83.8</cell><cell>72.3</cell><cell>81.5</cell><cell>81.6</cell></row><row><cell>SimpleBaseline [27]</cell><cell>ResNet-50</cell><cell>256 × 192</cell><cell>70.2</cell><cell>90.9</cell><cell>78.3</cell><cell>67.1</cell><cell>75.9</cell><cell>75.8</cell></row><row><cell>+UDP</cell><cell>ResNet-50</cell><cell>256 × 192</cell><cell>71.7 (+1.5)</cell><cell>91.1</cell><cell>79.6</cell><cell>68.6</cell><cell>77.5</cell><cell>77.2</cell></row><row><cell>SimpleBaseline [27]</cell><cell>ResNet-50</cell><cell>384 × 288</cell><cell>71.3</cell><cell>91.0</cell><cell>78.5</cell><cell>67.3</cell><cell>77.9</cell><cell>76.6</cell></row><row><cell>+UDP</cell><cell>ResNet-50</cell><cell>384 × 288</cell><cell>72.5 (+1.2)</cell><cell>91.1</cell><cell>79.7</cell><cell>68.8</cell><cell>79.1</cell><cell>77.9</cell></row><row><cell>SimpleBaseline [27]</cell><cell>ResNet-152</cell><cell>256 × 192</cell><cell>71.9</cell><cell>91.4</cell><cell>80.1</cell><cell>68.9</cell><cell>77.4</cell><cell>77.5</cell></row><row><cell>+UDP</cell><cell>ResNet-152</cell><cell>256 × 192</cell><cell>72.9 (+1.0)</cell><cell>91.6</cell><cell>80.9</cell><cell>70.0</cell><cell>78.5</cell><cell>78.4</cell></row><row><cell>SimpleBaseline [27]</cell><cell>ResNet-152</cell><cell>384 × 288</cell><cell>73.8</cell><cell>91.7</cell><cell>81.2</cell><cell>70.3</cell><cell>80.0</cell><cell>79.1</cell></row><row><cell>+UDP</cell><cell>ResNet-152</cell><cell>384 × 288</cell><cell>74.7 (+0.9)</cell><cell>91.8</cell><cell>82.1</cell><cell>71.5</cell><cell>80.8</cell><cell>80.0</cell></row><row><cell>HRNet [25]</cell><cell>HRNet-W32</cell><cell>256 × 192</cell><cell>73.5</cell><cell>92.2</cell><cell>82.0</cell><cell>70.4</cell><cell>79.0</cell><cell>79.0</cell></row><row><cell>+UDP</cell><cell>HRNet-W32</cell><cell>256 × 192</cell><cell>75.2 (+1.7)</cell><cell>92.4</cell><cell>82.9</cell><cell>72.0</cell><cell>80.8</cell><cell>80.4</cell></row><row><cell>HRNet [25]</cell><cell>HRNet-W32</cell><cell>384 × 288</cell><cell>74.9</cell><cell>92.5</cell><cell>82.8</cell><cell>71.3</cell><cell>80.9</cell><cell>80.1</cell></row><row><cell>+UDP</cell><cell>HRNet-W32</cell><cell>384 × 288</cell><cell>76.1 (+1.2)</cell><cell>92.5</cell><cell>83.5</cell><cell>72.8</cell><cell>82.0</cell><cell>81.3</cell></row><row><cell>HRNet [25]</cell><cell>HRNet-W48</cell><cell>256 × 192</cell><cell>74.3</cell><cell>92.4</cell><cell>82.6</cell><cell>71.2</cell><cell>79.6</cell><cell>79.7</cell></row><row><cell>+UDP</cell><cell>HRNet-W48</cell><cell>256 × 192</cell><cell>75.7 (+1.4)</cell><cell>92.4</cell><cell>83.3</cell><cell>72.5</cell><cell>81.4</cell><cell>80.9</cell></row><row><cell>HRNet [25]</cell><cell>HRNet-W48</cell><cell>384 × 288</cell><cell>75.5</cell><cell>92.5</cell><cell>83.3</cell><cell>71.9</cell><cell>81.5</cell><cell>80.5</cell></row><row><cell>+UDP</cell><cell>HRNet-W48</cell><cell>384 × 288</cell><cell>76.5 (+1.0)</cell><cell>92.7</cell><cell>84.0</cell><cell>73.0</cell><cell>82.4</cell><cell>81.6</cell></row><row><cell cols="3">the data processing pipeline. Techniques we study here includes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Unbiased Coordinate System Transformation (UCST), Flipping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Testing (FT), Shift the Network Output by One Pixel (SNOOP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">used in some state-of-the-arts [24], [25], [29], Extra Compensation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(EC) proposed in Section 3.1.6 for the residual error left by using</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SNOOP, Unbiased Keypoint Format Transformation in Combined</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Classification and Regression Form (UKFT-CCRF), Unbiased</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Keypoint Format Transformation in Classification Form (UKFT-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CF).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>The improvement of AP on CrowdPose test set when UDP is applied. † means unreported results in the original paper and trained with official implementation by us.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input size</cell><cell>IPS</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP E</cell><cell>AR M</cell><cell>AR H</cell></row><row><cell>SPPE [30]</cell><cell>ResNet-101</cell><cell cols="2">320 × 240 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Ablation study in top-down paradigm on COCO val set. UCST denotes Unbiased Coordinate System Transformation. SNOOP denotes Shift the Network Output by One Pixel. EC denotes Extra Compensation. UKFTCCRF denotes Unbiased Keypoint Format Transformation in Combined Classification and Regression Form, UKFTCF denotes Unbiased Keypoint Format Transformation in Classification Form.</figDesc><table><row><cell>ID FT UCST SNOOP EC UKFTCCRF UKFTCF</cell><cell>AP</cell></row><row><cell>A</cell><cell>74.5</cell></row><row><cell>B</cell><cell>74.4</cell></row><row><cell>C</cell><cell>73.3</cell></row><row><cell>D</cell><cell>75.7</cell></row><row><cell>E</cell><cell>75.6</cell></row><row><cell>F</cell><cell>75.8</cell></row><row><cell>G</cell><cell>74.5</cell></row><row><cell>H</cell><cell>76.8</cell></row><row><cell>I</cell><cell>76.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Ablation study of techniques in bottom-up paradigm on COCO val set. HNOR denotes Higher Network Output Resolution, UCST denotes Unbiased Coordinate System Transformation, UKFT-CF denotes Unbiased Keypoint Format Transformation in Classification Form and RNO denotes Resize the Network Output.</figDesc><table><row><cell cols="2">ID HNOR UCST UKFT-CF RNO IPS</cell><cell>AP</cell></row><row><cell>A</cell><cell>0.8</cell><cell>64.4</cell></row><row><cell>B</cell><cell>4.9</cell><cell>65.9</cell></row><row><cell>C</cell><cell>4.9</cell><cell>67.0</cell></row><row><cell>D</cell><cell>0.8</cell><cell>66.1</cell></row><row><cell>E</cell><cell>2.9</cell><cell>66.9</cell></row><row><cell>F</cell><cell>1.1</cell><cell>67.1</cell></row><row><cell>G</cell><cell>1.1</cell><cell>67.1</cell></row><row><cell>H</cell><cell>2.9</cell><cell>67.3</cell></row><row><cell>I</cell><cell>2.9</cell><cell>67.8</cell></row><row><cell>J</cell><cell>1.1</cell><cell>67.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Bottom-up methods with multi-scale</title>
		<imprint/>
	</monogr>
	<note>×2,×1,×0.5]) test as in HigherHRNet [26</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Locally connected network for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-task deep learning for real-time 3d human pose estimation and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d human pose machines with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1069" to="1082" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust 3d human pose estimation from single images or video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1227" to="1241" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="901" to="914" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Look into person: Joint body parsing &amp; pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="871" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attribute and-or grammar for joint parsing of human pose, parts and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1555" to="1569" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision-based estimation of mds-updrs gait scores for assessing parkinson&apos;s disease motor severity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Poston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pfefferbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2020</title>
		<editor>A. L. Martel, P. Abolmaesumi, D. Stoyanov, D. Mateus, M. A. Zuluaga, S. K. Zhou, D. Racoceanu, and L. Joskowicz</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="637" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fall detection based on key points of human-skeleton using openpose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">744</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Patient-specific pose estimation in clinical environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alasfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Devinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Melloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of translational engineering in health and medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">State-aware reidentification feature for multi-target multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting offset-guided network for pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detectand-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action machine: Toward person-centric action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1633" to="1637" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional relation network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking on multi-stage networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5700" to="5709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint coco and lvis workshop at eccv 2020: Coco keypoint challenge track technical report: Udp++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Whole-body human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="196" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local jointto-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Associative embedding: End-toend learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Human pose estimation for real-world crowded scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Golda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Cascade feature aggregation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07837</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adversarial learning of structure-aware fully convolutional networks for landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1654" to="1669" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with enhanced channel-wise and spatial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5674" to="5682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3028" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7773" to="7781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">A context-and-spatial aware network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
