<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In Defense of the Triplet Loss for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Visual Computing Institute RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Visual Computing Institute RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Visual Computing Institute RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">In Defense of the Triplet Loss for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person reidentification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, person re-identification (ReID) has attracted significant attention in the computer vision community. Especially with the rise of deep learning, many new approaches have been proposed to achieve this task <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b34">35]</ref>  <ref type="bibr" target="#b0">1</ref> . In many aspects person ReID is similar to image retrieval, where significant progress has been made and where deep learning has recently introduced a lot of changes. One prominent example in the recent literature is FaceNet <ref type="bibr" target="#b28">[29]</ref>, a convolutional neural network (CNN) used to learn an embedding for faces.</p><p>The key component of FaceNet is to use the triplet loss, as introduced by Weinberger and Saul <ref type="bibr" target="#b40">[41]</ref>, for training the CNN as an embedding function. The triplet loss optimizes the embedding space such that data points with the same identity are closer to each other than those with different identities. A visualization of such an embedding is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Several approaches for person ReID have already used some variant of the triplet loss to train their models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref>, with moderate success. The * Equal contribution. Ordering determined by a last minute coin flip. <ref type="bibr" target="#b0">1</ref> A nice overview of the field is given by a recent survey paper <ref type="bibr" target="#b50">[51]</ref>. recently most successful person ReID approaches argue that a classification loss, possibly combined with a verification loss, is superior for the task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b21">22]</ref>. Typically, these approaches train a deep CNN using one or multiple of these surrogate losses and subsequently use a part of the network as a feature extractor, combining it with a metric learning approach to generate final embeddings. Both of these losses have their problems, though. The classification loss necessitates a growing number of learnable parameters as the number of identities increases, most of which will be discarded after training. On the other hand, many of the networks trained with a verification loss have to be used in a cross-image representation mode, only answering the question "How similar are these two images?". This makes using them for any other task, such as clustering or retrieval, prohibitively expensive, as each probe has to go through the network paired up with every gallery image. In this paper we show that, contrary to current opinion, a plain CNN with a triplet loss can outperform current state-of-the-art approaches on the CUHK03 <ref type="bibr" target="#b20">[21]</ref>, Market-1501 <ref type="bibr" target="#b49">[50]</ref> and MARS <ref type="bibr" target="#b48">[49]</ref> datasets. The triplet loss allows us to perform end-to-end learning between the input image and the desired embedding space. This means we directly optimize the network for the final task, which renders an additional metric learning step obsolete. Instead, we can simply compare persons by computing the Euclidean distance of their embeddings.</p><p>A possible reason for the unpopularity of the triplet loss is that, when applied na√Øvely, it will indeed often produce disappointing results. An essential part of learning using the triplet loss is the mining of hard triplets, as otherwise training will quickly stagnate. However, mining such hard triplets is time consuming and it is unclear what defines "good" hard triplets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>. Even worse, selecting too hard triplets too often makes the training unstable. We show how this problem can be alleviated, resulting in both faster training and better performance. We systematically analyze the design space of triplet losses, and evaluate which one works best for person ReID. While doing so, we place two previously proposed variants <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref> into this design space and discuss them in more detail in Section 2. Specifically, we find that the best performing version has not been used before. Furthermore we also show that a margin-less formulation performs slightly better, while removing one hyperparameter.</p><p>Another clear trend seems to be the use of pretrained models such as GoogleNet <ref type="bibr" target="#b35">[36]</ref> or ResNet-50 <ref type="bibr" target="#b13">[14]</ref>. Indeed, pretrained models often obtain great scores for person ReID <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">52]</ref>, while ever fewer top-performing approaches use networks trained from scratch <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b3">4]</ref>. Some authors even argue that training from scratch is bad <ref type="bibr" target="#b9">[10]</ref>. However, using pretrained networks also leads to a design lock-in, and does not allow for the exploration of new deep learning advances or different architectures. We show that, when following best practices in deep learning, networks trained from scratch can perform competitively for person ReID. Furthermore, we do not rely on network components specifically tailored towards person ReID, but train a plain feed-forward CNN, unlike many other approaches that train from scratch <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b47">48]</ref>. Indeed, our networks using pretrained weights obtain the best results, but our far smaller architecture obtains respectable scores, providing a viable alternative for applications where person ReID needs to be performed on resource-constrained hardware, such as embedded devices.</p><p>In summary our contribution is twofold: Firstly we introduce variants of the classic triplet loss which render mining of hard triplets unnecessary and we systematically evaluate these variants. And secondly, we show how, contrary to the prevailing opinion, using a triplet loss and no special layers, we achieve state-of-the-art results both with a pretrained CNN and with a model trained from scratch. This highlights that a well designed triplet loss has a significant impact on the result, on par with other architectural novelties, hopefully enabling other researchers to gain the full potential of the previously often dismissed triplet loss. This is an important result, highlighting that a well designed triplet loss has a significant impact on model performance-on par with other architectural novelties-hopefully enabling other researchers to gain the full potential of the previously often dismissed triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning Metric Embeddings, the Triplet</head><p>Loss, and the Importance of Mining</p><p>The goal of metric embedding learning is to learn a function f Œ∏ (x) : R F ‚Üí R D which maps semantically similar points from the data manifold in R F onto metrically close points in R D . Analogously, f Œ∏ should map semantically different points in R F onto metrically distant points in R D . The function f Œ∏ is parametrized by Œ∏ and can be anything ranging from a linear transform <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b27">28]</ref> to complex non-linear mappings usually represented by deep neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. Let D(x, y) : R D √ó R D ‚Üí R be a metric function measuring distances in the embedding space. For clarity we use the shortcut notation D i,j = D(f Œ∏ (x i ), f Œ∏ (x j )), where we omit the indirect dependence of D i,j on the parameters Œ∏. As is common practice, all loss-terms are divided by the number of summands in a batch; we omit this term in the following equations for conciseness.</p><p>Weinberger and Saul <ref type="bibr" target="#b40">[41]</ref> explore this topic with the explicit goal of performing k-nearest neighbor classification in the learned embedding space and propose the "Large Margin Nearest Neighbor loss" for optimizing f Œ∏ :</p><formula xml:id="formula_0">L LMNN (Œ∏) = (1 ‚àí ¬µ)L pull (Œ∏) + ¬µL push (Œ∏),<label>(1)</label></formula><p>which is comprised of a pull-term, pulling data points i towards their target neighbor T (i) from the same class, and a push-term, pushing data points from a different class k further away:</p><formula xml:id="formula_1">L pull (Œ∏) = i,j‚ààT (i) D i,j ,<label>(2)</label></formula><formula xml:id="formula_2">L push (Œ∏) = a,n ya =yn m + D a,T (a) ‚àí D a,n + .<label>(3)</label></formula><p>Because the motivation was nearest-neighbor classification, allowing disparate clusters of the same class was an explicit goal, achieved by choosing fixed target neighbors at the onset of training. Since this property is harmful for retrieval tasks such as face and person ReID, FaceNet <ref type="bibr" target="#b28">[29]</ref> proposed a modification of L LMNN (Œ∏) called the "Triplet loss":</p><formula xml:id="formula_3">L tri (Œ∏) = a,p,n ya=yp =yn [m + D a,p ‚àí D a,n ] + .<label>(4)</label></formula><p>This loss makes sure that, given an anchor point x a , the projection of a positive point x p belonging to the same class (person) y a is closer to the anchor's projection than that of a negative point belonging to another class y n , by at least a margin m. If this loss is optimized over the whole dataset for long enough, eventually all possible pairs (x a , x p ) will be seen and be pulled together, making the pull-term redundant. The advantage of this formulation is that, while eventually all points of the same class will form a single cluster, they are not required to collapse to a single point; they merely need to be closer to each other than to any point from a different class. A major caveat of the triplet loss, though, is that as the dataset gets larger, the possible number of triplets grows cubically, rendering a long enough training impractical. To make matters worse, f Œ∏ relatively quickly learns to correctly map most trivial triplets, rendering a large fraction of all triplets uninformative. Thus mining hard triplets becomes crucial for learning. Intuitively, being told over and over again that people with differently colored clothes are different persons does not teach one anything, whereas seeing similarly-looking but different people (hard negatives), or pictures of the same person in wildly different poses (hard positives) dramatically helps understanding the concept of "same person". On the other hand, being shown only the hardest triplets would select outliers in the data unproportionally often and make f Œ∏ unable to learn "normal" associations, as will be shown in <ref type="table">Table 1</ref>. Examples of typical hard positives, hard negatives, and outliers are shown in the Supplementary Material. Hence it is common to only mine moderate negatives <ref type="bibr" target="#b28">[29]</ref> and/or moderate positives <ref type="bibr" target="#b30">[31]</ref>. Regardless of which type of mining is being done, it is a separate step from training and adds considerable overhead, as it requires embedding a large fraction of the data with the most recent f Œ∏ and computing all pairwise distances between those data points.</p><p>In a classical implementation, once a certain set of B triplets has been chosen, their images are stacked into a batch of size 3B, for which the 3B embeddings are computed, which are in turn used to create B terms contributing to the loss. Given the fact that there are up to 6B 2 ‚àí 4B possible combinations of these 3B images that are valid triplets, using only B of them seems wasteful. With this realization, we propose an organizational modification to the classic way of using the triplet loss: the core idea is to form batches by randomly sampling P classes (person identities), and then randomly sampling K images of each class (person), thus resulting in a batch of P K images. <ref type="bibr" target="#b1">2</ref> Now, for each sample a in the batch, we can select the hardest positive and the hardest negative samples within the batch when forming the triplets for computing the loss, which we call Batch Hard:</p><formula xml:id="formula_4">L BH (Œ∏; X) = all anchors P i=1 K a=1 m + hardest positive max p=1...K D f Œ∏ (x i a ), f Œ∏ (x i p ) (5) ‚àí min j=1...P n=1...K j =i D f Œ∏ (x i a ), f Œ∏ (x j n ) hardest negative + ,</formula><p>which is defined for a mini-batch X and where a data point x i j corresponds to the j-th image of the i-th person in the batch.</p><p>This results in P K terms contributing to the loss, a threefold 3 increase over the traditional formulation. Additionally, the selected triplets can be considered moderate triplets, since they are the hardest within a small subset of the data, which is exactly what is best for learning with the triplet loss.</p><p>This new formulation of sampling a batch immediately suggests another alternative, that is to simply use all possible P K(P K ‚àí K)(K ‚àí 1) combinations of triplets, which corresponds to the strategy chosen in <ref type="bibr" target="#b8">[9]</ref> and which we call Batch All: </p><formula xml:id="formula_5">L BA (Œ∏; X) =</formula><formula xml:id="formula_6">d i,a,p j,a,n = D f Œ∏ (x i a ), f Œ∏ (x i p ) ‚àí D f Œ∏ (x i a ), f Œ∏ (x j n ) .</formula><p>At this point, it is important to note that both L BH and L BA still exactly correspond to the standard triplet loss in the limit of infinite training. Both the max and min functions are continuous and differentiable almost everywhere, meaning they can be used in a model trained by stochastic (sub-)gradient descent without concern. In fact, they are already widely available in popular deep-learning frameworks for the implementation of max-pooling and the ReLU <ref type="bibr" target="#b10">[11]</ref> non-linearity. Most similar to our batch hard and batch all losses is the Lifted Embedding loss <ref type="bibr" target="#b31">[32]</ref>, which fills the batch with triplets but considers all but the anchor-positive pair as negatives: While <ref type="bibr" target="#b31">[32]</ref> motivates a "hard"-margin loss similar to L BH and L BA , they end up optimizing the smooth bound of it given in the above equation. Additionally, traditional 3B batches are considered, thus using all possible negatives, but only one positive pair per triplet. This leads us to propose a generalization of the Lifted Embedding loss based on P K batches which considers all anchor-positive pairs as follows:</p><formula xml:id="formula_7">L L (Œ∏; X) =</formula><formula xml:id="formula_8">L LG (Œ∏; X) = all anchors P i=1 K a=1 log all positives K p=1 p =a e D(f Œ∏ (x i a ),f Œ∏ (x i p )) (7) + log P j=1 j =i K n=1 e m‚àíD(f Œ∏ (x i a ),f Œ∏ (x j n )) all negatives + .</formula><p>Distance Measure. Throughout this section, we have referred to D(a, b) as the distance function between a and b in the embedding space. In most related works, the squared Euclidean distance</p><formula xml:id="formula_9">D (f Œ∏ (x i ), f Œ∏ (x j )) = f Œ∏ (x i ) ‚àí f Œ∏ (x j ) 2 2</formula><p>is used as metric, although nothing in the above loss definitions precludes using any other (sub-)differentiable distance measure. While we do not have a side-by-side comparison, we noticed during initial experiments that using the squared Euclidean distance made the optimization more prone to collapsing, whereas using the actual (non-squared) Euclidean distance was more stable. We hence used the Euclidean distance throughout all our experiments presented in this paper. In addition, squaring the Euclidean distance makes the margin parameter less interpretable, as it does not represent an absolute distance anymore.</p><p>Note that when forcing the embedding's norm to one, using the squared Euclidean distance corresponds to using the cosine-similarity, up to a constant factor of two. We did not use a normalizing layer in any of our final experiments. For one, it does not dramatically regularize the network by reducing the available embedding space: the space spanned by all D-dimensional vector of fixed norm is still a D ‚àí 1dimensional volume. Worse, an output-normalization layer can actually hide problems in the training, such as slowly collapsing or exploding embeddings.</p><p>Soft-margin. The role of the hinge function [m + ‚Ä¢] + is to avoid correcting "already correct" triplets. But in person ReID, it can be beneficial to pull together samples from the same class as much as possible <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b7">8]</ref>, especially when working on tracklets such as in MARS <ref type="bibr" target="#b48">[49]</ref>. For this purpose, it is possible to replace the hinge function by a smooth approximation using the softplus function: ln(1 + exp(‚Ä¢)), for which numerically stable implementations are commonly available as log1p. The softplus function has similar behavior to the hinge, but it decays exponentially instead of having a hard cut-off, we hence refer to it as the soft-margin formulation.</p><p>Summary. In summary, the novel contributions proposed in this paper are the batch hard loss and its soft margin version. In the following section we evaluate them experimentally and show that, for ReID, they achieve superior performance compared to both the traditional triplet loss and the previously published variants of it <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Our experimental evaluation is split up into three main parts. The first section evaluates different variations of the triplet loss, including some hyper-parameters, and identifies the setting that works best for person ReID. This evaluation is performed on a train/validation split we create based on the MARS training set. The second section shows the performance we can attain based on the selected variant of the triplet loss. We show state-of-the-art results on the CUHK03, Market-1501 and MARS test sets, based on a pretrained network and a network trained from scratch. Finally, the third section discusses advantages of training models from scratch with respect to real-world use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We focus on the Market-1501 <ref type="bibr" target="#b49">[50]</ref> and MARS <ref type="bibr" target="#b48">[49]</ref> datasets, the two largest person ReID datasets currently available. The Market-1501 dataset contains bounding boxes from a person detector which have been selected based on their intersection-over-union overlap with manually annotated bounding boxes. It contains 32 668 images of 1501 persons, split into train/test sets of 12 936/19 732 images as defined by <ref type="bibr" target="#b49">[50]</ref>. The dataset uses both singleand multi-query evaluation, we report numbers for both.</p><p>The MARS dataset originates from the same raw data as the Market-1501 dataset; however, a significant difference is that the MARS dataset does not have any manually annotated bounding boxes, reducing the annotation overhead. MARS consist of "tracklets" which have been grouped into person IDs manually. It contains 1 191 003 images split into train/test sets of 509 914/681 089 images, as defined by <ref type="bibr" target="#b48">[49]</ref>. Here, person ReID is no longer performed on a frame-to-frame level, but instead on a tracklet-to-tracklet level, where feature embeddings are pooled across a tracklet, thus it is inherently a multi-query setup.</p><p>We use the standard evaluation metrics for both datasets, namely the mean average precision score (mAP) and the cumulative matching curve (CMC) at rank-1 and rank-5. To compute these scores we use the evaluation code provided by <ref type="bibr" target="#b54">[55]</ref>.</p><p>Additionally, we show results on the CUHK03 [21] dataset for our pretrained network, using the single shot setup and average over the provided 20 train/test splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>Unless specifically noted otherwise, we use the same training procedure across all experiments and on all datasets. We performed all our experiments using the Theano <ref type="bibr" target="#b4">[5]</ref> framework, code is available at redacted.</p><p>We use the Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with the default hyperparameter values ( = 10 ‚àí3 , Œ≤ 1 = 0.9, Œ≤ 2 = 0.999) for most experiments. During initial experiments on our own MARS validation split (see Sec. 3.4), we ran multiple experiments for a very long time and monitored the loss and mAP curves. With this information, we decided to fix the following exponentially decaying training schedule, which does not disadvantage any setup, for all experiments presented in this paper:</p><formula xml:id="formula_10">(t) = 0 if t ‚â§ t 0 0 0.001 t‚àít 0 t 1 ‚àít 0 if t 0 ‚â§ t ‚â§ t 1<label>(8)</label></formula><p>with 0 = 10 ‚àí3 , t 0 = 15 000, and t 1 = 25 000, stopping training when reaching t 1 . We also set Œ≤ 1 = 0.5 when entering the decay schedule at t 0 , as is common practice <ref type="bibr" target="#b1">[2]</ref>.</p><p>In the Supplementary Material, we provide a detailed discussion of various interesting effects we regularly observed during training, providing hands-on guidance for other researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architectures</head><p>For our main results we use two different architectures, one based on a pretrained network and one which we train from scratch.</p><p>Pretrained. We use the ResNet-50 architecture and the weights provided by He et al. <ref type="bibr" target="#b13">[14]</ref>. We discard the last layer and add two fully connected layers for our task. The first has 1024 units, followed by batch normalization <ref type="bibr" target="#b15">[16]</ref> and ReLU <ref type="bibr" target="#b10">[11]</ref>, the second goes down to 128 units, our final embedding dimension. Trained with our batch hard triplet loss, we call this model TriNet. Due to the size of this network (25.74 M parameters), we had to limit our batch size to 72, containing P = 18 persons with K = 4 images each. For these pretrained experiments, 0 = 10 ‚àí3 proved to be too high, causing the models to diverge within few iterations. We thus reduced 0 to 3 ¬∑ 10 ‚àí4 which worked fine on all datasets.</p><p>Trained from Scratch. To show that training from scratch does not necessarily result in poor performance, we also designed a network called LuNet which we train from scratch. LuNet follows the style of ResNet-v2, but uses leaky ReLU nonlinearities, multiple 3 √ó 3 max-poolings with stride 2 instead of strided convolutions, and omits the final averagepooling of feature-maps in favor of a channel-reducing final res-block. An in-depth description of the architecture is given in the Supplementary Material. As the network is much more lightweight (5.00 M parameters) than its pretrained sibling, we sample batches of size 128, containing P = 32 persons with K = 4 images each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Triplet Loss</head><p>Our initial experiments test the different variants of triplet training that we discussed in Sec. 2. In order not to perform model-selection on the test set, we randomly sample a validation set of 150 persons from the MARS training set, leaving the remaining 475 persons for training. In order to make this exploration tractable, we run all of these experiments using the smaller LuNet trained from scratch on images downscaled by a factor of two. Since our goal here is to explore triplet loss formulations, as opposed to reaching top performance, we do not perform any data augmentation in these experiments. <ref type="table">Table 1</ref> shows the resulting mAP and rank-1 scores for the different formulations at multiple margin values, and with a soft-margin where applicable. Consistent with results reported in several recent papers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref>, the vanilla triplet loss with randomly sampled triplets performs poorly. When performing simple offline hard-mining (OHM), the scores sometimes increase dramatically, but the training also fails to learn useful embeddings for multiple margin values. This problem is well-known <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref> and has been discussed in Sec. 2. While the idea of learning embeddings using triplets is theoretically pleasing, this practical finnickyness, coupled with the considerable increase in training time due to non-parallelizable offline mining (from 7 h to 20 h in our experiments), makes learning with vanilla triplets rather unattractive.</p><p>Considering the long training times, it is nice to see that all proposed triplet re-formulations perform similarly to or better than the best OHM run. The key observation is that the (semi) hard-mining happens within the batch and thus comes at almost no additional runtime cost.</p><p>Perhaps surprisingly, the batch hard variant (Eq. 5) consistently outperforms the batch all variant (Eq. 6) previously used by several authors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>. We suspect this is due to the fact that in the latter, many of the possible triplets in a batch are zero, essentially "washing out" the few useful contributing terms during averaging. To test this hypothesis, we also ran experiments where we only average the non-zero loss terms (marked by = 0 in <ref type="table">Table 1</ref>  <ref type="table">Table 1</ref>: LuNet scores on our MARS validation split. The best performing loss at a given margin is bold, the best margin for a given loss is italic, and the overall best combination is highlighted in green. A * denotes runs trapped in a bad local optimum.</p><p>in the batch all case. Another interpretation of this modification is that it dynamically increases the weight of triplets which remain active as they get fewer. The lifted triplet loss L L as introduced by [32] performs competitively, but is slightly worse than most other formulations. As can be seen in the table, our generalization to multiple positives (Eq. 7), which makes it more similar to the batch all variant of the triplet, improves upon it overall.</p><p>The best score was obtained by the soft-margin variation of the batch hard loss. We use this loss in all our further triplet experiments. To clarify, here we merely seek the best triplet loss variation for person ReID, but do not claim that this variant works best across all fields. For other tasks such as image retrieval or clustering, additional experiments will have to be performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Performance Evaluation</head><p>Here, we present the main experiments of this paper. We perform all following experiments using the batch hard variant L BH of the triplet loss and the soft margin, since this setup performed best during the exploratory phase.</p><p>Batch Generation and Augmentation. Since our batch hard triplet loss requires slightly different mini-batches, we sample random P K-style batches by first randomly sampling P person identities uniformly without replacement. For each person, we then sample K images, without replacement whenever possible, otherwise replicating images as necessary.</p><p>We follow common practice by using random crops and random horizontal flips during training <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1]</ref>. Specifically, we resize all images of size H √ó W to 1 1 8 (H √ó W ), of which we take random crops of size H √ó W , keeping their aspect ratio intact. For all pretrained networks we set H = 256, W = 128 on Market-1501 and MARS and H = 256, W = 96 on CUHK03, whereas for the networks trained from scratch we set H = 128, W = 64.</p><p>We apply test-time augmentation in all our experiments. Following <ref type="bibr" target="#b18">[19]</ref>, we deterministically average over the em-beddings from five crops and their flips. This typically gives an improvement of 3% in the mAP score; a more detailed analysis can be found in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination of Embeddings.</head><p>For test-time augmentation, multi-query evaluation, and tracklet-based evaluation, the embeddings of multiple images need to be combined.</p><p>While the learned clusters have no reason to be Gaussian, their convex hull is trained to only contain positive points. Thus, a convex combination of multiple embeddings cannot get closer to a negative embedding than any of the original ones, which is not the case for a non-convex combination such as max-pooling. For this reason, we suggest combining triplet-based embeddings by using their mean. For example, combining tracklet-embeddings using max-pooling led to an 11.4% point decrease in mAP on MARS.</p><p>Comparison to State-of-the-Art. <ref type="table" target="#tab_2">Tables 2 and 3</ref> compare our results to a set of related, top performing approaches on Market-1501 and MARS, and CUHK03, respectively. We do not include approaches which are orthogonal to ours and could be integrated in a straightforward manner, such as various re-ranking schemes, data augmentation, and regularization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref>. These are included in more exhaustive tables in the Supplementary Material. The different approaches are categorized into three major types: Identification models (I) that are trained to classify person IDs, Verification models (V) that learn whether an image pair represents the same person, and methods such as ours that directly learn an Embedding (E).</p><p>We present results for both our pretrained network (TriNet) and the network we trained from scratch (LuNet). As can clearly be seen, TriNet outperforms all current methods. Especially striking is the jump from 41.5% mAP, obtained by another ResNet-50 model trained with triplets (DTL <ref type="bibr" target="#b9">[10]</ref>), to our 69.14% mAP score in <ref type="table" target="#tab_2">Table 2</ref>. Since Geng et al. <ref type="bibr" target="#b9">[10]</ref> do not discus all details of their training procedure when using the triplet loss, we could only speculate about the reasons for the large performance gap.  Our LuNet model, which we train from scratch, also performs very competitively, matching or outperforming most other baselines. While it does not quite reach the performance of our pretrained model, our results clearly show that with proper training, the flexibility of training models from scratch (see Sec. 3.6) should not be discarded.</p><p>To show that the actual performance boost is indeed gained by the triplet loss and not by other design choices, we train a ResNet-50 model with a classification loss. This model is very similar to the one used in <ref type="bibr" target="#b54">[55]</ref> and we thus refer to it as "IDE (R) ours", for which we also apply a metric learning step (XQDA <ref type="bibr" target="#b22">[23]</ref>). Unfortunately, especially difficult images caused frequent spikes in the loss, which ended up harming the optimization using Adam. After unsuccessfully trying lower learning rates and clipping extreme loss values, we resorted to Adadelta <ref type="bibr" target="#b43">[44]</ref>, another competitive optimization algorithm which did not exhibit these problems. While we combine embeddings through average pooling for our triplet based models, we found maxpooling and normalization to work better for the classification baseline, consistent with results reported in <ref type="bibr" target="#b48">[49]</ref>. As <ref type="table" target="#tab_2">Table 2</ref> shows, the performance of the resulting model "IDE (R) ours" is still on-par with similar models in the literature. However, the large gap between the identification-based model and our TriNet clearly demonstrates the advantages of using a triplet loss.</p><p>In line with the general trend in the vision community, all deep learning methods outperform shallow methods using hand-crafted features. While <ref type="table" target="#tab_2">Table 2</ref> only shows <ref type="bibr" target="#b44">[45]</ref> as a non-deep learning method, to the best of our knowledge all others perform worse.</p><p>We also evaluated how our models fare when combined with a recent re-ranking approach by Zhong et al. <ref type="bibr" target="#b54">[55]</ref>. This approach can be applied on top of any ranking methods and uses information from nearest neighbors in the gallery to improve the ranking result. As <ref type="table" target="#tab_2">Table 2</ref> shows, our approaches go well with this method and show similar improvements to those obtained by Zhong et al. <ref type="bibr" target="#b54">[55]</ref>.</p><p>Finally, we evaluate our models on Market-1501 with the provided 500k additional distractor images. The full experiment is described in the Supplementary Material. Even with these additional distractors, our triplet-based model outperforms a classification one by 8.4% mAP.</p><p>All of these results show that triplet loss embeddings are indeed a valuable tool for person ReID and we expect them to significantly change the way how research will progress in this field.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">To Pretrain or not to Pretrain?</head><p>As mentioned before, many methods for person ReID rely on pretrained networks, following a general trend in the computer vision community. Indeed, these models lead to impressive results, as we also confirmed in this paper with our TriNet model. However, pretrained networks reduce the flexibility to try out new advances in deep learning or to make task-specific changes in a network. Our LuNet model clearly suggests that it is also possible to train models from scratch and obtain competitive scores.</p><p>In particular, an interesting direction for ReID could be the usage of additional input channels such as depth information, readily available from cheap consumer hardware. However it is unclear how to best integrate such input data into a pretrained network in a proper way.</p><p>Furthermore, the typical pretrained networks are designed with accuracy in mind and do not focus on the memory footprint or the runtime of a method. Both are important factors for real-world robotic scenarios, where typically power consumption is a constraint and only less powerful hardware can be considered <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>. When designing a network from scratch, one can directly take this into consideration and create networks with a smaller memory footprint and faster evaluation times.</p><p>In principle, our pretrained model can easily be sped up by using half or quarter size input images, since the global average pooling in the ResNet will still produce an output vector of the same shape. This, however, goes hand in hand with the question of how to best adapt a pretrained network to a new task with different image sizes. The typical way of leveraging pretrained networks is to simply stretch images to the fixed expected input size used to train the network, typically 224 √ó 224 pixels. We used 256 √ó 128 instead in order to preserve the aspect ratio of the original image. However, for the Market-1501 dataset, this meant we had to upscale the images, while if we do not confine ourselves to pretrained networks we can simply adjust our architecture to the dataset size, as we did in the LuNet architecture. However, we hypothesize that a pretrained network has an "intrinsic scale," for which the learned filters work properly and thus simply using smaller input images will result in suboptimal performance. To show this, we retrain our TriNet with 128√ó64 and 64√ó32 images. As <ref type="table" target="#tab_5">Table 4</ref> clearly shows, the performance drops rapidly. At the original image scale, our LuNet model can almost match the mAP score of TriNet and already outperforms it when considering CMC scores. At an even smaller image size, LuNet significantly outperforms the pretrained model. Since the LuNet performance only drops by about ‚àº 3%, the small images still hold enough data to perform ReID, but the rather rigid pretrained weights can no longer adjust to such a data change. This shows that pretrained models are not a solution for arbitrary tasks, especially when one wants to train lightweight models for small images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We are not the first to use the triplet loss for person ReID. Ding et al. <ref type="bibr" target="#b8">[9]</ref> and Wang et al. <ref type="bibr" target="#b39">[40]</ref> use a batch generation and loss formulation which is very similar to our batch all formulation. Wang et al. <ref type="bibr" target="#b39">[40]</ref> further combine it with a pairwise verification loss. However, in the batch all case, it was important for us to average only over the active triplets (L BA =0 ), which they do not mention. This, in combination with their rather small networks, might explain their relatively low scores. Cheng et al. <ref type="bibr" target="#b7">[8]</ref> propose an "improved triplet loss" by introducing another pull term into the loss, penalizing large distances between positive images. This formulation is in fact very similar to the original one by Weinberger and Saul <ref type="bibr" target="#b40">[41]</ref>. We briefly experimented with a pull term, but the additional weighting hyper-parameter was not trivial to optimize and it did not improve our results. Several authors suggest learning attributes and ReID jointly <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref>, some of which integrate this into their embedding dimensions. This is an interesting research direction orthogonal to our work. Several other authors also defined losses over triplets of images <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26]</ref>, however, they use losses different from the triplet loss we defend in this paper, possibly explaining their lower scores. Finally, FaceNet <ref type="bibr" target="#b28">[29]</ref> uses a huge batch with moderate mining, which can only be done on the CPU, whereas we advocate hard mining in a small batch, which has a similar effect to moderate mining in a large batch, while fitting on a GPU and thus making training significantly more affordable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have shown that, contrary to the prevailing belief, the triplet loss is an excellent tool for person re-identification. We propose a variant that no longer requires offline hard negative mining at almost no additional cost. Combined with a pretrained network, we set the new state-of-the-art on three of the major ReID datasets. Furthermore, we show that training networks from scratch can lead to very competitive scores. We hope that in future work the ReID community will build on top of our results and shift more towards end-to-end learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Test-time Augmentation</head><p>As is good practice in the deep learning community <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref>, we perform test-time augmentation. From each image, we deterministically create five crops of size H √ó W : four corner crops and one center crop, as well as a horizontally flipped copy of each. The embeddings of all these ten images are then averaged, resulting in the final embedding for a person. <ref type="table" target="#tab_7">Table 5</ref> shows how five possible settings affect our scores on the Market-1501 dataset. As expected, the worst option is to scale the original images to fit the network input (first line), as this shows the network an image type it has never seen before. This option is directly followed by not using any test-time augmentation, i.e. just using the central crop. Simply flipping the center crop and averaging the two resulting embeddings already gives a big boost while only being twice as expensive. The four additional corner crops typically seem to be less effective, while more expensive, but using both augmentations together gives the best results. For the networks trained with a triplet loss, we gain about 3% mAP, while the network trained for identification only gains about 2% mAP. A possible explanation is that the feature space we learn with the triplet loss could be more suited to averaging than that of a classification network.   <ref type="figure" target="#fig_3">Figure 2</ref> shows several outliers in the Market-1501 and MARS datasets. Some issues in MARS are caused by the tracker-based annotations where bounding boxes sometimes span two persons and the tracker partially focuses on the wrong person. Additionally, some annotation mistakes can be found in both datasets; while some are obvious, some others are indeed very hard to spot! In <ref type="figure" target="#fig_4">Figure 3</ref> we show some of the most difficult queries along with their top-3 retrieved images (containing hard negatives), as well as their two hardest positives. While some mistakes are easy to spot by a human, others are indeed not trivial, such as the first row in <ref type="figure" target="#fig_4">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hard Positives, Hard Negatives and Outliers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments with Distractors</head><p>On top of the normal gallery set, the Market-1501 dataset provides an additional 500k distractors recorded at another time. In order to evaluate how such distractors affect performance, we randomly sample an increasing number of distractors and add them to the original gallery set. Here we compare to the results from Zheng et al. <ref type="bibr" target="#b51">[52]</ref>. Both our models show a similar behavior to that of their ResNet-50 baseline. Surprisingly, our LuNet model starts out with a slightly better mAP score than the baseline and ends up just below it, while consistently being better when considering the rank-1 score. This might indeed suggest that the inductive bias from pretraining helps during generalization to large amounts of unseen data. Nevertheless, all models seem to suffer under the increasing gallery set in a similar manner, albeit none of them fails badly. Especially the fact that in 74.70% of all single-image queries the first image out of 519 732 gallery images is correctly retrieved is an impressive result.</p><p>For reproducibility of the 500k distractor plot <ref type="figure" target="#fig_5">(Fig. 4)</ref>, <ref type="table" target="#tab_10">Table 6</ref> lists the values of the plot.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Notes on Network Training</head><p>Here we present and discuss several training-logs that display interesting behavior, as noted in the main paper. <ref type="bibr" target="#b18">19</ref>  This serves as practical advice of what to monitor for practitioners who choose to use a triplet-based loss in their training.</p><p>A typical training usually proceeds as follows: initially, all embeddings are pulled together towards their center of gravity. When they come close to each other, they will "pass" each other to join "their" clusters and, once this cross-over has happened, training mostly consists of pushing the clusters further apart and fine-tuning them. The collapsing of training happens when the margin is too large and the initial spread is too small, such that the embeddings get stuck when trying to pass each other.</p><p>Most importantly of all, if any type of hard-triplet mining is used, a stagnating loss curve by no means indicates stagnating progress. As the network learns to solve some hard cases, it will be presented with other hard cases and hence still keep a high loss. We recommend observing the fraction of active triplets in a batch, as well as the norms of the embeddings and all pairwise distances. <ref type="figure" target="#fig_6">Figures 5, 6</ref>, 7, and 8 all show different training logs. Note that while they all share the x-axis since the number of updates was kept the same throughout the experiments, the y-axes vary for clearer visualization. First, the topmost plot in each Subfigure (a) ("Value of criterion/penalty") shows all per-triplet values of the optimization criterion (the triplet loss) encountered in each mini-batch. This is shown again in the plot below it on the left ("loss"), with an overlaid light-blue line representing the batch-mean criterion value, and an overlaid dark-blue line representing the batch's 5percentile. To the right of it, the "% non-zero losses in batch" shows how many entries in the mini-batch had nonzero loss; values are computed up to a precision of 10 ‚àí5 , which explains how it can be below 100% in the soft-margin case. The single final vertical line present in some plots should be ignored as a plotting-artifact. Second, each Subfigure (b) (blue plots), monitors statistics about the embeddings computed during training. Different lines show 0, 5, 50, 95, and 100-percentiles within a mini-batch, thus visualizing the distribution of values. The top-left plot, "2-norm of embeddings", shows the norms of the embeddings, thus visualizing whether the embeddingspace shrinks towards 0 or expands. The top-right plot, "%tiles value of embedding entries" shows these same statistics over the individual numeric entries in the embedding vectors. The only use we found for this plot is noticing when embeddings collapse to all-zeros vs. some other value. Finally, the bottom-left plot, "Distance between embeddings", is the most revealing, as it shows the same percentiles over all pairwise distances between the embeddings within a mini-batch. Due to a bug, the x-axis is unfortunately mislabeled in some cases.</p><p>Let us now start by looking at the logs of two very suc-  cessful runs: the LuNet training from scratch on Market-1501 with the batch hard loss with margin 0.2 and in the soft-margin formulation, see <ref type="figure" target="#fig_6">Fig. 5</ref> and <ref type="figure" target="#fig_8">Fig. 6</ref>, respectively. The first observation is that, although they reach similar final scores, they learn significantly different embeddings. Looking at the embedding distances and norms, it is clear that the soft-margin formulation keeps pushing the embeddings apart, whereas the 0.2-margin keeps the norm of embeddings and their distances bounded. The effect of exponentially decaying the learning-rate is also clearly visible: starting at 15 000 updates, both the loss as well as the number of non-zero entries in a mini-batch start to strongly decrease again, before finally converging from 20 000 to 25 000 updates. A network getting stuck only happened to us with too weak network architectures, or when using offline hard mining (OHM), the latter can be seen in  Next, let us turn to <ref type="figure" target="#fig_10">Fig. 7</ref>, which shows the traininglogs of a very small net (not further specified in this paper). We can clearly see that the network first pulls all embeddings towards their center of gravity, as evidenced by the quickly decreasing embedding norms, entries, as well as distances in the first few hundred updates. (More visible when zooming-in on a computer.) Once they are all close to each other, the networks really struggles to make them all "pass each other" to reach "their" clusters. As soon as this difficult phase is overcome, the embeddings are spread around the space to quickly decrease the loss. This is where the training becomes "fragile" and prone to collapsing: if the embeddings never pass each other, training gets stuck. This behavior can also be observed in <ref type="figure" target="#fig_6">Figures 5 and 6</ref>, although to a much lesser extent, as the network is powerful enough to quickly overcome this difficult phase.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extended Comparison Tables</head><p>We show extended versions of the two state-of-the-art comparison tables in the main paper. We add additional methods that were left out due to space reasons, or because the approaches are orthogonal to ours. The latter could be integrated with our approach in a straightforward manner. Market-1501 and Mars results are shown in <ref type="table" target="#tab_13">Table 7</ref> and CUHK03 results in <ref type="table" target="#tab_14">Table 8</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. LuNet's Architecture</head><p>The details of the LuNet architecture for training from scratch can be seen in <ref type="table" target="#tab_16">Table 9</ref>. The input image has three channels and spatial dimensions 128√ó64. Most Res-blocks are of the "bottleneck" type <ref type="bibr" target="#b14">[15]</ref>, meaning for given numbers n 1 , n 2 , n 3 in the table, they consist of a 1√ó1 convolution from the number of input channels n 1 to the number of intermediate channels n 2 , followed by a 3√ó3 convolution keeping the number of channel constant, and finally another 1√ó1 convolution going from n 2 channels to n 3 channels. Only the last Res-block, whose exact filter sizes are given in the table, is an exception to this. All ReLUs, including those in Res-blocks, are leaky <ref type="bibr" target="#b26">[27]</ref> by a factor of 0.3; although we do not have side-by-side experiments comparing the benefits, we expect them to be minor. All convolutional weights are initialized following He et al. <ref type="bibr" target="#b12">[13]</ref>, whereas we initialized the final Linear layers following Glorot et al. <ref type="bibr" target="#b10">[11]</ref>. Batch-normalization <ref type="bibr" target="#b15">[16]</ref> is essential to train such a network, and makes the exact initialization less important.   G. Full t-SNE Visualization <ref type="figure" target="#fig_13">Figure 9</ref> shows the full Barnes-Hut t-SNE visualization from which the teaser image ( <ref type="figure" target="#fig_0">Fig. 1</ref> in the paper) was cropped. We used a subset of 6000 images from the Market-1501 test-set and a perplexity of 5000 for this visualization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A small crop of the Barnes-Hut t-SNE<ref type="bibr" target="#b37">[38]</ref> of our learned embeddings for the Market-1501 test-set. The triplet loss learns semantically meaningful features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a,p)‚ààX D a,p +log n‚ààX n =a,n =p e m‚àíDa,n + e m‚àíDp,n + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Some outliers in the Mars (top two rows) andMarket-1501 (bottom row) datasets. The first row shows high image overlap between tracklets of two persons. The second row shows a very hard example where a person was wrongly matched across tracklets. The last row shows a simple annotation mistake.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Some of the hardest queries. The leftmost column shows the query image, followed by the top 3 retrieved images and the two ground truth matches with the highest distance to the query images, i.e. the hardest positives. Correctly retrieved images have a green border, mistakes (i.e. hard negatives) have a red border.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>500k distractor set results. Solid lines represent the mAP score, dashed lines the rank-1 score. See Supplementary Material for values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Training-log of LuNet on Market1501 using the batch hard triplet loss with margin 0.2. The embeddings stay bounded, as expected from a triplet formulation, and there is a lot of progress even when the loss stays seemingly flat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Training-log of the loss and active triplet count. (b) Training-log of the embeddings in the minibatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Training-log of LuNet on Market1501 using the batch hard triplet loss with soft margin. The embeddings keep moving apart as even the loss shows a steady downward trend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Training-log of a very small network on Market-1501 using the batch hard triplet loss with soft margin. The difficult "packed" phase is clearly visible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) Training-log of the loss and active triplet count. (b) Training-log of the embeddings in the minibatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Training-log of LuNet on MARS when using offline hard mining (OHM) with margin 0.1. This is one of the runs that collapsed and never got past the difficult phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Barnes-Hut t-SNE<ref type="bibr" target="#b37">[38]</ref> of our learned embeddings for the Market-1501 test-set. Best viewed when zoomed-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Scores on both the Market-1501 and MARS datasets. The top and middle contain our scores and those of the current state-of-the-art respectively. The bottom contains several methods with re-ranking [55]. The different types represent the optimization criteria, where I stands for identification, V for verification and E for embedding. All our scores include test-time augmentation. The best scores are bold.‚Ä† : Concurrent work only published on arXiv.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Scores on CUHK03 for TriNet and a set of recent top performing methods. The best scores are highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell>TriNet</cell><cell></cell><cell>LuNet</cell><cell></cell></row><row><cell></cell><cell cols="2">mAP rank-1 rank-5</cell><cell cols="3">mAP rank-1 rank-5</cell></row><row><cell>256 √ó 128</cell><cell>69.14</cell><cell>84.92 94.21</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>128 √ó 64</cell><cell>62.52</cell><cell>79.45 91.06</cell><cell>60.71</cell><cell>81.38</cell><cell>92.34</cell></row><row><cell>64 √ó 32</cell><cell>47.42</cell><cell>68.08 85.84</cell><cell>57.18</cell><cell>78.21</cell><cell>90.94</cell></row></table><note>‚Ä† : Concurrent work only published on arXiv.*: The method was trained on several additional datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>The effect of input size on mAP and CMC scores.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The effect of test-time augmentation on Market-1501.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Values for the 500k distractor plot.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Scores on both the Market-1501 and MARS datasets. The top and middle contain our scores and those of the current state-of-the-art respectively. The bottom contains several methods with re-ranking<ref type="bibr" target="#b54">[55]</ref>. The different types represent the optimization criteria, where I stands for identification, V for verification and E for embedding. All our scores include test-time augmentation. The best scores are bold. ‚Ä† : Concurrent work only published on arXiv.TriNet E 89.63 99.01 87.58 98.17</figDesc><table><row><cell></cell><cell>Type</cell><cell cols="2">Labeled r-1 r-5</cell><cell cols="2">Detected r-1 r-5</cell></row><row><cell>Gated siamese CNN [39]</cell><cell>V</cell><cell>-</cell><cell>-</cell><cell cols="2">61.8 86.7</cell></row><row><cell>DGD* [42]</cell><cell cols="2">I 75.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LOMO + Null Space [45]</cell><cell cols="5">E 62.55 90.05 54.70 84.75</cell></row><row><cell>SSM [3]</cell><cell cols="3">-76.6 94.6</cell><cell cols="2">72.7 92.4</cell></row><row><cell>CAN [25]</cell><cell cols="3">E 77.6 95.2</cell><cell cols="2">69.2 88.5</cell></row><row><cell>Latent Parts (Fusion) [20]</cell><cell cols="5">I 74.21 94.33 67.99 91.04</cell></row><row><cell>Spindle Net* [48]</cell><cell cols="3">I 88.5 97.8</cell><cell>-</cell><cell>-</cell></row><row><cell>JLML [22]</cell><cell cols="3">I 83.2 98.0</cell><cell cols="2">80.6 96.9</cell></row><row><cell>SVDNet [35]</cell><cell>I</cell><cell>-</cell><cell>-</cell><cell>81.8</cell><cell>-</cell></row><row><cell>CNN + DCGAN [54]</cell><cell>I</cell><cell>-</cell><cell>-</cell><cell cols="2">84.6 97.6</cell></row><row><cell>DPFL [7]</cell><cell cols="2">I 82.8</cell><cell>-</cell><cell>82.0</cell><cell>-</cell></row><row><cell>DTL  ‚Ä† [10]</cell><cell cols="2">I+V 85.4</cell><cell>-</cell><cell>84.1</cell><cell>-</cell></row><row><cell>ResNet 50 (I+V)  ‚Ä† [52]</cell><cell>I+V</cell><cell>-</cell><cell>-</cell><cell cols="2">83.4 97.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Scores on CUHK03 for TriNet and a set of recent top performing methods. The best scores are highlighted in bold.</figDesc><table /><note>‚Ä† : Concurrent work only published on arXiv.*: The method was trained on several additional datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>MaxPool pool 3√ó3, stride (2√ó2), padding (1√ó1)</figDesc><table><row><cell>Type</cell><cell>Size</cell></row><row><cell>Conv</cell><cell>128√ó7√ó7√ó3</cell></row><row><cell>Res-block</cell><cell>128, 32, 128</cell></row><row><cell>Res-block</cell><cell>128, 32, 128</cell></row><row><cell>Res-block</cell><cell>128, 32, 128</cell></row><row><cell>Res-block</cell><cell>128, 64, 256</cell></row><row><cell>MaxPool</cell><cell>pool 3√ó3, stride (2√ó2), padding (1√ó1)</cell></row><row><cell>Res-block</cell><cell>256, 64, 256</cell></row><row><cell>Res-block</cell><cell>256, 64, 256</cell></row><row><cell>MaxPool</cell><cell>pool 3√ó3, stride (2√ó2), padding (1√ó1)</cell></row><row><cell>Res-block</cell><cell>256, 64, 256</cell></row><row><cell>Res-block</cell><cell>256, 64, 256</cell></row><row><cell>Res-block</cell><cell>256, 128, 512</cell></row><row><cell>MaxPool</cell><cell>pool 3√ó3, stride (2√ó2), padding (1√ó1)</cell></row><row><cell>Res-block</cell><cell>512, 128, 512</cell></row><row><cell>Res-block</cell><cell>512, 128, 512</cell></row><row><cell>MaxPool</cell><cell>pool 3√ó3, stride (2√ó2), padding (1√ó1)</cell></row><row><cell>Res-block</cell><cell>512√ó(3√ó3√ó512), 128√ó(3√ó3√ó512)</cell></row><row><cell>Linear</cell><cell>1024√ó512</cell></row><row><cell cols="2">Batch-Norm 512</cell></row><row><cell>ReLU</cell><cell></cell></row><row><cell>Linear</cell><cell>512√ó128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>The architecture of LuNet.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In all experiments we choose B, P , and K in such a way that 3B is close to P K, e.g. 3 ¬∑ 42 ‚âà 32 ¬∑ 4.3 Because P K ‚âà 3B, see footnote 2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Training-log of the loss and active triplet count. (b) Training-log of the embeddings in the minibatch.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This work was funded, in parts, by ERC Starting Grant project CV-SUPER (ERC-2012-StG-307432) and the EU project STRANDS (ICT-2011-600623). We would also like to thank the authors of the Market-1501 and MARS datasets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Improved Deep Learning Architecture for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer Normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Looking Beyond Appearances: Synthetic Training Data for Deep CNNs in Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rognhaugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03153</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Theano: new features and speed improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Multi-task Deep Network for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person Re-Identification by Deep Learning Multi-Scale Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shaogang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV W. on Cross-domain Human Identification</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Transfer Learning for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Sparse Rectifier Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burbridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lacerda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mudrov√°</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hebesberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>K√∂rtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The STRANDS Project: Long-Term Autonomy in Everyday Environments. RAM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="146" to="156" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving Deep Into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint Learning for Attribute-Consistent Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person Re-Identification by Deep Joint Learning of Multi-Loss Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Person Re-identification by Local Maximal Occurrence Representation and Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Endto-End Comparative Attention Networks for Person Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-Scale Triplet CNN for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person Re-Identification by Deep Learning Attribute-Complementary Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embedding Deep Metric for Person Re-identification: A Study Against Large Variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Lifted Structured Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Attributes Driven Multi-camera Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Neural Networks with Inexact Matching for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2667" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">SVDNet for Pedestrian Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Socially Aware Service Robot for Passenger Guidance and Help in Busy Airports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="607" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using Tree-Based Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint Learning of Single-image and Cross-image Representations for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Divide and Fuse: A Re-ranking Approach for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning a Discriminative Null Space for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning Compact Appearance Representation for Video-based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06294</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00384</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Deep Mutual Learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">MARS: A Video Benchmark for Large-Scale Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scalable Person Re-Identification: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Person Reidentification: Past, Present and Future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A Discriminatively Learned CNN Embedding for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05666</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Pedestrian Alignment Network for Large-scale Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00408</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Re-ranking Person Re-identification with k-reciprocal Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<title level="m">Random Erasing Data Augmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-based Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
