<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OBLIQUE DECISION TREES FROM DERIVATIVES OF RELU NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-He</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Lab MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Lab MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OBLIQUE DECISION TREES FROM DERIVATIVES OF RELU NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show how neural models can be used to realize piece-wise constant functions such as decision trees. The proposed architecture, which we call locally constant networks, builds on ReLU networks that are piece-wise linear and hence their associated gradients with respect to the inputs are locally constant. We formally establish the equivalence between the classes of locally constant networks and decision trees. Moreover, we highlight several advantageous properties of locally constant networks, including how they realize decision trees with parameter sharing across branching / leaves. Indeed, only M neurons suffice to implicitly model an oblique decision tree with 2 M leaf nodes. The neural representation also enables us to adopt many tools developed for deep networks (e.g., DropConnect (Wan et al., 2013)) while implicitly training decision trees. We demonstrate that our method outperforms alternative techniques for training oblique decision trees in the context of molecular property classification and regression tasks. 1 1 Our implementation and data are available at https://github.com/guanghelee/iclr20-lcn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Decision trees <ref type="bibr" target="#b2">(Breiman et al., 1984</ref>) employ a series of simple decision nodes, arranged in a tree, to transparently capture how the predicted outcome is reached. Functionally, such tree-based models, including random forest <ref type="bibr" target="#b3">(Breiman, 2001)</ref>, realize piece-wise constant functions. Beyond their status as de facto interpretable models, they have also persisted as the state of the art models in some tabular <ref type="bibr" target="#b27">(Sandulescu &amp; Chiru, 2016)</ref> and chemical datasets <ref type="bibr" target="#b38">(Wu et al., 2018)</ref>. Deep neural models, in contrast, are highly flexible and continuous, demonstrably effective in practice, though lack transparency. We merge these two contrasting views by introducing a new family of neural models that implicitly learn and represent oblique decision trees.</p><p>Prior work has attempted to generalize classic decision trees by extending coordinate-wise cuts to be weighted, linear classifications. The resulting family of models is known as oblique decision trees <ref type="bibr" target="#b19">(Murthy et al., 1993)</ref>. However, the generalization accompanies a challenging combinatorial, non-differentiable optimization problem over the linear parameters at each decision point. Simple sorting procedures used for successively finding branch-wise optimal coordinate cuts are no longer available, making these models considerably harder to train. While finding the optimal oblique decision tree can be cast as a mixed integer linear program <ref type="bibr" target="#b1">(Bertsimas &amp; Dunn, 2017)</ref>, scaling remains a challenge.</p><p>In this work, we provide an effective, implicit representation of piece-wise constant mappings, termed locally constant networks. Our approach exploits piece-wise linear models such as ReLU networks as basic building blocks. Linearity of the mapping in each region in such models means that the gradient with respect to the input coordinates is locally constant. We therefore implicitly represent piece-wise constant networks through gradients evaluated from ReLU networks. We prove the equivalence between the class of oblique decision trees and these proposed locally constant neural models. However, the sizes required for equivalent representations can be substantially different. For example, a locally constant network with M neurons can implicitly realize an oblique decision tree whose explicit form requires 2 M −1 oblique decision nodes. The exponential complexity reduc-Published as a conference paper at ICLR 2020 tion in the corresponding neural representation illustrates the degree to which parameters are shared across the locally constant regions.</p><p>Our locally constant networks can be learned via gradient descent, and they can be explicitly converted back to oblique decision trees for interpretability. For learning via gradient descent, however, it is necessary to employ some smooth annealing of piece-wise linear activation functions so as to keep the gradients themselves continuous. Moreover, we need to evaluate the gradients of all the neurons with respect to the inputs. To address this bottleneck, we devise a dynamic programming algorithm which computes all the necessary gradient information in a single forward pass. A number of extensions are possible. For instance, we can construct approximately locally constant networks by switching activation functions, or apply helpful techniques used with normal deep learning models (e.g., DropConnect <ref type="bibr" target="#b35">(Wan et al., 2013)</ref>) while implicitly training tree models.</p><p>We empirically test our model in the context of molecular property classification and regression tasks <ref type="bibr" target="#b38">(Wu et al., 2018)</ref>, where tree-based models remain state-of-the-art. We compare our approach against recent methods for training oblique decision trees and classic ensemble methods such as gradient boosting <ref type="bibr" target="#b10">(Friedman, 2001)</ref> and random forest. Empirically, a locally constant network always outperforms alternative methods for training oblique decision trees by a large margin, and the ensemble of locally constant networks is competitive with classic ensemble methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Locally constant networks are built on a mixed integer linear representation of piece-wise linear networks, defined as any feed-forward network with a piece-wise linear activation function such as ReLU <ref type="bibr" target="#b21">(Nair &amp; Hinton, 2010)</ref>. One can specify a set of integers encoding the active linear piece of each neuron, which is called an activation pattern <ref type="bibr" target="#b24">(Raghu et al., 2017)</ref>. The feasible set of an activation pattern forms a convex polyhedron in the input space <ref type="bibr" target="#b16">(Lee et al., 2019)</ref>, where the network degenerates to a linear model. The framework motivates us to leverage the locally invariant derivatives of the networks to construct a locally constant network. The activation pattern is also exploited in literature for other purposes such as deriving robustness certificates <ref type="bibr" target="#b36">(Weng et al., 2018)</ref>. We refer the readers to the recent work <ref type="bibr" target="#b16">(Lee et al., 2019)</ref> and the references therein.</p><p>Locally constant networks use the gradients of deep networks with respect to inputs as the representations to build discriminative models. Such gradients have been used in literature for different purposes. They have been widely used for local sensitivity analysis of trained networks <ref type="bibr" target="#b29">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b30">Smilkov et al., 2017)</ref>. When the deep networks model an energy function <ref type="bibr" target="#b15">(LeCun et al., 2006)</ref>, the gradients can be used to draw samples from the distribution specified by the normalized energy function <ref type="bibr" target="#b7">(Du &amp; Mordatch, 2019;</ref><ref type="bibr" target="#b31">Song &amp; Ermon, 2019)</ref>. The gradients can also be used to train generative models <ref type="bibr" target="#b11">(Goodfellow et al., 2014)</ref> or perform knowledge distillation <ref type="bibr" target="#b32">(Srinivas &amp; Fleuret, 2018)</ref>.</p><p>The class of locally constant networks is equivalent to the class of oblique decision trees. There are some classic methods that also construct neural networks that reproduce decision trees <ref type="bibr" target="#b28">(Sethi, 1990;</ref><ref type="bibr" target="#b4">Brent, 1991;</ref><ref type="bibr" target="#b6">Cios &amp; Liu, 1992)</ref>, by utilizing step functions and logic gates (e.g., AND/NEGATION) as the activation function. The methods were developed when back-propagation was not yet practically useful, and the motivation is to exploit effective learning procedures of decision trees to train neural networks. Instead, our goal is to leverage the successful deep models to train oblique decision trees. Recently, <ref type="bibr" target="#b39">Yang et al. (2018)</ref> proposed a network architecture with arg max activations to represent classic decision trees with coordinate cuts, but their parameterization scales exponentially with input dimension. In stark contrast, our parameterization only scales linearly with input dimension (see our complexity analyses in §3.7).</p><p>Learning oblique decision trees is challenging, even for a greedy algorithm; for a single oblique split, there can be D k=0 N k different ways to separate N data points in D-dimensional space <ref type="bibr" target="#b34">(Vapnik &amp; Chervonenkis, 1971</ref>) (cf. N D possibilities for coordinate-cuts). Existing learning algorithms for oblique decision trees include greedy induction, global optimization, and iterative refinements on an initial tree. We review some representative works, and refer the readers to the references therein.</p><p>Optimizing each oblique split in greedy induction can be realized by coordinate descent <ref type="bibr" target="#b20">(Murthy et al., 1994)</ref> or a coordinate-cut search in some linear projection space <ref type="bibr" target="#b18">(Menze et al., 2011;</ref><ref type="bibr" target="#b37">Wickramarachchi et al., 2016)</ref>. However, the greedy constructions tend to get stuck in poor local optimum.</p><p>There are some works which attempt to find the global optimum given a fixed tree structure by formulating a linear program <ref type="bibr" target="#b0">(Bennett, 1994)</ref> or a mixed integer linear program <ref type="bibr" target="#b1">(Bertsimas &amp; Dunn, 2017)</ref>, but the methods are not scalable to ordinary tree sizes (e.g., depth more than 4). The iterative refinements are more scalable than global optimization, where CART <ref type="bibr" target="#b2">(Breiman et al., 1984)</ref> is the typical initialization. <ref type="bibr" target="#b5">Carreira-Perpinán &amp; Tavallali (2018)</ref> develop an alternating optimization method via iteratively training a linear classifier on each decision node, which yield the state-of-theart empirical performance, but the approach is only applicable to classification problems. <ref type="bibr" target="#b22">Norouzi et al. (2015)</ref> proposed to do gradient descent on a sub-differentiable upperbound of tree prediction errors, but the gradients with respect to oblique decision nodes are unavailable whenever the upperbound is tight. In contrast, our method conducts gradient descent on a differentiable relaxation, which is gradually annealed to a locally constant network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we introduce the notation and basics in §3.1, construct the locally constant networks in §3.2-3.3, analyze the networks in §3.4-3.5, and develop practical formulations and algorithms in §3.6-3.7. Note that we will propose two (equivalent) architectures of locally constant networks in §3.3 and §3.6, which are useful for theoretical analyses and practical purposes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NOTATION AND BASICS</head><p>The proposed approach is built on feed-forward networks that yield piece-wise linear mappings.</p><p>Here we first introduce a canonical example of such networks, and elaborate its piece-wise linearity. We consider the densely connected architecture <ref type="bibr" target="#b14">(Huang et al., 2017)</ref>, where each hidden layer takes as input all the previous layers; it subsumes other existing feed-forward architectures such as residual networks <ref type="bibr" target="#b12">(He et al., 2016)</ref>. For such a network f θ : R D → R L with the set of parameters θ, we denote the number of hidden layers as M and the number of neurons in the i th layer as N i ; we denote the neurons in the i th layer, before and after activation, as z i ∈ R Ni and a i ∈ R Ni , respectively, where we sometimes interchangeably denote the input instance x as a 0 ∈ R N0 with N 0 D. To simplify exposition, we denote the concatenation of (a 0 , a 1 , . . . , a i ) asã i ∈ RÑ i withÑ i i j=0 N i , ∀i ∈ {0, 1, . . . , M }. The neurons are defined via the weight matrix W i ∈ R Ni×Ñi−1 and the bias vector b i ∈ R Ni in each layer i ∈ [M ] {1, 2, . . . , M }. Concretely,</p><formula xml:id="formula_0">a 0 x, z i W iãi−1 + b i , a i σ(z i ), ∀i ∈ [M ],<label>(1)</label></formula><p>where σ(·) is a point-wise activation function. Note that both a and z are functions of the specific instance denoted by x, where we drop the functional dependency to simplify notation. We use the set I to denote the set of all the neuron indices in this network</p><formula xml:id="formula_1">{(i, j)|j ∈ [N i ], i ∈ [M ]}.</formula><p>In this work, we will use ReLU <ref type="bibr" target="#b21">(Nair &amp; Hinton, 2010)</ref> as a canonical example for the activation function</p><formula xml:id="formula_2">a i j = σ(z i ) j max(0, z i j ), ∀(i, j) ∈ I,<label>(2)</label></formula><p>but the results naturally generalize to other piece-wise linear activation functions such as leaky <ref type="bibr">ReLU (Maas et al., 2013)</ref>. The output of the entire network f θ (x) is the affine transformation from all the hidden layersã M with the weight matrix W M +1 ∈ R L×Ñ M and bias vector b M +1 ∈ R L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LOCAL LINEARITY</head><p>It is widely known that the class of networks f θ (·) yields a piece-wise linear function. The results are typically proved via associating the end-to-end behavior of the network with its activation pattern -which linear piece in each neuron is activated; once an activation pattern is fixed across the entire network, the network degenerates to a linear model and the feasible set with respect to an activation pattern is a natural characterization of a locally linear region of the network.</p><p>Formally, we define the activation pattern as the collection of activation indicator functions for each neuron o i j : R D → {0, 1}, ∀(i, j) ∈ I (or, equivalently, the derivatives of ReLU units; see below) 2 : Here the locally constant networks have 1 neuron per layer. We show the locally constant networks on the LHS, the raw mappings in the middle, and the equivalent oblique decision trees on the RHS.</p><formula xml:id="formula_3">o i j = ∂a i j ∂z i j I[z i j ≥ 0], ∀(i, j) ∈ I,<label>(3)</label></formula><p>where I[·] is the indicator function. Note that, for mathematical correctness, we define ∂a i j /∂z i j = 1 at z i j = 0; this choice is arbitrary, and one can change it to ∂a i j /∂z i j = 0 at z i j = 0 without affecting most of the derivations. Given a fixed activation patternō i j ∈ {0, 1}, ∀(i, j), we can specify a feasible set in R D that corresponds to this activation pattern</p><formula xml:id="formula_4">{x ∈ R D |o i j =ō i j , ∀(i, j) ∈ I} (note that each o i j is a function of x).</formula><p>Due to the fixed activation pattern, the non-linear ReLU can be re-written as a linear function for all the inputs in the feasible set. For example, for anō i j = 0, we can re-write a i j = 0 × z i j . As a result, the network has a consistent end-to-end linear behavior across the entire feasible set. One can prove that all the feasible sets partition the space R D into disjoint convex polyhedra 3 , which realize a natural representation of the locally linear regions. Since we will only use the result to motivate the construction of locally constant networks, we refer the readers to <ref type="bibr" target="#b16">Lee et al. (2019)</ref> for a detailed justification of the piece-wise linearity of such networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CANONICAL LOCALLY CONSTANT NETWORKS</head><p>Since the ReLU network f θ (x) is piece-wise linear, it immediately implies that its derivatives with respect to the input x is a piece-wise constant function. Here we use J</p><formula xml:id="formula_5">x f θ (x) ∈ R L×D to denote the Jacobian matrix (i.e., [J x f θ (x)] i,j = ∂f θ (x) i /∂x j )</formula><p>, and we assume the Jacobian is consistent with Eq. (3) at the boundary of the locally linear regions. Since any function taking the piecewise constant Jacobian as input will remain itself piece-wise constant, we can construct a variety of locally constant networks by composition.</p><p>However, in order to simplify the derivation, we first make a trivial observation that the activation pattern in each locally linear region is also locally invariant. More broadly, any invariant quantity in each locally linear region can be utilized so as to build locally constant networks. We thus define the locally constant networks as any composite function that leverage the local invariance of piece-wise linear networks. For the theoretical analyses, we consider the below architecture.</p><p>Canonical architecture. We denoteõ M ∈ {0, 1}Ñ M as the concatenation of (o 1 , . . . , o M ). We will use the composite function g(õ M ) as the canonical architecture of locally constant networks for theoretical analyses, where g :</p><formula xml:id="formula_6">{0, 1}Ñ M → R L is simply a table.</formula><p>Before elucidating on the representational equivalence to oblique decision trees, we first show some toy examples of the canonical locally constant networks and their equivalent mappings in <ref type="figure" target="#fig_0">Fig. 1</ref>, which illustrates their constructions when there is only 1 neuron per layer (i.e., z i = z i 1 , and similarly for o i and a i ). When M = 1, o 1 = 1 ⇔ x 1 − x 2 + 1 ≥ 0, thus the locally constant network is equivalent to a linear model shown in the middle, which can also be represented as an oblique decision tree with depth = 1. When M &gt; 1, the activations in the previous layers control different linear behaviors of a neuron with respect to the input, thus realizing a hierarchical structure as an oblique decision tree. For example, for</p><formula xml:id="formula_7">M = 2, o 1 = 0 ⇔ z 1 &lt; 0 ⇒ z 2 = −4x 1 + x 2 + 4 and o 1 = 1 ⇔ z 1 ≥ 0 ⇒ z 2 = −3x 2 + 8;</formula><p>hence, it can also be interpreted as the decision tree on the RHS, where the concrete realization of z 2 depends on the previous decision variable z 1 ≥ 0. Afterwards, we can map either the activation patterns on the LHS or the decision patterns on the RHS to an output value, which leads to the mapping in the middle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">REPRESENTATIONAL EQUIVALENCE</head><p>In this section, we prove the equivalence between the class of oblique decision trees and the class of locally constant networks. We first make an observation that any unbalanced oblique decision tree can be re-written to be balanced by adding dummy decision nodes 0 x ≥ −1. Hence, we can define the class of oblique decision trees with the balance constraint:</p><p>Definition 1. The class of oblique decision trees contains any functions that can be procedurally defined (with some depth T ∈ Z &gt;0 ) for x ∈ R D :</p><formula xml:id="formula_8">1. r 1 I[ω ∅ x + β ∅ ≥ 0],</formula><p>where ω ∅ ∈ R D and β ∅ ∈ R denote the weight and bias of the root decision node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For</head><formula xml:id="formula_9">i ∈ (2, 3, . . . , T ), r i I[ω r1:i−1 x+β r1:i−1 ≥ 0],</formula><p>where ω r1:i−1 ∈ R D and β r1:i−1 ∈ R denote the weight and bias for the decision node after the decision pattern r 1:i−1 .</p><formula xml:id="formula_10">3. v : {0, 1} T → R L outputs the leaf value v(r 1:T ) associated with the decision pattern r 1:T .</formula><p>The class of locally constant networks is defined by the canonical architecture with finite M and</p><formula xml:id="formula_11">N i , ∀i ∈ [M ]</formula><p>. We first prove that we can represent any oblique decision tree as a locally constant network. Since a typical oblique decision tree can produce an arbitrary weight in each decision node (cf. the structurally dependent weights in the oblique decision trees in <ref type="figure" target="#fig_0">Fig. 1</ref>), the idea is to utilize a network with only 1 hidden layer such that the neurons do not constrain one another. Concretely, Theorem 2. The class of locally constant networks ⊇ the class of oblique decision trees.</p><p>Proof. For any oblique decision tree with depth T , it contains 2 T − 1 weights and biases. We thus construct a locally constant network with M = 1 and N 1 = 2 T − 1 such that each pair of (ω, β) in the oblique decision tree is equal to some W 1 k,: and b 1 k in the constructed locally constant network.</p><p>For each leaf node in the decision tree, it is associated with an output value y ∈ R L and T decisions; the decisions can be written as</p><formula xml:id="formula_12">W 1 idx[j],: x + b 1 idx[j] ≥ 0 for j ∈ {1, 2, . . . , T } and W 1 idx[j],: x + b 1 idx[j] &lt; 0 for j ∈ {T + 1, T + 2, . . . , T } for some index function idx : [T ] → [2 T − 1]</formula><p>and some T ∈ {0, 1, . . . , T }. We can set the table g(·) of the locally constant network as</p><formula xml:id="formula_13">y, if o 1 idx[j] = 1(⇔ W 1 idx[j],: x + b 1 idx[j] ≥ 0), for j ∈ {1, 2, . . . , T }, and o 1 idx[j] = 0(⇔ W 1 idx[j],: x + b 1 idx[j] &lt; 0), for j ∈ {T + 1, T + 2, . . . , T }.</formula><p>As a result, the constructed locally constant network yields the same output as the given oblique decision tree for all the inputs that are routed to each leaf node, which concludes the proof.</p><p>Then we prove that the class of locally constant networks is a subset of the class of oblique decision trees, which simply follows the construction of the toy examples in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Theorem 3. The class of locally constant networks ⊆ the class of oblique decision trees.</p><p>Proof. For any locally constant network, it can be re-written to have 1 neuron per layer, by expanding any layer with N i &gt; 1 neurons to be N i different layers such that they do not have effective intra-connections. Below the notation refers to the converted locally constant network with 1 neuron per layer. We define the following oblique decision tree with T = M for x ∈ R D :</p><formula xml:id="formula_14">1. r 1 o 1 1 = I[ω ∅ x + β ∅ ≥ 0] with ω ∅ = W 1 1,: and β ∅ = b 1 1 . 2. For i ∈ (2, 3, . . . , M ), r i I[ω r1:i−1 x + β r1:i−1 ≥ 0], where ω r1:i−1 = ∇ x z i 1 and β r1:i−1 = z i 1 − (∇ x z i 1 ) x. Note that r i = I[z i 1 ≥ 0] = o i 1 . 3. v = g.</formula><p>Note that, in order to be a valid decision tree, ω 1:ri−1 and β 1:ri−1 have to be unique for all x that yield the same decision pattern r 1:i−1 . To see this, for i ∈ (2, 3, . . . , M ), as r 1:i−1 = (o 1 1 , . . . , o i−1 1 ), we know each z i 1 is a fixed affine function given an activation pattern for the preceding neurons, so ∇ x z i 1 and z i 1 − x ∇ x z i 1 are fixed quantities given a decision pattern r 1:i−1 . Since r 1:M =õ M and v = g, we conclude that they yield the same mapping.</p><p>Despite the simplicity of the proof, it has some practical implications: Remark 4. The proof of Theorem 3 implies that we can train a locally constant network with M neurons, and convert it to an oblique decision tree with depth M (for interpretability). Remark 5. The proof of Theorem 3 establishes that, given a fixed number of neurons, it suffices (representationally) to only consider the locally constant networks with one neuron per layer.</p><p>Remark 5 is important for learning small locally constant networks (which can be converted to shallow decision trees for interpretability), since representation capacity is critical for low capacity models. In the remainder of the paper, we will only consider the setting with N i = 1, ∀i ∈ [M ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">STRUCTURALLY SHARED PARAMETERIZATION</head><p>Although we have established the exact class-level equivalence between locally constant networks and oblique decision trees, once we restrict the depth of the locally constant networks M , it can no longer re-produce all the decision trees with depth M . The result can be intuitively understood by the following reason: we are effectively using M pairs of (weight, bias) in the locally constant network to implicitly realize 2 M − 1 pairs of (weight, bias) in the corresponding oblique decision tree. Such exponential reduction on the effective parameters in the representation of oblique decision trees yields "dimension reduction" of the model capacity. This section aims to reveal the implied shared parameterization embedded in the oblique decision trees derived from locally constant networks.</p><p>In this section, the oblique decision trees and the associated parameters refer to the decision trees obtained via the proof of Theorem 3. We start the analysis by a decomposition of ω r1:i among the preceding weights ω ∅ , ω r1:1 , . . . , ω r1:r−1 . To simplify notation, we denote ω r1:0 ω ∅ . Since ω r1:i = ∇ x z i+1 1 and z i+1 1 is an affine transformation of the vector (a 0 , a 1 1 , . . . , a i 1 ),</p><formula xml:id="formula_15">ω r1:i = ∇ x z i+1 1 = W i+1 1,1:D + i k=1 W i+1 1,D+k × ∂a k 1 ∂z k 1 ×∇ x z k 1 = W i+1 1,1:D + i k=1 W i+1 1,D+k ×r k ×ω r 1:k−1 ,</formula><p>where we simply re-write the derivatives in terms of tree parameters. Since W i+1 1,1:D is fixed for all the ω r1:i , the above decomposition implies that, in the induced tree, all the weights ω r1:i in the same depth i are restricted to be a linear combination of the fixed basis W i+1 1,1:D and the corresponding preceding weights ω r1:0 , . . . , ω r1:i−1 . We can extend this analysis to compare weights in same layer, and we begin the analysis by comparing weights whose 0 distance in decision pattern is 1. To help interpret the statement, note that ω r1:j−1 is the weight that leads to the decision r j (or r j ; see below). Lemma 6. For an oblique decision tree with depth T &gt; 1, ∀i ∈ [T − 1] and any r 1:i , r 1:i such that r k = r k for all k ∈ [i] except that r j = r j for some j ∈ [i], we have ω r1:i − ω r 1:i = α × ω r1:j−1 , for some α ∈ R.</p><p>The proof involves some algebraic manipulation, and is deferred to Appendix A.1. Lemma 6 characterizes an interesting structural constraint embedded in the oblique decision trees realized by locally constant networks, where the structural discrepancy r j in decision patterns (r 1:i versus r 1:i ) is reflected on the discrepancy of the corresponding weights (up to a scaling factor α). The analysis can be generalized for all the weights in the same layer, but the message is similar. Proposition 7. For the oblique decision tree with depth T &gt; 1, ∀i ∈ [T − 1] and any r 1:i , r 1:i such that r k = r k for all k ∈ [i] except for n ∈ [i] coordinates j 1 , . . . , j n ∈ [i], we have</p><formula xml:id="formula_16">ω r1:i − ω r 1:i = n k=1 α k × ω r1:j k −1 , for some α k ∈ R, ∀k ∈ [n].<label>(4)</label></formula><p>The statement can be proved by applying Lemma 6 multiple times.</p><p>Discussion. Here we summarize this section and provide some discussion. Locally constant networks implicitly represent oblique decision trees with the same depth and structurally shared parameterization. In the implied oblique decision trees, the weight of each decision node is a linear combination of a shared weight across the whole layer and all the preceding weights. The analysis explains how locally constant networks use only M weights to model a decision tree with 2 M − 1 decision nodes; it yields a strong regularization effect to avoid overfitting, and helps computation by exponentially reducing the memory consumption on the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">STANDARD LOCALLY CONSTANT NETWORKS AND EXTENSIONS</head><p>The simple structure of the canonical locally constant networks is beneficial for theoretical analysis, but the structure is not practical for learning since the discrete activation pattern does not exhibit gradients for learning the networks. Indeed, ∇õM g(õ M ) is undefined, which implies that ∇ W i g(õ M ) is also undefined. Here we present another architecture that is equivalent to the canonical architecture, but exhibits sub-gradients with respect to model parameters and is flexible for model extension.</p><p>Standard architecture. We assume N i = 1, ∀i ∈ [M ]. We denote the Jacobian of all the neurons after activationã M as J xã M ∈ R M ×D , and denote J xã M as the vectorized version. We then define the standard architecture as g</p><formula xml:id="formula_17">φ ( J xã M ), where g φ : R (M ×D) → R L is a fully-connected network.</formula><p>We abbreviate the standard locally constant networks as LCN. Note that each a i 1 is locally linear and thus the Jacobian J xã M is locally constant. We replaceõ M with J xã M as the invariant representation for each locally linear region 4 , and replace the table g with a differentiable function g φ that takes as input real vectors. The gradients of LCN with respect to parameters is thus established through the derivatives of g φ and the mixed partial derivatives of the neurons (derivatives of J xã M ).</p><p>Fortunately, all the previous analyses also apply to the standard architecture, due to a fine-grained equivalence between the two architectures. Theorem 8. Given any fixed f θ , any canonical locally constant network g(õ M ) can be equivalently represented by a standard locally constant network g φ ( J xã M ), and vice versa.</p><p>Since f θ and g control the decision nodes and leaf nodes in the associated oblique decision tree, respectively (see Theorem 3), Theorem 8 essentially states that both architectures are equally competent for assigning leaf nodes. Combining Theorem 8 with the analyses in §3.4, we have class-level equivalence among the two architectures of locally constant networks and oblique decision trees. The analyses in §3.5 are also inherited since the analyses only depend on decision nodes (i.e., f θ ).</p><p>The core ideas for proving Theorem 8 are two-fold: 1) we find a bijection between the activation patternõ M and the Jacobian J xã M , and 2) feed-forward networks g φ can map the (finitely many) Jacobian J xã M as flexibly as a table g. The complete proof is deferred to Appendix A.2.</p><p>Discussion. The standard architecture yields a new property that is only partially exhibited in the canonical architecture. For all the decision and leaf nodes which no training data is routed to, there is no way to obtain learning signals in classic oblique decision trees. However, due to shared parameterization (see §3.5), locally constant networks can "learn" all the decision nodes in the implied oblique decision trees (if there is a way to optimize the networks), and the standard architecture can even "learn" all the leaf nodes due to the parameterized output function g φ .</p><p>Extensions. The construction of (standard) locally constant networks enables several natural extensions due to the flexibility of the neural architecture and the interpretation of decision trees. The original locally linear networks (LLN) f θ , which outputs a linear function instead of a constant function for each region, can be regarded as one extension. Here we discuss two examples.</p><p>• Approximately locally constant networks (ALCN): we can change the activation function while keeping the model architecture of LCN. For example, we can replace ReLU max(0, x) with softplus log(1 + exp(x)), which will lead to an approximately locally constant network, as the softplus function has an approximately locally constant derivative for inputs with large absolute value. Note that the canonical architecture (tabular g) is not compatible with such extension.</p><p>• Ensemble locally constant networks (ELCN): since each LCN can only output 2 M different values, it is limited for complex tasks like regression (akin to decision trees). We can instead use an additive ensemble of LCN or ALCN to increase the capacity. We use g </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">COMPUTATION AND LEARNING</head><p>In this section, we discuss computation and learning algorithms for the proposed models. In the following complexity analyses, we assume g φ to be a linear model. , which can be computationally challenging to obtain. Existing automatic differentiation (e.g., back-propagation) only computes the gradient of a scalar output. Instead, here we propose an efficient dynamic programming procedure which only requires a forward pass:</p><formula xml:id="formula_18">1. ∇ x a 1 1 = o 1 1 × W 1 . 2. ∀i ∈ {2, . . . , M }, ∇ x a i 1 = o i 1 × (W i 1,1:D + i−1 k=1 W i 1,D+k ∇ x a k 1 ),</formula><p>The complexity of the dynamic programming is Θ(M 2 ) due to the inner-summation inside each iteration. Straightforward back-propagation re-computes the partial solutions ∇ x a k 1 for each ∇ x a i 1 , so the complexity is Θ(M 3 ). We can parallelize the inner-summation on a GPU, and the complexity of the dynamic programming and straightforward back-propagation will become Θ(M ) and Θ(M 2 ), respectively. Note that the complexity of a forward pass of a typical network is also Θ(M ) on a GPU. The time complexity of learning LCN by (stochastic) gradient descent is thus Θ(M τ ), where τ denotes the number of iterations. In contrast, the computation of existing oblique decision tree training algorithms is typically data-dependent and thus the complexity is hard to characterize.</p><p>Training LCN and ALCN. Even though LCN is sub-differentiable, whenever o i 1 = 0, the network does not exhibit useful gradient information for learning each locally constant representation ∇ x a i 1 (note that J xã M = [∇ x a 1 1 , . . . , ∇ x a M 1 ]), since, operationally, o i 1 = 0 implies a i 1 ← 0 and there is no useful gradient of ∇ x a i 1 = ∇ x 0 = 0 with respect to model parameters. To alleviate the problem, we propose to leverage softplus as an infinitely differentiable approximation of ReLU to obtain meaningful learning signals for ∇ x a i 1 . Concretely, we conduct the annealing during training:</p><formula xml:id="formula_19">a i 1 = λ t max(0, z i 1 ) + (1 − λ t ) log(1 + exp(z i 1 )), ∀i ∈ [M ], λ t ∈ [0, 1],<label>(5)</label></formula><p>where λ t is an iteration-dependent annealing parameter. Both LCN and ALCN can be constructed as a special case of Eq. (5). We train LCN with λ t equal to the ratio between the current epoch and the total epochs, and ALCN with λ t = 0. Both models are optimized via stochastic gradient descent.</p><p>We also include DropConnect <ref type="bibr" target="#b35">(Wan et al., 2013)</ref> to the weight matrices W i ← drop(W i ) during training. Despite the simple structure of DropConnect in the locally constant networks, it entails a structural dropout on the weights in the corresponding oblique decision trees (see §3.5), which is challenging to reproduce in typical oblique decision trees. In addition, it also encourages the exploration of parameter space, which is easy to see for the raw LCN: the randomization enables the exploration that flips o i 1 = 0 to o i 1 = 1 to establish effective learning signal. Note that the standard DropOut <ref type="bibr" target="#b33">(Srivastava et al., 2014)</ref> is not ideal for the low capacity models that we consider here.</p><p>Training ELCN. Since each ensemble component is sub-differentiable, we can directly learn the whole ensemble through gradient descent. However, the approach is not scalable due to memory constraints in practice. Instead, we propose to train the ensemble in a boosting fashion:</p><p>1. We first train an initial locally constant network g <ref type="figure" target="#fig_0">,[1]</ref> ).</p><formula xml:id="formula_20">[1] φ ( J xã M</formula><p>2. For each iteration e ∈ {2, 3, . . . , E}, we incrementally optimize e e=1 g <ref type="bibr">[e]</ref> φ ( J xã M, <ref type="bibr">[e]</ref> ).</p><p>Note that, in the second step, only the latest model is optimized, and thus we can simply store the predictions of the preceding models without loading them into the memory. Each partial ensemble can be directly learned through gradient descent, without resorting to complex meta-algorithms such as adaptive boosting <ref type="bibr" target="#b9">(Freund &amp; Schapire, 1997)</ref> or gradient boosting <ref type="bibr" target="#b10">(Friedman, 2001)</ref>.  <ref type="table">Table 2</ref>: Main results. The 1 st section refers to (oblique) decision tree methods, the 2 nd section refers to single model extensions of LCN, the 3 rd section refers to ensemble methods, and the last section is GCN. The results of GCN are copied from <ref type="bibr" target="#b38">(Wu et al., 2018)</ref>, where the results in SIDER and Tox21 are not directly comparable due to lack of standard splittings. The best result in each section is in bold letters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>Here we evaluate the efficacy of our models (LCN, ALCN, and ELCN) using the chemical property prediction datasets from MoleculeNet <ref type="bibr" target="#b38">(Wu et al., 2018)</ref>, where random forest performs competitively. We include 4 (multi-label) binary classification datasets and 1 regression dataset. The statistics are available in <ref type="table" target="#tab_1">Table 1</ref>. We follow the literature to construct the feature <ref type="bibr" target="#b38">(Wu et al., 2018)</ref>. Specifically, we use the standard Morgan fingerprint <ref type="bibr" target="#b26">(Rogers &amp; Hahn, 2010)</ref>, 2,048 binary indicators of chemical substructures, for the classification datasets, and 'grid features' (fingerprints of pairs between ligand and protein, see <ref type="bibr" target="#b38">Wu et al. (2018)</ref>) for the regression dataset. Each dataset is splitted into (train, validation, test) sets under the criterion specified in MoleculeNet.</p><p>We compare LCN and its extensions (LLN, ALCN, and ELCN) with the following baselines:</p><p>• (Oblique) decision trees: CART <ref type="bibr" target="#b2">(Breiman et al. (1984)</ref>), <ref type="bibr">HHCART (Wickramarachchi et al. (2016)</ref>; oblique decision trees induced greedily on linear projections), and TAO (Carreira-Perpinán &amp; Tavallali (2018); oblique decision trees trained via alternating optimization). • Tree ensembles: RF <ref type="bibr" target="#b3">(Breiman (2001)</ref>; random forest) and <ref type="bibr">GBDT (Friedman (2001)</ref>; gradient boosting decision trees). • Graph networks: <ref type="bibr">GCN (Duvenaud et al. (2015)</ref>; graph convolutional networks on molecules).</p><p>For decision trees, LCN, LLN, and ALCN, we tune the tree depth in {2, 3, . . . , 12}. For LCN, LLN, and ALCN, we also tune the DropConnect probability in {0, 0.25, 0.5, 0.75}. Since regression tasks require precise estimations of the prediction values while classification tasks do not, we tune the number of hidden layers of g φ in {0, 1, 2, 3, 4} (each with 256 neurons) for the regression task, and simply use a linear model g φ for the classification tasks. For ELCN, we use ALCN as the base model, tune the ensemble size E ∈ {2 0 , 2 1 , . . . , 2 6 } for the classification tasks, and E ∈ {2 0 , 2 1 , . . . , 2 9 } for the regression task. To train our models, we use the cross entropy loss for the classification tasks, and mean squared error for the regression task. Other minor details are available in Appendix B.</p><p>We follow the chemistry literature <ref type="bibr" target="#b38">(Wu et al., 2018)</ref> to measure the performance by AUC for classification, and root-mean-squared error (RMSE) for regression. For each dataset, we train a model for   <ref type="figure" target="#fig_2">Fig. 2a</ref> is an ablation study for LCN and <ref type="figure" target="#fig_2">Fig. 2b-2c</ref> compare different training methods.</p><p>each label, compute the mean and standard deviation of the performance across 10 different random seeds, and report their average across all the labels within the dataset. The results are in <ref type="table">Table 2</ref>.</p><p>Among the (oblique) decision tree training algorithms, our LCN achieves the state-of-the-art performance. The continuous extension (ALCN) always improves the empirical performance of LCN, which is expected since LCN is limited for the number of possible outputs (leaf nodes). Among the ensemble methods, the proposed ELCN always outperforms the classic counterpart, GBDT, and sometimes outperforms RF. Overall, LCN is the state-of-the-art method for learning oblique decision trees, and ELCN performs competitively against other alternatives for training tree ensembles.</p><p>Empirical analysis. Here we analyze the proposed LCN in terms of the optimization and generalization performance in the large HIV dataset. We conduct an ablation study on the proposed method for training LCN in <ref type="figure" target="#fig_2">Figure 2a</ref>. Direct training (without annealing) does not suffice to learn LCN, while the proposed annealing succeed in optimization; even better optimization and generalization performance can be achieved by introducing DropConnect, which corroborates our hypothesis on the exploration effect during training in §3.7 and its well-known regularization effect. Compared to other methods <ref type="figure" target="#fig_2">(Fig. 2b)</ref>, only TAO has a comparable training performance. In terms of generalization ( <ref type="figure" target="#fig_2">Fig. 2c)</ref>, all of the competitors do not perform well and overfit fairly quickly. In stark contrast, LCN outperforms the competitors by a large margin and gets even more accurate as the depth increases. This is expected due to the strong regularization of LCN that uses a linear number of effective weights to construct an exponential number of decision nodes, as discussed in §3.5. Some additional analysis and the visualization of the tree converted from LCN are included in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND CONCLUSION</head><p>We create a novel neural architecture by casting the derivatives of deep networks as the representation, which realizes a new class of neural models that is equivalent to oblique decision trees. The induced oblique decision trees embed rich structures and are compatible with deep learning methods. This work can be used to interpret methods that utilize derivatives of a network, such as training a generator through the gradient of a discriminator <ref type="bibr" target="#b11">(Goodfellow et al., 2014)</ref>. The work opens up many avenues for future work, from building representations from the derivatives of neural models to the incorporation of more structures, such as the inner randomization of random forest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>GH and TJ were in part supported by a grant from Siemens Corporation. The authors thank Shubhendu Trivedi and Menghua Wu for proofreading, and thank the anonymous reviewers for their helpful comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>A.1 PROOF OF LEMMA 6</p><p>Proof. We fix j and do induction on i. Without loss of generality, we assume 1 = r j = r j = 0.</p><p>If i = j, since r j = 0, we have</p><formula xml:id="formula_21">ω r1:i = W i+1 1,1:D + i k=1 W i+1 1,D+k × r k × ω r 1:k−1 , ω r 1:1 = W i+1 1,1:D + i−1 k=1 W i+1 1,D+k × r k × ω r 1:k−1 .</formula><p>Hence, we have ω r1:i − ω r 1:1 = (W i+1 1,D+i × r i ) × ω r1:i−1 = α × ω r1:j−1 . We assume the statement holds for up to some integer i ≥ j:</p><formula xml:id="formula_22">ω r1:i − ω r 1:i = α × ω r1:j−1 , for some α ∈ R.</formula><p>For i + 1, we have</p><formula xml:id="formula_23">ω r1:i+1 =W i+2 1,1:D + i+1 k=1 W i+2 1,D+k × r k × ω r 1:k−1 =W i+2 1,1:D + j−1 k=1 W i+2 1,D+k × r k × ω r 1:k−1 + W i+2 1,D+j × r j × ω r1:j−1 + i+1 k=j+1 W i+2 1,D+k × r k × ω r 1:k−1 =W i+2 1,1:D + j−1 k=1 W i+2 1,D+k × r k × ω r 1:k−1 + W i+2 1,D+j × r j × ω r1:j−1 + i+1 k=j+1 W i+2 1,D+k × r k × (ω r 1:k−1 + α k × ω r1:j−1 ), for some α k ∈ R =W i+2 1,1:D + i+1 k=1 W i+2 1,D+k × r k × ω r 1:k−1 + (W i+2 1,D+j × r j + i+1 k=j+1 W i+2 1,D+k × r k × α k ) × ω r1:j−1 =ω r 1:i+1 + α × ω r1:j−1 , for some α ∈ R B IMPLEMENTATION DETAILS</formula><p>Here we provide the full version of the implementation details.</p><p>For the baseline methods:</p><p>• CART, HHCART, and TAO: we tune the tree depth in {2, 3, . . . , 12}.</p><p>• RF: we use the scikit-learn <ref type="bibr" target="#b23">(Pedregosa et al., 2011)</ref> implementation of random forest. We set the number of estimators as 500.</p><p>• GBDT: we use the scikit-learn <ref type="bibr" target="#b23">(Pedregosa et al., 2011)</ref> implementation of gradient boosting trees. We tune the number of estimators in {2 3 , 2 4 , . . . , 2 10 }.</p><p>For LCN, LLN, and ALCN, we run the same training procedure. For all the datasets, we tune the depth in {2, 3, . . . , 12} and the DropConnect probability in {0, 0.25, 0.5, 0.75}. The models are optimized with mini-batch stochastic gradient descent with batch size set to 64. For all the classification tasks, we set the learning rate as 0.1, which is annealed by a factor of 10 for every 10 epochs (30 epochs in total). For the regression task, we set the learning rate as 0.0001, which is annealed by a factor of 10 for every 30 epochs (60 epochs in total).</p><p>Both LCN and ALCN have an extra fully-connected network g φ , which transforms the derivatives J xã M to the final outputs. Since regression tasks require precise estimation of prediction values while classification tasks do not, we tune the number of hidden layers of g φ in {0, 1, 2, 3, 4} (each with 256 neurons) for the regression dataset, and simply use a linear g φ for the classification datasets.</p><p>For ELCN, we fix the depth to 12 and tune the number of base models E ∈ {2 0 , 2 1 , . . . , 2 6 } for the classification tasks, and E ∈ {2 0 , 2 1 , . . . , 2 9 } for the regression task. We set the DropConnect probability as 0.75 to encourage strong regularization for the classification tasks, and as 0.25 to impose mild regularization for the regression task (because regression is hard to fit). We found stochastic gradient descent does not suffice to incrementally learn the ELCN, so we use the AMSGrad optimizer <ref type="bibr" target="#b25">(Reddi et al., 2018)</ref> instead. We set the batch size as 256 and train each partial ensemble for 30 epochs. The learning rate is 0.01 for the classification tasks, and 0.0001 for the regression task.</p><p>To train our models, we use the cross entropy loss for the classification tasks, and mean squared error for the regression task. In this section, we investigate the learning of "unobserved branching / leaves" discussed in §3.6.</p><p>The "unobserved branching / leaves" refer to the decision and leaf nodes of the oblique decision tree converted from LCN, such that there is no training data that are routed to the nodes. It is impossible for traditional (oblique) decision tree training algorithms to learn the values of such nodes (e.g., the output value of a leaf node in the traditional framework is based on the training data that are routed to the leaf node). However, the shared parameterization in our oblique decision tree provides a means to update such unobserved nodes during training (see the discussion in §3.6).</p><p>Since the above scenario in general happens more frequently in small datasets than in large datasets, we evaluate the scenario on the small Bace dataset (binary classification task). Here we empirically analyze a few things pertaining to the unobserved nodes:</p><p>• # of training patterns: the number of distinct end-to-end activation / decision patterns r 1:M encountered in the training data. • # of testing patterns: the number of distinct end-to-end activation / decision patterns r 1:M encountered ib the testing data. • # of testing patterns -training patterns: the number of distinct end-to-end activation / decision patterns r 1:M that is only encountered in the testing data but not in the training data. • Ratio of testing points w/ unobserved patterns: the number of testing points that yield unobserved patterns divided by the total number of testing points. • Testing performance -observed patterns: here we denote the number of testing data as n, the prediction and label of the i th asŷ i ∈ [0, 1] and y i ∈ {0, 1}, respectively. We collect the subset of indices I of the testing data such that their activation / decision patterns r 1:M are observed in the training data, and then compute the performance of their predictions. Since the original performance is measured by AUC, here we generalize AUC to measure a subset of points I as: .</p><p>When I = [n], the above measure recovers AUC. • Testing performance -unobserved patterns: the same as above, but use I for the testing data such that their activation / decision patterns r 1:M are unobserved in the training data.</p><p>The results are in <ref type="table" target="#tab_4">Table 3</ref>. There are some interesting findings. For example, there is an exponential number of possible patterns, but the number of patterns that appear in the dataset is quite small. The ratio of testing points with unobserved patterns is also small, but these unobserved branching / leaves seem to be controlled properly. They do not lead to completely different performance compared to those that are observed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 VISUALIZATION</head><p>Here we visualize the learned locally constant network on the HIV dataset in the representation of its equivalent oblique decision tree in <ref type="figure" target="#fig_4">Fig. 3</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Toy examples for the equivalent representations of the same mappings for different M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Space complexity. The space complexity of LCN is Θ(M D) for representing decision nodes and Θ(M DL) for representing leaf nodes. In contrast, the space complexity of classic oblique decision trees is Θ((2 M − 1)D) for decision nodes and Θ(2 M L) for leaf nodes. Hence, our representation improves the space complexity over classic oblique decision trees exponentially.Computation and time complexity. LCN and ALCN are built on the gradients of all the neurons J xã M = [∇ x a M 1 , . . . , ∇ x a 1 1 ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Empirical analysis for oblique decision trees on the HIV dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>y i &gt; y j ] I[ŷ i &gt;ŷ j ] + 0.5I[ŷ i =ŷ j ] + I[y i &lt; y j ] I[ŷ i &lt;ŷ j ] + 0.5I[ŷ i =ŷ j ] i∈I n j=1 I[y i &gt; y j ] + I[y i &lt; y j ])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Since the dimension of Morgan fingerprint (Rogers &amp; Visualization of learned locally constant network in the representation of oblique decision trees using the proof of Theorem 3. The number in the leaves indicates the ranking of output probability among the 16 leaves (the exact value is not important). See the descriptions in Appendix C.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>( J xã  M,[e]  ) to denote a base model in the ensemble, and denote the ensemble with E models as</figDesc><table><row><cell>[e]</cell></row><row><cell>E e=1 g φ ( J xã [e] M,[e] ).</cell></row></table><note>φ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics</figDesc><table><row><cell>Dataset</cell><cell>Bace</cell><cell>HIV</cell><cell>SIDER</cell><cell>Tox21</cell><cell>PDBbind</cell></row><row><cell>Task</cell><cell cols="5">(Multi-label) binary classification Regression</cell></row><row><cell>Number of labels</cell><cell>1</cell><cell>1</cell><cell>27</cell><cell>12</cell><cell>1</cell></row><row><cell>Number of data</cell><cell cols="2">1,513 41,127</cell><cell>1,427</cell><cell>7,831</cell><cell>11,908</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Analysis for "unobserved decision patterns" of LCN in the Bace dataset.</figDesc><table><row><cell>Depth</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell></row><row><cell># of possible patterns</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell></row><row><cell># of training patterns</cell><cell>72</cell><cell>58</cell><cell>85</cell><cell>103</cell><cell>86</cell></row><row><cell># of testing patterns</cell><cell>32</cell><cell>31</cell><cell>48</cell><cell>49</cell><cell>40</cell></row><row><cell># of testing patterns -training patterns</cell><cell>5</cell><cell>2</cell><cell>11</cell><cell>8</cell><cell>11</cell></row><row><cell>Ratio of testing points w/ unobserved patterns</cell><cell>0.040</cell><cell>0.013</cell><cell>0.072</cell><cell>0.059</cell><cell>0.079</cell></row><row><cell>Testing performance -observed patterns</cell><cell cols="5">0.8505 0.8184 0.8270 0.8429 0.8390</cell></row><row><cell>Testing performance -unobserved patterns</cell><cell cols="5">0.8596 0.9145 0.8303 0.7732 0.8894</cell></row><row><cell cols="5">C SUPPLEMENTARY EMPIRICAL ANALYSIS AND VISUALIZATION</cell><cell></cell></row><row><cell>C.1 SUPPLEMENTARY EMPIRICAL ANALYSIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that each o i j is again a function of x, where we omit the dependency for brevity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The boundary of the polyhedron depends on the specific definition of the activation pattern, so, under some definition in literature, the resulting convex polyhedra may not be disjoint in the boundary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In practice, we also include each bias a i 1 − (∇xa i 1 ) x, which is omitted here to simplify exposition.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The proof follows by induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROOF OF THEOREM 8</head><p>Proof. We first prove that we can represent any g φ ( J xã M ) as g(õ M ). Note that for any x mapping to the same activation patternõ M , the Jacobian J xã M is constant. Hence, we may re-write the standard architecture g φ ( J xã M ) as g φ ( J(õ M )), where J(õ M ) is the Jacobian corresponding to the activation patternõ M . Then we can set g(·) g φ ( J(·)), which concludes the first part of the proof.</p><p>To prove the other direction, we first prove that we can also write the activation pattern as a function of the Jacobian. We prove this by layer-wise induction (note thatõ  The derivation implies that we may re-write the canonical architecture g(õ M ) as g(õ( J xã M )),</p><p>is the activation pattern corresponding to the Jacobian J xã M . Hence, it suffices to establish that there exists a feed-forward network g φ such that g φ ( J xã M ) = g(õ( J xã M )) for at most 2 M distinct J xã M , which can be found by the Theorem 2.5 of <ref type="bibr" target="#b13">Hornik et al. (1989)</ref> or the Theorem 1 of <ref type="bibr" target="#b40">Zhang et al. (2017)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Global tree optimization: A non-greedy decision tree algorithm. Computing Science and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kristin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="156" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimal classification trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="1039" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Wadsworth and Brooks</publisher>
			<pubPlace>Monterey, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast training algorithms for multilayer neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="354" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alternating optimization of decision trees, with application to learning sparse oblique trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Miguel A Carreira-Perpinán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tavallali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1211" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A machine learning method for generation of a neural network architecture: A continuous id3 algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Cios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="280" to="291" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3603" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Predicting Structured Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards robust, locally linear deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-He</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On oblique random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bjoern H Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Splitthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Koethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="453" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Oc1: A randomized algorithm for building oblique decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sreerama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kasif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salzberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beigel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="322" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A system for induction of oblique decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sreerama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kasif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient non-greedy optimization of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1729" to="1737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Predicting the future relevance of research institutions-the winning solution of the kdd cup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Sandulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Chiru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02728</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Entropy nets: from decision trees to neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishwar Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sethi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1605" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge transfer with Jacobian matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4723" to="4731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chervonenkis</surname></persName>
		</author>
		<idno type="DOI">10.1137/1116025</idno>
		<ptr target="https://doi.org/10.1137/1116025" />
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="264" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Tsui-Wei Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duane</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<title level="m">Towards fast computation of certified robustness for relu networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hhcart: An oblique decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dc Wickramarachchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><forename type="middle">Garcia</forename><surname>Morillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06988</idno>
		<title level="m">Deep neural decision trees</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">here we only visualize the top-K weights (in terms of the absolute value) for each decision node. We also normalize each weight such that the 1 norm of each weight is 1. Since the task is evaluated by ranking (AUC), we visualize the leaf nodes in terms of the ranking of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>is quite high (2,048. output probability among the leaf nodes (the higher the more likely</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Our main contribution here is the algorithm that transforms an LCN to an oblique decision tree, rather than the visualization of oblique decision trees</title>
		<imprint/>
	</monogr>
	<note>Note that a complete visualization requires some engineering efforts. so we only provide the initial visualization as a proof of concept</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
