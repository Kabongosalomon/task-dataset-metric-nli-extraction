<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CODAH: An Adversarially-Authored Question Answering Dataset for Common Sense</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-07-26">26 Jul 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chen</surname></persName>
							<email>y-chen@u.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">D &amp;apos;</forename><surname>Arcy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisa</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
							<email>jared.fern@u.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>ddowney@eecs.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CODAH: An Adversarially-Authored Question Answering Dataset for Common Sense</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-07-26">26 Jul 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Commonsense reasoning is a critical AI capability, but it is difficult to construct challenging datasets that test common sense. Recent neural question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks. These systems do not possess human-level common sense, but are able to exploit limitations of the datasets to achieve human-level scores.</p><p>We introduce the CODAH dataset, an adversarially-constructed evaluation dataset for testing common sense. CODAH forms a challenging extension to the recently-proposed SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video. To produce a more difficult dataset, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-ofthe-art neural question answering systems. Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation). We create 2.8k questions via this procedure and evaluate the performance of multiple state-of-the-art question answering systems on our dataset. We observe a significant gap between human performance, which is 95.3%, and the performance of the best baseline accuracy of 67.5% by the BERT-Large model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Enabling commonsense reasoning in machines is a longstanding challenge in AI. The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text.</p><p>The Situations With Adversarial Generations (SWAG) dataset <ref type="bibr" target="#b12">(Zellers et al., 2018)</ref> introduced a large-scale benchmark for commonsense question answering in the form of multiple choice sentence completion questions describing situations as observed in video. However, while SWAG was constructed to be resistant to certain baseline algorithms, powerful subsequent methods were able to perform very well on the dataset. In particular, the development of the transformer architecture <ref type="bibr" target="#b10">(Vaswani et al., 2017)</ref> has led to powerful pre-trained language model representations, including the OpenAI Transformer Language Model <ref type="bibr" target="#b7">(Radford et al., 2018)</ref> and the Bidirectional Encoder Representations from Transformers (BERT) model <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>. BERT achieved new state-of-the-art performance on SWAG that exceeded even that of a human expert. However, BERT does not possess human-level common sense in general, as our experiments demonstrate. It is instead able to exploit regularities in the SWAG dataset to score high. This motivates the construction of additional datasets that pose new challenges, and serve as more reliable benchmarks for commonsense reasoning systems.</p><p>In this work, we introduce the COmmonsense Dataset Adversarially-authored by Humans (CODAH) for commonsense question answering in the style of SWAG multiple choice sentence completion. We propose a novel method for question generation, in which human annotators are educated on the workings of a state-of-the-art question answering model, and are asked to submit questions that adversarially target the weaknesses. Annotators are rewarded for submissions in which the model fails to identify the correct sentence completion both before and after fine-tuning on a sample of the submitted questions, encouraging the creation of questions that are not easily learnable.</p><p>We experimentally demonstrate that CODAH's generation procedure produces a dataset with a large gap between system performance and human expert accuracy, even when using state-ofthe-art pre-trained language models with and without fine-tuning on the large SWAG dataset. Using a model initially fine-tuned on SWAG, we find that the OpenAI GPT-1 and BERT neural question answering models yield 65.5% and 69.5% accuracy, respectively, on the CODAH dataset in cross-validation. Thus, cross-validating on CO-DAH can form a challenging additional evaluation for SWAG-style commonsense QA systems. Human evaluators achieve 95.3% accuracy, which is substantially higher than the 85.0% <ref type="bibr" target="#b12">(Zellers et al., 2018)</ref> and 87.7% <ref type="bibr" target="#b3">(Ghaeini et al., 2018)</ref> human performance on the SWAG and SNLI natural language inference tasks. The high human performance suggests that answers to the CODAH questions are in fact commonsense knowledge. Finally, we also analyze differences in performance across questions that target different types of commonsense reasoning, including quantitative, negation, and object reference, showing consistency in performance for BERT and GPT on the proposed categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prior work in question answering has largely focused on the development of reading comprehension-based question answering and resulted in the creation of several large datasets for factoid extraction such as SQuAD <ref type="bibr" target="#b9">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b8">(Rajpurkar et al., , 2018</ref> and the Google Natural Questions datasets <ref type="bibr" target="#b5">(Kwiatkowski et al., 2019)</ref>. In these tasks, extraction of correct answers from the provided context requires little external world knowledge, understanding of intents, or other commonsense knowledge.</p><p>Earlier work has established multiple benchmarks for natural language inference and linguistic entailment with the release SNLI <ref type="bibr" target="#b0">(Bowman et al., 2015)</ref> and MultiNLI datasets <ref type="bibr" target="#b11">(Williams et al., 2018)</ref>. In these tasks, systems must identify whether a hypothesis agrees with or contradicts a provided premise. In these datasets, determining entailment solely relies upon the provided premise and does not require a question answering system to utilize external knowledge. More recently, the SWAG dataset <ref type="bibr" target="#b12">(Zellers et al., 2018)</ref> directly targets natural language inference that leverages commonsense knowledge. SWAG multiple choice completion questions are constructed using a video caption as the ground truth with incorrect counterfactuals created using adversarially-filtered generations from an LSTM language model. State-of-the-art models for natural language inference have rapidly improved and approach human performance, which leaves little room for continued improvement on current benchmarks.</p><p>Generation of adversarial examples has also been used to increase the robustness of NLP systems as part of the Build it, Break It, The Language Edition Workshop <ref type="bibr" target="#b2">(Ettinger et al., 2017)</ref>. In this workshop, builders designed systems for Sentiment Analysis and Question Answering Driven Semantic Role Labeling tasks and were evaluated on the accuracy of their models on adversarial test cases designed by breakers. Whereas Build It Break It adversarial generation required submissions to match the format of a starter dataset and offered limited adversarial access to the target NLP systems, the CODAH construction procedure allows for entirely new questions and provide adversaries with a target model throughout the submission process, allowing workers to experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The CODAH Dataset</head><p>Our dataset contains multiple choice sentence completion questions in the format of the SWAG dataset. Examples of the questions are shown in <ref type="table" target="#tab_0">Table 1</ref>. Each question consists of a prompt sentence, the subject of the subsequent sentence, and four candidate completions, such that exactly one candidate completion is consistent with common sense. This task definition allows for easy evaluation by many state-ofthe-art models, such as BERT and GPT-1, and enables us to utilize the large SWAG dataset for pre-training. The full dataset is available at https://github.com/Websail-NU/CODAH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Production</head><p>We collected questions via a Web-based system. Participants were asked to compose a complete question, including the prompt, subject, and the four candidate completions. They would then be presented with the response of a pre-trained BERT model to their question. The pre-trained model consisted of a BERT-base model fine-tuned on the SWAG training set for 3 epochs with a batch size of 8. This model achieved 80.68% accuracy on Category Description Example</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Idioms</head><p>Including phrases whose meaning cannot be readily interpreted from the meaning of constituent parts A man on his first date wanted to break the ice. He drank all of his water. threw the ice at the wall. looked at the menu. made a corny joke.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negation</head><p>Including negators to dictate the meaning of the sentence</p><p>The man's rebuttal was clearly not nonsensical. The rebuttal has nothing to do with sense. had some reasons associated with it. did not make any sense. was funny.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polysemy</head><p>Testing the understanding of multiple meanings of a single word</p><p>An architect retrieves his compass. He computes the area of a circle explores the open sea draws building dimensions on a canvas uses his compass to find the north cardinal direction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Requiring understanding of reference to one of multiple subjects</p><p>Rose is walking the dog while Joseph cooks dinner. Rose is following a new recipe. enjoys the fresh air. wags her tail with joy. cuts tomatoes for the soup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Involving basic arithmetic calculations or comparisons</head><p>A woman is walking two dogs and carrying a cat on her way to her car. She puts all three animals in the back seat before driving off. puts all four animals in the back seat before driving off. puts both animals in the back seat before driving off. puts all nine animals in the back seat before driving off. the SWAG validation set. The ability to obtain real-time feedback about the model's answers allowed participants to explore areas of weakness and design challenging questions. All submitted questions were added to the dataset, whether they fooled the baseline model or not.</p><p>Annotators were provided explicit incentives to produce questions that the model answered incorrectly. The vast majority of submissions were contributed by university computer science students, who were familiar with neural network question answering systems. Students were rewarded with extra credit points for submitting valid questions that fooled the baseline model. Further, students could earn an equal number of extra credit points for questions that fooled the model when evaluated in cross-validation, after fine-tuning on other submitted questions. This protocol was designed to encourage the creation of challenging and valid commonsense questions that are also free from stylistic annotation artifacts or redundancy, which would reduce the difficulty of the questions after fine-tuning and reduce the returns on their submissions. A small portion of the dataset was submitted anonymously by other individuals.</p><p>We received a total of 4,149 raw questions, which were read and cleaned by four annotators (the authors). During cleaning, the answer choice order was shuffled and model's output answer were hidden from the annotator. We removed submissions with multiple or no distinctive commonsense answers, spelling or grammatical errors, incorrect answers, as well as duplicate submissions. The remaining questions were judged natural and easily answerable from common sense with minimal ambiguity and dispute. The cleaning operation produced our current 2,801-question dataset.</p><p>Our 2,801-question dataset contains submissions from 116 named participants. The median, mean and standard deviation of the number of valid questions submitted by named individuals are 20.00, 21.38, and 13.86. The most prolific contributor submitted 86 questions. Anonymous participants contributed 321 questions, which is 11% of the final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the dataset on state-of-the-art neural question answering systems built on the BERT and GPT-1 architecture and provide multiple baselines. The models and experiment setups are discussed below. We also analyze the questions to identify distinctive categories of commonsense reasoning that provide a finer-grained understanding of model performances. In addition, the ablation experiments on dataset size and the use of fine-tuning on SWAG data allow us to further understand the impact of the relatively small size of CODAH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question Categorization</head><p>One of our goals is to analyze how system and human performance varies across questions in CODAH that employ different types of common sense. Therefore, we identified a small number of unambiguous categories of common sense, such as questions involving quantitative reasoning or negation. These categories only apply to a portion of the questions in our dataset, but have the advantage of being unambiguous and in many cases predictive of low system performance. In earlier attempts to devise categories to cover all questions, similar to analysis performed for textual entailment (LoBue and Yates, 2011), we found the inter-annotator agreement on such complete categorizations to be substantially lower (at &lt;0.4), even after iterating on category definitions.</p><p>We manually inspected all questions in our dataset and annotated each with one or more category labels, representing all types of reasoning required to identify the correct answer and eliminate incorrect ones. The descriptions and examples of these categories are found in <ref type="table" target="#tab_0">Table 1</ref>. Four human annotators (the authors) categorized the questions, and we calculated a Feiss' Kappa score of 0.63 between the annotators over an additional 50 questions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">OpenAI GPT-1</head><p>We also evaluate a pre-trained GPT model implemented in PyTorch. As described in <ref type="bibr" target="#b7">Radford et al. (2018)</ref>, this model consists of a 12-layer decoder transformer with 12 attention heads and 3,072dimensional hidden states. Our fine-tuning configuration is the same as described in the original paper: a batch size of 32, learning rate of 6.25e-5, linear learning rate decay over 3 epochs (with warmup over 0.2% of training), and λ of 0.5 (where λ is a tuning coefficient that balances language-modeling loss and multiple-choice loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Evaluation</head><p>We evaluate the models on several different train and test configurations described below. The CO-DAH dataset is evaluated in 5-fold stratified crossvalidation which balances the distribution of question categories in each fold.</p><p>• CODAH: Cross-validation fine-tuning on the CODAH dataset. The CODAH 80% experiment represents the standard cross-validation setting on the full dataset, training on 80% of the data in each fold and evaluating on the remaining 20%. The 60%, 40% and 20% ablation experiments are trained on a smaller portion of the CODAH dataset for each fold, but are evaluated in on the same test set which consists of 20% of the full dataset. The question categories are balanced in both training set and test set. This makes the results from the experiments more comparable with each other. Three trials are conducted for all settings; the mean and standard deviation of the model accuracy are reported in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>• SWAG+CODAH: Fine-tuned on SWAG first, then fine-tuned again in cross-validation on CODAH. Ablation experiments are conducted in the same way as in the CODAHonly setting above, with the same dataset splits for training. The mean and standard deviation of the three trials are reported in <ref type="table" target="#tab_4">Table  3</ref>.</p><p>• SWAG only: Fine-tuned on SWAG and evaluated on CODAH. Only one trial is conducted.</p><p>• Answer only: Cross-validation fine-tuning on the full CODAH dataset with the questions left blank (in both training and testing). Only one trial is conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">BERT Hyperparameter Search</head><p>The authors of BERT suggest a hyperparameter grid search to perform when training BERT on new datasets. We perform a slightly modified version of this grid search in 5-fold cross validation over the training set in each fold of the cross-validation experiments described in Section 4.3. That is, the complete grid search is run ten times, once for each fold of the CODAH and SWAG+CODAH settings. For the answer-only setting, we use the optimal hyperparameters found for the CODAH-only setting. Also, when training the initial SWAG model we use the hyperparameters recommended in the BERT paper, namely a batch size of 16, learning rate of 2e-5, and 3 epochs.</p><p>In our initial experiments, we found that a lower learning rate and more training epochs produced higher accuracy on CODAH, so we replaced the 5e-5 learning rate in the original grid search with 1e-5, and we added a 6-epoch setting. The final hyperparameter grid is as follows:</p><p>• Batch size: 16, 32 • Learning rate: 1e-5, 2e-5, 3e-5 • Number of epochs: 3, 4, 6 In addition, we observed that in rare cases BERT fails to train; that is, after several training epochs it has accuracy approximately equal to that of random guessing. To combat this, any time BERT fails to achieve 30% or greater accuracy on the test set for a given fold, we run it again with a different random seed. If it fails five times, take the average of those five trials as the final accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>Results for the above configurations are shown in <ref type="table" target="#tab_4">Table 3</ref>. As a baseline, we evaluate both models on the full SWAG training and validation sets, providing an accuracy of 84.2% on BERT and 80.2% on GPT. To adjust for the difference in size between our dataset and SWAG, we also train the models on a sample of 2,241 SWAG questions (the size of the training set in each of CODAH's crossvalidation folds) and evaluate them on the full SWAG validation set. This produces an accuracy of 75.2% for BERT (using the cross-validation grid search) and 63.6% for GPT.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Human Evaluation</head><p>For each category, we measure the accuracy of the BERT and GPT models trained on SWAG+CODAH. We also measure human accuracy as a baseline. Human accuracy was calculated as the mean accuracy of three human annotators, covering 707 dataset questions in total. Human annotators answered 95.3% of questions correctly, presenting a 7-fold reduction in error compared to the fine-turned BERT model. Interannotator agreement was computed over a set of 50 additional questions with a pairwise average Cohen-Kappa score of 0.89, which is interpreted as almost perfect agreement by some guidelines.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Based on our experiments, we find that model performance on CODAH is substantially lower than those seen on SWAG, which has seen models achieve over 85% accuracy. We observed a decrease of 14.7% on both models between the accuracy on SWAG and the accuracy on our SWAG+CODAH setting. This is especially significant since human error on CODAH is 4.7%-less than a third of the 15% expert error on the SWAG dataset. This suggests that CODAH is challenging to our QA systems because of the difficult commonsense reasoning involved, and not because of ambiguity or intractability in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Question Categories</head><p>The logic categories including Quantitative and Negation are especially difficult for our models, seeing some of the lowest accuracies from both models, in contrast to the 99.0% weighted average human accuracy on these categories. Surprisingly, both models performed very well on the Idioms category, suggesting that our neural systems may be capable of learning idioms just like other semantic knowledge. Further identification of additional distinctive and interesting categories that cover the entire dataset may prove very useful in directing our efforts towards aspects of our commonsense QA systems that require the most attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Annotation Artifacts</head><p>Annotation artifacts are known to exist in many datasets and may be exploited by supervised models to achieve inflated performances <ref type="bibr" target="#b4">(Gururangan et al., 2018)</ref>. In CODAH, we did not explicitly filter questions with artifacts or try to detect them. We instead incentivize the question authors, who have some knowledge of how the learners work, to avoid introducing noticeable artifacts in their submissions, as explained in Section 3.1.</p><p>Our results show that artifacts do not provide sufficient signal for state-of-the-art neural models to come close to human-level accuracy on our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Answer-Only Baseline</head><p>In the answer-only experiment (where questions are omitted during training and testing), we found that BERT achieves 52.2% accuracy and GPT-1 achieves 53.4% accuracy, both of which are approximately the equivalent of narrowing four random options down to two. By comparing this to the CODAH experiment setting, we can interpret these results as an indication of the extent to which the signal was in the answers. While this could be due to artifacts, such as the right answer commonly being of a certain length, we also observed that in many cases, distinguishing between reasonable and ridiculous answers (without seeing the premise) is a part of commonsense reasoning. For example, a commonsense reasoner would be able to rule out the choice "picks up his phone and calls his mom to tell her he doesn't have his phone" without seeing the premise, as a contradiction is contained in the answer. Similarly, "kicks a field goal, celebrates by transforming into a fish, and then quits football" is unlikely to be veracious regardless of the hidden subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Dataset Size</head><p>Our experiments show that CODAH forms a challenging extension to the existing SWAG dataset. Even when we train a system to perform near human-level on SWAG, and then fine-tune on CO-DAH, the system still struggles to answer CO-DAH questions correctly. However, CODAH is also smaller than SWAG, raising the question of whether CODAH remains more difficult than SWAG when training set size is equalized. When we restrict to a subset of SWAG of the same number of questions as CODAH, we find that SWAG and CODAH have comparable accuracy for GPT (63.6% on reduced-size SWAG vs. 62.3% for CO-DAH) but on BERT there is larger gap (75.2% accuracy on reduced-size SWAG, vs. 67.5% accuracy on CODAH). This suggests that CODAH questions are somewhat more difficult than SWAG questions when data set size is equalized, but the performance gap is much smaller than comparing with full-size SWAG. In addition, our ablation experiments where we train on different sizes of CO-DAH suggest that BERT and GPT performance improve with more data on both the CODAH-only and SWAG+CODAH settings, but the rate of improvement slows down as data size increases. The slowdown in improvement is more significant for BERT than GPT. Our results suggest two recommendations for dataset construction which we hope to evaluate in future work. The first is, rather than using a single protocol to collect one monolithic dataset, the community may be able to obtain more challenging data by aggregating a variety of distinct, independently-gathered datasets that follow a similar format. For example, pre-training on SWAG and evaluating on CODAH forms a more challenging benchmark than training and testing on SWAG alone. Secondly, if we wish to use our adversarial collection approach to grow CODAH to tens of thousands of examples, we should update our system as new data arrives, so that contributors are able to tune their questions to remain difficult for the strongest, most up-to-date version of the system. Under such a data collection scheme, we may need to increase the reward for fooling the model in cross-validation compared to that for fooling the current model (whereas, these two rewards were equal in CODAH), in order to disincentivize adversarial attacks that manipulate the current model to make it easy to fool on subsequent questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present CODAH, a commonsense question answering dataset that is adversarially-constructed by allowing humans to view feedback from a pre-trained model and use this information to design challenging commonsense questions. Our experimental results show that CODAH questions present a complementary extension of the SWAG dataset, testing additional modes of common sense.</p><p>We identify specific categories of commonsense questions to determine types of reasoning that are more challenging for existing models. In particular, we note that Quantitative questions have low accuracy for both BERT and GPT. A more detailed analysis into why models struggle to reason about numbers as well as development of more detailed categories of commonsense reasoning are items for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Question categories, descriptions, and examples</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>shows the distribution of labels over the entire dataset.</figDesc><table><row><cell>Category</cell><cell cols="2">Count Percentage</cell></row><row><cell>Idioms</cell><cell>249</cell><cell>8.8</cell></row><row><cell>Reference</cell><cell>133</cell><cell>4.8</cell></row><row><cell>Polysemy</cell><cell>108</cell><cell>3.9</cell></row><row><cell>Negation</cell><cell>116</cell><cell>4.1</cell></row><row><cell>Quantitative</cell><cell>87</cell><cell>3.1</cell></row><row><cell>Other</cell><cell>2108</cell><cell>75.3</cell></row><row><cell>Total</cell><cell>2801</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Distribution of question categories.</figDesc><table><row><cell>4.2 Models</cell></row><row><cell>4.2.1 BERT</cell></row><row><cell>We evaluate a pre-trained BERT-Large</cell></row><row><cell>(Devlin et al., 2018) implemented in PyTorch</cell></row><row><cell>on the CODAH dataset. This model consists of</cell></row><row><cell>a 24-layer network, with 1,024 hidden units per</cell></row><row><cell>layer, 16-heads and a total of 340M parameters.</cell></row><row><cell>For fine-tuning, we choose hyperparameters as</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of BERT and GPT on different training settings when tested on CODAH. Numbers in parentheses represent the standard deviation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>displays the accuracy of the human annotators and neural networks on each category.</figDesc><table><row><cell>Category</cell><cell>Human %</cell><cell>BERT %</cell><cell>GPT-1 %</cell></row><row><cell>Quantitative</cell><cell>97.6</cell><cell cols="2">58.2 (3.69) 48.7 (1.76)</cell></row><row><cell>Polysemy</cell><cell>91.7</cell><cell cols="2">63.9 (1.60) 56.2 (2.87)</cell></row><row><cell>Negation</cell><cell>100</cell><cell cols="2">65.2 (2.17) 62.4 (2.63)</cell></row><row><cell>Idioms</cell><cell>97.5</cell><cell cols="2">71.4 (1.23) 71.9 (1.75)</cell></row><row><cell>Reference</cell><cell>100</cell><cell cols="2">72.4 (0.87) 70.7 (1.99)</cell></row><row><cell>Other</cell><cell>94.9</cell><cell cols="2">70.2 (0.28) 65.9 (0.31)</cell></row><row><cell>Total</cell><cell>95.3</cell><cell cols="2">69.5 (0.34) 65.5 (0.24)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Class-wise and overall accuracy of human annotators and neural network models, sorted by BERT performance on the proposed categories. Numbers in parentheses represent the standard deviation.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, Yiben Yang, and Chandra Bhagavatula for helpful comments and feedback. We also thank the students of Northwestern EECS 349 Fall 2018, whose creativity and insight made this work possible. This work was supported in part by NSF Grant IIS-1351029 and the Allen Institute for Artificial Intelligence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards linguistically generalizable nlp systems: A workshop and shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</title>
		<meeting>the First Workshop on Building Linguistically Generalizable NLP Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dr-bilstm: Dependent reading bidirectional lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1460" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics</title>
		<editor>Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Types of common-sense knowledge needed for recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lobue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="329" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
