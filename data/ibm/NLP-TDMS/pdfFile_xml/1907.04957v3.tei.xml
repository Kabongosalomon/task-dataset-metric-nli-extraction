<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision-and-Dialog Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Murray</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Cakmak</surname></persName>
							<email>mcakmak@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Vision-and-Dialog Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robots navigating in human environments should use language to ask for assistance and be able to understand human responses. To study this challenge, we introduce Cooperative Vision-and-Dialog Navigation, a dataset of over 2k embodied, human-human dialogs situated in simulated, photorealistic home environments. The Navigator asks questions to their partner, the Oracle, who has privileged access to the best next steps the Navigator should take according to a shortest path planner. To train agents that search an environment for a goal location, we define the Navigation from Dialog History task. An agent, given a target object and a dialog history between humans cooperating to find that object, must infer navigation actions towards the goal in unexplored environments. We establish an initial, multi-modal sequence-to-sequence model and demonstrate that looking farther back in the dialog history improves performance. Sourcecode and a live interface demo can be found at https://cvdn.dev/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialog-enabled smart assistants, which communicate via natural language and occupy human homes, have seen widespread adoption in recent years. These systems can communicate information, but do not manipulate objects or actuate. By contrast, manipulation-capable and mobile robots are still largely deployed in industrial settings, but do not interact with human users. Dialogenabled robots can bridge this gap, with natural language interfaces helping robots and non-experts collaborate to achieve their goals <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Navigating successfully from place to place is a fundamental need for a robot in a human environment and can be facilitated, as with smart assistants, through dialog. To study this challenge, we introduce Cooperative Vision-and-Dialog Navigation (CVDN), an English language dataset situated in the Matterport Room-2-Room (R2R) simulation environment <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>  <ref type="figure">(Figure 1</ref>). CVDN can be used to train navigation agents, such as language teleoperated home and office robots, that ask targeted questions about where to go next when unsure. Additionally, CVDN can be used to train agents that can answer such questions given expert knowledge of the environment to enable automated language guidance for humans in unfamiliar places (e.g., asking for directions in an office building). The photorealistic environment used in CVDN may enable agents trained in simulation to conduct and understand dialog from humans to transfer those skills to the real world. The dialogs in CVDN contain nearly three times as many words as R2R instructions, and cover average path lengths more than three times longer than paths in R2R.</p><p>In Section 2 we situate the Vision-and-Dialog Navigation paradigm. After introducing CVDN (Section 3), we create the Navigation from Dialog History (NDH) task with over 7k instances from CVDN dialogs (Section 4). We evaluate an initial, sequence-to-sequence model on this task (Section 5). The sequence-to-sequence model encodes the human-human dialog so far and uses it to infer navigation actions to get closer to a goal location. We find that agents perform better with more dialog history and when mixing human and planner supervision during training. We conclude with next directions for creating tasks from CVDN, such as two learning agents that must be trained cooperatively, and more nuanced models for NDH, where our initial sequence-to-sequence model leaves headroom between its performance and human-level performance (Section 6). <ref type="figure">Figure 1</ref>: In Cooperative Vision-and-Dialog Navigation, two humans are given a hint about an object t o in the goal room. The Navigator moves (N ) through the simulated environment to find the goal room, and can stop at any time to type a question (Q) to the Oracle. The Oracle has a privileged view of the best next steps (O) according to a shortest path planner, and uses that information to answer (A) the question. The dialog continues until the Navigator stops in the goal room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work and Background</head><p>Dialogs in CVDN begin with an underspecified, ambiguous instruction analogous to what robots may encounter in a home environment (e.g., "Go to the room with the bed"). Dialogs include both navigation and question asking / answering to guide the search, akin to a robot agent asking for clarification when moving through a new environment. <ref type="table">Table 1</ref> summarizes how CVDN combines the strengths and difficulties of a subset of existing navigation and question answering tasks.</p><p>Vision-and-Language Navigation. Early, simulator-based Vision-and-Language Navigation (VLN) tasks use language instructions that are unambiguous-designed to uniquely describe the goal-and fully specified-describing the steps necessary to reach the goal <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. In a more recent setting, a simulated quadcopter drone uses low-level controls to follow a route described in natural language <ref type="bibr" target="#b9">[10]</ref>. In photorealistic simulation environments, agents can navigate high-definition scans of indoor scenes <ref type="bibr" target="#b6">[7]</ref> or large, outdoor city spaces <ref type="bibr" target="#b10">[11]</ref>. In interactive question answering <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> settings, the language context is a single question (e.g., "What color is the car?") that requires navigation to answer. The questions serve as underspecified instructions, but are unambiguous (e.g., there is only one car whose color can be asked about). These questions are generated from templates rather than human language. In CVDN, input is an underspecified hint about the goal location (e.g., "The goal room has a sink") requiring exploration and dialog to resolve. Rather than single instructions, CVDN includes two-sided, human-human dialogs.</p><p>Question Answering and Dialog. In Visual Question Answering (VQA), agents answer language questions about a static image. These tasks exist for templated language on rendered images <ref type="bibr" target="#b13">[14]</ref> and human language on real-world images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Later extensions feature two-sided dialog, where a series of question-answer pairs provide context for the next question <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Question answering in natural language processing is a long-studied task for questions about static text documents (e.g., the Stanford QA Dataset <ref type="bibr" target="#b19">[20]</ref>). Recently, this paradigm was extended to two-sided dialogs via human-human, question-answer pairs about a document <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Questions in these datasets are unambiguous: they have a right answer that can be inferred from the context. By contrast, CVDN conversations begin with a hint about the goal location that is always ambiguous and requires cooperation between participants. Contrasting VQA, because CVDN extends navigation the visual context is temporally dynamic-new visual observations arrive at each timestep.</p><p>Task-oriented Dialog. In human-robot collaboration, robot language requests for human help can be generated to elicit non-verbal human help (e.g, moving a table leg to be within reach for the robot) <ref type="bibr" target="#b0">[1]</ref>. However, humans may use language to respond to robot requests for help in task-oriented Dataset -Language Context--Visual Context-Human Amb UnderS Temporal Real-world Temporal Shared MARCO <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, DRIF <ref type="bibr" target="#b9">[10]</ref> 1I Dynamic -R2R <ref type="bibr" target="#b6">[7]</ref>, Touchdown <ref type="bibr" target="#b10">[11]</ref> 1I Dynamic -EQA <ref type="bibr" target="#b11">[12]</ref>, IQA <ref type="bibr" target="#b12">[13]</ref> 1Q Dynamic -CLEVR <ref type="bibr" target="#b13">[14]</ref> -1Q Static -VQA <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> -1Q Static -CLEVR-Dialog <ref type="bibr" target="#b17">[18]</ref> -2D Static VisDial <ref type="bibr" target="#b18">[19]</ref> -2D Static VLNA <ref type="bibr" target="#b23">[24]</ref>, HANNA <ref type="bibr" target="#b24">[25]</ref> 1D Dynamic TtW <ref type="bibr" target="#b25">[26]</ref> 2D Dynamic CVDN 2D Dynamic <ref type="table">Table 1</ref>: Compared to existing datasets involving vision and language input for navigation and question answering, CVDN is the first to include two-sided dialogs held in natural language, with the initial navigation instruction being both ambiguous (Amb) and underspecified (UnderS), and situated in a photorealistic, visual navigation environment viewed by both speakers. For temporal language context, we note single navigation instructions (1I) and questions (1Q) versus 1-sided (1D) and 2-sided (2D) dialogs.</p><p>dialogs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>. Recent work adds requesting navigation help as an action, but the response either comes in the form of templated language that encodes gold-standard planner action sequences <ref type="bibr" target="#b23">[24]</ref> or as an automatic generation trained from human instructions and coupled with a visual goal frame as additional supervision <ref type="bibr" target="#b24">[25]</ref>. Past work introduced Talk the Walk (TtW) <ref type="bibr" target="#b25">[26]</ref>, where two humans communicate to reach a goal location in a photorealistic, outdoor environment. In TtW, the guiding human does not have an egocentric view of the environment, but an abstracted semantic map, and so language grounding centers around semantic elements like "bank" and "restaurant" rather than visual features, and the target location is unambiguously shown to the guide from the start. In CVDN, a Navigator human generates language requests for help, and an Oracle human answers in language conditioned on higher-level, visual observations of what a shortest-path planner would do next, with both players observing the same, egocentric visual context. In some ways, CVDN echoes several older human-human, spoken dialog corpora like the HCRC Map Task <ref type="bibr" target="#b27">[28]</ref>, SCARE <ref type="bibr" target="#b28">[29]</ref>, and CReST <ref type="bibr" target="#b29">[30]</ref>, but these are substantially smaller and have fewer and less rich environments.</p><p>Background: Matterport Simulator and the Room-2-Room Task. We build on the R2R task <ref type="bibr" target="#b6">[7]</ref> and train navigation agents using the same simulator and API. MatterPort contains 90 3D house scans, with each scan S divided into visual panoramas p ∈ S (nodes which a navigation agent can occupy) accompanied by an adjacency matrix A S . We differentiate between the steps and distance between p and q-steps represent the number of intervening nodes d h , while distance is defined in meters as d m .</p><p>Step distance d h (p, q) is the number of hops through A S to get from node p to node q. The distance in meters d m (p, q) is defined as physical distance if A S [p, q] = 1 or the shortest route between p and q otherwise. On average, 1 step corresponds to 2.25 meters.</p><p>At each timestep, an agent emits a navigation action taken in the simulated environment. The actions are to turn left or right, tilt up or down, move forward to an adjacent node, or stop. After taking any action except stop, the agent receives a new visual observation from the environment. The forward action is only available if the agent is facing an adjacent node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Cooperative Vision-and-Dialog Navigation Dataset</head><p>We collect 2050 human-human navigation dialogs, comprising over 7k navigation trajectories punctuated by question-answer exchanges, across 83 MatterPort <ref type="bibr" target="#b5">[6]</ref> houses. <ref type="bibr" target="#b0">1</ref> We prompt with initial instructions that are both ambiguous and underspecified. An ambiguous navigation instruction is one that requires clarification because it can refer to more than one possible goal location. An underspecified navigation instruction is one that does not describe the route to the goal.</p><p>Dialog Prompts. A dialog prompt is a tuple of the house scan S, a target object t o to be found, a starting position p 0 , and a goal region G j . We use the MatterPort object segmentations to get region Proportional Frequency <ref type="figure">Figure 2</ref>: The distributions of steps taken by human Navigators versus a shortest path planner (Left), the number of word tokens from the Navigator and the Oracle (Center), and the number of utterances in dialogs across the CVDN dataset.</p><p>locations for household objects, as in prior work <ref type="bibr" target="#b23">[24]</ref>. We define a set of 81 unique object types that appear in at least 5 unique houses and appear between 2 and 4 times per such house. <ref type="bibr" target="#b1">2</ref> Each dialog begins with a hint, such as "The goal room contains a plant," which by construction is both ambiguous (there are two to four rooms with a plant) and underspecified (the path to the room is not described by the hint).</p><p>Given a house scan S and a target object t o , a dialog prompt is created for every goal region G j in the house containing an instance of t o . Goal regions are sets of nodes that occupy the same room in a house scan. The starting node p 0 is chosen to maximize the distance between p 0 and the goal regions G 0:|G| containing t o . Formally,</p><formula xml:id="formula_0">p 0 = argmax p∈S   j min pi∈Gj (d h (p, p i ) 2 )   .</formula><p>Crowdsourced Data Collection. We gathered human-human dialogs through Amazon Mechanical Turk. <ref type="bibr" target="#b2">3</ref> In each Human Intelligence Task (HIT), workers read about the roles of Navigator and Oracle and could practice using the navigation interface. Pairs of workers were connected to one another via a chat interface.</p><p>Every dialog was instantiated via a randomly chosen prompt (S, t o , p 0 , G j ), with the Navigator starting at panorama p 0 and both workers instructed via the text: "Hint: The goal room contains a t o ." The dialog begins with the Navigator's turn. On the Navigator's turn, they could navigate, type a natural language question to ask the Oracle, or guess that they had found the goal room. Incorrect guesses disabled further navigation and forced the Navigator to ask a question to the Oracle. Throughout navigation, the Oracle was shown the steps being taken as a mirror of the Navigator's interface, so that both workers were always aware of the current visual frame. On the Oracle's turn, they could view an animation depicting the next 5 hops through the navigation graph towards the goal room according to a shortest path planner and communicate back to the Navigator via natural language ( <ref type="figure">Figure 1</ref>). Five hops was chosen because this is slightly shorter than the 6 hop average path in the R2R dataset, for which human annotators were able to provide reasonable language descriptions. Each HIT paid $1.25 per worker, the entire dataset collection cost over $7k.</p><p>After successfully locating the goal room, workers rated their partner's cooperativeness (from 1 to 5). Workers who failed to maintain a 4 or higher average peer rating were disallowed from taking more of our HITs. On average, dialog participants' mean peer rating is 4.52 out of 5 across CVDN.</p><p>Analysis. The CVDN dataset has longer routes and language contexts than the R2R task. The dialogs exhibit complex phenomena that require both dialog and navigation history to resolve. <ref type="figure">Figure 2</ref> shows the distributions of path lengths, word counts, and number of utterances across dialogs in the CVDN dataset. Human (25.0 ± 12.9) and planner (17.4 ± 7.0) path lengths are on average more than three times longer, and have higher variance, than the path lengths in R2R (6.0 ± 0.85). Average word counts for navigators <ref type="bibr">(33.5)</ref>   a factor of three. Dialogs average about 6 utterances each (3 question and answer exchanges), with a fraction being much longer-up to 26 utterances. Some dialogs have no exchanges (about 5%): the Navigator was able to find the goal location by intuition alone given the hint. Because more than one room always contains t o , these are 'lucky' guesses.</p><p>We randomly sampled 100 dialogs with at least one QA exchange and annotated whether each utterance (out of 342 per speaker) exhibited certain phenomena ( <ref type="table" target="#tab_2">Table 2</ref>). Over half the utterances from both Navigator and Oracle roles, and over 90% of all dialogs, contain egocentric references requiring the agent's position and orientation to interpret. Some Oracle answers require the Navigator question to resolve (e.g., when the answer is just a confirmation). Some utterances need dialog history from previous exchanges or past visual navigation information. More than 10% of dialogs exhibit conversational repair, when speakers try to rectify mistakes. Speakers sometimes establish rapport with off-topic comments and jokes. Both speakers, especially those in the Navigator role, sometimes send vacuous communications, but this is limited to a smaller percentage of dialogs.</p><p>Models attempting to perform navigation, ask questions, or answer questions about an embodied environment must grapple with these types of phenomena. For example, an agent may need to attend not just to the last QA exchange, but to the entire dialog and navigation history in order to correctly follow instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Navigation from Dialog History Task</head><p>CVDN facilities training agents for navigation, question asking, and question answering. In this paper, we focus on navigation. The ability to navigate successfully given dialog history is key to any future work in the Vision-and-Dialog Navigation paradigm. Every dialog is a sequence of Navigator question and Oracle answer exchanges, with Navigator steps following each exchange. We use this structure to divide dialogs into Navigation from Dialog History (NDH) instances.</p><p>In particular, CVDN instances are each comprised of a repeating sequence &lt; N 0 , Q 1 , A 1 , N 1 , . . . , Q k , A k , N k &gt; of navigation actions, N , questions asked by the Navigator, Q, and answers from the Oracle, A. Because sending a question or answer ends the worker's turn, every question Q i and answer A i is a single string of tokens. For each dialog with prompt (S, t o , p 0 , G j ), an NDH instance is created for each of 0 ≤ i ≤ k. The input is t o and a (possibly empty) history of questions and answers (Q 1:i , A 1:i ). The task is to predict navigation actions that bring the agent closer to the goal location G j , starting from the terminal node of N i−1 (or p 0 , for N 0 ). We extract 7415 NDH instances from the 2050 navigation dialogs in CVDN.  <ref type="table">ResNet  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE   LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE  LE</ref> LE LE LE LE <ref type="figure">Figure 3</ref>: We use a sequence-to-sequence model with an LSTM encoder that takes in learnable token embeddings (LE) of the dialog history. The encoder conditions an LSTM decoder for predicting navigation actions that takes in fixed ResNet embeddings of visual environment frames. Here, we demarcate subsequences in the input (e.g., t o ) compared during input ablations.</p><p>We divide these instances into training, validation, and test folds, preserving the R2R folds by house scan. This division is further done by dialog, such that for every dialog in CVDN the NDH instances created from it all belong to the same fold. As in R2R, we split the validation fold into seen and unseen house scans, depending on whether the scan is present in the training set. This results in 4742 training, 382 seen validation, 907 unseen validation, and 1384 unseen test instances.</p><p>We provide two forms of supervision for the NDH task: N i , the navigation steps taken by the Navigator after question-answer exchange i, and O i , the shortest-path steps shown to the Oracle and used as context to provide answer A i . In each instance of the task, i indexes the QA exchange in the dialog from which the instance is drawn (with i = 0 an empty QA followed by initial navigation steps). Across NDH instances, the N i steps range in length from 1 to 40 (average 6.63), and the O i steps range in length from 0 to 5 (average 4.35). The Navigator often continues farther than what the Oracle describes, using their intuition about the house layout to seek the target object.</p><p>We evaluate performance on this task by measuring how much progress the agent makes towards G j . Let e(P ) be the end node of path P , b(P ) the beginning, andP the path inferred by the navigation agent. Then the progress towards the goal is defined as the reduction (in meters) from the distance to the goal region G j at b(P ) versus at e(P ). Because G j is a set of nodes, we take the minimum distance min p∈Gj (d m (p, q)) as the distance between q and region G j . Note that this is a topological distance (e.g., we measure the distance around a wall, rather than straight through it).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Anderson et al. <ref type="bibr" target="#b6">[7]</ref> introduced a sequence-to-sequence model to serve as a learning baseline in the R2R task. We formulate a similar model to encode an entire dialog history, rather than a single navigation instruction, as an initial learning baseline for the NDH task. The dialog history is encoded using an LSTM and used to initialize the hidden state of an LSTM decoder whose observations are visual frames from the environment, and whose outputs are actions in the environment <ref type="figure">(Figure 3</ref>).</p><p>We replace words that occur fewer than 5 times with an UNK token. The resulting vocabulary sizes are 1042 language tokens in the training fold and 1181 tokens in the combined training and validation folds. We also use special NAV and ORA tokens to preface a speaker's tokens, TAR to preface the target object token, and EOS to indicate the end of the input sequence. During training, an embedding is learned for every token and given as input to the encoder LSTM. For visual features, we embed the visual frame as the penultimate layer of an Imagenet-pretrained ResNet-152 model <ref type="bibr" target="#b30">[31]</ref>.</p><p>When evaluating against the validation folds, we train only on the training fold. When evaluating against the test fold, we train on the union of the training and validation folds. We ablate the distance of dialog history encoded, and introduce a mixed planner and human supervision strategy at training time. We hypothesize both that encoding a longer dialog history and using mixed-supervision steps will increase the amount the agent progresses towards the goal.</p><p>Training. Given supervision from an end node e(P * ), the agent infers navigation actions to form pathP . We train all agents with student-forcing for 20000 iterations of batch size 100, and evaluate validation performance every 100 iterations (see the Appendix for details). The best performance across all epochs is reported for validation folds. At each timestep the agent executes its inferred actionâ, and is trained using cross entropy loss against the action a * that is next along the shortest  <ref type="table">Table 3</ref>: Average agent progress towards the goal location when trained using different path end nodes for supervision. Among sequence-to-sequence ablations, bold indicates most progress across available language input, and blue indicates most progress across supervision signals.</p><p>path to the end node e(P * ). Using the whole navigation path, P * , as supervision rather than only the end node has been considered in other work <ref type="bibr" target="#b31">[32]</ref>. At test time, the agents are trained up to the epoch that achieved the best performance on the unseen validation fold and then evaluated (e.g., test fold evaluations are run only once per agent).</p><p>Recall that for each NDH instance, the path shown to the Oracle during QA exchange i, O i , and the path taken by the Navigator after that exchange, N i , are given. We define the mixed supervision path M i as N i when e(O i ) ∈ N i , and O i otherwise. This new form of supervision has parallels to previous works on learning from imperfect or adversarial human demonstrations. One common solution is to use imperfect human demonstrations to learn an initial policy which is then refined with Reinforcement Learning (RL) <ref type="bibr" target="#b32">[33]</ref>. Learning performance can be improved by first assigning a confidence measure to the demonstrations and only including those demonstrations that pass a certain threshold <ref type="bibr" target="#b33">[34]</ref>. While we leave the evaluation of more sophisticated RL methods to future work, the mixed supervision described above can be thought of as using a simple binary confidence heuristic to threshold the human demonstrations.</p><p>Baselines and Ablations. We compare the sequence-to-sequence agent to a full-state information shortest path agent, to a non-learning baseline, and to unimodal baselines. The Shortest Path agent takes the shortest path to the supervision goal at inference time, and represents the best a learning agent could do under a given form of supervision. The non-learning Random agent chooses a random heading and walks up to 5 steps forward (as in <ref type="bibr" target="#b6">[7]</ref>). Random baselines can be outperformed by unimodal model ablations-agents that consider only visual input, only language input, or neither-on VLN tasks <ref type="bibr" target="#b34">[35]</ref>. So, we also compare our agent to unimodal baselines where agents have zeroed out visual features in place of the V ResNet features at each decoder timestep (visionless baseline) and/or empty language inputs to the encoder (language-less baseline). To examine the impact of dialog history, we consider agents with access to the target object t o ; the last Oracle answer A i ; the prefacing Navigator question Q i ; and the full dialog history <ref type="figure">(Figure 3)</ref>.</p><p>Results. <ref type="table">Table 3</ref> shows agent performances given different forms of supervision. We ran paired t-tests between all model ablations within each supervision paradigm and across paradigms, and applied the Benjamini-Yekutieli procedure to control the false discovery rate (details in the Appendix).</p><p>Using all dialog history significantly outperforms unimodal ablations in unseen environments. The Shortest Path agent performance with Navigator supervision N i approximates human performance on NDH, because e(N i ) is the node reached by the human Navigator after QA exchange i during data collection. The sequence-to-sequence models establish an initial, multimodal baseline for NDH, with headroom remaining compared to human performance, especially in unseen environments. Using all dialog history, rather than just the last question or question-answer exchange, is needed to achieve statistically significantly better performance than using the target object alone in unseen test environments. This supports our hypothesis that dialog history is beneficial for understanding the context of the latest navigation instruction A i . Models trained with mixed supervision always statistically significantly outperform those trained with oracle or navigator supervision. This supports our hypothesis that using human demonstrations only when they appear trustworthy increases agent progress towards the goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We introduce Cooperative Vision-and-Dialog Navigation: 2050 human-human, situated navigation dialogs in a photorealistic, simulated environment. The dialogs contain complex phenomena that require egocentric visual grounding and referring to both dialog history and past navigation history for context. CVDN is a valuable resource for studying in-situ navigation interactions, and for training agents that both navigate human environments and ask questions when unsure, as well as those that provide verbal assistance to humans navigating in unfamiliar places.</p><p>We then define the Navigation from Dialog History task. Our evaluations show that dialog history is relevant for navigation agents to learn a mapping between dialog-based instructions and correct navigation actions. Further, we find that using a mixed form of both human and planner supervision combines the best of each: long-range exploration of an environment according to human intuition to find the goal, and short-range accuracy aligned with language input.</p><p>Limitations. The CVDN dataset builds on the Room-to-Room task in the MatterPort Simulator <ref type="bibr" target="#b6">[7]</ref>. We would like to use CVDN to train real world agents for dialog and navigation. Simply fine-tuning on real world data may not be sufficient. Real-world robot navigation relies on laser scan depths, not just RGB information, and invokes lower quality egocentric vision, sensor noise, and localization issues. While the simulation provides photorealistic environments, it suffers from discrete, graph-based navigation, requiring a real world navigable environment to be mapped and divided into topological waypoints. Human-human dialogs collected in high-fidelity, continuous motion simulators (e.g., <ref type="bibr" target="#b35">[36]</ref>) or using virtual reality technology may facilitate easier transfer to physical robot platforms. However, sharing a simulation environment with the existing R2R task means that models for dialog history tasks like NDH may benefit from pretraining on R2R.</p><p>Future Work. The sequence-to-sequence model used in our experiments serves as an initial learning baseline for the NDH task. Moving forward, by formulating NDH as a sequential decision process we can use RL to shape the agent's policy, as in recent VLN work <ref type="bibr" target="#b36">[37]</ref>. Dialog analysis also suggests that there is relevant information in the historical navigation actions which are not considered by the initial model. Jointly conditioning dialog and navigation history may help resolve past reference instructions like "Go back to the stairwell and go up one flight of steps," and could involve cross-modal attention alignment.</p><p>The CVDN dataset also provides a scaffold for navigation-centered question asking and question answering tasks. In our future work, we will explore training two agents in tandem: one to navigate and ask questions when lost, and another to answer those questions. This will facilitate end-to-end evaluation on CVDN, and will differ from all existing VLN tasks by involving two, trained agents engaged in task-oriented dialog.  <ref type="figure">Figure 4</ref> gives the distributions of target objects t o across the dialogs in CVDN. The most frequent objects are those that are both frequent across houses and typically number between 2 and 4 per house, and often have a one-to-one correspondence with bedrooms and bathrooms. Proportional Frequency <ref type="figure">Figure 4</ref>: The distribution of the 81 target objects t o in dialogs across CVDN. <ref type="figure" target="#fig_1">Figure 5</ref> gives the intersection-over-union (IoU) of paths in CVDN within the same scan, comparing them against those in R2R and human performance per-dialog. The average path IoU across a scan is the average number of navigation nodes in the intersection of two paths over the union of nodes in those paths, across all paths in the scan. Compared to R2R, the paths in the dialogs of CVDN share more navigation nodes per scan because of the way starting panoramas p 0 were chosen-to maximize the distance to potential goal regions. Many CVDN paths start at or near the same remote p 0 nodes in, e.g., basements, rooftops, and lawns. Per-dialog, we measure the IoU between human Navigator and shortest path planner trajectories and find that there is substantially more overlap than between two paths in the same scan, indicating that humans follow closer to the shortest path than to an average walk through the scan (e.g., they are not just memorizing previous dialog trajectories). <ref type="figure" target="#fig_2">Figure 6</ref> gives path data for the NDH task. Compared to R2R, path lengths using shortest path supervision (O i ) are on average shorter than those in R2R, because paths shown to the Oracle were at most length 5. By contrast, human Navigator paths (N i ) are substantially longer than those seen in R2R. We also examine the distribution of the number of hops progressed towards the goal per NDH instance across Oracle shortest path, human navigator, and mixed supervision (M i ). While the planner always moves towards the goal (or stands still, if the Navigator is already in the goal region), human Navigators sometimes move farther away from the goal, though in general make more progress than the planner. Using mixed supervision, fewer trajectories move "backwards"; the simple heuristic of whether a Navigator walked over the last node in the Oracle's described shortest path shifts the distribution weight farther towards positive goal progress. Stepwise Progress Towards Goal </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Additional NDH Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">NDH Model Performance Statistical Comparisons</head><p>We ran paired t-tests between all model ablations within each supervision paradigm (e.g., comparing all mixed supervision models to one another), and across paradigms (e.g., comparing the full dialog history model trained with mixed supervision to the one with navigator path supervision). Data pairs are NDH the distances progressed towards the goal on the same instance (i.e., dialog history and goal) between two conditions. This results in hundreds of comparison tests, so we apply a Benjamini-Yekutieli procedure to control the false discovery rate. Because the tests are not all independent, but some are, we estimate c(m) under an arbitrary dependence assumption as c(m) = m i+1</p><p>1 i , where m is the number of tests run. We choose a significance threshold of α &lt; 0.05. Rather than report the hundreds of individual p-values, we highlight salient results below.</p><p>Different forms of supervision With one exception, in all environments (seen validation, unseen validation, and unseen test), across ablations of language context (i.e., full model using all history down to model using only the target object as dialog context), the differences in progress towards the goal under oracle, navigator, and mixed supervision are statistically significantly different. The only exception is the difference between oracle and navigator path supervision in unseen validation environments with the last answer only (i.e., row 16 of <ref type="table">Table 3</ref>) (p = 0.006). Models trained with mixed supervision almost always achieve the most progress. For brevity, below we discuss further comparisons between models trained with mixed supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different amounts of dialog history</head><p>In unseen validation and test environments, using all dialog history statistically significantly outperforms using only the target object, but not only the last answer (p = 0.773 in validation, p = 0.035 in test) or the last question-answer exchange (p = 0.143 in validation, p = 0.560 in test). Notably, in test environments, a statistically significant difference compared to using only the target object is observed only when using all dialog history. In seen validation houses, adding additional dialog history does not result in statistically significant gains, reflecting the representative power of the vision-only unimodal baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unimodal ablations</head><p>In unseen validation and test environments, the model using all dialog history statistically significantly outperforms the unimodal baselines in all cases except the language-only unimodal model in the unseen validation houses (p = 0.011). In seen validation houses, this model statistically significantly outperforms the language-only and zero (no language, no vision) unimodal ablations. This result does not hold for the vision-only baseline, which is able to memorize the familiar houses for use at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Sequence-to-Sequence Model Training</head><p>Hyperparameters. We use the training hyperparameters (optimizer, learning rate, hidden state sizes, etc.) presented in Anderson et al. <ref type="bibr" target="#b6">[7]</ref> when training our sequence-to-sequence agents. We adjust the maximum input sequence length for language encoding based on the amount of dialog history available: 3 for t o only (e.g., TAR tag, the target itself, and EOS); 70 for A i ; 120 for adding Q i ; and 720 (e.g., 120 times 6 turns of history) for Q 1:k , A 1:k . We increase the maximum episode length (e.g., the maximum number of navigation actions) depending on the supervision being used: 20 for oracle O i (the same as in R2R) and 60 for navigator N i and mixed M i .</p><p>Teacher-versus Student-Forcing. We use student-forcing when training all of our sequence-tosequence agents. Anderson et al. <ref type="bibr" target="#b6">[7]</ref> found that student-forcing improved agent performance in unseen environments. Further, Thomason et al. <ref type="bibr" target="#b34">[35]</ref> found that agents trained via teacher-forcing were outperformed by their unimodal ablations (i.e., they did not learn to incorporate both language and vision supervision, instead memorizing unimodal priors). Thus, we see no value in evaluating multi-modal agents trained via teacher-forcing in this setting.</p><p>Language Encoding. It is common in sequence-to-sequence architectures to reverse the input sequence of tokens during training, because the tokens relevant for the first decoding actions are likely also the first in the input sequence. Reversing the sequence means those relevant tokens have been seen more recently by the encoder, and this strategy was employed in prior work <ref type="bibr" target="#b6">[7]</ref>. Following this intuition, we preserve the order of the dialog history during encoding, so that the most recent utterances are read just before decoding, but reverse the tokens at the utterance level (e.g., Q i in <ref type="figure">Figure 3</ref> is represented as sequence "&lt;NAV&gt; ? upstairs go I Should").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Naive Dialog History Encoding</head><p>We naively concatenated an encoded navigation history N H (via an LSTM taking in ResNet embeddings of past navigation frames) to the encoded dialog history, then learned a feed-forward shrinking layer to initialize the decoder <ref type="table">(Table 4</ref>). We hypothesize that there is some signal in this information, but we discover that naive concatenation does not improve performance in seen or unseen environments. We suspect that a modeling approach which learns an attention alignment between the navigation history and dialog history could make better use of the additional signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq-2-Seq Inputs</head><p>Goal Progress (m) ↑ Q1:i−1 Fold NH V to Ai Qi A1:i−  <ref type="table">Table 4</ref>: Average sequence-to-sequence agent performance when the agent encodes the entire navigation history N H compared against the Shortest Path upper bound and the agent encoding all dialog history across different supervision signals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Left: The IoU of nodes in the paths of human Navigator and shortest path planner trajectories in CVDN versus those in R2R when comparing paths in the same scan. Right: The IoU of Navigator and shortest path planner trajectories in the same scan versus the IoU of player and shortest path planner trajectories across a dialog.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Left: The distributions of path lengths by human Navigator and the shortest path planner provided as supervision in NDH instances versus path lengths in R2R supervision. Right: The progress per NDH instance made towards the goal (in steps) by the human Navigator, the shortest path planner, and the mixed-supervision path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and oracles (48.1) sum to an average 81.6 words per dialog, again exceeding the Room-to-Room average of 29 words per instruction by nearly Oracle: Turn slightly to your right and go forward down the hallway Navigator: Should I turn left down the hallway ahead? Oracle: Through the lobby. So go through the door next to the green towel. Go to the left door next to the two yellow lights. Walk straight to the end of the hallway and stop . . . Navigator: Are these the yellow lights you were talking about? Oracle: You were there briefly but left. There is a turntable behind you a bit. Enter the bedroom next to it. Navigator: I am to the 'rear' of the zebra. Nice one.Oracle: Ok hold your nose and go to the left of the zebra, through the livingroom and kitchen and towards the bedroom you can see</figDesc><table><row><cell></cell><cell cols="2">Dia Nav</cell><cell>Ora Example</cell></row><row><cell cols="4">Ego 92.5 52.9 65.8 Needs Q 13.0 -3.9 Oracle: ya</cell></row><row><cell cols="4">Needs Dialog History 1.0 Needs Nav 3.5 0.4 History 14.0 1.5 3.4 Repair 12.5 1.6 3.4 Oracle: I am so sorry I meant for you to look over to the right not</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the left</cell></row><row><cell>Off-topic</cell><cell>3.0</cell><cell>5.4</cell><cell>5.1 past that</cell></row><row><cell>Vacuous</cell><cell cols="2">6.0 22.7</cell><cell>2.3 Navigator: Ok, now where?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The average percent of Dialogs, as well as individual Navigator and Oracle utterances, exhibiting each phenomena out of 100 hand-annotated dialogs. Two authors annotated each dialog and reached an agreement of Cohen's κ = .738 across all phenomena labels.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A demonstration video of the data collection interface: https://youtu.be/BonlITv_PKw.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also cut odd ("soffet") and non-specific ("wall") objects, and merge similar object names (e.g., "potted plant" and "plant") to cut down the initial 929 object types to these salient 81. Some houses do not have objects that meet our criteria, so CVDN represents only 83 of the 90 total MatterPort houses.<ref type="bibr" target="#b2">3</ref> https://cvdn.dev/. Connect with two tabs to start a dialog with yourself.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by the ARO (W911NF-16-1-0121) and the NSF (IIS-1252835, IIS-1562364). We thank the authors of Anderson et al. <ref type="bibr" target="#b6">[7]</ref> for creating an extensible base for further research in VLN using the MatterPort 3D simulator, and our coworkers Yonatan Bisk, Mohit Shridhar, Ramya Korlakai Vinayak, and Aaron Walsman for helpful discussions and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Asking for help using inverse semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language to action: Towards interactive task learning with physical agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saba-Sadiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving grounded natural language understanding through human-robot dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sinapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yedidsion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning from humanrobot interactions in modeled scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murnane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breitmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dempster-shafer theoretic resolution of referential ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="414" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Matterport3D: Learning from RGB-D data in indoor environments. 3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Walk the talk: Connecting language, knowledge, and action in route instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stankiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mapping navigation instructions to continuous control actions with position visitation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Touchdown: Natural language navigation and spatial reasoning in visual street environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Embodied question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">IQA: Visual question answering in interactive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CLEVR-Dialog: A diagnostic dataset for multi-round reasoning in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">QuAC: Question answering in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interpretation of natural language rules in conversational machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CoQa: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vision-based navigation with language-based assistance via imitation learning with indirect intervention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<title level="m">Talk the walk: Navigating new york city through grounded dialogue. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A research platform for multi-robot dialogue with humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nogar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bloecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M E</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kowtko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sotillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The HCRC Map Task Corpus</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SCARE: a Situated Corpus with Annotated Referring Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Shockley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheutz</surname></persName>
		</author>
		<title level="m">The Indiana &quot;Cooperative Remote Search Task&quot; (CReST) Corpus. In LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stay on the path: Instruction fidelity in vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Integrating reinforcement learning with human demonstrations of varying ability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Suay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chernova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving reinforcement learning with confidence-based demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shifting the Baseline: Single Modality Performance on Visual Navigation &amp; QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herrasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to navigate unseen environments: Back translation with environmental dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
