<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Joint Sequence Fusion Model for Video Question Answering and Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
							<email>js.kim@vision.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
							<email>gunhee@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Joint Sequence Fusion Model for Video Question Answering and Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multimodal Retrieval; Video Question and Answering</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach named JSFusion (Joint Sequence Fusion) that can measure semantic similarity between any pairs of multimodal sequence data (e.g. a video clip and a language sentence). Our multimodal matching network consists of two key components. First, the Joint Semantic Tensor composes a dense pairwise representation of two sequence data into a 3D tensor. Then, the Convolutional Hierarchical Decoder computes their similarity score by discovering hidden hierarchical matches between the two sequence modalities. Both modules leverage hierarchical attention mechanisms that learn to promote well-matched representation patterns while prune out misaligned ones in a bottom-up manner. Although the JSFusion is a universal model to be applicable to any multimodal sequence data, this work focuses on video-language tasks including multimodal retrieval and video QA. We evaluate the JS-Fusion model in three retrieval and VQA tasks in LSMDC, for which our model achieves the best performance reported so far. We also perform multiple-choice and movie retrieval tasks for the MSR-VTT dataset, on which our approach outperforms many state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, various video-language tasks have drawn a lot of interests in computer vision research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, including video captioning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, video question answering (QA) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and video retrieval for a natural language query <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. To solve such challenging tasks, it is important to learn a hidden join representation between word and frame sequences, for correctly measuring their semantic similarity. Video classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> can be a candidate solution, but tagging only a few labels to a video may be insufficient to fully relate multiple latent events in the video to a language description. Thanks to recent advance of deep representation learning, many methods for multimodal semantic embedding (e.g. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>) have been proposed. However, most of existing methods embed each of visual and language information into a single vector, which is often insufficient especially for a video and a natural sentence. <ref type="bibr">With</ref>   the two sequence modalities, it is hard to directly compare multiple relations between subsets of sequence data (i.e. matchings between subevents in a video and short phrases in a sentence), for which hierarchical matching is more adequate. There have been some attempts to learn representation of hierarchical structure of natural sentences and visual scenes (e.g. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> using recursive neural networks), but they require groundtruth parse trees or segmentation labels.</p><p>In this paper, we propose an approach that can measure semantic similarity between any pairs of multimodal sequence data, by learning bottom-up recursive matches via attention mechanisms. We apply our method to tackle several video question answering and retrieval tasks. Our approach, named as Joint Sequence Fusion (JSFusion) model, consists of two key components. First, the Joint Semantic Tensor (JST) performs dense Hadamard products between frames and words and encodes all pairwise embeddings between the two sequence data into a 3D tensor. JST further takes advantage of learned attentions to refine the 3D matching tensor. Second, the Convolutional Hierarchical Decoder (CHD) discovers local alignments on the tensor, by using a series of attention-based decoding modules, consisting of convolutional layers and gates. These two attention mechanisms promote well-matched representation patterns and prune out misaligned ones in a bottom-up manner. Finally, CHD obtains hierarchical composible representations of the two modalities, and computes a semantic matching score of the sequence pair.</p><p>We evaluate the performance of our JSFusion model on multiple video question answering and retrieval tasks on LSMDC <ref type="bibr" target="#b0">[1]</ref> and MSR-VTT <ref type="bibr" target="#b1">[2]</ref> datasets. First, we participate in three challenges of LSMDC: multiple-choice test, movie retrieval, and fill-in-the-blank, which require the model to correctly measure a semantic matching score between a descriptive sentence and a video clip, or to predict the most suitable word for a blank in a sentence for a query video. Our JSFusion model achieves the best accuracies reported so far with significant margins for the lsmdc tasks. Second, we newly create multiple-choice and movie retrieval annotations for the MSR-VTT dataset, on which our approach also outperforms many state-of-the-art methods in diverse video topics (e.g. TV shows, web videos, and cartoons).</p><p>We summarize the contributions of this work as follows.</p><p>1. We propose the Joint Sequence Fusion (JSFusion) model, consisting of two key components: JST and CHD. To the best of our knowledge, it is a first attempt to leverage recursively learnable attention modules for measuring semantic matching scores between multimodal sequence data. Specifically, we propose two different attention models, including soft attention in JST and Conv-layers and Conv-gates in CHD. 2. To validate the applicability of our JSFusion model, especially on video question answering and retrieval, we participate in three tasks of LSMDC <ref type="bibr" target="#b0">[1]</ref>, and achieve the best performance reported so far. We newly create video retrieval and QA benchmarks based on MSR-VTT <ref type="bibr" target="#b1">[2]</ref> dataset, on which our JSFusion outperforms many state-of-the-art VQA models. Our source code and benchmark annotations are publicly available in our project page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work can be uniquely positioned in the context of two recent research directions: video retrieval and video question answering. Video retrieval with natural language sentences. Visual information retrieval with natural language queries has long been tackled via joint visuallanguage embedding models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. In the video domain, it is more difficult to learn latent relations between a sequence of frames and a sequence of descriptive words, given that a video is not simply a multiple of images. Recently, there has been much progress in this line of research. Several deep video-language embedding methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> has been developed by extending image-language embeddings <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Other recent successful methods benefit from incorporating concept words as semantic priors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref>, or relying on strong representation of videos like RNN-FV <ref type="bibr" target="#b29">[30]</ref>. Another dominant approach may be leveraging RNNs or their variants like LSTM to encode the whole multimodal sequences (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>).</p><p>Compared to these existing methods, our model first finds dense pairwise embeddings between the two sequences, and then composes higher-level similarity matches from fine-grained ones in a bottom-up manner, leveraging hierarchical attention mechanisms. This idea improves our model's robustness especially for local subset matching (e.g. at the activity-phrase level), which places our work in a unique position with respect to previous works.</p><p>Video question answering. VQA is a relatively new problem at the intersection of computer vision and natural language research <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Video-based VQA is often recognized as a more difficult challenge than image-based one, because video VQA models must learn spatio-temporal reasoning to answer problems, which requires large-scale annotated data. Fortunately, large-scale video QA datasets have been recently emerged from the community using crowdsourcing on various sources of data (e.g. movies for MovieQA <ref type="bibr" target="#b9">[10]</ref> and animated GIFs for TGIF-QA <ref type="bibr" target="#b10">[11]</ref>). Rohrbach et al. <ref type="bibr" target="#b0">[1]</ref> extend the LSMDC movie description dataset to the VQA domain, introducing several new tasks such as multiplechoice <ref type="bibr" target="#b11">[12]</ref> and fill-in-the-blank <ref type="bibr" target="#b33">[34]</ref>.</p><p>The multiple-choice problem is, given a video query and five descriptive sentences, to choose a single best answer in the candidates. To tackle this problem, ranking losses on deep representation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> or nearest neighbor search on the joint space <ref type="bibr" target="#b29">[30]</ref> are exploited. Torabi et al. <ref type="bibr" target="#b11">[12]</ref> use the temporal attention on the joint representation between the query videos and answer choice sentences. Yu et al. <ref type="bibr" target="#b8">[9]</ref> use LSTMs to sequentially feed the query and the answer embedding conditioned on detected concept words. The fill-in-the-blank task is, given a video and a sentence with a single blank, to select a suitable word for the blank. To encode the sentential query sentence on the video context, MergingLSTMs <ref type="bibr" target="#b34">[35]</ref> and LR/RL LSTMs <ref type="bibr" target="#b35">[36]</ref> are proposed. Yu et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref> attempt to detect semantic concept words from videos and integrate them with Bidirectional LSTM that encodes the language query. However, most previous approaches tend to focus too much on the sentence information and easily ignore visual cues. On the other hand, our model focuses on learning multi-level semantic similarity between videos and sentences, and consequently achieves the best results reported so far in these two QA tasks, as will be presented in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Joint Sequence Fusion Model</head><p>We first explain the preprocessing steps for describing videos and sentences in section 3.1, and then discuss the two key components of our JSFusion model in section 3.2-3.4, respectively. We present the training procedure of our model in section 3.5, and its applications to three video-language tasks in section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>Sentence representation. We encode each sentence in a word level. We first define a vocabulary dictionary V by collecting the words that occur more than three times in the dataset. (e.g. the dictionary size is |V| = 16, 824 for LSMDC). We ignore the words that are not in the dictionary. Next we use the pretrained glove.42B.300d <ref type="bibr" target="#b36">[37]</ref> to obtain the word embedding matrix E ∈ R d×|V| where d = 300 is the word embedding dimension. We denote the description of each sentence by {w m } M m=1 where M is the number of words in the sentence. We limit the maximum number of words per sentence to be M max = 40. If a sentence is too long, we discard the remaining excess words, because only 0.07% of training sentences excess this limit, and no performance gain is observed for larger M max . Throughout this paper, we use m for denoting the word index.</p><p>Video representation. We sample a video at five fps, to reduce the frame redundancy while minimizing information loss. We employ CNNs to encode both visual and audial information in videos. For visual description, we extract the  Fill-in-the-blank QA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>On the road, the car speeds toward a truck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: A car speeds towards a truck</head><p>A:</p><p>house On the road, the car speeds toward a truck.  (b) CHD learns hierarchical relation patterns between the sequences, using a series of convolutional decoding module which shares parameters for each stage. is Hadamard product, ⊕ is addition, and ⊗ is multiplication between representation and attentions described in Eq.(2)-(4). We omit some fully-connected layers for visualization purpose.</p><p>feature map of each frame from the pool5 layer (R 2,048 ) of ResNet-152 <ref type="bibr" target="#b37">[38]</ref> pretrained on ImageNet. For audial information, we extract the feature map using the VGGish <ref type="bibr" target="#b38">[39]</ref> followed by PCA for dimensionality reduction (R 128 ). We then concatenate both features as the video descriptor {v n } N n=1 ∈ R 2,156×N where N is the number of frames in the video. We limit the maximum number of frames to be N max = 40. If a video is too long, we select N max equidistant frames. We observe no performance gain for larger N max . We use n for denoting the video frame index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Joint Semantic Tensor</head><p>The Joint Semantic Tensor (JST) first composes pairwise representations between two multimodal sequences into a 3D tensor. Next, JST applies a self-gating mechanism to the 3D tensor to refine it as an attention map that discovers finegrained matching between all pairwise embeddings of the two sequences while pruning out unmatched joint representations Sequence encoders. Give a pair of multimodal sequences, we first represent them using encoders. We use bidirectional LSTM networks (BLSTM) encoder <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> for word sequence and CNN encoder for video frames. It is often advantageous to consider both future and past contexts to represent each element in a sequence, which motivates the use of BLSTM encoders. {h f t } T t=1 and {h b t } T t=1 are the forward and backward hidden states of the BLSTM, respectively: where we set h b t , h f t ∈ R 512 , with initializing them as zeros:</p><formula xml:id="formula_0">h f t = LSTM(x t , h f t−1 ), h b t = LSTM(x t , h b t+1 ),<label>(1)</label></formula><formula xml:id="formula_1">h b T +1 = h f 0 = 0.</formula><p>Finally, we obtain the representation of each modality at each step by concatenating the forward/backward hidden states and the input features:</p><formula xml:id="formula_2">x w,t = [h f w,t , h b w,t , w t ] for words. For visual domain, we use 1-d CNN encoder represen- tation for v t , h cnn ∈ R 2,048 instead, x v,t = [h cnn v,t , v t ].</formula><p>Attention-based joint embedding. We then feed the output of the sequence encoder into fully-connected (dense) layer [D1] for each modality separately, which results in</p><formula xml:id="formula_3">D1 v (x v ), D1 w (x w ) ∈ R d D1 , where d D1 is a hidden dimen- sion of [D1].</formula><p>We summarize the details of all the layers in our JSFusion model in <ref type="table" target="#tab_3">Table 1</ref>. Throughout the paper, we denote fully-connected layers as Dk and convolutional layers as Convk.</p><p>Next, we compute attention weights α and representation γ, from which we obtain the JST as a joint embedding between every pair of sequential features:</p><formula xml:id="formula_4">j nm = α nm γ nm , where α nm = σ(w T D2(t nm )), γ nm = D4(D3(t nm )), (2) t nm = D1 v (x v,n ) D1 w (x w,m ).<label>(3)</label></formula><p>is a hadamard product, σ is a sigmoid function, and w ∈ R d D2 is a learnable parameter. Since the output of the sequence encoder represents each frame conditioned on the neighboring video (or each word conditioned on the whole sentence), the attention α is expected to figure out which pairs should be more weighted for the joint embedding among all possible pairs. For example of <ref type="figure" target="#fig_2">Fig.2</ref>, expectedly, α 3,6 (v 3 , w 6 ) &gt; α <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6</ref> (v 8 , w 6 ), if w 6 is truck, and the third video frame contains the truck while the eighth frame does not.</p><p>From Eq. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Convolutional Hierarchical Decoder</head><p>The Convolutional Hierarchical Decoder (CHD) computes a compatibility score for a pair of multimodal sequences by exploiting the compositionality in the joint vector space of JST. We pass the JST tensor through a series of a convolutional (Conv) layer and a Conv-gating block, whose learnable kernels progressively find matched embeddings from those of each previous layer. That is, starting from the JST tensor, the CHD recursively activates the weights of positively aligned pairs than negatively aligned ones. Specifically, we apply three sets of Conv layer and Conv-gating to the JST:</p><formula xml:id="formula_5">J (k) = Convk(J (k−1) ) · σ(ConvGk(J (k−1) ))<label>(4)</label></formula><p>for k = 1, 2, 3. We initialize J (0) = J from the JST, and [Convk] is the k-th Conv layer for joint representation, [ConvGk] is the k-th Conv-gating layer for matching filters, whose details are summarized in <ref type="table" target="#tab_3">Table 1</ref>. We apply mean pooling to J <ref type="bibr" target="#b2">(3)</ref> to obtain a single video-sentence vector representation J out (e.g. R 17×17×256 → R 1×1×256 ). Finally, we compute similarity matching score by feeding J out into four dense layers [D5, D6, D7, D8]:</p><formula xml:id="formula_6">score =W D8 (D7(D6(D5(J out )))) + b D8<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">Dk(x) = tanh(W Dk x + b Dk ), k = 5, 6, 7.</formula><p>We use the tanh activation for all dense layers except [D8].  At test time, when it comes to compute a similarity score, the Conv-gating layers prune out misaligned patterns; if the pair is negative where there is no common aligned structure in the two sequences, as shown in the right of <ref type="figure" target="#fig_4">Fig.3(b)</ref>, most elements of J (k) have very low values. As a result, the CHD can selectively filter lower-layer information that needs to be propagated to the final-layer representation, and the final layer of CHD assigns a high score only if the jointly aligned patterns are significant between the sequence data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">An Illustrative Example of How the JSFusion Model Works</head><p>The motivation behind the JSFusion model is that long sequence data like videos and sentences are too complicated to compare them in a single vector space, although most previous approaches depend on single LSTM embedding such as neural visual semantic embedding <ref type="bibr" target="#b18">[19]</ref> and previous LSMDC winners <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9]</ref>. Instead, in our approach, JST first composes a dense pairwise 3D tensor representation between multimodal sequence data, from which CHD then exploits convolutional gated layers to learn multi-stage similarity matching. Therefore, our JST model can be more robust for detecting partial matching between short phrases and subhots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>We train our JSFusion model using the ranking loss. Each training batch consists of L video-sentence pairs, including a single positive pair and L − 1 randomly sampled negative pairs. We use batch shuffling in every training epoch. Finally, we train the model using a max-margin structured loss objective as follows:</p><formula xml:id="formula_8">L = k L l=1 max(0, S k,l − S k,l * + ∆) + λ||θ|| 2<label>(6)</label></formula><p>where l * is the answer pair among L candidates, λ is a hyperparameter and θ denotes weight parameters. This objective encourages a positive video-sentence pair to have a higher score than a misaligned negative pair by a margin ∆. We use λ = 0.0005, ∆ = 10 in our experiments. We train all of our models using the Adam optimizer <ref type="bibr" target="#b41">[42]</ref>, with an initial learning rate in the range of 10 −4 . For regularization, we apply batch normalization <ref type="bibr" target="#b42">[43]</ref> to every dense layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation of Video-Language Models</head><p>We below discuss how the JSFusion model is implemented for three videolanguage tasks, video retrieval, multiple-choice test, and fill-in-the-blank. We apply the same JSFusion model to both video retrieval and multiple-choice test with slightly different hyperparameter settings. For the fill-in-the-blank, we make a minor modification in our model to predict a word for a blank in the middle of the sentence.</p><p>For retrieval. The retrieval model takes a query sentence and ranks 1,000 test videos according to the relevance between the query and videos. For training, we set L = 10 as the size of each training batch. At test, for each query sentence k, we compute scores {S k,l } l for all videos l in the test set. From the score matrix, we can rank the videos for the query. As will be presented in section 4.3 and 4.4, our method successfully finds hierarchical matching patterns between complex natural language query and video frames with sounds.</p><p>For multiple-choice test. The multiple-choice model takes a video and five choice sentences among which only one is the correct answer. Since our model can calculate the compatibility score between the query video and each sentence choice, we use the same model as the retrieval task. We simply select the choice with the highest score as an answer. For training, we set L = 10 so that each training batch contains 10 pairs of videos and sentences, which include only a single correct sentence, four wrong choices, and 5 randomly selected sentences from other training data.</p><p>For fill-in-the-blank. The fill-in-the-blank model takes a video and a sentence with one blank, and predict a correct word for the blank. Since this task requires more difficult inference (i.e. selecting a word out of vocabulary V, instead of computing a similarity score), we make two modifications as follows. First, we use deeper dimensions for layers: <ref type="table" target="#tab_3">Table 1</ref>.</p><formula xml:id="formula_9">d D1 = d D5 = d D6 = d D7 = 1, 024, d D2 = d D3 = d D4 = 2, 048, d D8 = |V|, d Conv1 1 = d Conv2 1 = d Conv3 1 = 1, 024, instead of the numbers in</formula><p>Second, we add a skip-connection part to our model, which is illustrated as the green paths of <ref type="figure" target="#fig_2">Figure 2</ref>. Letting b as the blank position in the query sentence, we use the BLSTM output from the blank word token BLANK as a sentential context of the blank position: t b = D1 w (w b ). We make a summation between the output of [D7] ∈ R 1,024 and the sentential context t b ∈ R 1,024 , and then feed it into [D8] to predict a word.</p><p>For training, we set the batch size as L = 32. We use the different objective, the cross-entropy loss, because this task is classification rather than ranking:</p><formula xml:id="formula_10">L = − log p(y) + λ||θ|| 2<label>(7)</label></formula><p>where θ denotes weight parameters and λ = 0.0005. We use dropout with a rate of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We report the experimental results of JSFusion models for the three tasks of LSMDC <ref type="bibr" target="#b0">[1]</ref> and two tasks of MSR-VTT <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LSMDC Dataset and Tasks</head><p>The LSMDC 2017 consists of four video-language tasks for movie understanding and captioning, among which we focus on the three tasks in our experiments:  <ref type="table">Table 2</ref>. Performance comparison for the movie retrieval task using Recall@k (R@k, higher is better) and Median Rank (MedR, lower is better). We report the results on the two datasets of LSMDC <ref type="bibr" target="#b0">[1]</ref> (L) and MSR-VTT <ref type="bibr" target="#b1">[2]</ref> (M).</p><p>movie retrieval, multiple-choice test, and fill-in-the-blank. The challenge provides a subset of the LSMDC dataset, which contains a parallel corpus of 118,114 sentences and 118,081 video clips of about 4-5 seconds long sampled from 202 movies. We strictly follow the evaluation protocols of the challenge. We defer more details of the dataset and challenge rules to <ref type="bibr" target="#b0">[1]</ref> and the homepage 1 .</p><p>Multiple-choice test. Given a video query and five candidate captions, the goal is to find the correct one for the query out of five possible choices. The correct answer is the groundtruth (GT) caption and four other distractors are randomly chosen from other captions that have different activity-phrase labels from the correct answer. The evaluation metric is the percentage of correctly answered test questions out of 10,053 public-test data.</p><p>Movie retrieval. The test set consists of 1,000 video/activity phrase pairs sampled from the LSMDC17 public-test data. Then, the objective is, given a short query activity-phrase (e.g. answering phone), to find its corresponding video out of 1,000 test videos. The evaluation metrics include Recall@1, Re-call@5, Recall@10, and Median Rank (MedR). The Recall@k means the percentage of GT videos in the first k retrieved videos, and the MedR indicates the median rank of GT videos. The challenge winner is determined by the metric of Recall@10.</p><p>Movie fill-in-the-blank. This track is related to visual question answering. The task is, given a video clip and a sentence with a blank in it, to predict a single Multiple-Choice Accuracy Dataset L M LSTM-fusion 52.8 38.3 SA-G+SA-FC7 <ref type="bibr" target="#b11">[12]</ref> 55.1 55.8 LSTM+SA-FC7 <ref type="bibr" target="#b11">[12]</ref> 56.3 59.1 C+LSTM+SA-FC7 <ref type="bibr" target="#b11">[12]</ref> 58.1 60.2 VSE-LSTM <ref type="bibr" target="#b18">[19]</ref> 63.0 67.3 SNUVL <ref type="bibr" target="#b28">[29]</ref> 63.1 65.4 ST-VQA-Sp.Tp <ref type="bibr" target="#b10">[11]</ref> 63.5 66.1 EITanque <ref type="bibr" target="#b29">[30]</ref> 63.7 65.5 CT-SAN <ref type="bibr" target="#b8">[9]</ref> 63.8 66.4 MLB <ref type="bibr" target="#b44">[45]</ref> 69.0 76. Fill-in-the-Blank Accuracy Text-only BLSTM <ref type="bibr" target="#b33">[34]</ref> 32.0 Text-only Human <ref type="bibr" target="#b33">[34]</ref> 30.2 GoogleNet-2D + C3D <ref type="bibr" target="#b33">[34]</ref> 35.7 Ask Your Neurons <ref type="bibr">[46]</ref> 33.2 Merging-LSTM <ref type="bibr" target="#b34">[35]</ref> 34.2 SNUVL <ref type="bibr" target="#b28">[29]</ref> 38.0 CT-SAN <ref type="bibr" target="#b8">[9]</ref> 41.9 LR/RL LSTMs <ref type="bibr" target="#b35">[36]</ref> 40.9 LR/RL LSTMs (Ensemble) <ref type="bibr" target="#b35">[36]</ref> 43  <ref type="table">Table 3</ref>. Left: Performance comparison for the multiple-choice test using the accuracy in percentage. We report the results on the two datasets of LSMDC (L) and MSR-VTT (M). Right: Accuracy comparison (in percentage) for the movie fill-in-the-blank task.</p><p>correct word for the blank. The test set includes 30,000 examples from 10,000 clips (i.e. about 3 blanks per sentence). The evaluation metric is the prediction accuracy (i.e. the percentage of predicted words that match with GTs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MSR-VTT-(RET/MC) Dataset and Tasks</head><p>The MSR-VTT <ref type="bibr" target="#b1">[2]</ref> is a large-scale video description dataset. It collects 118 videos per query of 257 popular queries, and filters manually to 7,180 videos. From the videos, it selects 10K video clips with 41.2 hours and 200K clip-sentence pairs. Based on the MSR-VTT dataset, we newly create two video-text matching tasks: (i) multiple-choice test and (ii) video retrieval. The task objectives for these tasks are identical to those of corresponding tasks in the LSMDC benchmark. To collect annotations for the two tasks, we exactly follow the protocols that are used in the LSMDC dataset, as described in <ref type="bibr" target="#b11">[12]</ref>.</p><p>Multiple-choice test: We generate 2,990 questions in total for the multiplechoice test, using all the test video clips of MSR-VTT. For each test video, we use the associated GT caption for the correct answer, while randomly sampled descriptions from other test data for four negative choices.</p><p>Video retrieval: For retrieval, we first sample 1,000 pairs of video clips and description queries from the test set of MSR-VTT We use 1,000 as the size of the test set, following the LSMDC benchmark. As a result, the retrieval task is to find out the video that corresponds to the query caption out of 1000 candidates. <ref type="table">Table 2</ref>-3 summarize the results of our experiments for the three video-language tasks. For LSMDC experiments, we report the results in the published papers and the official leaderboard of LSMDC 2017 2 . For MSR-VTT experiments, we run some participants of LSMDC, including SNUVL, EITanque, VSE-LSTM, ST-VQA-Sp.Tp and CT-SAN, using the source codes provided by the original authors. We implement the other baselines by ourselves, only except Miech et al. that require an additional person tracker, which is unavailable to use. Other variants of our method will be discussed in details below in the ablation study. <ref type="table">Table 2</ref>-3 clearly show that our JSFusion achieves the best performance with significant margins from all the baselines over the three tasks on both datasets. That is, the two components of our approach, JST and CHD, indeed helps measure better the semantic similarity between multimodal sequences than a wide range of state-of-the-art models, such as a multimodal embedding method (VSE-LSTM), a spatio-temporal attention-based QA model (ST-VQA-Sp.Tp), and a language model based QA inference (Text-only BLSTM). Encouragingly, the JSFusion single model outperforms even the ensemble method of runner-up (LR/RL LSTMs) in the fill-in-the-blank task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results</head><p>Among baselines, multimodal low-rank bilinear attention network (MLB) <ref type="bibr" target="#b44">[45]</ref> is competitive. The main differences of our model from (MLB) are two-fold. First, JSFusion embeds both a video and a sentence to feature sequences, whereas (MLB) represents the sentence as a single feature. Second, JSFusion uses the self-gating to generate fine-grained matching between all pairwise embeddings of the two sequences, while (MLB) uses the attention to find a position in the visual feature space that best fits for the sentence vector. Moreover, JSFusion consistently shows better performance than (MLB) in all experiments.</p><p>Ablation study. We conduct ablation experiments on different variants of our JSFusion model and present the results in <ref type="table">Table 2</ref>-3. As one naive variant of our model, we test a simple LSTM baseline (LSTM-fusion) that only carries out the Hadamard product on a pair of final states of video and language LSTM encoders. That is, (LSTM-fusion) is our JSFusion model that has neither JST nor CHD, which are the two main contributions of our model. We train (LSTM-fusion) in the same way as done for the JSFusion model in section 3.5. As easily expected, the performance of (LSTM-fusion) is significantly worse than our JSFusion in all the tasks.</p><p>To further validate the contribution of each component, we remove or replace key components of our model with simpler ones. To understand the effectiveness of BLSTM encoding, we test two baselines: (JSTfc) that replaces BLSTM with fully-connected layers and (JSTlstm) that replaces BLSTM with LSTM. (JSTmax) and (JSTmean) denote our variants that use max pooling and mean pooling, instead of the Convk convolutional layers in CHD. That is, they use fixed max/mean pooling operations instead of convolutions with learnable kernels. These comparisons reveal that the proposed CHD is critical to improve  the performance of JSFusion nontrivially on all the tasks on both datasets. We also compare our model with (JSFusion-noattention) that discards Conv-gating operations of CHD. (JSFusion-noattention) shows nontrivial performance drops as MC (acc): 4.1%p, 4.2%p, RET (R@10): 5.7%p, 3.7%p for LSMDC and MSR-VTT, respectively. Finally, we test our model with using no audio information denoted by (JSFusion-noaudio), which is also much better than other baselines but only slightly worse than our original model. <ref type="figure" target="#fig_7">Fig.4</ref> illustrates qualitative results of our JSFusion algorithm with correct (left) and near-miss (right) examples for each task. In each set, we show natural lan-guage query and sampled frames of a video. We present both groundtruth (GT), our prediction (Ours). Movie retrieval. <ref type="figure" target="#fig_7">Fig.4(a)</ref> is an example that our model can understand human behaviors like gaze. <ref type="figure" target="#fig_7">Fig.4(b)</ref> shows the model's failure to distinguish a small motion (e.g. facial expression), and simply retrieve the videos containing the face of a woman. <ref type="figure" target="#fig_7">Fig.4(c)</ref> shows that our model successfully catches the features of horses in both web videos and 3D animation, and correctly select the highest ranking video by focusing on the word stall. In <ref type="figure" target="#fig_7">Fig.4(d)</ref>, although the model can retrieve relevant videos of cooking with bowl, it fails to find out the answer video that contains the query description of baking mix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>Movie multiple-choice test. <ref type="figure" target="#fig_7">Fig.4</ref>(e) delivers an evidence that our model uses the whole sentence for computing matching scores, because the model successfully chooses 5 instead of 1 that shares the same phrases (e.g. shakes his head). <ref type="figure" target="#fig_7">Fig.4(f)</ref> is an example of focusing on a wrong video subsequence, where our model chooses the word club by looking at a subsequence with crowded people, but the answer is related to another subsequence with grandmother. <ref type="figure" target="#fig_7">Fig.4(g)</ref> is an example that the model learns words in a phrase. Choice 4 can be very tempting, since it contains the word kids, tv and show. But our model successfully choose the right answer by identifying that kids tv show and kids in tv show mean differently. <ref type="figure" target="#fig_7">Fig.4(h)</ref> shows that our model fails to distinguish the details.</p><p>Movie fill-in-the-blank. In <ref type="figure" target="#fig_7">Fig.4(i)</ref>, the model successfully finds the answer by using both structural information of a sentence and a video (e.g. door is a likely word after shuts the). <ref type="figure" target="#fig_7">Fig.4(j)</ref> is an example that the model focuses too much on the word picture that follows the blank, instead of visual information, and thus choose a wrong answer framed picture rather than flash picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed the Joint Sequence Fusion (JSFusion) model for measuring hierarchical semantic similarity between two multimodal sequence data. The two key components of the model, Joint Semantic Tensor (JST) and Convolutional Hierarchical Decoder (CHD), are easily adaptable in many video-and-language tasks, including multimodal matching or video question answering. We demonstrated that our method significantly improved the performance of video understanding through natural language description. Our method achieved the best performance in challenge tracks of LSMDC, and outperformed many state-of-the-art models for VQA and retrieval tasks on the MSR-VTT dataset.</p><p>Moving forward, we plan to expand the applicability of JSFusion; since our model is usable to any multimodal sequence data, we can explore other retrieval tasks of different modalities, such as videos-to-voices or text-to-human motions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The intuition of the Joint Sequence Fusion (JSFusion) model. Given a pair of a video clip and a language query, Joint Semantic Tensor (in purple) encodes a pairwise joint embedding between the two sequence data, and Convolutional Hierarchical Decoder (in blue) discovers hierarchical matching relations from JST. Our model is easily adaptable to many video QA and retrieval tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of Joint Sequence Fusion (JSFusion) model. Blue paths indicate the information flows for multimodal similarity matching tasks, while green paths for the fill-in-the-blank task. (a) JST composes pairwise joint representation of language and video sequences into a 3D tensor, using a soft-attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(2)-(3), we obtain JST in a form of 3D tensor: J = [j n,m ] n=1:Nmax m=1:Mmax and J ∈ R Nmax×Mmax×d D4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Attention examples for (a) Joint Semantic Tensor (JST) and (b) Convolutional Hierarchical Decoder (CHD). Higher values are shown in darker. (a) JST assigns high weights on positively aligned joint semantics in the two sequence data. Attentions are highlighted darker where words coincide well with frames. (b) Each layer in CHD assigns high weights to where structure patterns are well matched between the two sequence data. For a wrong pair of sequences, a series of Conv-gating (ConvG2) prune out misaligned patterns with low weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3</head><label>3</label><figDesc>illustrates with actual examples how the attentions of JST and CHD work.Fig.3(a) visualizes the learned attention weights α nm in Eq.(2) of all pairs between frames in a video and words in a positive and a negative sentence. The attentions are highlighted with higher values (shown in darker) when the words coincide better with the content in the frames, dominantly in a positive pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 (</head><label>3</label><figDesc>b) shows the output J (k) of each Conv layer and Conv-gating block in Eq.(4) for the same example. During training, each Conv layer learns to compose joint embedding from the ones in the lower layer, while the Convgating layer learns frequent matching patterns in the training pairs of videos and sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative examples of the three video-language tasks: movie retrieval on LSMDC (a)-(b) and MSR-VTT-RET (c)-(d), multiple-choice on LSMDC (e)-(f) and MSR-VTT-MC (g)-(h), and (i)-(j) fill-in-the-blank on LSMDC. The left column shows correct examples, while the right column shows near-miss examples. In (b),(d), we show our retrieval ranks of the GT clips (in the red box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>pale blue sky road lined with tall evergreens car speeds down</head><label></label><figDesc>single vectors for arXiv:1808.02559v1 [cs.CV] 7 Aug 2018Now, the car speeds down an empty road lined with tall evergreens that just into the pale blue sky.</figDesc><table><row><cell>Video sequence</cell><cell cols="2">= { 1 , 2 ⋯ , }</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Similarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>matching</cell></row><row><cell>Language Sequence</cell><cell>= { 1 , 2 ⋯ ,</cell><cell>}</cell><cell>1</cell><cell>Now the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>car</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>speeds</cell></row><row><cell>Joint Semantics</cell><cell></cell><cell></cell><cell></cell><cell>down …</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>blue</cell><cell>Word</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sky</cell><cell>prediction</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Blanked query</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>The detailed setting of layers in the JSFusion model. No padding is used for each layer. Dk means a fully-connected dense layer, and Convk and ConGk indicate convolutional and convolutional-gating layer, respectively.</figDesc><table><row><cell>FC layers</cell><cell>size</cell><cell>Conv layer</cell><cell>kernel/stride</cell><cell>channel</cell></row><row><cell>D1 v ,D1 w</cell><cell>512</cell><cell>Conv1</cell><cell>3 × 3 / 1</cell><cell>256</cell></row><row><cell>D2</cell><cell>512</cell><cell>ConvG1</cell><cell>3 × 3 / 1</cell><cell>1</cell></row><row><cell>D3, D4</cell><cell>512</cell><cell>Conv2</cell><cell>3 × 3 / 1</cell><cell>256</cell></row><row><cell>D5</cell><cell>256</cell><cell>ConvG2</cell><cell>3 × 3 / 1</cell><cell>1</cell></row><row><cell>D6</cell><cell>256</cell><cell>Conv3</cell><cell>3 × 3 / 2</cell><cell>256</cell></row><row><cell>D7</cell><cell>128</cell><cell>ConvG3</cell><cell>3 × 3 / 2</cell><cell>1</cell></row><row><cell>D8</cell><cell>1</cell><cell>MeanPool</cell><cell>17 × 17 / 17</cell><cell>256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>①Grandma watches her eyes wide and round behind her glassesyoung boy speaks to the judges on the tv show the voice</head><label></label><figDesc>SOMEONE steps back into the elevator. ② SOMEONE offers her hands, guides her sister up from her seat. ③</figDesc><table><row><cell></cell><cell cols="2">Correct</cell><cell></cell><cell>Wrong</cell></row><row><cell cols="3">Q : Meeting SOMEONE's sincere gaze, SOMEONE takes a</cell><cell cols="2">Q : The woman scowls suspiciously.</cell></row><row><cell>shaky breath.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Q : A brown horse in a stall.</cell><cell>(a)</cell><cell cols="2">(b) Q : A baking mix is stirred in a bowl while oil is added.</cell><cell>12th</cell></row><row><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell>(d)</cell><cell>5th</cell></row><row><cell cols="3">① Looking away, SOMEONE shakes his head.</cell><cell></cell></row><row><cell cols="3">② SOMEONE lands on top of him.</cell><cell></cell></row><row><cell cols="3">③ In the audience, SOMEONE's goateed friend and SOMEONE</cell><cell></cell></row><row><cell cols="3">share a table with SOMEONE.</cell><cell cols="2">. (GT Answer)</cell></row><row><cell cols="3">④ SOMEONE opens the gate and steps through.</cell><cell>④ At the club.</cell></row><row><cell cols="3">⑤ SOMEONE smiles tightly and SOMEONE shakes his</cell><cell cols="2">⑤ SOMEONE rests her head on SOMEONE's shoulder and frowns</cell></row><row><cell>head. (GT Answer)</cell><cell></cell><cell>(e)</cell><cell>uneasily.</cell><cell>(f)</cell></row><row><cell cols="3">① Screen recording of someone searching the internet with voice</cell><cell cols="2">① A video game player rides a motorcycle</cell></row><row><cell cols="3">over of man with indian accent</cell><cell cols="2">② A group of girls are talking about colors</cell></row><row><cell cols="3">② A comic about a young girl with super powers</cell><cell cols="2">③ There is a man in black dressing talking in front of a monitor</cell></row><row><cell cols="3">③ A .</cell><cell cols="2">④ There is a lady is sitting before the food sandwich</cell></row><row><cell>(GT Answer)</cell><cell></cell><cell></cell><cell cols="2">⑤ Adding ingredients to a pizza (GT Answer)</cell></row><row><cell cols="2">④ Scene from a kids tv show</cell><cell></cell><cell></cell></row><row><cell cols="3">⑤ There is a black car moving without control</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(g)</cell><cell></cell><cell>(h)</cell></row><row><cell cols="3">Blank Sentence : He enters the headmistress' office and shuts the</cell><cell cols="2">Blank Sentence : SOMEONE takes a _____ picture.</cell></row><row><cell>_____.</cell><cell></cell><cell></cell><cell>Answer : flash</cell><cell>Our result : framed</cell></row><row><cell>Answer : door</cell><cell cols="2">Our result : door</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(i)</cell><cell></cell><cell>(j)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/site/describingmovies/lsmdc-2017.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">FIB : https://competitions.codalab.org/competitions/11691#results. Multichoice : https://competitions.codalab.org/competitions/11491#results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="46">. Tzeng, E., Hoffman, J., Darrell, T., Saenko, K.: Simultaneous Deep Transfer Across Domains and Tasks. In: ICCV. (2015)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Jisung Kim and Antoine Miech for helpful comments about the model. This research was supported by Brain Research Program by National Research Foundation of Korea (NRF) (2017M3C7A1047860). Gunhee Kim is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03705</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Movie Description</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Msr-vtt: A Large Video Description Dataset for Bridging Video and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collecting Highly Parallel Data for Paraphrase Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-shot Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Long-Short Story of Movie Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: GCPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sequence to Sequence -Video to Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kate</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<title level="m">MovieQA: Understanding Stories in Movies through Question-Answering. In: CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<title level="m">Learning Language-Visual Embedding for Movie Understanding with Natural-Language</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02367</idno>
		<title level="m">Learning Joint Representations of Videos and Sentences with Web Image Search</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<title level="m">Space-time Interest Points. In: ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Realistic Human Actions from Movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">UCF101: A Dataset of 101 Human Actions Classes from Videos in the Wild</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Activitynet: A Large-scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<title level="m">Devise: A Deep Visual-semantic Embedding Model. In: NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Grounded Compositional Semantics for Finding and Describing Images with Sentences. TACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In: ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Parsing with Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<editor>ACL.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Framing Image Description as a Ranking Task: Data, models and Evaluation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visual Semantic Search: Retrieving Videos via Complex Textual Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06361</idno>
		<title level="m">Order-embeddings of Images and Language</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Natural Language Object Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generation and Comprehension of Unambiguous Object Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Video Captioning and Retrieval Models with Semantic Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02947</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Temporal Tessellation for Video Annotation and Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A Dataset and Exploration of Models for Understanding Video Data Through Fill-in-the-blank Questionanswering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07810</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04062</idno>
		<title level="m">Video Fill in the Blank with Merging LSTMs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Video Fill In the Blank using LR/RL LSTMs with Spatial-Temporal Attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<title level="m">CNN Architectures for Large-Scale Audio Classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bidirectional Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TSP</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning from Video and Text via Large-scale Discriminative Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hadamard Product for Low-rank Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
