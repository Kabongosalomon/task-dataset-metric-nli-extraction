<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with GNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Courant Institute of Mathematical Sciences New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
							<email>soledad.villar@nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Courant Institute of Mathematical Sciences Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
							<email>bruna@cims.nyu.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Courant Institute of Mathematical Sciences Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with GNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have achieved lots of success on graph-structured data. In the light of this, there has been increasing interest in studying their representation power. One line of work focuses on the universal approximation of permutation-invariant functions by certain classes of GNNs, and another demonstrates the limitation of GNNs via graph isomorphism tests. Our work connects these two perspectives and proves their equivalence. We further develop a framework of the representation power of GNNs with the language of sigma-algebra, which incorporates both viewpoints. Using this framework, we compare the expressive power of different classes of GNNs as well as other methods on graphs. In particular, we prove that order-2 Graph G-invariant networks fail to distinguish non-isomorphic regular graphs with the same degree. We then extend them to a new architecture, Ring-GNNs, which succeeds on distinguishing these graphs as well as for social network datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the expressivity of graph neural networks in terms of function approximation. Since we could argue that many if not most functions on a graph that we are interested in are invariant or equivariant to permutations of the nodes in the graph, GNNs are usually designed to be invariant or equivariant, and therefore the natural question is whether certain classes GNNs can approximate any continuous and invariant or equivariant functions. Recent work <ref type="bibr" target="#b17">[18]</ref> showed the universal approximation of G-invariant networks, constructed based on the linear invariant and equivariant layers studied in <ref type="bibr" target="#b16">[17]</ref>, if the order of the tensor involved in the networks can grow as the graph gets larger. Such a dependence on the graph size was been theoretically overcame by the very recent work <ref type="bibr" target="#b12">[13]</ref>, though there is no known upper bound on the order of the tensors involved. With potentially very-high-order tensors, these models that are guaranteed of univeral approximation are not quite feasible in practice.</p><p>The foundational part of this work aims at building the bridge between graph isomorphism testing and invariant function approximation, the two main perspectives for studying the expressive power of graph neural networks. We demonstrate an equivalence between the the ability of a class of GNNs to distinguish between any pairs of non-isomorphic graph and its power of approximating any (continuous) invariant functions, for both the case with finite feature space and the case with continuous feature space. Furthermore, we argue that the concept of sigma-algebras on the space of graphs is a natural description of the power of graph neural networks, allowing us to build a taxonomy of GNNs based on how their respective sigmas-algebras interact. Building on this theoretical framework, we identify an opportunity to increase the expressive power of order-2 Ginvariant networks with computational tractability, by considering a ring of invariant matrices under addition and multiplication. We show that the resulting model, which we refer to as Ring-GNN, is able to distinguish between non-isomorphic regular graphs where order-2 G-invariant networks provably fail. We illustrate these gains numerically in synthetic and real graph classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of main contributions:</head><p>• We show the equivalence between graph isomorphism testing and approximation of permutation-invariant functions for analyzing the expressive power of graph neural networks.</p><p>• We introduce a language of sigma algebra for studying the representation power of graph neural networks, which unifies both graph isomorphism testing and function approximation, and use this framework to compare the power of some GNNs and other methods.</p><p>• We propose Ring-GNN, a tractable extension of order-2 Graph G-invariant Networks that uses the ring of matrix addition and multiplication. We show this extension is necessary and sufficient to distinguish Circular Skip Links graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Graph Neural Networks and graph isomorphism. Graph isomorphism is a fundamental problem in theoretical computer science. It amounts to deciding, given two graphs A, B, whether there exists a permutation π such that πA = Bπ. There exists no known polynomial-time algorithm to solve it, but recently Babai made a breakthrough by showing that it can be solved in quasi-polynomial-time <ref type="bibr" target="#b0">[1]</ref>.</p><p>Recently <ref type="bibr" target="#b29">[30]</ref> introduced graph isomorphism tests as a characterization of the power of graph neural networks. They show that if a GNN follows a neighborhood aggregation scheme, then it cannot distinguish pairs of non-isomorphic graphs that the 1-WL test fails to distinguish. Therefore this class of GNNs is at most as powerful as the 1-WL test. They further propose the Graph Isomorphism Networks (GINs) based on approximating injective set functions by multi-layer perceptrons (MLPs), which can be as powerful as the 1-WL test. Based on k-WL tests <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref> proposes k-GNN, which can take higher-order interactions among nodes into account. Concurrently to this work, <ref type="bibr" target="#b15">[16]</ref> proves that order-k invariant graph networks are at least as powerful as the k-WL tests, and similarly to us, it and augments order-2 networks with matrix multiplication. They show they achieve at least the power of 3-WL test. <ref type="bibr" target="#b19">[20]</ref> proposes relational pooling (RP), an approach that combines permutation-sensitive functions under all permutations to obtain a permutation-invariant function. If RP is combined with permutation-sensitive functions that are sufficiently expressive, then it can be shown to be a universal approximator. A combination of RP and GINs is able to distinguish certain non-isomorphic regular graphs which GIN alone would fail on. A drawback of RP is that its full version is intractable computationally, and therefore it needs to be approximated by averaging over randomly sampled permutations, in which case the resulting functions is not guaranteed to be permutation-invariant.</p><p>Universal approximation of functions with symmetry. Many works have discussed the function approximation capabilities of neural networks that satisfy certain symmetries. <ref type="bibr" target="#b1">[2]</ref> studies the symmetry in neural networks from the perspective of probabilistic symmetry and characterizes the deterministic and stochastic neural networks that satisfy certain symmetry. <ref type="bibr" target="#b23">[24]</ref> shows that equivariance of a neural network corresponds to symmetries in its parameter-sharing scheme. <ref type="bibr" target="#b30">[31]</ref> proposes a neural network architecture with polynomial layers that is able to achieve universal approximation of invariant or equivariant functions. <ref type="bibr" target="#b16">[17]</ref> studies the spaces of all invariant and equivariant linear functions, and obtained bases for such spaces. Building upon this work, <ref type="bibr" target="#b17">[18]</ref> proposes the G-invariant network for a symmetry group G, which achieves universal approximation of G-invariant functions if the maximal tensor order involved in the network to grow as n(n−1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, but such high-order tensors are prohibitive in practice. Upper bounds on the approximation power of the G-invariant networks when the tensor order is limited remains open except for when G = A n <ref type="bibr" target="#b17">[18]</ref>. The very recent work <ref type="bibr" target="#b12">[13]</ref> extends the result to the equivariant case, although it suffers from the same problem of possibly requiring high-order tensors. Within the computer vision literature, this problem has also been addressed, in particular <ref type="bibr" target="#b10">[11]</ref> proposes an architecture that can potentially express all equivariant functions.</p><p>To the best our knowledge, this is the first work that shows an explicit connection between the two aforementioned perspectives of studying the representation power of graph neural networks -graph isomorphism testing and universal approximation. Our main theoretical contribution lies in showing an equivalence between them, for both finite and continuous feature space cases, with a natural generalization of the notion of graph isomorphism testing to the latter case. Then we focus on the Graph G-invariant network based on <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, and showed that when the maximum tensor order is restricted to be 2, then it cannot distinguish between non-isomorphic regular graphs with equal degrees. As a corollary, such networks are not universal. Note that our result shows an upper bound on order 2 G-invariant networks, whereas concurrently to us, <ref type="bibr" target="#b15">[16]</ref> provides a lower bound by relating to k-WL tests. Concurrently to <ref type="bibr" target="#b15">[16]</ref>, we propose a modified version of order-2 graph networks to capture higher-order interactions among nodes without computing tensors of higher-order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph isomorphism testing and universal approximation</head><p>In this section we show that there exists a very close connection between the universal approximation of permutation-invariant functions by a class of functions, and its ability to perform graph isomorphism tests. We consider graphs with nodes and edges labeled by elements of a compact set X ⊂ R. We represent graphs with n nodes by an n by n matrix G ∈ X n×n , where a diagonal term G ii represents the label of the ith node, and a non-diagonal G ij represents the label of the edge from the ith node to the jth node. An undirected graph will then be represented by a symmetric G.</p><p>Thus, we focus on analyzing a collection C of functions from X n×n to R. We are especially interested in collections of permutation-invariant functions, defined so that f (π Gπ) = f (G), for all G ∈ X n×n , and all π ∈ S n , where S n is the permutation group of n elements. For classes of functions, we define the property of being able to discriminate non-isomorphic graphs, which we call GIso-discriminating, which as we will see generalizes naturally to the continuous case.</p><formula xml:id="formula_0">Definition 1. Let C be a collection of permutation-invariant functions from X n×n to R. We say C is GIso-discriminating if for all non-isomorphic G 1 , G 2 ∈ X n×n (denoted G 1 G 2 ), there exists a function h ∈ C such that h(G 1 ) = h(G 2 )</formula><p>. This definition is illustrated by figure 2 in the appendix. Definition 2. Let C be a collection of permutation-invariant functions from X n×n to R. We say C is universally approximating if for all permutation-invariant function f from X n×n to R, and for all &gt; 0, there exists h f, ∈ C such that f − h f, ∞ := sup G∈X n×n |f (G) − h(G)| &lt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Finite feature space</head><p>As a warm-up we first consider the space of graphs with a finite set of possible features for nodes and edges, X = {1, . . . , M }. Theorem 1. Universally approximating classes of functions are also GIso-discriminating.</p><p>Proof. Given G 1 , G 2 ∈ X n×n , we consider the permutation-invariant function 1 G1 : X n×n → R such that 1 G1 (G) = 1 if G is isomorphic to G 1 and 0 otherwise. Therefore, it can be approximated with = 0.1 by a function h ∈ C. Then h is a function that distinguishes G 1 from G 2 , as in Definition 1. Hence C is GIso-discriminating.</p><p>To obtain a result on the reverse direction, we first introduce the concept of an augmented collection of functions, which is especially natural when C is a collection of neural networks. Definition 3. Given C, a collection of functions from X n×n to R, we consider an augmented collection of functions also from X n×n to R consisting of functions that map an input graph G to N N ([h 1 (G), ..., h d (G)]) for some finite d, where N N is a feed-forward neural network / multi-layer perceptron, and h 1 , ..., h d ∈ C. When N N is restricted to have L layers, we denoted this augmented collection by C +L . In this work, we consider ReLU as the nonlinear activation function in the neural networks. Remark 1. If C L0 is the collection of feed-forward neural networks with L 0 layers, then C +L</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L0</head><p>represents the collection of feed-forward neural networks with L 0 + L layers. Remark 2. If C is a collection of permutation-invariant functions, so is C +L . Theorem 2. If C is GIso-discriminating, then C +2 is universal approximating.</p><p>The proof is simple and it is a consequence of the following lemmas that we prove in Appendix A. Lemma 1. If C is GIso-discriminating, then for all G ∈ X n×n , there exists a functionh G ∈ C +1 such that for all G ,h G (G ) = 0 if and only if G G . Lemma 2. Let C be a class of permutation-invariant functions from X n×n to R satisfying the consequences of Lemma 1, then C +1 is universally approximating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extension to the case of continuous (Euclidean) feature space</head><p>Graph isomorphism is an inherently discrete problem, whereas universal approximation is usually more interesting when the input space is continuous. With our definition 1 of GIso-discriminating, we can achieve a natural generalization of the above results to the scenarios of continuous input space. All proofs for this section can be found in Appendix A.</p><p>Let X be a compact subset of R, and we consider graphs with n nodes represented by G ∈ K = X n×n ; that is, the node features are {G ii } i=1,...,n and the edge features are {G ij } i,j=1,...,n;i =j . Theorem 3. If C is universally approximating, then it is also GIso-discriminating</p><p>The essence of the proof is similar to that of Theorem 1. The other direction -showing that pairwise discrimination can lead to universal approximation -is less straightforward. As an intermediate step between, we make the following definition: Definition 4. Let C be a class of functions K → R. We say it is able to locate every isomorphism class if for all G ∈ K and for all &gt; 0 there exists h G ∈ C such that:</p><formula xml:id="formula_1">• for all G ∈ K, h G (G ) ≥ 0; • for all G ∈ K, if G G, then h G (G ) = 0; and • there exists δ G &gt; 0 such that if h G &lt; δ G , then ∃π ∈ S n such that d(π(G ), G) &lt; , where</formula><p>d is the Euclidean distance defined on R n×n Lemma 3. If C, a collection of continuous permutation-invariant functions from K to R, is GIsodiscriminating, then C +1 is able to locate every isomorphism class.</p><p>Heuristically, we can think of the h G in the definition above as a "loss function" that penalizes the deviation of G from the equivalence class of G. In particular, the second condition says that if the loss value is small enough, then we know that G has to be close to the equivalence class of G. Lemma 4. Let C be a class of permutation-invariant functions K → R. If C is able to locate every isomorphism class, then C +2 is universally approximating.</p><p>Combining the two lemmas above, we arrive at the following theorem: Theorem 4. If C, a collection of continuous permutation-invariant functions from K to R, is GIsodiscriminating, then C +3 is universaly approximating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A framework of representation power based on sigma-algebra 4.1 Introducing sigma-algebra to this context</head><p>Let K = X n×n be a finite input space. Let Q K := K/ be the set of isomorphism classes under the equivalence relation of graph isomorphism. That is, for all</p><formula xml:id="formula_2">τ ∈ Q K , τ = {π Gπ : π ∈ Γ n } for some G ∈ K.</formula><p>Intuitively, a maximally expressive collection of permutation-invariant functions, C, will allow us to know exactly which isomorphism class τ a given graph G belongs to, by looking at the outputs of certain functions in the collection applied to G. Heuristically, we can consider each function in C as a "measurement", which partitions that graph space K according to the function value at each point. If C is powerful enough, then as a collection it will partition K to be as fine as Q K . If not, it is going to be coarser than Q K . These intuitions motivate us to introduce the language of sigma-algebra.</p><p>Recall that an algebra on a set K is a collection of subsets of K that includes K itself, is closed under complement, and is closed under finite union. Because K is finite, we have that an algebra on K is also a sigma-algebra on K, where a sigma-algebra further satisfies the condition of being closed under countable unions. Since Q K is a set of (non-intersecting) subsets of K, we can obtain the algebra generated by Q K , defined as the smallest algebra that contains Q K , and use σ(Q K ) to denote the algebra (and sigma-algebra) generated by Q K .</p><formula xml:id="formula_3">Observation 1. If f : X n×n → R is a permutation-invariant function, then f is measurable with respect to σ(Q K ), and we denote this by f ∈ M[σ(Q K )]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now consider a class of functions</head><formula xml:id="formula_4">C that is permutation-invariant. Then for all f ∈ C, f ∈ M[σ(Q K )].</formula><p>We define the sigma-algebra generated by f as the set of all the pre-images of Borel sets on R under f , and denote it by σ(f ). It is the smallest sigma-algebra on K that makes f measurable. For a class of functions C, σ(C) is defined as the smallest sigma-algebra on K that makes all functions in C measurable. Because here we assume K is finite, it does not matter whether C is a countable collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reformulating graph isomorphism testing and universal approximation with sigma-algebra</head><p>We restrict our attention to the case of finite feature space. Given a graph G ∈ X n×n , we use E(G) to denote its isomorphism class, {G ∈ X n×n : G G}. The following results are proven in Section B Theorem 5. If C is a class of permutation-invariant functions on X n×n and C is GIso-discriminating, then σ(C) = σ(Q K )</p><p>Together with Theorem 1, the following is an immediate consequence:</p><formula xml:id="formula_5">Corollary 1.</formula><p>If C is a class of permutation-invariant functions on X n×n and C achieves universal approximation, then σ(C) = σ(Q K ).</p><formula xml:id="formula_6">Theorem 6. Let be C a class of permutation-invariant functions on X n×n with σ(C) = σ(Q K ).</formula><p>Then C is GIso-discriminating.</p><p>Thus, this sigma-algebra language is a natural notion for characterizing the power of graph neural networks, because as shown above, generating the finest sigma-algebra σ(Q K ) is equivalent to being GIso-discriminating, and therefore to universal approximation.</p><p>Moreover, when C is not GIso-discriminating or universal, we can evaluate its representation power by studying σ(C), which gives a measure for comparing the power of different GNN families. Given two classes of functions</p><formula xml:id="formula_7">C 1 , C 2 , there is σ(C 1 ) ⊆ σ(C 2 ) if and only if M[σ(C 1 )] ⊆ M[σ(C 2 )</formula><p>] if and only if C 1 is less powerful than C 2 in terms of representation power.</p><p>In Appendix C we use this notion to compare the expressive power of different families of GNNs as well as other algorithms like 1-WL, linear programming and semidefinite programming in terms of their ability to distinguish non-isomorphic graphs. We summarize our findings in <ref type="figure">Figure 1</ref>.</p><formula xml:id="formula_8">sGNN(I, A) LP ≡ 1 − W L ≡ GIN SDP MPNN * sGNN(I, D, A, {min{A t , 1}} T t=1 ) order 2 G-invariant networks * spectral methods</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SoS hierarchy</head><p>Ring-GNN <ref type="figure">Figure 1</ref>: Relative comparison of function classes in terms of their ability to solve graph isomorphism. * Note that, on one hand GIN is defined by <ref type="bibr" target="#b29">[30]</ref> as a form of message passing neural network justifying the inclusion GIN → MPNN. On the other hand <ref type="bibr" target="#b16">[17]</ref> shows that message passing neural networks can be expressed as a modified form of order 2 G-invariant networks (which may not coincide with the definition we consider in this paper). Therefore the inclusion GIN → order 2 G-invariant networks has yet to be established rigorously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ring-GNN: a GNN defined on the ring of equivariant functions</head><p>We now investigate the G-invariant network framework proposed in <ref type="bibr" target="#b17">[18]</ref> (see Appendix D for its definition and a description of an adapted version that works on graph-structured inputs, which we call the Graph G-invariant Networks). The architecture of G-invariant networks is built by interleaving compositions of equivariant linear layers between tensors of potentially different orders and point-wise nonlinear activation functions. It is a powerful framework that can achieve universal approximation if the order of the tensor can grow as n(n−1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, where n is the number of nodes in the graph, but less is known about its approximation power when the tensor order is restricted. One particularly interesting subclass of G-invariant networks is the ones with maximum tensor order 2, because <ref type="bibr" target="#b16">[17]</ref> shows that it can approximate any Message Passing Neural Network <ref type="bibr" target="#b7">[8]</ref>. Moreover, it is both mathematically cumbersome and computationally expensive to include equivariant linear layers involving tensors with order higher than 2.</p><p>Our following result shows that the order-2 Graph G-invariant Networks subclass of functions is quite restrictive. The proof is given in Appendix D. Theorem 7. Order-2 Graph G-invariant Networks cannot distinguish between non-isomorphic regular graphs with the same degree.</p><p>Motivated by this limitation, we propose a GNN architecture that extends the family of order-2 Graph G-invariant Networks without going into higher order tensors. In particular, we want the new family to include GNNs that can distinguish some pairs of non-isomorphic regular graphs with the same degree. For instance, take the pair of Circular Skip Link graphs G 8,2 and G 8,3 , illustrated in <ref type="figure">Figure 5</ref>. Roughly speaking, if all the nodes in both graphs have the same node feature, then because they all have the same degree, the updates of node states in both graph neural networks based on neighborhood aggregation and the WL test will fail to distinguish the nodes. However, the power graphs 1 of G 8,2 and G 8,3 have different degrees. Another important example comes from spectral methods that operate on normalized operators, such as the normalized Laplacian ∆ = I − D −1/2 AD −1/2 , where D is the diagonal degree operator. Such normalization preserves the permutation symmetries and in many clustering applications leads to dramatic improvements <ref type="bibr" target="#b26">[27]</ref>.</p><p>This motivates us to consider a polynomial ring generated by the matrices that are the outputs of permutation-equivariant linear layers, rather than just the linear space of those outputs. Together with point-wise nonlinear activation functions such as ReLU, power graph adjacency matrices like min(A 2 , 1) can be expressed with suitable choices of parameters. We call the resulting architecture the Ring-GNN 2 . <ref type="figure">Figure 2</ref>: The Circular Skip Link graphs G n,k are undirected graphs in n nodes q 0 , . . . , q n−1 so that (i, j) ∈ E if and only if |i − j| ≡ 1 or k (mod n). In this figure we depict (left) G 8,2 and (right) G <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3</ref> . It is very easy to check that G n,k and G n ,k are not isomorphic unless n = n and k ≡ ±k (mod n). Both 1-WL and G-invariant networks fail to distinguish them.</p><p>Definition 5 (Ring-GNN). Given a graph in n nodes with both node and edge features in R d , we represent it with a matrix A ∈ R n×n×d . <ref type="bibr" target="#b16">[17]</ref> shows that all linear equivariant layers from R n×n to R n×n can be expressed as <ref type="bibr" target="#b14">15</ref> are the 15 basis functions of all linear equivariant functions from R n×n to R n×n , L <ref type="bibr" target="#b15">16</ref> and L 17 are the basis for the bias terms, and θ ∈ R 17 are the parameters that determine L. Generalizing to an equivariant linear layer from R n×n×d to R n×n×d , we set</p><formula xml:id="formula_9">L θ (A) = 15 i=1 θ i L i (A) + 17 i=16 θ i L i , where the {L i } i=1,...,</formula><formula xml:id="formula_10">L θ (A) ·,·,k = d k=1 15 i=1 θ k,k ,i L i (A ·,·,i ) + 17 i=16 θ k,k ,i L i , with θ ∈ R d×d ×17 .</formula><p>With this formulation, we now define a Ring-GNN with T layers. First, set A (0) = A. In the t th layer, let</p><formula xml:id="formula_11">B (t) 1 = ρ(L α (t) (A (t) )) B (t) 2 = ρ(L β (t) (A (t) ) · L γ (t) (A (t) )) A (t+1) = k (t) 1 B (t) 1 + k (t) 2 B (t) 2 where k (t) 1 , k (t) 2 ∈ R, α (t) , β (t) , γ (t) ∈ R d (t) ×d (t) ×17</formula><p>are learnable parameters. If a scalar output is desired, then in the general form, we set the output to be θ S i,j A</p><formula xml:id="formula_12">(T ) ij + θ D i,i A (T ) ii + i θ i λ i (A (T ) ), where θ S , θ D , θ 1 , . . . , θ n ∈ R are trainable parameters, and λ i (A (T ) ) is the i-th eigenvalue of A (L) .</formula><p>Note that each layer is equivariant, and the map from A to the final scalar output is invariant. A Ring-GNN can reduce to an order-2 Graph G-invariant Network if k (t) 2 = 0. With J + 1 layers and suitable choices of the parameters, it is possible to obtain min(A 2 J , 1) in the (J + 1) th layer. Therefore, we expect it to succeed in distinguishing certain pairs of regular graphs that order-2 Graph G-invariant Networks fail on, such as the Circular Skip Link graphs. Indeed, this is verified in the synthetic experiment presented in the next section. The normalized Laplacian can also be obtained, since the degree matrix can be inverted by taking the reciprocal on the diagonal, and then entry-wise inversion and square root on the diagonal can be approximated by MLPs.</p><p>The terms in the output layer involving eigenvalues are optional, depending on the task. For example, in community detection spectral information is commonly used <ref type="bibr" target="#b14">[15]</ref>. We could also take a fixed number of eigenvalues instead of the full spectrum. In the experiments, Ring-GNN-SVD includes the eigenvalue terms while Ring-GNN does not, as explained in appendix E. Computationally, the complexity of running the forward model grows as O(n 3 ), dominated by matrix multiplications and possibly singular value decomposition for computing the eigenvalues. We note also that Ring-GNN can be augmented with matrix inverses or more generally with functional calculus on the spectrum of any of the intermediate representations <ref type="bibr" target="#b2">3</ref> while keeping O(n 3 ) computational complexity. Finally, note that a Graph G-invariant Network with maximal tensor order d will have complexity at least O(n d ). Therefore, the Ring-GNN explores higher-order interactions in the graph that order-2 Graph G-invariant Networks neglects while remaining computationally tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The different models and the detailed setup of the experiments are discussed in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Classifying Circular Skip Links (CSL) graphs</head><p>The following experiment on synthetic data demonstrates the connection between function fitting and graph isomorphism testing. The Circular Skip Links graphs are undirected regular graphs with node degree 4 <ref type="bibr" target="#b19">[20]</ref>, as illustrated in <ref type="figure">Figure 5</ref>. Note that two CSL graphs G n,k and G n ,k are not isomorphic unless n = n and k ≡ ±k (mod n). In the experiment, which has the same setup as in <ref type="bibr" target="#b19">[20]</ref>, we fix n = 41, and set k ∈ {2, <ref type="bibr">3, 4, 5, 6, 9, 11, 12, 13, 16}</ref>, and each k corresponds to a distinct isomorphism class. The task is then to classify a graph G n,k by its skip length k.</p><p>Note that since the 10 classes have the same size, a naive uniform classifier would obtain 0.1 accuracy. As we see from <ref type="table">Table 1</ref>, both GIN and G-invariant network with tensor order 2 do not outperform the naive classifier. Their failure in this task is unsurprising: WL tests are proved to fall short of distinguishing such pairs of non-isomorphic regular graphs <ref type="bibr" target="#b3">[4]</ref>, and hence neither can GIN <ref type="bibr" target="#b29">[30]</ref>; by the theoretical results from the previous section, order-2 Graph G-invariant network are unable to distinguish them either. Therefore, their failure as graph isomorphism tests is consistent with their failure in this classification task, which can be understood as trying to approximate the function that maps the graph to their class labels.</p><p>It should be noted that, since graph isomorphism tests are not entirely well-posed as classfication tasks, the performance of GNN models could vary due to randomness. But the fact that Ring-GNNs achieve a relatively high maximum accuracy (compared to RP for example) demonstrates that as a class of GNNs it is rich enough to contain functions that distinguish the CSL graphs to a large extent.  <ref type="table">Table 1</ref>: (left) Accuracy of different GNNs at classifying CSL (see Section 6.1). We report the best performance and worst performance among 10 experiments. (right) Accuracy of different GNNs at classifying real datasets (see Section 6.1). We report the best performance among all epochs on a 10-fold cross validation dataset, as was done in <ref type="bibr" target="#b29">[30]</ref>. †: Reported performance by <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">IMDB datasets</head><p>We use the two IMDB datasets (IMDBBINARY, IMDBMULTI) to test different models in realworld scenarios. Since our focus is on distinguishing graph structures, these datasets are suitable as they do not contain node features, and hence the adjacency matrix contains all the input data. IMDBBINARY dataset has 1000 graphs, with average number of nodes 19.8 and 2 classes. The dataset is randomly partitioned into 900/100 for training/validation. IMDBMULTI dataset has 1500 graphs, with average number of nodes 13.0 and 3 classes. The dataset is randomly partitioned into 1350/150 for training/validation. All models are evaluated via 10-fold cross validation and best accuracy is calculated through averaging across folds followed by maximizing along epochs <ref type="bibr" target="#b29">[30]</ref>. Importantly, the architecture hyper-parameter of Ring-GNN we use is close to that provided in <ref type="bibr" target="#b16">[17]</ref> to show that order-2 G-invariant Network is included in model family we propose. The results show that Ring-GNN models achieve higher performance than Order-2 G-invariant networks in both datasets. Admittedly its accuracy does not reach that of the state-of-the-art. However, the main goal of this part of our work is not necessarily to invent the best-performing GNN through hyperparameter optimization, but rather to propose Ring-GNN as an augmented version of order-2 Graph G-invariant Networks and show experimental results that support the theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work we address the important question of organizing the fast-growing zoo of GNN architectures in terms of what functions they can and cannot represent. We follow the approach via the graph isomorphism test, and show that is equivalent to the other perspective via function approximation. We leverage our graph isomorphism reduction to augment order-2 G-invariant nets with the ring of operators associated with matrix multiplication, which gives provable gains in expressive power with complexity O(n 3 ), and is amenable to efficiency gains by leveraging sparsity in the graphs.</p><p>Our general framework leaves many interesting questions unresolved. First, a more comprehensive analysis on which elements of the algebra are really needed depending on the application. Next, our current GNN taxonomy is still incomplete, and in particular we believe it is important to further discern the abilities between spectral and neighborhood-aggregation-based architectures. Finally, and most importantly, our current notion of invariance (based on permutation symmetry) defines a topology in the space of graphs that is too strong; in other words, two graphs are either considered equal (if they are isomorphic) or not. Extending the theory of symmetric universal approximation to take into account a weaker metric in the space of graphs, such as the Gromov-Hausdorff distance, is a natural next step, that will better reflect the stability requirements of powerful graph representations to small graph perturbations in real-world applications.</p><p>A Proofs on universal approximation and graph isomorphism Lemma 1. If C is GIso-discriminating, then for all G ∈ X n×n , there exists a functionh G ∈ C +1 such that for all G ,h G (G ) = 0 if and only if G G .</p><p>Proof of Lemma 1. Given G, G ∈ X n×n with G G , let h G,G ∈ C be the function that distinguishes this pair, i.e. h G,G (G) = h G,G (G ). Then define a function h G,G by</p><formula xml:id="formula_13">h G,G (G * ) = |h G,G (G * ) − h G,G (G)| = max(h G,G (G * ) − h G,G (G), 0) + max(h G,G (G) − h G,G (G * ), 0) (1) Note that if G * G, then h G,G (G * ) = h G,G (G), and so h G,G (G * ) = 0. If G * G , then h G,G (G * ) &gt; 0. Otherwise, h G,G (G * ) ≥ 0. Next, define a functionh G byh G (G * ) = G ∈X n×n ,G G h G,G (G * ). If G * G, we havẽ h G (G * ) = 0, whereas if G * G thenh G (G * ) &gt; 0.</formula><p>Thus, it suffices to show thath G ∈ C +1 . We take the finite subcollection of functions, {h G,G } G ∈X n×n ,G G , and feed the input graph G to each of them to obtain a vector of outputs. By equation 1, h G,G (G * ) can be obtained from h G,G (G * ) by passing through one ReLU layer. Finally, a finite summation across G G yieldsh G (G * ). Therefore,h G ∈ C +1 , ∀G ∈ X n×n .</p><p>Lemma 2 Let C be a class of permutation-invariant functions from X n×n to R so that for all G ∈ X n×n , there existsh G ∈ C satisfyingh G (G ) = 0 if and only if G G . Then C +1 is universally approximating.</p><p>Proof of Lemma 2. In fact, in the finite feature setting we can obtain a stronger result: for all f that is permutation-invariant, f ∈ C +1 , and so no approximation is needed.</p><p>We first use theh G 's to construct all the indicator functions 1 G G * as functions of G * on X n×n . To achieve this, because X n×n is finite, ∀G, we let δ G = 1 2 min G ∈X n×n ,G G |h G (G )| &gt; 0. We then introduce a "bump" function from R to R with parameters a and b, ψ a,b (x) = ψ((x − b)/a), where ψ(x) = max(x − 1, 0) + max(x + 1, 0) − 2 max(x, 0). Then ψ a,b (b) = 0, and supp(ψ a,b ) = (b − a, b + a). Now, we define a function ϕ G from X = {1, ..., M } to R by ϕ G (G * ) = ψ δ G ,0 (h G (G * )). Note that ϕ G (G * ) = 1 G G * as a function of G * on X n×n . Given f , thanks to the finiteness of the input space X n×n , we decompose it as f (</p><formula xml:id="formula_14">G * ) = ( 1 |Sn| G∈X n×n 1 G G * )f (G * ) = 1 |Sn| G∈X n×n f (G)1 G G * = 1 |Sn| G∈X n×n f (G)ϕ G (G * ).</formula><p>The right hand side can be realized in C +1 , since we can first take the finite collection of functions {h G } G∈X n×n and obtain {h G (G * )} G∈X n×n . Then, with an MLP with one hidden layer, we can obtain {ϕ G (G * )} G∈X n×n , a linear combination of which gives the right hand side, since each "f (G)" within the summation is a constant. Theorem 3. If C is universally approximating, then it is also GIso-discriminating</p><formula xml:id="formula_15">Proof of Theorem 3. ∀G 1 , G 2 ∈ K, if G 1 G 2 , define f 1 (G) = min π∈Sn d(G 1 , π Gπ)</formula><p>. It is a continuous and permutation-invariant function on K, and therefore can be approximated by a function h ∈ C to within = 1 2 f 1 (G 2 ) &gt; 0 accuracy. Then h is a function that can discriminate between G 1 and G 2 .</p><p>Lemma 3. If C, a collection of continuous permutation-invariant functions from K to R, is pairwise distinguishing, then C +1 is able to locate every isomorphism class.  <ref type="figure">B(G , )</ref>, the open -ball in K under the Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 3. Fix any</head><formula xml:id="formula_16">G ∈ K. ∀G G ∈ K, ∃h G,G ∈ C such that h G,G (G) = h G,G (G ). For each G , define a set A G as h −1 G,G ((h G,G (G ) − |h G,G (G )−h G,G (G)| 2 , h G,G (G ) + e e</formula><formula xml:id="formula_17">= {h G,G , h G,G } is GIso-discriminating. |h G,G (G )−h G,G (G)| 2 )) ⊆ K. Obviously G ∈ A G and G does not. Since h G,G is assumed continu- ous, A G is an open set for each G G. If G G, define A G =</formula><p>Thus, {A G } G ∈K is an open cover of K. Since K is compact, ∃ a finite subset K 0 of K such that {A G } G ∈K0 also covers K.</p><p>Hence, ∀G * ∈ K, ∃G ∈ K 0 such that G * ∈ A G . Moreover, ∀G * ∈ K \ ( G ∈E(G) A G ) = K \ ( π∈Sn B(π Gπ, )), where E(G) represents the equivalence class of graphs in K consisting of graphs isomorphic to G, ∃G ∈ K 0 \ E(G) such that G * ∈ A G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now define a functionh</head><formula xml:id="formula_18">G on K byh G (G * ) = G ∈K0\E(G) h G,G (G * ), where h G,G (G * ) = max( 2 3 |h G,G (G) − h G,G (G )| − |h G,G (G * ) − h G,G (G )|, 0).</formula><p>Since each h G,G in continuous, h G is also continuous. Thus, we can show thath G is the desired function in Definition 4:</p><formula xml:id="formula_19">• h G,G is nonnegative ∀G, G , and henceh G is nonnegative on K • If G * G, then as each h G,G is permutation invariant, there is h G,G (G * ) = h G,G (G),</formula><p>and hence h G,G (G * ) = 0. Thus,h G (G * ) = 0.</p><formula xml:id="formula_20">• If ∀π ∈ S n , d(π G * π, G) ≥ , then G * ∈ K \ G ∈E(G) A G . Therefore, ∃G ∈ K \ E(G) such that G * ∈ A G , which implies that |h G,G (G * ) − h G,G (G )| &lt; 1 2 |h G,G (G) − h G,G (G )| &lt; 2 3 |h G,G (G) − h G,G (G )|. Therefore, 2 3 |h G,G (G) − h G,G (G )|−|h G,G (G * )−h G,G (G )| &gt; 1 6 |h G,G (G)−h G,G (G )| &gt; 0, and soh G (G * ) ≥ h G,G (G * ) &gt; 1 6 |h G,G (G) − h G,G (G )|. Define δ G = 1 6 min G ∈K0\E(G) |h G,G (G) − h G,G (G )| &gt; 0.</formula><p>Then ifh G (G * ) &lt; δ G , it has to be the case that G * ∈ G ∈E(G) A G = π∈Sn B(π Gπ, ), implying that ∃π ∈ S n such that d(G * , π Gπ) &lt; .</p><p>Finally, it is clear thath G can be realized in C +1 . Lemma 4. Let C be a class of permutation-invariant functions K → R. If C is able to locate every isomorphism class, then C +2 is universally approximating.</p><p>Proof of Lemma 4. Consider any f that is continuous and permutation-invariant. Since K is compact, f is uniformly continuous on K. Therefore, ∀ &gt; 0, ∃r &gt; 0 such that ∀G 1 , <ref type="figure">([0, a)</ref>).</p><formula xml:id="formula_21">G 2 ∈ K, if d(G 1 , G 2 ) &lt; r, then |f (G 1 ) − f (G 2 )| &lt; . Given ∀G ∈ K, choose the function h G in definition 2. Use h −1 G (a) to denote h −1 G</formula><p>Then</p><formula xml:id="formula_22">∃δ G such that h −1 G (δ G ) ⊆ B(G, r), where B(G, r) is the ball in K centered at G with radius r (in Euclidean distance). Since h G is continuous, h −1 G (δ G ) is open. Therefore, {h −1 G (δ G )} G∈K is an open cover of K. Because K is compact, ∃ a finite subset K 0 ⊆ K such that {h −1 G (δ G )} G∈K0 also covers K. ∀G 0 ∈ K 0 , define another function ϕ G0 (G ) = δ G0 − h G0 (G ) if h G0 (G ) &lt; δ G0 and 0 oth- erwise. Therefore, supp(ϕ G0 ) = h −1 G0 (δ G0 ). Let ϕ(G ) = G * ∈K0 ϕ G * (G ), and then define ψ G0 (G ) = ϕ G 0 (G ) ϕ(G ) . Note that ∀G ∈ K, since {h −1 G (δ G )} G∈K0 covers K, ∃G * ∈ K 0 such that G ∈ h −1 G * (δ G * ) = supp(ϕ G * )</formula><p>, and so the denominator &gt; 0. Therefore, ψ G0 is well defined on K, and supp(ψ G0 ) = supp(ϕ G0 ) = h −1 G0 (δ G0 ). Moreover, ∀G ∈ K, G0∈K0 ψ G0 (G ) = 1. Therefore, the set of functions {ψ G0 } G0∈K0 is a "partition of unity", with respect to the open cover</p><formula xml:id="formula_23">{h −1 G (δ G )} G∈K0 .</formula><p>Back to the function f that we want to approximate. We want to express it in away that resembles what a neural network can do. With the set of functions {ψ G0 } G0∈K0 , we have</p><formula xml:id="formula_24">f (G ) = G0∈K0 f (G )ψ G0 (G ) = G0∈K0 G ∈h −1 G 0 (δ G 0 ) f (G )ψ G0 (G ) If G ∈ h −1 G0 (δ G0 , then d(G , G 0 ) &gt; r, and therefore |f (G ) − f (G 0 )| &lt; . Hence, we can usē h(G ) = G0∈K0 f (G 0 )ψ G0 (G ) to approximate f (G ), because |f (G ) − G0∈K0 f (G 0 )ψ G0 (G )| =|f (G ) − G0∈K0 G ∈h −1 G 0 (δ G 0 ) f (G 0 )ψ G0 (G )| = G0∈K0 G ∈h −1 G 0 (δ G 0 ) |f (G ) − f (G 0 )|ψ G0 (G ) &lt;<label>(2)</label></formula><p>Finally, we need to show how to approximateh with functions from C augmented with a multi-layer perceptron. We start with {h G0 } G0∈K ⊆ C, and apply them to the input graph G . Then, for each of h G0 G () apply an MLP with one hidden layer to obtain ϕ G0 (G ), and use one node to store. their sum, ϕ(G ). We then use an MLP with one hidden layer to approximate division, obtaining ψ G0 (G ). Finally,h(G ) is approximated by a linear combination of {ψ G0 (G )} G0∈K , since each f (G 0 ) is a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs of Section 4.2</head><p>Theorem 5. If C is a class of permutation-invariant functions on X n×n and C is GIso-discriminating, then σ(C) = σ(Q K )</p><p>Proof of Theorem 5. If C is GIso-discriminating, then given a G ∈ X n×n , ∀G</p><formula xml:id="formula_25">G, ∃h G ∈ C and b G ∈ R such that E(G) = ∩ G G h −1 G ({b G })</formula><p>, which is a finite intersection of sets in σ(C). Hence, E(G) ∈ σ(f G ) ⊆ σ(C). Therefore, Q K ⊆ σ(C), and hence σ(Q K ) ⊆ σ(C). Moreover, since σ(g) ⊆ σ(Q K ) for all g ∈ C, there is σ(C) ⊆ σ(Q K ) Theorem 6. Let be C a class of permutation-invariant functions on X n×n with σ(C) = σ(Q K ). Then C is GIso-discriminating.</p><p>Proof of Theorem 6. Suppose not. This implies that Q K σ(C), and hence ∃τ = E(G) ∈ Q K such that τ / ∈ σ(C). Note that τ is an equivalence class of graphs that are isomorphic to each other. Then consider the smallest subset in σ(C) that contains τ , defined as S(τ ) = T ∈σ(C) τ ⊆T T.</p><p>Since K is a finite space, σ(C) is also finite, and hence this is a finite intersection. Since a sigmaalgebra is closed under finite intersection, there is S(τ ) ∈ σ(C). As τ / ∈ σ(C), we know that τ S(τ ). Then, ∃G G such that G ∈ S(τ ). Then there does not exist any function h in C such that h(G) = h(G ), since otherwise the pre-image of some interval in R under h will intersect with only E(G) but not E(G ). Contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Comparison of expressive power of families of functions via graph isomorphism</head><p>Given two classes of functions C 1 , C 2 , such as two classes of GNNs, there are four possibilities regarding their relative representation power, using the language of sigma-algebra developed in the main text:</p><formula xml:id="formula_26">• σ(C 1 ) = σ(C 2 ) • σ(C 1 ) σ(C 2 ) • σ(C 2 ) σ(C 1 )</formula><p>• Not comparable / None of the above (i.e., σ(C 1 ) σ(C 2 ) and σ(C 1 ) σ(C 2 ))</p><p>In this section we summarize some results from the literature and show partial relationships between different GNNs architectures in terms of their ability to distinguish non-isomorphic graphs (in the context of the sigma algebra introduced in Section 4). For simplicity, in this section we assume that graphs are given by an adjacency matrix (no node nor edge features are considered). We illustrate our findings in <ref type="figure">Figure 1</ref>.</p><p>• sGNN(M). We consider spectral GNNs as the ones used in <ref type="bibr" target="#b4">[5]</ref> for community detection.</p><p>In this context we focus on the simplified version where the GNNs are defined as</p><formula xml:id="formula_27">v 0 =1 n v t+1 =ρ M ∈M M v t θ t M where θ t M ∈ R dt×dt+1 learnable parameters, v t ∈ R n×dt output : d L i=1 v L i .</formula><p>Usually M is a set of operators related to the graph. In this context we consider M = {I, A} and M (J) = {I, D, A, min{A 2 t , 1}, t = 2, . . .}. The operators min{A 2 t , 1} allow the model to distinguish regular graphs that order 2 G-invariant networks cannot distinguish, such as the Circular Skip Link graphs. • Linear Programming (LP). This is not a GNN but the natural linear programming relaxation for graph isomorphism. Namely given a pair graphs with adjacency matrix A, B ∈ {0, 1} n×n LP (A, B) = min P A − BP 1 subject to P 1 n = 1 n , P 1 n = 1 n , P ≥ 0.</p><p>The natural sigma algebra to consider here is σ(∪ A∈X n×n {LP (A, ·)}). Two graphs are said to be fractionally isomorphic is LP (A, B) = 0 (i.e. the LP cannot distinguish them). <ref type="bibr" target="#b22">[23]</ref> showed that two graphs are fractionally isomorphic if and only if they cannot be distinguished by 1-WL.</p><p>• Semidefinite Programming (SDP). The semidefinite programming relaxation of quadratic assignment from <ref type="bibr" target="#b31">[32]</ref> is based on the following observation: P A − BP 2 F = P A 2 F + BP 2 F − 2 trace(P AP B ) and trace(vec(P ) vec(P ) A ⊗ B ) where ⊗ is the Kronecker product operator and vec takes an n × n matrix and flattens it into an n 2 × 1 vector. The resulting semidefinite relaxation considers the vector x := [1, vec(P ) ] and relaxes the rank 1 matrix xx into a positive semidefinite matrix. By including the constraints corresponding to the LP in xx one makes sure that solution of the SDP is always in the feasible set of the LP, therefore the LP is less expressive than the SDP.</p><p>• Sum-of-Squares (SoS) hierarchy. One can consider the hierarchy of relaxations coming from sum-of-squares (SoS). In the context of graph isomorphism, it is known that graph isomorphism is a hard problem for this hierarchy <ref type="bibr" target="#b21">[22]</ref>. In particular the Lasserre/SoS hierarchy requires 2 Ω (n) to solve graph isomorphism (in the same sense that o(n)-WL fails to solve graph isomorphism <ref type="bibr" target="#b3">[4]</ref>).</p><p>• Spectral methods. If we consider the function that takes a graph and outputs the set of eigenvalues of its adjacency matrix, such function is permutation invariant. A priori one may think that such function, being highly non-linear, is more expressive than any form message passing GNN. In fact, regular graphs are not distinguished by 1-WL or order 2 G-invariant networks and may be distinguished by their eigenvalues (like the Circular Skip Link graphs). However, 1-WL and this particular spectral method are not comparable (a simple example is provided in <ref type="figure">Figure 2</ref> of <ref type="bibr" target="#b22">[23]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Graph G-invariant Networks with maximum tensor order 2</head><p>In this section we prove Theorem 7 that says that graph G-invariant Networks with tensor order 2 cannot distinguish between non-isomorphic regular graphs with the same degree.</p><p>First, we need to state our definition of the order-2 Graph G-invariant Networks. In general, given G ∈ R n×n , we let A (0) = G, d (0) = 1, and</p><formula xml:id="formula_28">A (t+1) = σ(L (t) (A (t) ))</formula><p>and outputs m • h • A (L) , where each L (t) is an equivariant linear layer from R n×n×d (t) to</p><formula xml:id="formula_29">R n×n×d (t+1)</formula><p>, σ is a point-wise activation function, h is an invariant linear layer from R n×n to R, and m is an MLP. d (t) is the feature dimension in layer t, interpreted as the dimension of the hidden state attached to each pair of nodes. For simplicity of notations, in the following proof we assume that d (t) = 1, ∀t = 1, ..., L, and thus each A (t) is essentially a matrix. The following results can be extended to the cases where d (t) &gt; 1, by adding more subscripts in the proof.</p><p>Given an unweighted graph G, let E ⊆ [n] 2 be the edge set of G, i.e., <ref type="bibr">(u, v</ref></p><formula xml:id="formula_30">) ∈ E if u = v and G uv = 1; set S ⊆ [n] 2 to be {(u, u)} u∈[n] 2 ; and let N = [n] 2 \ (E ∪ S). Thus, E ∪ N ∪ S = [n] 2 .</formula><p>Lemma 5. Let G, G be the adjacency matrices of two unweighted regular graphs with the same degree d, and let A (t) , E, N, S and A (t) , E , N , S be defined as above for G and G , respectively. Then ∀n ≤ L, ∃ξ</p><formula xml:id="formula_31">(t) 1 , ξ (t) 2 , ξ (t) 3 ∈ R such that A (t) uv = ξ (t) 1 1 (u,v)∈E + ξ (t) 2 1 (u,v)∈N + ξ (t) 3 1 (u,v)∈S , and A (t) uv = ξ (t) 1 1 (u,v)∈E + ξ (t) 2 1 (u,v)∈N + ξ (t) 3 1 (u,v)∈S</formula><p>Proof. We prove this lemma by induction. For t = 0, A (0) = G and A (0) = G . Since the graph is unweighted, G uv = 1 if u = v and (u, v) ∈ E, and 0 otherwise. Similar is true for G . Therefore, we can set ξ Next, we consider the inductive steps. Assume that the conditions in the lemma are satisfied for layer t − 1. To simplify the notation, we use A, A to stand for A (t−1) , A (t−1) , and we assume to satisfy the inductive hypothesis with ξ 1 , ξ 2 and ξ 3 . We thus want to show that if L is any equivariant linear, then σ(L(A)), σ(L(A )) also satisfies the inductive hypothesis. Also, in the following, we use p 1 , p 2 , q 1 , q 2 to refer to nodes, a, b to refer to pairs of nodes, λ to refer to any equivalence class of 2-tuples (i.e. pairs) of nodes, and µ to refer to any equivalence class of 4-tuples of nodes. ∀a = (p 1 , p 2 ), b = (q 1 , q 2 ) ∈ [n] 2 , let E(a, b) denote the equivalence class of 4-tuples containing (p 1 , p 2 , q 1 , q 2 ), and let E(b) represent the equivalence class of 2-tuples containing (q 1 , q 2 ). Two 4tuples (u, v, w, x), (u , v , w , x ) are considered equivalent if ∃π ∈ S n such that π(u) = u , π(v) = v , π(w) = w , π(x) = x . Similarly is equivalence between 2-tuples defined. By equation 9(b) in <ref type="bibr" target="#b16">[17]</ref>, using the notations of T, B, C, w, β defined there, L is described by, given A as an input as b as the subscript index on the output, </p><formula xml:id="formula_32">L(A) b = (n,n) a=(p1,p2)=(1,1) T a,b A a + Y b = a,µ w µ B µ a,b A a + λ β λ C λ b = µ ( a∈[n] 2 (a,b)∈µ A a )w µ + β E(b)<label>(3)</label></formula><formula xml:id="formula_33">ξ 3 = m E (b, µ)ξ 1 + m N (b, µ)ξ 2 + m S (b, µ)ξ 3<label>(4)</label></formula><p>where m E (b, µ) is defined as the total number of distinct a ∈ [n] 2 that satisfies (a, b) ∈ µ and a ∈ E, and similarly for m N (b, µ) and m S (b, µ). Formally, for example, m E (b, µ) = card{a ∈ [n] 2 : (a, b) ∈ µ, a ∈ E}.</p><p>Since E ∪ N ∪ S = [n] 2 , b belongs to one of E, N and S. Thus, let</p><formula xml:id="formula_34">τ (b) = E if b ∈ E, τ (b) = N if b ∈ N and τ (b) = S if b ∈ S.</formula><p>It turns out that if A is the adjacency matrix of a undirected regular graph with degree d, then m E (b, µ), m N (b, µ), m S (b, µ) can be instead written (with an abuse of notation) as m E (τ (b), µ), m N (τ (b), µ), m S (τ (b), µ), meaning that for a fixed µ, the values of m E , m N and m S only depend on which of the three sets (E, N or S) b is in, and changing b to a different member in the set τ (b) won't change the three numbers. In fact, for each τ (b) and µ, the three numbers can be computed as functions of n and d using simple combinatorics, and their values are seen in the three tables 2, 3 and 4. An illustration of these numbers is given in <ref type="figure">Figure D</ref>. <ref type="figure">E(1, 1)</ref>. Hence, we can write β τ (b) instead of β E(b) without loss of generality. Then in particular, this means that</p><formula xml:id="formula_35">Therefore, we have L(A) b = µ w µ (m E (τ (b), µ)+m N (τ (b), µ)+m S (τ (b), µ))+β E(b) . Moreover, notice that τ (b) determines E(b): if τ (b) = E or N , then E(b) = E(1, 2); if τ (b) = S, then E(b) =</formula><formula xml:id="formula_36">L(A) b = L(A) b if τ (b) = τ (b ). Therefore, L(A) b = ξ 1 1 b∈E +ξ 2 1 b∈N + ξ 3 1 b∈S , where ξ 1 = µ w µ (m E (E, µ)+m N (E, µ)+m S (E, µ))+β E , ξ 2 = µ w µ (m E (N, µ)+ m N (N, µ) + m S (N, µ)) + β N , and ξ 3 = µ w µ (m E (S, µ) + m N (S, µ) + m S (S, µ)) + β S .</formula><p>Similarly, L(A ) b = ξ 1 1 b∈E + ξ 2 1 b∈N + ξ 3 1 b∈S . But importantly, ∀ equivalence class of 4-tuples, µ, and ∀λ 1 , λ 2 ∈ {E, N, S}, m λ1 (λ 2 , µ) = m λ1 (λ 2 , µ), as both of them can be obtained from the same entry of the same table. Therefore, ξ 1 = ξ 1 , ξ 2 = ξ 2 , ξ 3 = ξ 3 .</p><p>Finally, let ξ * 1 = σ(ξ 1 ), ξ * 2 = σ(ξ 2 ), and ξ * 3 = σ(ξ 3 ). Then, there is σ(L(A)) b = ξ * 1 1 b∈E + ξ * 2 1 b∈N + ξ * 3 1 b∈S , and σ(L(A )) b = ξ * 1 1 b∈E + ξ * 2 1 b∈N + ξ * 3 1 b∈S , as desired.  <ref type="figure" target="#fig_0">E(1, 2, 3, 4)</ref>), m E <ref type="figure" target="#fig_0">(E, E(1, 2, 3, 2)</ref>), m E <ref type="figure" target="#fig_0">(E, E(1, 2, 3, 1)</ref>), m E <ref type="figure" target="#fig_0">(E, E(1, 2, 2, 3)</ref>) and m E <ref type="figure" target="#fig_0">(E, E(1, 2, 1, 3)</ref>) of G 8,2 and G 8,3 . In either graph, twice the total number of black edges equal m E (E, E(1, 2, 3, 4)) = 18 (it is twice because each undirected edge corrspond to two pairs (p 1 , p 2 ) and (p 2 , p 1 ), which combined with (q 1 , q 2 ) both belongs to E(1, 2, 3, 4)); the total number of of red edges, 3, equals both m E (E, E(1, 2, 2, 3)) and m E (E, <ref type="figure" target="#fig_0">E(1, 2, 1, 3)</ref>); the total number of green edges, also 3, equals both m E (E, E(1, 2, 3, 2)), m E (E, E(1, 2, 3, 1)).  <ref type="table">Table 2</ref>: m E Since h is an invariant function, h acting on A (L) essentially computes the sum of all the diagonal terms (i.e., for b ∈ S) and the sum of all the off-diagonal terms (i.e., for b ∈ E ∪N ) of A (L) separately and then adds the two sums with two weights. If G, G are regular graphs with the same degree, then |E| = |E |, |S| = |S | and |N | = |N |. Therefore, by the lemma, there is h(A (L) ) = h(A (L) ), and as a consequence m(h(A (L) )) = m(h(A (L) )). µ m N (E, µ) m N (N, µ) m N (S, µ) (1, 2, 3, 4) (n − 4)(n − d − 1) (n − 4)(n − d − 1) + 2 0 (1, 1, 2, 3) 0 0 0 (1, 2, 2, 3) n − d − 1 n − d − 2 0 (1, 2, 1, 3) n − d − 1 n − d − 2 0 (1, 2, 3, 2) n − d − 1 n − d − 2 0 (1, 2, 3, 1) n − d − 1 n − d − 2 0 (1, 1, 1, 2) 0 0 0 (1, 1, 2, 1) 0 0 0 (1, 2, 1, 2) 0 1 0 (1, 2, 2, 1) 0 1 0 (1, 2, 3, 3) 0 0 (n − 2)(n − d − 1) (1, 1, 2, 2) 0 0 0 (1, 2, 2, 2) 0 0 n − d − 1 (1, 2, 1, 1) 0 0 n − d − 1 (1, 1, 1, 1) 0 0 0 Total n(n − d − 1) n(n − d − 1) n(n − d − 1) <ref type="table">Table 3</ref>: m N µ m S (E, µ) m S (N, µ) m S (S, µ) (1, 2, 3, 4) 0 0 0 (1, 1, 2, 3) n − 2 n − 2 0 (1, 2, 2, 3) 0 0 0 (1, 2, 1, 3) 0 0 0 (1, 2, 3, 2) 0 0 0 (1, 2, 3, 1) 0 0 0 (1, 1, 1, 2) 1 1 0 (1, 1, 2, 1) 1 1 0 (1, 2, 1, 2) 0 0 0 (1, 2, 2, 1) 0 0 0 (1, 2, 3, 3) 0 0 0 (1, 1, 2, 2) 0 0 n − 1 (1, 2, 2, 2) 0 0 0 (1, 2, 1, 1) 0 0 0 (1, 1, 1, 1) 0 0 1 Total n n n <ref type="table">Table 4</ref>: m S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Specific GNN Architectures</head><p>In section 6, we show experiments on synthetic and real datasets with several related architectures. Here are some explanations for them.</p><p>• Ring-GNN: As defined in the main text. The architecture (number of hidden layers, feature dimensions) is taken to be the same os the Order-2 Graph G-invariant Networks. For the experiments on the IMDB datasets, each k (t) 1 is initialized independently under N (0, 1), and each k (t) 2 is initialized independently under N (0, 0.01). They are trained using the Adam optimizer with learning rate 0.00001. The initialization of k (t) 2 and the learning rate were manually tuned, following the heuristic that Ring-GNN reduces to Order-2 Graph G-invariant Networks when k (t) 2 = 0, and that since Ring-GNN added more operators, a smaller learning rate is likely more appropriate.</p><p>• Ring-GNN-SVD: Compared with above Ring-GNN model, a Singular Value Decomposition layer is added between Ring layers and fully-connected layers. SVD layer takes as input batch-size×channels matrices and as output batch-size×channels×5 top eigenvalues.</p><p>Considering computation complexity and condition numbers, this model has only two Ring layers and careful initialization. For IMDB datasets, Ring layers have numbers of channels in {16, 32} and the model is trained using Adam optimizer with learning rate of 0.001 for 350 epochs. For CSL dataset, Ring layers have numbers of channels in {4, 8} and the model is trained using Adam optimizer with learning rate of 0.001 for 1000 epochs. In both cases, each k (t) 1 is initializated independently under N (0, 0.5) and each k (t) 2 is initializated independently under N (0, 0.005). It is also noted, since often easily dropping into ill condition when using back propagation of SVD, we clip gradient values when training. Moreover, from prospective of computation resources, Nvidia V100 and P40 are much more numerically robust than 1080Ti and CPU in this task.</p><p>For the experiments with Circular Skip Links graphs, each model is trained and evaluated using 5-fold cross-validation. For Ring-GNN, in particular, we performed training + cross-validation 20 times with different random seeds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustrating the definition of GIso-discriminating. G, G and G are mutually nonisomorphic, and each of the big circles with dashed boundary represents an equivalence class under graph isomorphism. h G,G is a permutation-invariant function that obtains different values on equivalence class of G and on that of G , and similar h G,G . If the graph space has only these three equivalence classes of graphs, then C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>q q 1 1 q q 2 2 q q 1 1 q q 2 2 Figure 4 :</head><label>12124</label><figDesc>m E (E,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>µ m E (E, µ) m E (N, µ) m E (S, µ) (1, 2, 3, 4) (n − 4)d + 2 (n − 4)d 0</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If A is the adjacency matrix of a graph, its power graph has adjacency matrix min(A 2 , 1). The matrix min(A 2 , 1) has been used in<ref type="bibr" target="#b4">[5]</ref> in graph neural networks for community detection and in<ref type="bibr" target="#b20">[21]</ref> for the quadratic assignment problem.<ref type="bibr" target="#b1">2</ref> We call it Ring-GNN since the main object we consider is the ring of matrices, but technically we can express an associative algebra since our model includes scalar multiplications.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">When A = A (0) is an undirected graph, one easily verifies that A (t) contains only symmetric matrices for each t.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Haggai Maron for fruitful discussions and for pointing us towards G-invariant networks as powerful models to study representational power in graphs. This work was partially supported by NSF grant RI-IIS 1816753, NSF CAREER CIF 1845360, the Alfred P. Sloan Fellowship, Samsung GRP and Samsung Electronics. SV was partially funded by EOARD FA9550-18-1-7007 and the Simons Collaboration Algorithms and Geometry.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph isomorphism in quasipolynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-eighth annual ACM symposium on Theory of Computing</title>
		<meeting>the forty-eighth annual ACM symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="684" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06082</idno>
		<title level="m">Probabilistic symmetry and invariant neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internation Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mapping images to scene graphs with permutation-invariant structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshiko</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7211" to="7221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04943</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spectral redemption in clustering sparse networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Neeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Sly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenka</forename><surname>Zdeborová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="20935" to="20940" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipman</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yaron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11136</idno>
		<title level="m">Provably powerful graph networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nimrod</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09342</idno>
		<title level="m">On the universality of invariant networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks. Association for the Advancement of Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02541</idno>
		<title level="m">Relational pooling for graph representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A note on learning algorithms for quadratic assignment with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hardness of robust graph isomorphism, lasserre gaps, and asymmetry of random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1659" to="1677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Motakuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Edward R Scheinerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fractional isomorphism of graphs. Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="247" to="265" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Equivariance through parametersharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Universal approximations of invariant maps by neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Yarotsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10306</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semidefinite programming relaxations for the quadratic assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">E</forename><surname>Karisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Rendl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Wolkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Optimization</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="109" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">min(A 2 i−1 , 1)}, i ∈ {1, 2, 5}. In our experiments, the sGN N models have 5 layers and hidden layer dimension (i.e. d k ) 64</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">sGNNs with operators from family {I, D, min(A 2 0 , 1)</title>
		<imprint/>
	</monogr>
	<note>They are trained using the Adam optimizer with learning rate 0.01</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Line Graph Neural Networks proposed by [5]. In our experiments, the sGN N models have 5 layers and hidden layer dimension (i.e. d k ) 64</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Lgnn</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>They are trained using the Adam optimizer with learning rate 0.01</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph Isomorphism Network by [30]. We took their performance results on the IMDB datasets reported in [30], and their performance results on the Circular Skip Link graphs experiments reported in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Gin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Rp-Gin</surname></persName>
		</author>
		<title level="m">Graph Isomorphism Network combined with Relational pooling by</title>
		<imprint/>
	</monogr>
	<note>20]. We took the reported results reported in [20] for the Circular Skip Link graphs experiment</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">• Order-2 Graph G-invariant Networks: G-invariant networks based on</title>
		<ptr target="https://github.com/Haggaim/InvariantGraphNetworks" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
