<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fahad</roleName><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action localization is a challenging computer vision problem with numerous real-world applications. Most existing methods require laborious frame-level supervision to train action localization models. In this work, we propose a framework, called 3C-Net, which only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count. We introduce a novel formulation to learn discriminative action features with enhanced localization capabilities. Our joint formulation has three terms: a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability and a counting loss term to delineate adjacent action sequences, leading to improved localization. Comprehensive experiments are performed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our approach sets a new state-of-the-art for weakly-supervised temporal action localization on both datasets. On the THU-MOS14 dataset, the proposed method achieves an absolute gain of 4.6% in terms of mean average precision (mAP), compared to the state-of-the-art <ref type="bibr" target="#b15">[16]</ref>. Source code is available at https://github.com/naraysa/3c-net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action localization in untrimmed videos is a challenging problem due to intra-class variations, cluttered background, variations in video duration, and changes in viewpoints. In temporal action localization, the task is to find the start and end time (temporal boundaries or extent) of actions in a video. Most existing action localization approaches are based on strong supervision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>, requiring manually annotated ground-truth temporal boundaries of actions during training. However, framelevel action boundary annotations are expensive compared to video-level action label annotations. Further, unlike object boundary annotations in images, manual annotations of temporal action boundaries are more subjective and prone to large variations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>. Here, we focus on learning to temporally localize actions using only video-level supervision, commonly referred to as weakly-supervised learning.</p><p>Weakly-supervised temporal action localization has been investigated using different types of weak labels, e.g., action categories <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14]</ref>, movie scripts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref> and sparse spatio-temporal points <ref type="bibr" target="#b12">[13]</ref>. Recently, Paul et al. <ref type="bibr" target="#b15">[16]</ref> proposed an action localization approach, demonstrating stateof-the-art results, using video-level category labels as the weak supervision. In their approach <ref type="bibr" target="#b15">[16]</ref>, a formulation based on co-activity similarity loss is introduced which distinguishes similar and dissimilar temporal segments (regions) in paired videos containing same action categories. This leads to improved action localization results. However, the formulation in <ref type="bibr" target="#b15">[16]</ref> puts a constraint on the mini-batch, used for training, to mostly contain paired videos with actions belonging to the same category. In this work, we look into an alternative formulation that allows the mini-batch to contain diverse action samples during training.</p><p>We propose a framework, called 3C-Net, using a novel formulation to learn discriminative action features with enhanced localization capabilities using video-level supervision. As in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, our formulation contains a classification loss term that ensures the inter-class separability of learned features, for video-level action classification. However, this separability at the global video-level alone is insufficient for accurate action localization, which is generally a local temporal-context classification. This can be observed in <ref type="figure">Fig. 1</ref>, where the network trained with classification loss alone, denoted as 'CLS', localizes multiple instances of an action (central portion of the timeline) as a single instance. We therefore introduce two additional loss terms in our formulation that ensure both the discriminability of action categories at the global-level and separability of instances at the local-level.</p><p>The first additional term in our formulation is the center loss <ref type="bibr" target="#b29">[30]</ref>, introduced here for multi-label action classification. Originally designed for the face recognition problem <ref type="bibr" target="#b29">[30]</ref>, the objective of the center loss term is to reduce the intra-class variations in the feature representation of the training samples. This is achieved by learning the classspecific centers and penalizing the distance between the fea-Ground-truth (GT) CLS CLS + CL Ours (CLS+CL+CT) <ref type="figure">Figure 1</ref>. Predicted action proposals for a video clip containing PoleVault action category from THUMOS14 dataset. Sample frames from the video are shown in the top row. Frames containing actions have a blue border. GT indicates the ground-truth segments in the video containing the action. The network trained with classification loss term alone (CLS) inaccurately merges the four actions instances in the middle as a single instance. The network trained on classification and center loss terms (denoted as CLS + CL) improves the action localization but only partially delineates the merged action instances. The proposed 3C-Net framework, denoted as Ours (CLS + CL + CT), trained using a joint formulation of classification, center and counting loss terms, delineates the adjacent action instances in the middle. White regions in the timeline indicate background regions which do not contain actions of interest.</p><p>tures and their respective class centers. However, the standard center loss operates on training samples representing single-label instances. This prohibits its direct applicability in our multi-label action localization settings. We therefore propose to use a class-specific attention-based feature aggregation scheme to utilize multi-label action videos for training with center loss. As a result, a discriminative feature representation is obtained for improved localization. This improvement over 'CLS' can be observed in <ref type="figure">Fig. 1</ref>, where the network trained using the classification and center loss terms, denoted as 'CLS + CL', partially solves the incorrect grouping of multiple action instances.</p><p>The final term in our formulation is a counting loss term, which enhances the separability of action instances at the local-level. Count information has been previously exploited in the image domain for object delineation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>. In this work, the counting loss term incorporates information regarding the frequency of an action category in a video. The proposed loss term minimizes the distance between the predicted action count in a video and the ground-truth count. Consequently, the prediction scores sum up to a positive value within action instances and zero otherwise, leading to improved localization. This can be observed in <ref type="figure">Fig. 1</ref>, where the proposed 3C-Net trained using all the three loss terms, denoted as 'Ours (CLS + CL + CT)', delineates all four adjacent action instances, thereby leading to improved localization. Our counting term utilizes video-level action count and does not require user-intensive action location information (e.g. temporal boundaries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>We introduce a weakly-supervised action localization framework, 3C-Net, with a novel formulation. Our formulation consists of a classification loss to ensure inter-class separability, a multi-label center loss to enhance the feature discriminability and a counting loss to improve the separability of adjacent action instances. The three loss terms in our formulation are jointly optimized in an end-to-end fashion. To the best of our knowledge, we are the first to propose a formulation containing center loss for multi-label action videos and counting loss to utilize video-level action count information for weakly-supervised action localization.</p><p>We perform comprehensive experiments on two benchmarks: THUMOS14 <ref type="bibr" target="#b8">[9]</ref> and ActivityNet 1.2 <ref type="bibr" target="#b2">[3]</ref>. Our joint formulation significantly improves the baseline containing only classification loss term. Further, our approach sets a new state-of-the-art on both datasets and achieves an absolute gain of 4.6% in terms of mAP, compared to the best existing weakly-supervised method on THUMOS14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Temporal action localization in untrimmed videos is a challenging problem that has gained significant attention in recent years. This is evident in popular challenges, such as THUMOS <ref type="bibr" target="#b8">[9]</ref> and ActivityNet <ref type="bibr" target="#b2">[3]</ref>, where a separate track is dedicated to the problem of temporal action localization in untrimmed videos. Weakly-supervised action localization mitigates the need for temporal action boundary annotations and is therefore an active research problem. In the standard settings, only action category labels are available to train a localization model. Existing approaches have investigated different weak supervision strategies for action localization. The work of <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> use action category labels in videos for temporal localization, whereas <ref type="bibr" target="#b12">[13]</ref> uses point-level supervision to spatio-temporally localize the actions. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2]</ref> exploit the order of actions in a video as a weak supervision cue. The work of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref> use video subtitles and movie scripts to obtain coarse temporal localization for training, while <ref type="bibr" target="#b0">[1]</ref> utilizes actor-action pairs extracted from scripts for learning spatial actor-action localization. Recent work of <ref type="bibr" target="#b7">[8]</ref> shows that object counting with image-level supervision is less expensive, in terms of annotation cost, compared to instance-level supervision (e.g., bounding-box). In this work, we propose to use action instance count as an additional cue for weakly-supervised action localization.</p><p>State-of-the-art weakly-supervised action localization methods utilize both appearance and motion features, typi-cally extracted from backbone networks trained for the action recognition task. The work of <ref type="bibr" target="#b27">[28]</ref> proposes a framework that consists of a classification and a selection module for classifying the actions and detecting the relevant temporal segments, respectively. The approach uses a two-stream Temporal Segment Network <ref type="bibr" target="#b28">[29]</ref> as its backbone and employs a classification loss for training. In <ref type="bibr" target="#b13">[14]</ref>, a two-stream architecture is used to learn temporal class activation maps and a class-agnostic temporal attention. Their combination is then used to localize the human actions. Classification and sparsity-based losses are used to learn the activation maps and temporal attention, respectively. Recently, <ref type="bibr" target="#b15">[16]</ref> proposed a framework to learn temporal localization from video-level labels, where a classification loss and a triplet loss for matching similar segments of an action category in paired videos is employed. In this work, we propose a joint formulation with explicit loss terms to ensure the separability of learned action features, enhance the feature discriminability and delineate adjacent action instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first describe the feature extraction scheme used in our approach. We then present our overall architecture followed by a detailed description of the different loss terms in the proposed formulation. Feature Extraction: As in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, we use Inflated 3D (I3D) features extracted from the RGB and flow I3D deep networks <ref type="bibr" target="#b3">[4]</ref>, trained on the Kinetics dataset, to encode appearance and motion information, respectively. A video is divided into non-overlapping segments, each consisting of 16 frames. The input to the RGB and flow I3D networks are the color and the corresponding optical flow frames of a segment, respectively. A D-dimensional output I3D feature per segment, from each of the two networks, is used as input to the respective RGB and flow streams in our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>Our overall 3C-Net architecture is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. In our approach, both appearance (RGB) and motion (flow) features are processed in parallel streams. The two streams are then fused at a later stage of the network. Both streams are structurally identical in design. Each stream in our network comprises of three fully-connected (FC) layers. Guided by the center loss <ref type="bibr" target="#b29">[30]</ref>, the first two FC layers learn to transform the I3D features into a discriminative intermediate feature representation. The final FC layer projects the intermediate features into the action category space under the guidance of the classification loss. The outputs of the final FC layer represent the sequence of classification scores for each action over time. This class-specific 1D representation, similar to the 2D class activation map in object detection <ref type="bibr" target="#b33">[34]</ref>, is called temporal class activation map (T-CAM), as in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Given a training video v i , let y i ∈ R Nc denote the ground-truth multi-hot vector indicating the presence or absence of an action category in</p><formula xml:id="formula_0">v i , where i ∈ [1, N ].</formula><p>Here, N is the number of videos and N c is the number of action classes in the dataset. Let x a i , x f i ∈ R si×D denote the intermediate features (outputs of the second FC layer) in the two streams, respectively. Here, s i denotes the length (number of segments) of the video v i . The output of the final FC layers represent the T-CAMs, denoted by C a i , C f i ∈ R si×Nc , for the RGB and flow streams, respectively. The two T-CAMs (C a i and C f i ) are weighted by learned class-specific parameters, w a , w f ∈ R Nc , and later combined by addition to result in the final T-CAM, C F i ∈ R si×Nc . The learning of the final T-CAM, C F i is guided by the classification and counting loss terms. Consequently, our 3C-Net framework is trained using the overall loss formulation,</p><formula xml:id="formula_1">L = L cls + αL center + βL count<label>(1)</label></formula><p>where L cls , L center and L count denote the classification loss, center loss and counting loss terms, respectively. The respective weights for the center loss and counting loss terms are denoted by α and β. Next, we describe the three loss terms utilized in the proposed formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Classification Loss</head><p>The classification loss term is used in our formulation to ensure the inter-class separability of the features at the video-level and tackles the problem of multi-label action classification in the video. We utilize the cross-entropy classification loss as in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b15">16]</ref>, to recognize different action categories in a video. The number of segments per video varies greatly in untrimmed videos. Hence, the top-k values per category (where k = s i /8 , is proportional to the length, s i , of the video) of a T-CAM 1 (C a i ) are selected, as in <ref type="bibr" target="#b15">[16]</ref>. This results in a representation of size k × N c , for the video. Further, a temporal averaging is performed on this representation to obtain a class-specific encoding, r a i ∈ R Nc , for the T-CAM, C a i . Consequently, a probability mass function (pmf), p a i ∈ R Nc , is computed using</p><formula xml:id="formula_2">p a i (j) = exp(r a i (j)) l exp(r a i (l))<label>(2)</label></formula><p>where j ∈ [1, N c ] denotes the action category. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the 'Classification Module (CLS)' performs topk temporal pooling, averaging and category-wise softmax operations and outputs a predicted pmf, p a i for an input, C a i . The multi-hot encoded ground-truth action labels y i , are l 1 -normalized to generate a ground-truth pmf, q i . The classification loss is then represented as the cross-entropy between p a i and q i . Let L a cls = −E[q T i log(p a i )] denote the classification loss for the RGB stream, where p a i is the pmf computed from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Center Loss for Multi-label Classification</head><p>We adapt and integrate the center loss term <ref type="bibr" target="#b29">[30]</ref> in our overall formulation to cluster the features of different categories such that the same action category features are grouped together. The center loss learns the cluster centers of each action class and penalizes the distance between the features and the corresponding class centers. The objective of the classification loss, commonly employed in action localization, is to ensure the inter-class separability of learned features, whereas the center loss aims to enhance their discrminability through action-specific clustering and minimizing the intra-class variations. However, the standard center loss, originally proposed for face recognition <ref type="bibr" target="#b29">[30]</ref>, operates on training samples representing single-label instances. This hinders its usage in multi-label weakly-supervised action localization settings, where training samples (videos) contain multiple action categories. To counter this issue, we employ an attention-based per-class feature aggregation strategy to utilize videos with multiple action categories for training with the center loss. To the best of our knowledge, we are the first to introduce the center loss with multi-label training samples for weakly supervised action localization.</p><p>In the proposed 3C-Net framework, the center loss is ap-plied on the features 1 , x a i (output of the penultimate FC layer as in <ref type="figure" target="#fig_0">Fig. 2)</ref>. Typically, videos vary in length (s i ) and contain multiple action classes. Additionally, the action duration may be relatively short in untrimmed videos. Hence, aggregating category-specific features by considering only the high attention regions of those categories in the video is required. We perform the feature aggregation step on x a i and compute a single feature f a i (j) ∈ R D if y i (j) = 0 (i.e., if the action category j is present in video v i ). In the case of action categories which are not present in a video, the feature aggregation step is not performed, since these categories will not have a meaningful feature representation in that video. To this end, we first compute the attention, a a i ∈ R si×Nc , over time t, for a category j, using</p><formula xml:id="formula_3">a a i (t, j) = exp(C a i (t, j)) l exp(C a i (l, j))<label>(4)</label></formula><p>where C a i represents the RGB stream T-CAM for video v i . A threshold, τ j =median(a a i (j)) is used to set the attention weights less than τ j to 0 (i.e. a a i (t, j) = 0, if a a i (t, j) &lt; τ j )). Here, s i is the length of the video. This thresholding enables feature aggregation from category-specific high-attention regions of the video. The resulting aggregated features, f a i (j), are then used with the center loss. The aggregated feature f a i (j) is computed using</p><formula xml:id="formula_4">f a i (j) = t a a i (t, j)x a i (t) t a a i (t, j)<label>(5)</label></formula><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the 'Center Loss Module (CL)' implements Eq. 4 and 5 for each stream, using the outputs of the FC layers of the respective stream. Let c a j ∈ R D be the cluster center associated with the action category j. Following <ref type="bibr" target="#b29">[30]</ref>, the center loss and the update for center c a j , used in our multi-label formulation, are given by,</p><formula xml:id="formula_5">L a center = 1 N i j:yi(j)=1 ||f a i (j) − c a j || 2 2 (6) ∆c a j = i:yi(j)=1 (c a j − f a i (j)) 1 + i y i (j)<label>(7)</label></formula><p>For every category j, present in a mini-batch, the corresponding center, c a j is updated using its ∆c a j during training. The loss for the flow stream, L f center , is also computed in a similar manner. The total center loss is then given by,</p><formula xml:id="formula_6">L center = L a center + L f center<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Counting Loss</head><p>In this work, we propose to use auxiliary count information in addition to standard action category labels for weakly-supervised action localization. Here, count refers to the number of instances of an action category occurring in a video. As discussed earlier, integrating count information enhances the feature representation and delineation of temporally adjacent action instances in the video, leading to an improved temporal localization. In our 3C-Net framework, the counting loss is applied on the final T-CAM, C F i . To compute the predicted count, first, the element-wise product of the category-specific temporal attention and the final T-CAM, C F i , is performed. The resulting attentionweighted T-CAM is equivalent to a density map <ref type="bibr" target="#b5">[6]</ref> of the action category, and its summation yields the predicted count of that category. Let the attention for action category j be a F i (j), which is computed using the final T-CAM, similar to Eq. 4. The predicted count for category j is given by,</p><formula xml:id="formula_7">m i (j) = t a F i (t, j)C F i (t, j)<label>(9)</label></formula><p>where m i (j) represents the sum of activation weighted by the temporal attention, over time for the j th action category. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the 'Counting Module (CT)' implements Eq. 4 and 9 for the final T-CAM, C F i . Temporal attention weighting ignores the background video segments not containing the action category j.</p><p>In the context of action localization, we observe that videos with a higher action count tend to have higher errors in count prediction during training. Training with absolute error results in an inferior T-CAM, since the mini-batch loss will be dominated by the count prediction error for the videos with a higher action count. To tackle this issue, we use a simple yet effective weighting strategy, where errors are inversely weighted depending on the action count in a video. A lower weight is assigned when the action count in a video is high and vice versa. The weighting penalizes the count error (ce) more at lower ground-truth count (GTC) compared to the same magnitude of ce at higher GTC. E.g., ce = 1 at GTC of 5 is emphasized over ce = 1 at GTC of 100. To obtain a relative error for per-category count prediction, we divide the absolute error by the GTC of the categories present in the video. Absolute error is used for the action categories that are not present in a video to ensure that their predicted count is zero. The counting loss is then given by,</p><formula xml:id="formula_8">L + count = 1 N i j:ni(j)&gt;0 |m i (j) − n i (j)| n i (j) L − count = 1 N i j:ni(j)=0 |m i (j)| L count = L + count + λL − count<label>(10)</label></formula><p>where n i ∈ R Nc is the ground-truth count label and λ is a hyper-parameter, typically set to 10 −3 to compensate for the ratio of positive to negative instances for an action class.</p><p>To summarize, the loss terms in our overall formulation enhance the separability and discriminability of the learned features and improve the delineation of adjacent action instances. Consequently, a disrcriminative and improved T-CAM representation is obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Classification and Localization using T-CAM</head><p>After training the 3C-Net, the CLS module (see <ref type="figure" target="#fig_0">Fig. 2</ref> and Eq. 2) is used to compute the action-class scores (pmf) at the video-level using the final T-CAM, for the action classification task. Similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b15">16]</ref>, we use the computed pmf without threshold, for evaluation. For the action localization task, detections are obtained using a similar approach used in <ref type="bibr" target="#b15">[16]</ref>. Detections in a video are generated for the action categories with average top-k score above 0 (i.e. for categories in set {j : r F i (j) &gt; 0}, where r F i is computed as in Sec. 3.2 using the final T-CAM). For a category j in the obtained set, continuous video segments between successive time instants when T-CAM goes above and below threshold η, correspond to a valid action detection. The resulting detections of an action category are non-overlapping. A weighted sum of the highest T-CAM value with in the detection and the category score for the video, corresponds to the score of a detection. The detection with the highest score that is overlapping (above IoU threshold) with the ground-truth is considered true-positive during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets: The proposed 3C-Net is evaluated for temporal action localization on two challenging datasets containing untrimmed videos with varying degree of activity duration. THUMOS14 <ref type="bibr" target="#b8">[9]</ref> dataset contains 1010 validation and 1574 test videos from 101 action categories. Out of these, 20 categories have temporal annotations in 200 validation and 213 test videos. The dataset is challenging, as it contains an average of 15 activity instances per video. Similar to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, we use the validation set for training and test set for evaluating our framework. ActivityNet 1.2 <ref type="bibr" target="#b2">[3]</ref> dataset has 4819 training, 2383 validation and 2480 testing videos from 100 activity categories. Note that the test set annotations for this dataset are withheld. There are an average of 1.5 activity instances per video. As in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref>, we use the training set to train and validation set to test our approach. Count Labels: The ground-truth count labels for the videos in both datasets are generated using the available temporal action segments information. The total number of segments of an action category in a video is the ground-truth count video-label for the respective category. This was done to use the available annotations and avoid re-annotations. However, for a new dataset, action count can be independently annotated, without requiring action segment information. Evaluation Metric: We follow the standard protocol, provided with the two datasets, for evaluation. The evaluation protocol is based on mean Average Precision (mAP) for different intersection over union (IoU) values for the action localization task. For the multi-label action classification task, we use the mAP computed from the predicted videolevel scores for evaluation. Implementation Details: We use an alternate mini-batch training approach to train the proposed 3C-Net framework. Since, the count labels are available at the video-level, all the segments of a video are required for count prediction. We use random temporal cropping of videos in alternate mini-batches to improve the generalization. Thus, the classification and center losses are used for every mini-batch training and the counting loss is applied only on the alternate mini-batches containing the full-length video features.</p><p>In our framework, a TV-L1 optical flow <ref type="bibr" target="#b31">[32]</ref> is used to generate the optical flow frames of the video. The I3D features of size D = 1024 per segment of 16 video frames are obtained after spatio-temporal average pooling of Mixed 5c layers from the RGB and Flow I3D networks. These I3D features are then used as input to our framework. As in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, the backbone networks are not finetuned. Our 3C-Net is trained with a mini-batch size of 32 using the Adam <ref type="bibr" target="#b10">[11]</ref> optimizer with 10 −4 learning rate and 0.005 weight decay. The centers c j are learned using the SGD optimizer with 0.1 learning rate. For both datasets, we set α in Eq. 1 to 10 −3 since the center loss penalty is a squared error loss with a higher magnitude compared to other loss terms. We set β in Eq. 1 to 1 and 0.1 for the THUMOS14 and ActivityNet 1.2 datasets, respectively. η is set to 0.5[min(C F i (j))+max(C F i (j))] for a j th category  <ref type="table">Table 1</ref>. Action localization performance comparison (mAP) of our 3C-Net with state-of-the-art methods on THUMOS14 dataset. Superscript '+' for a method denotes that strong supervision is required for training. Our 3C-Net outperforms existing weaklysupervised methods and achieves an absolute gain of 4.6%, at IoU=0.5, compared to the best weakly-supervised result <ref type="bibr" target="#b15">[16]</ref>.</p><p>T-CAM in THUMOS14. Due to the nature of actions in ActivityNet 1.2, W-TALC <ref type="bibr" target="#b15">[16]</ref> approach uses the Savitzky-Golay filter <ref type="bibr" target="#b18">[19]</ref> for post-processing the T-CAMs. Here, we use a learnable temporal convolution filtering (kernel size=13, dilation=2) and set η to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">State-of-the-art comparison</head><p>Temporal Action Localization: Tab. 1 shows the comparison of our 3C-Net method with existing approaches in literature on the THUMOS14 dataset. Superscript '+' for a method in Tab. 1 denotes that frame-level labels (strong supervision) are required for training. Our approach is denoted as '3C-Net'. We report mAP scores at different IoU thresholds. Both UntrimmedNets <ref type="bibr" target="#b27">[28]</ref> and Autoloc <ref type="bibr" target="#b21">[22]</ref> use TSN <ref type="bibr" target="#b28">[29]</ref> as the backbone, whereas STPN <ref type="bibr" target="#b13">[14]</ref> and W-TALC <ref type="bibr" target="#b15">[16]</ref> use I3D networks similar to our framework. The STPN approach obtains an mAP of 16.9 at IoU=0.5, while W-TALC achieves an mAP of 22.0. Our approach CLS + CL, without any count supervision, outperforms all existing weakly-supervised action localization approaches. With the integration of count supervision, our 3C-Net achieves an absolute gain of 4.6%, in terms of mAP at IoU=0.5, over W-TALC <ref type="bibr" target="#b15">[16]</ref>. Further, a consistent improvement in performance is also obtained at other IoU thresholds. Tab. 2 shows the state-of-the-art comparison on the Ac-tivityNet 1.2 dataset. We follow the standard evaluation protocol <ref type="bibr" target="#b2">[3]</ref> by reporting the mean mAP scores at different thresholds (0.5:0.05:0.95). Among the existing methods, the SSN approach <ref type="bibr" target="#b32">[33]</ref> relies on frame-level annotations (strong supervision, denoted by superscript '+' in Tab. 2) for training and achieves a mean mAP score of 26.6. Our baseline approach, trained with the classification loss alone, achieves a mean mAP of 18.2. With only the center loss adaption, our approach achieves a mean mAP of 21.1 and surpasses all existing weakly-supervised methods.  <ref type="table">Table 2</ref>. Action localization performance comparison (mean mAP) of our 3C-Net with state-of-the-art methods on the ActivityNet 1.2 dataset. The mean mAP is denoted by Avg * . Note that SSN <ref type="bibr" target="#b32">[33]</ref> requires frame-level labels (strong supervision) for training. Our 3C-Net outperforms all existing weakly-supervised methods and obtains an absolute gain of 3.7% in terms of mean mAP, compared to the state-of-the-art weakly-supervised W-TALC <ref type="bibr" target="#b15">[16]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baseline Comparison and Ablation Study</head><p>Baseline comparison: Tab. 4 shows the action localization performance comparison on THUMOS14 (at IoU=0.5). We also show the impact of progressively integrating one contribution at a time in our 3C-Net framework. The baseline (CLS) trained using classification loss alone obtains a mAP score of 19.1. The integration of our multi-label center loss term (CLS + CL) significantly improves the performance by obtaining a mAP score of 24.6. The action localization performance is further improved to 26.6 mAP, by the integration of our counting loss term (CLS + CL + CT).</p><p>Ablation study: <ref type="figure" target="#fig_2">Fig. 3</ref> shows the results with respect to  . This results (orange bar) in a drop of 2.5% mAP. Next, we observe that retaining center loss term only in the flow stream results in a drop of 2.1% mAP (purple bar). Retaining the center loss term only in the RGB stream results in a drop of 1.9% mAP (green bar). Afterwards, we observe that removing the negative category counting loss in Eq. 10 results in a drop of 1.5% mAP (blue bar). Further, replacing the relative error for counting loss with absolute error deteriorates the results by 1.2% mAP (red bar). These results show that both our design choices and different loss terms contribute in the overall performance of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Analysis</head><p>We now present the qualitative analysis of our 3C-Net approach. <ref type="figure">Fig. 4</ref> shows the qualitative temporal action localization results of our 3C-Net on example videos from the THUMOS14 and ActivityNet 1.  <ref type="figure">Figure 4</ref>. Qualitative temporal action localization results of our 3C-Net approach on example videos from the THUMOS14 and ActivityNet 1.2 datasets. For each video, we show the example frames in the top row, ground-truth segments indicating the action instances as GT and the class-specific confidence scores over time as T-CAM (for brevity, only the thresholded T-CAM is shown). Action segments predicted using the T-CAM are denoted as Detection. Examples show different scenarios: multiple instances of same action (first video), visuallysimilar multiple action categories (second video) and long duration activities (third and fourth video). Our approach achieves promising localization performance on these variety of actions.</p><p>dataset contain long duration activities from Playing Violin and Parallel Bars categories. Observing the T-CAM progression in both videos, we see that the proposed framework detects the action instances reasonably well. For Playing Violin video, prediction with respect to the second instance is correctly detected, while the first instance is partially detected. This is due to imprecise annotation of the first instance which has some segments without the playing activity. In Parallel Bars video, a single action instance is annotated. However, the video contains an activity instance followed by background segments without any action and ends with the replay of the first action instance. This progression of activity-background-activity has been clearly identified by our approach as observed in the T-CAM. These results suggest the effectiveness of our approach for the problem of temporal action localization. We observe common failure reasons to be extreme scale change, visually similar actions confusion and temporally quantized segments for I3D feature generation. Few failure instances in <ref type="figure">Fig. 4</ref> are: de-tections having minimal overlap with the GT (first two detected instances of ThrowDiscus), false detections (third and fourth detected instances of ThrowDiscus) and multiple detections (first two detected instances of Parallel Bars).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel formulation with classification loss, center loss and counting loss terms for weakly-supervised action localization. We first proposed to use a class-specific attention-based feature aggregation strategy to utilize multilabel videos for training with center loss. We further introduced a counting loss term to leverage video-level action count information. To the best of our knowledge, we are the first to propose a formulation with multi-label center loss and action counting loss terms for weakly-supervised action localization. Experiments on two challenging datasets clearly demonstrate the effectiveness of our approach for both action localization and classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>C a i . The loss for the flow stream T-CAM Our overall architecture (3C-Net) with different loss terms (classification, center and counting), and the associated modules. The architecture is based on a two-stream model (RGB and flow) with an associated backbone feature extractor in each stream. Both streams are structurally identical and consist of two fully-connected layers (FC). The outputs of the final FC layer in both streams are the temporal class activation maps (T-CAM), C a for RGB and C f for flow. The two T-CAMs are weighted by class-specific parameters (w a and w f ) and combined in a late fusion manner. The resulting T-CAM, C F , is used for inference. The modules for the different loss terms do not have learnable parameters and are shown separately in the bottom row with sample inputs and corresponding outputs for clarity. Both center (L a center , L f center ) and classification (L a cls , L f cls ) losses are applied to each of the two streams (C a and C f ) whereas the classification (L F cls ) and counting (Lcount) loss are applied to the fused representation (C F ). Superscripts a, f and F denote appearance (RGB), flow and final, respectively. Color-coded arrows denote the association between the features in the network and the respective modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>C f i and the final T-CAM C F i , are computed in a similar manner. The total classification loss, L cls , is then given by,L cls = L a cls + L f cls + L F cls(3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Ablation study with respect to difference design choices and different loss terms in our action localization framework on the THUMOS14 dataset. See text for details. different design choices and impact of different loss terms in our action localization framework on the THUMOS14 dataset. All the experiments are conducted independently and show the deviation in performance relative to the proposed 3C-Net framework. The localization performance of our final proposed 3C-Net framework is shown as yellow bar. First, we show the impact of removing the classification loss in both the streams and retaining it only for the final T-CAM (C F i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2 datasets. For each video, example frames are shown in the top row. GT denotes the ground-truth segments. The category-specific confidence scores over time are indicated by T-CAM. Detection denotes the action segments predicted using the T-CAM. The top two videos are from THUMOS14. The multiple instances of HighJump action (first video) are accurately localized by our 3C-Net. The second video contains visually similar multiple actions (Shotput and ThrowDiscus) and has overlapping ground-truth annotations. In this case, 3C-Net mostly localizes the two actions accurately. The bottom two examples from the ActivityNet 1.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Baseline action localization performance comparison (mAP) on THUMOS14 at IoU=0.5. Our 3C-Net achieves an absolute gain of 7.5% in terms of mAP, compared to the baseline.</figDesc><table><row><cell cols="3">Baseline: CLS CLS + CL 3C-Net: CLS + CL + CT</cell></row><row><cell>19.1</cell><cell>24.6</cell><cell>26.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For brevity, the loss computation is explained in detail for the RGB stream using the superscript a (denoting appearance) for the variables.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding actors and actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object counting and instance segmentation with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Fahad Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">C-wsl: Count-guided weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The thumos challenge on action recognition for videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The lear submission at thumos 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wtalc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling the temporal extent of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smoothing and differentiation of data by simplified least squares procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Savitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytical chemistry</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1627" to="1639" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Autoloc: weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
