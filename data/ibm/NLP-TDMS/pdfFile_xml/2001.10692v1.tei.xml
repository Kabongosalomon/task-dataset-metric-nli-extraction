<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Xinlei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection has seen quick progress thanks to advances in deep learning on point clouds. A few recent works have even shown state-of-the-art performance with just point clouds input (e.g. VOTENET). However, point cloud data have inherent limitations. They are sparse, lack color information and often suffer from sensor noise. Images, on the other hand, have high resolution and rich texture. Thus they can complement the 3D geometry provided by point clouds. Yet how to effectively use image information to assist point cloud based detection is still an open question. In this work, we build on top of VOTENET and propose a 3D detection architecture called IMVOTENET specialized for RGB-D scenes. IMVOTENET is based on fusing 2D votes in images and 3D votes in point clouds. Compared to prior work on multi-modal detection, we explicitly extract both geometric and semantic features from the 2D images. We leverage camera parameters to lift these features to 3D. To improve the synergy of 2D-3D feature fusion, we also propose a multi-tower training scheme. We validate our model on the challenging SUN RGB-D dataset, advancing state-of-the-art results by 5.7 mAP. We also provide rich ablation studies to analyze the contribution of each design choice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognition and localization of objects in a 3D environment is an important first step towards full scene understanding. Even such low dimensional scene representation can serve applications like autonomous navigation and augmented reality. Recently, with advances in deep networks for point cloud data, several works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b38">39]</ref> have shown state-of-the-art 3D detection results with point cloud as the only input. Among them, the recently proposed VOTENET <ref type="bibr" target="#b30">[31]</ref> work by Qi et al., taking 3D geometry input only, showed remarkable improvement for indoor object recognition compared with previous works that exploit *: equal contributions. †: work done while at Facebook. all RGB-D channels. This leads to an interesting research question: Is 3D geometry data (point clouds) sufficient for 3D detection, or is there any way RGB images can further boost current detectors?</p><p>By examining the properties of point cloud data and RGB image data (see for example <ref type="figure" target="#fig_0">Fig. 1</ref>), we believe the answer is clear: RGB images have value in 3D object detection. In fact, images and point clouds provide complementary information. RGB images have higher resolution than depth images or LiDAR point clouds and contain rich textures that are not available in the point domain. Additionally, images can cover "blind regions" of active depth sensors which often occur due to reflective surfaces. On the other hand, images are limited in the 3D detection task as they lack absolute measures of object depth and scale, which are exactly what 3D point clouds can provide. These observations, strengthen our intuition that images can help point cloud-based 3D detection.</p><p>However, how to make effective use of 2D images in a 3D detection pipeline is still an open problem. A naïve way is to directly append raw RGB values to the point clouds -since the point-pixel correspondence can be established through projection. But since 3D points are much sparser, in doing so we will lose the dense patterns from the image domain. In light of this, more advanced ways to fuse 2D and 3D data have been proposed recently. One line of work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref> uses mature 2D detectors to provide initial proposals in the form of frustums. This limits the 3D search space for estimating 3D bounding boxes. However, due to its cascaded design, it does not leverage 3D point clouds in the initial detection. In particular, if an object is missed in 2D, it will be missed in 3D as well. Another line of work <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b10">11]</ref> takes a more 3D-focused way to concatenate intermediate ConvNet features from 2D images to 3D voxels or points to enrich 3D features, before they are used for object proposal and box regression. The downside of such systems is that they do not use 2D images directly for localization, which can provide helpful guidance for detection objects in 3D.</p><p>In our work, we build upon the successful VOTENET architecture <ref type="bibr" target="#b30">[31]</ref> and design a joint 2D-3D voting scheme for 3D object detection named IMVOTENET. It takes advantage of the more mature 2D detectors <ref type="bibr" target="#b35">[36]</ref> but at the same time still reserves the ability to propose objects from the full point cloud itself -combining the best of both lines of work while avoiding the drawbacks of each. A key motivation for our design is to leverage both geometric and semantic/texture cues in 2D images <ref type="figure" target="#fig_0">(Fig. 1</ref>). The geometric cues come from accurate 2D bounding boxes in images, such as the output by a 2D detector. Instead of solely relying on the 2D detection for object proposal <ref type="bibr" target="#b31">[32]</ref>, we defer the proposal process to 3D. Given a 2D box, we generate 2D votes on the image space, where each vote connects from the object pixel to the 2D amodal box center. To pass the 2D votes to 3D, we lift them by applying geometric transformations based on the camera intrinsic and pixel depth, so as to generate "pseudo" 3D votes. These pseudo 3D votes become extra features appended to seed points in 3D for object proposals. Besides geometric cues from the 2D votes, each pixel also passes semantic and texture cues to the 3D points, as either features extracted per-region, or ones extracted per-pixel.</p><p>After lifting and passing all the features from the images to 3D, we concatenate them with the 3D point features from a point cloud backbone network <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Next, following the VOTENET pipeline, those points with the fused 2D and 3D features generate 3D Hough votes <ref type="bibr" target="#b11">[12]</ref> -not limited by 2D boxes -toward object centers and aggregate the votes to produce the final object detections in 3D. As the seed features have both 2D and 3D information, they are intuitively more informative for recovering heavily truncated objects or objects with few points, as well as more confident in distinguishing geometrically similar objects.</p><p>In addition, we recognize that when fusing 2D and 3D sources, one has to carefully balance the information from two modalities to avoid one being dominated by the other. To this end, we further introduce a multi-towered network structure with gradient blending <ref type="bibr" target="#b45">[46]</ref> to ensure our network makes the best use of both the 2D and 3D features. During testing, only the main tower that operates on the joint 2D-3D features are used, minimizing the sacrifice on efficiency.</p><p>We evaluate IMVOTENET on the challenging SUN RGB-D dataset <ref type="bibr" target="#b40">[41]</ref>. Our model achieves the state-of-theart results while showing a significant improvement (+5.7 mAP) over the 3D geometry only VOTENET, validating the usefulness of image votes and 2D features. We also provide extensive ablation studies to demonstrate the importance of each individual component. Finally, we also explore the potential of using color to compensate for sparsity in depth points, especially for the case of lower quality depth sensors or for cases where depth is estimated from a moving monocular camera (SLAM), showing potential of our method to more broader use cases.</p><p>To summarize, the contributions of our work are:</p><p>1. A geometrically principled way to fuse 2D object detection cues into a point cloud based 3D detection pipeline.</p><p>2. The designed deep network IMVOTENET achieves state-of-the-art 3D object detection performance on SUN RGB-D.</p><p>3. Extensive analysis and visualization to understand various design choices of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Advances in 3D sensing devices have led to a surge of methods designed to identify and localize objects in a 3D scene. The most relevant lines of work are detection with point clouds and detection with full RGB-D data. We also briefly discuss a few additional relevant works in the area of multi-modal data fusion.</p><p>3D object detection with point clouds. To locate objects using purely geometric information, one popular line of methods is based on template matching using a collection of clean CAD models either directly <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24]</ref>, or through extracted features <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b1">2]</ref>. More recent methods are based on point cloud deep nets <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31]</ref>. In the context of 3D scene understanding, there have also been promising results on semantic and instance segmentation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref>. Most relevant to our work are PointRCNN <ref type="bibr" target="#b38">[39]</ref> and Deep Hough Voting (VOTENET) <ref type="bibr" target="#b30">[31]</ref> which demonstrated stateof-the-art 3D object detection in outdoor and indoor scenes, respectively. Notably, these results are achieved without using the RGB input. To leverage this additional information, we propose a way to further boost detection performance in this work.</p><p>3D object detection with RGB-D data. Depth and color channels both contain useful information that can be useful for 3D object detection. Prior methods for fusing those two modalities broadly fall into three categories: 2D-driven, 3D-driven, and feature concatenation. The first type of  </p><formula xml:id="formula_0">K x (3 + F + F')</formula><p>Output: 3D boxes <ref type="figure">Figure 2</ref>. 3D object detection pipeline for IMVOTENET. Given RGB-D input (with the depth image converted to a point cloud), the model initially have two separate branches: one for 2D object detection on the image and the other for point cloud feature extraction (with a PointNet++ <ref type="bibr" target="#b33">[34]</ref> backbone) on the point clouds. Then we lift 2D image votes as well as semantic and texture cues to the 3D seed points (the fusion part). The seed points with concatenated image and point cloud features then generate votes towards 3D object centers and also propose 3D bounding boxes with its features (the joint tower). To push for more effective multi-modality fusion, we have two other towers that take image features only (image tower) and point cloud features only (point tower) for voting and box proposals.</p><p>method <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b47">48]</ref> starts with object detecions in the 2D image, which are then used to guide the search space in 3D. By 3D-driven, we refer to methods that first generate region proposals in 3D and then utilize 2D features to make a prediction, such as the Deep Sliding Shapes <ref type="bibr" target="#b42">[43]</ref>. Recently more works focus on fusing 2D and 3D features earlier in the process such as Multi-modal Voxelnet <ref type="bibr" target="#b44">[45]</ref>, AVOD <ref type="bibr" target="#b15">[16]</ref>, multi-sensor <ref type="bibr" target="#b19">[20]</ref> and 3D-SIS <ref type="bibr" target="#b10">[11]</ref>. However, all these mostly perform fusion through concatenation of 2D features to 3D features. Our proposed method is more closely related to the third type, but differs from it in two important aspects. First, we propose to make explicit use of geometric cues from the 2D detector and lift them to 3D in the form of pseudo 3D votes. Second, we use a multi-tower architecture <ref type="bibr" target="#b45">[46]</ref> to balance features from both modalities, instead of simply training on the concatenated features.</p><p>Multi-modal fusion in learning. How to fuse signals from multiple modalities is an open research problem in other areas than 3D object detection. For example, the main focus of vision and language research is on developing more effective ways to jointly reason over visual data and texts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref> for tasks like visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. Another active area of research is video+sound <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8]</ref>, where the additional sound track can either provide supervision signal <ref type="bibr" target="#b28">[29]</ref>, or propose interesting tasks to test joint understanding of both streams <ref type="bibr" target="#b50">[51]</ref>. Targeting at all such tasks, a recent gradient blending approach <ref type="bibr" target="#b45">[46]</ref> is proposed to make the multi-modal network more robust (to over-fitting and different convergence rates), which is adopted in our approach too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ImVoteNet Architecture</head><p>We design a 3D object detection solution suited for RGB-D scenes, based on the recently proposed deep Hough voting framework (VOTENET <ref type="bibr" target="#b30">[31]</ref>) by passing geometric and semantic/texture cues from 2D images to the voting process (as illustrated in <ref type="figure">Fig. 2</ref>). In this section, after a short summary of the original VOTENET pipeline, we describe how to build '2D votes' with the assistance of 2D detectors on RGB and explain how the 2D information is lifted to 3D and passed to the point cloud to improve the 3D voting and proposal. Finally, we describe our multi-tower architecture for fusing 2D and 3D detection with gradient blending <ref type="bibr" target="#b45">[46]</ref>. More implementation details are provided in supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep Hough Voting</head><p>VOTENET <ref type="bibr" target="#b30">[31]</ref> is a feed-forward network that consumes a 3D point cloud and outputs object proposals for 3D object detection. Inspired by the seminal work on the generalized Hough transform <ref type="bibr" target="#b2">[3]</ref>, VOTENET proposes an adaptation of the voting mechanism for object detection to a deep learning framework that is fully differentiable.</p><p>Specifically, it is comprised of a point cloud feature extraction module that enriches a subsampled set of scene points (called seeds) with high-dimensional features (bottom of <ref type="figure">Fig. 2</ref> from N ×3 input points to K×(3+F ) seeds). These features are then pushed through a Multi-Layer-Perceptron (MLP) to generate votes. Every vote is both a point in the 3D space with its Euclidean coordinates (3dim) supervised to be close to the object center, and a feature vector learned for the final detection task (F-dim). The votes form a clustered point cloud near object centers and are then processed by another point cloud network to generate object proposals and classification scores. This process is equivalent to the pipeline in <ref type="figure">Fig. 2</ref> with just the point tower and without the image detection and fusion.</p><p>VOTENET recently achieved state-of-the-art results on indoor 3D object detection in RGB-D <ref type="bibr" target="#b30">[31]</ref>. Yet, it is solely based on point cloud inputs and neglects the image channels which, as we show in this work, are a very useful source of information. In IMVOTENET, we leverage the additional image information and propose a lifting module from 2D votes to 3D that improves detection performance. Next, we explain how to get 2D votes in images and how we lift its geometric cues to 3D together with semantic/texture cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Votes from 2D Detection</head><p>We generate image votes based on a set of candidate boxes from 2D detectors. An image vote, in its geometric part, is simply a vector connecting an image pixel and the center of the 2D object bounding box that pixel belongs to (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Each image vote is also augmented with its semantic and texture cues from the features of its source pixel, such that each image vote has F dimension in total as in the fusion block in <ref type="figure">Fig. 2</ref>.</p><p>To form the set of boxes given an RGB image, we apply an off-the-shelf 2D detector (e.g. Faster R-CNN <ref type="bibr" target="#b35">[36]</ref>) pre-trained on color channels of the RGB-D dataset. The detector outputs the M most confident bounding boxes and their corresponding classes. We assign each pixel within a detected box a vote to the box center. Pixels inside multiple boxes are given multiple votes (corresponding 3D seed points are duplicated for each of them), and those outside of any box are padded with zeros. Next we go to details on how we derive geometric, semantic and texture cues.</p><p>Geometric cues: lifting image votes to 3D The translational 2D votes provide useful geometric cues for 3D object localization. Given the camera matrix, the 2D object center in the image plane becomes a ray in 3D space connecting the 3D object center and the camera optical center ( <ref type="figure" target="#fig_0">Fig. 1</ref>). Adding this information to a seed point can effectively narrow down the 3D search space of the object center to 1D.</p><p>In details, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, given an object in 3D with its detected 2D bounding box in the image plane, we denote the 3D object center as C and its projection onto the image as c. A point P on the object surface is associated with its projected point p in the image place, hence knowing the 2D vote to the 2D object center c, we can reduces the search space for the 3D center to a 1D position on the ray OC. We now derive the computation we follow to pass the ray information to the a 3D seed point. Defining P =(x 1 , y 1 , z 1 ) in the camera coordinate, and p=(u 1 , v 1 ), c=(u 2 , v 2 ) in the image plane coordinate, we seek to recover the 3D object center C=(x 2 , y 2 , z 2 ) (voting target for the 3D point P ). The true 3D vote from P to C is:</p><formula xml:id="formula_1"># » P C = (x 2 − x 1 , y 2 − y 1 , z 2 − z 1 ).<label>(1)</label></formula><p>The 2D vote, assuming a simple pin-hole camera <ref type="bibr" target="#b0">1</ref> with focal length f , can be written as:</p><formula xml:id="formula_2">#» pc = (u 2 − u 1 , v 2 − v 1 ) = (∆u, ∆v) = (f ( x 2 z 2 − x 1 z 1 ), f ( y 2 z 2 − y 1 z 1 )).<label>(2)</label></formula><p>We further assume the depth of the surface point P is similar to the center point C. This is a reasonable assumption for most objects when they are not too close to the camera. Then, given z 1 ≈z 2 , we compute # » P C ,</p><formula xml:id="formula_3"># » P C = ( ∆u f z 1 , ∆v f z 1 , 0),<label>(3)</label></formula><p>which we refer to as a pseudo 3D vote, as C lies on the ray OC and is in the proximity of C. This pseudo 3D vote provides information about where the 3D center is relative to the point surface point P .</p><p>To compensate for the error caused by the depth approximation (z 1 ≈ z 2 ), we pass the ray direction as extra information to the 3D surface point. The error (along the X-axis) caused by the approximated depth, after some derivation, can be expressed by</p><formula xml:id="formula_4">err x =∆x − ∆x = x 2 z 2 (z 2 − z 1 ).<label>(4)</label></formula><p>Hence, if we input the direction of the ray # » OC: (x 2 /z 2 , y 2 /z 2 ), the network should have more information to estimate the true 3D vote by estimate the depth different ∆z = z 2 − z 1 . As we do not know the true 3D object center C, we can use the ray direction of # » OC which aligns with # » OC after all, where</p><formula xml:id="formula_5"># » OC = # » OP + # » P C = (x 1 + ∆u f z 1 , y 1 + ∆v f z 1 , z 1 ).<label>(5)</label></formula><p>Normalizing and concatenating with the pseudo vote, the image geometric features we pass to the seed point P are: In light of this, we provide additional region-level features extracted per bounding box as semantic cues for 3D points. For all the 3D seed points that are projected within the 2D box, we pass a vector representing that box to the point. If a 3D seed point falls into more than one 2D boxes (i.e., when they overlap), we duplicate the seed point for each of the overlapping 2D regions (up to a maximum number of K). If a seed point is not projected to any 2D box, we simply pass an all-zero feature vector for padding.</p><formula xml:id="formula_6">( ∆u f z 1 , ∆v f z 1 , # » OC # » OC ).<label>(6)</label></formula><p>It is important to note that the 'region features' here include but are not limited to features extracted from RoI pooling operations <ref type="bibr" target="#b35">[36]</ref>. In fact, we find representing each box with a simple one-hot class vector (with a confidence score for that class) is already sufficient to cover the semantic information needed for disambiguation in 3D. It not only gives a light-weight input (e.g. 10-dim <ref type="bibr" target="#b43">[44]</ref> vs. 1024dim <ref type="bibr" target="#b20">[21]</ref>) that performs well, but also generalizes to all other competitive (e.g. faster) 2D detectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22</ref>] that do not explicitly use RoI but directly outputs classification scores. Therefore, we use this semantic cue by default.</p><p>Texture cues Different from the depth information that spreads sparsely in the 3D space, RGB images can capture high-resolution signals at a dense, per-pixel level in 2D. While region features can offer a high-level, semantic-rich representation per bounding box, it is complementary and equally important to use the low-level, texture-rich representations as another type of cues. Such cues can be passed to the 3D seed points via a simple mapping: a seed point gets pixel features from the corresponding pixel of its 2D projection 2 .</p><p>Although any learned, convolutional feature maps with spatial dimensions (height and width) can serve our purpose, by default we still use the simplest texture feature by feeding in the raw RGB pixel-values directly. Again, this choice is not only light-weight, but also makes our pipeline independent of 2D networks.</p><p>Experimentally, we show that even with such minimalist choice of both our semantic and texture cues, significant performance boost over geometric-only VOTENET can be achieved with our multi-tower training paradigm, which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Fusion and Multi-tower Training</head><p>With lifted image votes and its corresponding semantic and texture cues (K × F in the fusion block in <ref type="figure">Fig. 2)</ref> as well as the point cloud features with the seed points K × F , each seed point can generate 3D votes and aggregate them to propose 3D bounding boxes (through a voting and proposal module similar to that in <ref type="bibr" target="#b30">[31]</ref>). Yet it takes extra care to optimize the deep network to fully utilize cues from all modalities. As a recent paper <ref type="bibr" target="#b45">[46]</ref> mentions, without a careful strategy, multi-modal training can actually result in degraded performance as compared to a single modality training. The reason is that different modalities may learn to solve the task at different rates so, without attention, certain features may dominate the learning and result in overfitting. In this work, we follow the gradient blending strategy introduced in [46] to weight the gradient for different modality towers (by weighting the loss functions).</p><p>In our multi-tower formulation, as shown in <ref type="figure">Fig. 2</ref>, we have three towers taking seed points with three sets of features: point cloud features only, image features only and joint features. Each tower has the same target task of detecting 3D objects -but they each have their separate 3D voting and box proposal network parameters as well as their separate losses. The final training loss is the weighted sum of three detection losses:</p><formula xml:id="formula_7">L = w img L img + w point L point + w joint L joint .<label>(7)</label></formula><p>Within the image tower, while image features alone cannot localize 3D objects, we have leveraged surface point geometry and camera intrinsic to have pseudo 3D votes that are good approximations to the true 3D votes. So combining this image geometric cue with other semantic/texture cues we can still localize objects in 3D with image features only.</p><p>Note that, although the multi-tower structure introduces extra parameters, at inference time we no longer need to compute for the point cloud only and the image only towers -therefore there is minimal computation overhead.  <ref type="table" target="#tab_0">Table 1</ref>. 3D object detection results on SUN RGB-D v1 val set. Evaluation metric is average precision with 3D IoU threshold 0.25 as proposed by <ref type="bibr" target="#b40">[41]</ref>. Note that both COG <ref type="bibr" target="#b36">[37]</ref> and 2D-driven <ref type="bibr" target="#b16">[17]</ref> use room layout context to boost performance. The evaluation is on the SUN RGB-D v1 data for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first compare our model with previous state-of-the-art methods on the challenging SUN RGB-D dataset (Sec. 4.1). Next, we provide visualizations of detection results showing how image information helps boost the 3D recognition (Sec. 4.2). Then, we present an extensive set of analytical experiments to validate our design choices (Sec. 4.3). Finally, we test our method in the conditions of very sparse depth, and demonstrate its robustness (Sec. 4.4) in such scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparing with State-of-the-art Methods</head><p>Benchmark dataset. We use SUN RGB-D <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b40">41]</ref> as our benchmark for evaluation, which is a single-view 3 RGB-D dataset for 3D scene understanding. It consists of ∼10K RGB-D images, with ∼5K for training. Each image is annotated with amodal oriented 3D bounding boxes. In total, 37 object categories are annotated. Following standard evaluation protocol <ref type="bibr" target="#b42">[43]</ref>, we only train and report results on the 10 most common categories. To feed the data to the point cloud backbone network, we convert the depth images to point clouds using the provided camera parameters. The RGB image is aligned to the depth channel and is used to query corresponding image regions from scene 3D points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods in comparison.</head><p>We compare IMVOTENET with previous methods that use both geometry and RGB. Moreover, since previous state-of-the-art (VOTENET <ref type="bibr" target="#b30">[31]</ref>) used only geometric information, to better appreciate the improvement due to our proposed fusion and gradient blending modules we add two more strong baselines by extending the basic VOTENET with additional features from image.</p><p>Among the previous methods designed for RGB-D, 2Ddriven <ref type="bibr" target="#b16">[17]</ref>, PointFusion <ref type="bibr" target="#b47">[48]</ref> and F-PointNet <ref type="bibr" target="#b31">[32]</ref> are all cascaded systems that rely on 2D detectors to provide proposals for 3D. Deep Sliding Shapes <ref type="bibr" target="#b42">[43]</ref> designs a Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> style 3D CNN network to generate 3D proposals from voxel input and then combines 3D and 2D RoI features for box regression and classification. COG <ref type="bibr" target="#b36">[37]</ref> is a sliding shape based detector using 3D HoG like feature extracted from RGB-D data.</p><p>As for the variations of VOTENET <ref type="bibr" target="#b30">[31]</ref>, the first one, '+RGB', directly appends the the RGB values as a threedimensional vector to the point cloud features (of the seed points). For the second one ('+region feature'), we use the same pre-trained Faster R-CNN (as in our model) to obtain the region-level one-hot class confidence feature, and concatenate it to the seed points inside that 2D box frustum. These two variations can also be viewed as ablated versions of our method.</p><p>Results. <ref type="table" target="#tab_0">Table 1</ref> shows the per-class 3D object detection results on SUN RGB-D. We can see that our model outperforms all previous methods by large margins. Especially, it improves upon the previously best model VOTENET by 5.7 mAP, showing effectiveness of the lifted 2D image votes. It gets better results on nearly all categories and has the biggest improvements on object categories that are often occluded (+12.5 AP for bookshelves) or geometrically similar to the others (+11.6 AP for dressers and +7.7 AP for nightstands).</p><p>Compared to the variations of the VOTENET that also uses RGB data, our method also shows significant advantages. Actually we find that naively appending RGB values to the point features resulted in worse performance, likely due to the over-fitting on RGB values. Adding region features as a one-hot score vector helps a bit but is still inferior compared to our method that more systematically leverage image votes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results and Discussion</head><p>In <ref type="figure">Fig. 4</ref>  <ref type="figure">Figure 4</ref>. Qualitative results showing how image information helps. First row: the bookshelf is detected by IMVOTENET thanks to the cues from the 2D detector; Second row: the black sofa has barely any depth points due to its material, but leveraging images, we can detect it; Third row: with 2D localization cues and semantics, we detect the desk and chairs in the back which are even missed by ground truth annotations. Best viewed in color with zoom in. and semantic help. We see a cluttered bookshelf that was missed by the VOTENET but thanks to the 2D detection in the images, we have enough confidence to recognize it in our network. The image semantics also help our network to avoid the false positive chair as that in the VOTENET output (coffee table and candles confused the network there). The second example shows how images can compensate depth sensor limitations. Due to the color and material of the black sofa, there is barely any depth point captured for it. While VOTENET completely misses the sofa, our network is able to pick it up. The third example shows how image cues can push the limit of 3D detection performance, by recovering far away objects (the desk and chairs in the back) that are even missed in the ground truth annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis Experiments</head><p>In this subsection, we show extensive ablation studies on our design choices and discuss how different modules affect the model performance. For all experiments we report mAP@0.25 on SUN RGB-D as before.</p><p>Analysis on geometric cues. To validate that geometric cues lifted from 2D votes help, we ablate geometric features (as in Eq. 6) passed to the 3D seed points in <ref type="table" target="#tab_0">Table 2a</ref>. We see that from row 1 to row 3, not using any 2D geometric cue results in a 2.2 point drop. On the other hand, not using the ray angle resulted in a 1.2 point drop, indicating the ray angle helps provide corrective cue to the pseudo 3D votes.</p><p>Analysis on semantic cues. <ref type="table" target="#tab_0">Table 2b</ref> shows how different types of region features from the 2D images affect 3D detection performance. We see that the one-hot class score vector (probability score for the detected class, other classes set to 0), though simple, leads to the best result. Directly using the 1024-dim RoI features from the Faster R-CNN network actually got the worst number likely due to the optimization challenge to fuse this high-dim feature with the rest point features. Reducing the 1024-dim feature to 64-dim helps but is still inferior to the simple one-hot score feature.</p><p>Analysis on texture cues. (c) Ablation studies on 2D texture cues. We experiment with different pixel-level features including RGB values (default) and learned representations from the feature pyramid.  Gradient blending. <ref type="table" target="#tab_0">Table 3</ref> studies how tower weights affect the gradient blending training. We ablate with a few sets of representative weights ranging from single tower training (the first row), dominating weights for each of the tower (2nd to 4th rows) and our best set up. It is interesting to note that even with just the image features (the 1st row, 4th column) i.e. the pseudo votes and semantic/texture cues from the images, we can already outperform several previous methods (see <ref type="table" target="#tab_0">Table 1</ref>), showing the power of our fusion and voting design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Detection with Sparse Point Clouds</head><p>While depth images provide dense point clouds for a scene (usually 10k to 100k points), there are other scenarios that only sparse points are available. One example is when the point cloud is computed through visual odometry <ref type="bibr" target="#b26">[27]</ref> or Structure from Motion (SfM) <ref type="bibr" target="#b14">[15]</ref> where 3D point positions are triangulated by estimating poses of a monocular camera in multiple views. With such sparse data, it is valuable to have a system that can still achieve decent detection performance.</p><p>To analyze the potential of our model with sparse point clouds, we simulate scans with much less points through two types of point sub-sampling: uniformly random subsampling (remove existing points with a uniform distribution) and ORB <ref type="bibr" target="#b37">[38]</ref>   <ref type="table" target="#tab_0">Table 4</ref>. Sparse point cloud experiment, where we sub-sample the number of points in the cloud either via random uniform sampling or with ORB key points <ref type="bibr" target="#b37">[38]</ref>. In such cases, our IMVOTENET significantly outperforms purely geometry based VOTENET.</p><p>ple ORB key points on the image and only keep 3D points that project close to those 2D key points). In <ref type="table" target="#tab_0">Table 5</ref>, we present detection results with different distribution and density of point cloud input. We see that in the column of "point cloud", with decreased number of points, 3D detection performance quickly drops. On the other hand, we see including image cues significantly improves performance. This improvement is most significant when the sampled points are from ORB key points that are more non-uniformly distributed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we have explored how image data can assist a voting-based 3D detection pipeline. The VOTENET detector we build upon relies on a voting mechanism to effectively aggregate geometric information in point clouds. We have demonstrated that our new network, IMVOTENET, can leverage extant image detectors to provide both geometric and semantic/texture information about an object in a format that can be integrated into the 3D voting pipeline. Specifically, we have shown how to lift 2D geometric information to 3D, using knowledge of the camera parameters and pixel depth. IMVOTENET significantly boosts 3D object detection performance exploiting multi-modal training with gradient blending, especially in settings when the point cloud is sparse or unfavorably distributed.</p><p>In this section, we explain the details in the IMVOTENET architecture. Sec. B.1 provides details in the point cloud deep net as well as the training procedure. Further details on the 2D detector and 2D votes are described in Sec. B.2 while details on lifting 2D votes with general camera parameters are described in Sec. B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Point Cloud Network</head><p>Input and data augmentation. The point cloud backbone network takes a randomly sampled point cloud of a SUN RGB-D <ref type="bibr" target="#b40">[41]</ref> depth image with 20k points. Each point has its XY Z coordinate as well as its height (distance to floor). The floor height is estimated as the 1% percentile of heights of the all points. Similar to <ref type="bibr" target="#b30">[31]</ref>, we augment the input point cloud by randomly sub-sampling the points from the depth image points on-the-fly. Points are also randomly flipped in both horizontal directions and randomly rotated along the up-axis by Uniform <ref type="bibr">[-30,30]</ref> degrees. Points are also randomly scaled by Uniform[-.85, 1.15]. Note that the point height and the camera extrinsic are updated accordingly with the augmentation.</p><p>Network architecture. We adopt the same Point-Net++ <ref type="bibr" target="#b33">[34]</ref> backbone network as that in <ref type="bibr" target="#b30">[31]</ref> with four set abstraction (SA) layers and two feature propagation/upsamplng (FP) layers. With input of N ×4 where N =20k, the output of the backbone network is a set of seed points of K×(3 + C) where K=1024 and C=256.</p><p>As for voting, different from VOTENET that directly predicts votes from the seed points, here we fuse lifted image votes and the seed points before voting. As each seed point can fall into multiple 2D detection boxes, we duplicate a seed point q times if it falls in q overlapping boxes. Each duplicated seed point has its feature augmented with a concatenation of the following image vote features: 5-dim lifted geometric cues (2 for the vote and 3 for the ray angle), 10-dim (per-class) semantic cues and 3-dim texture cues. In the end the fused seed point has 3-dim XY Z coordinate and a 274-dim feature vector.</p><p>The voting layer takes the seed point and maps its features to votes through a multi-layer perceptron (MLP) with FC output sizes of 256, 256 and 259, where the last FC layer outputs XYZ offset and feature residuals (with regard to the 256-dim seed feature) for the votes. As in <ref type="bibr" target="#b30">[31]</ref>, the proposal module is another set abstraction layer that takes in the generated votes and generate proposals of shape K × <ref type="figure">(5+2N H+4N S+N C)</ref> where K is the number of total duplicated seed points and the output dimension consists of 2 objectness scores, 3 center regression values, 2N H numbers for heading regression (N H heading bins) and 4N S numbers for box size regression (N S box anchors) and N C numbers for semantic classification.</p><p>Training procedure. We pre-train the 2D detector as described more in Sec. B.2 and use the extracted image votes as extra input to the point cloud network. We train the point cloud deep net with the Adam optimizer with batch size 8 and an initial learning rate of 0.001. The learning rate is decayed by 10× after 80 epochs and then decayed by another 10× after 120 epochs. Finally, the training stops at 140 epochs as we find further training does not improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. 2D Detector and 2D Cues</head><p>2D detector training. While IMVOTENET can work with any 2D detector, in this paper we choose Faster R-CNN <ref type="bibr" target="#b35">[36]</ref>, which is the current dominant framework for bounding box detection in RGB. The detector we used has a basic ResNet-50 <ref type="bibr" target="#b9">[10]</ref> backbone with Feature Pyramid Networks (FPN) <ref type="bibr" target="#b20">[21]</ref> constructed as {P 2 , P 3 , . . . , P 6 }. It is pre-trained on the COCO train2017 dataset <ref type="bibr" target="#b22">[23]</ref> achieving a val2017 AP of 41.0. To adapt the COCO detector to the specific dataset for 2D detection, we further fine-tune the model using ground truth 2D boxes from the training set of SUN-RGBD before applying the model only using the color channels. The fine-tuning lasts for 4K iterations, with the learning rate reduced by 10× at 3K-th iteration starting from 0.01. The batch size, weight decay, and momentum are set as 8, 1e-4, and 0.9, respectively. Two data augmentation techniques are used: 1) standard left-right flipping; and 2) scale augmentation by randomly sample the shorter side of the input image from [480,600]. The resulting detector achieves a mAP (at overlap 0.5) of 58.5 on val set.</p><p>Note that we specifically choose not to use the most advanced 2D detectors (e.g. based on ResNet-152 <ref type="bibr" target="#b9">[10]</ref>) just for the sake of performance improvement. As our experimental results shown in the main paper, even with this simple baseline Faster R-CNN, we can already see significant boost thanks to the design of IMVOTENET.</p><p>2D boxes. To infer 2D boxes using the detector, we first resize the input image to a shorter side of 600 before feeding into the model. Then top 100 detection boxes across all classes for an image is aggregated. We further reduce the number of 2D boxes per image by filtering out any detection with a confidence score below 0.1. Two things to note about the 2D boxes used while training IMVOTENET: 1) we could also train with ground truth 2D boxes, however we empirically found that including them for training hurts performance, likely due to the different detection statistics at test time; 2) as the pre-training for the 2D detector is also performed on the same training set, it generally gives better detection results on SUN RGB-D train set images, to reduce the effect of over-fitting, we randomly dropped 2D boxes with a probability of 0.5.</p><p>Alternative semantic cues. Other than the default semantic cue to represent each 2D box region as the one-hot classification score vector (the detected class has the value of the confidence score from the detector, all other locations have zeros), we further experimented with dense RoI features extracted from that region. Two variants are reported in the paper, with the 1024-dim one being the output from the last FC layer before region classification and regression. For the 64-dim one, we insert an additional FC layer before the final output layers so that region information is compressed into the 64-dim vector. The added layer is pre-trained with the 2D detector (resulting in a val mAP of 57.9) and fixed when training IMVOTENET.</p><p>Alternative texture cues. The default texture cue is the raw RGB values (normalized to [-1,1]). Besides this simple texture cue, we also experimented more advanced perpixel features. One handy feature that preserves such spatial information is the feature maps P k from FPN that fuse topdown and lateral connections <ref type="bibr" target="#b20">[21]</ref>. Here k is the index to the layers in the feature pyramid, which also designates the feature strides and size. For example, P 2 has a stride of 2 2 =4 for both height and width; and a spatial size of roughly 1/16 of the input image 4 . For P 3 the strides are 2 3 =8. All feature maps have a channel size of 256, which becomes the input dimension when used as texture cues for IMVOTENET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Image Votes Lifting</head><p>In the main paper we derived the lifting process to transform a 2D image vote to a 3D pseudo vote without considering the camera extrinsic. As the point cloud sampled from depth image points is transformed to the upright coordinate before feeding to the point cloud network (through camera extrinsic R as a rotational matrix), the 3D pseudo vote also needs to be transformed to the same coordinate. <ref type="figure">Fig. 5</ref> shows the surface point P , object center C and the end point of the pseudo vote C . Since the point cloud is in the upright coordinate, the point cloud deep net can only estimate the depth displacement of P and C along the Z upright direction (it cannot estimate the depth displacement along the Z camera direction as the rotational angles from camera to upright coordinate are unknown to the network). Therefore, <ref type="bibr" target="#b3">4</ref> Note that different from 2D box detection, we feed the images directly to the model without resizing to shorter-side 600 to compute FPN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O Zupright</head><p>Yupright C P C' Zcamera C'' Ycamera <ref type="figure">Figure 5</ref>. Image vote lifting with camera extrinsic. Here we show surface point P and object center C in two coordinate systems: camera coordinate and upright coordinate (OY is along gravitational direction).</p><p># » P C is the true 3D vote. # » P C is the pseudo vote as calculated in the main paper and # » P C is the transformed pseudo vote finally used in feature fusion.</p><p>we need to calculate a new pseudo vote # » P C where C is on the ray OC and P C is perpendicular to the OZ upright .</p><p>To calculate the C we need to firstly transform P and C to the upright coordinate. Then assume P =(x p , y p , z p ) and C =(x c , y c , z c ) in the upright coordinate, we can compute:</p><formula xml:id="formula_8">C = (z p x c z c , z p y c y c , z p ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of Sparse Points</head><p>In the Sec. 4.4 of the main paper we showed how image information and IMVOTENET model can be specially helpful in detections with sparse point clouds. Here in <ref type="table" target="#tab_0">Table. 5</ref> we visualize the sampled sparse point clouds on three example SUN RGB-D images. We project the sampled points to the RGB images to show their distribution and density. We see that the 20k points in the first row have a dense and uniform coverage of the entire scene. After randomly subsampling the points to 5k and 1k points in the second and third rows respectively, we see the coverage is much more sparse but still uniform. In contrast the ORB key point based sampling (the last two rows) results in very uneven distribution of points where they are clustered around corners and edges. The non-uniformity and low coverage of ORB key points makes it especially difficult to recognize objects in point cloud only. That's also where our IMVOTENET model showed the most significant improvement upon VOTENET.  <ref type="table" target="#tab_0">Table 5</ref>. Sparse point cloud visualization. We show projected point clouds on three SUN RGB-D images and compare point density and distribution among random sampling (to 20k, 5k and 1k points) and ORB key points based sampling (to 5k and 1k points). For ORB key point sampling, we firstly detect ORB key points in the RGB images with an ORB key point detector and then keep 3D points that are projected near those key points. Best viewed in color with zoom in.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Voting using both an image and a point cloud from an indoor scene. The 2D vote reduces the search space of the 3D object center to a ray while the color texture in image provides a strong semantic prior. Motivated by the observation, our model lifts the 2D vote to 3D to boost 3D detection performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the pseudo 3D vote. In the figure, P is a surface point in 3D, C is the unknown object center while p and c are their projections on the image plane respectively. C is the pseudo 3D center and the vector # » P C is the pseudo 3D vote.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table table N</head><label>table</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>K x F'</cell><cell></cell><cell></cell></row><row><cell>image</cell><cell></cell><cell>geometric cues</cell><cell></cell><cell>semantic cues …</cell><cell>texture cues</cell></row><row><cell>RGB:</cell><cell></cell><cell></cell><cell></cell><cell>&lt;table&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>joint tower</cell><cell>(train &amp; test)</cell></row><row><cell>D: point cloud</cell><cell>projection</cell><cell>…</cell><cell cols="2">2D features</cell><cell>…</cell></row><row><cell></cell><cell></cell><cell>K x 3</cell><cell></cell><cell></cell><cell>K x F</cell></row><row><cell>points</cell><cell>K seeds</cell><cell cols="2">coordinates</cell><cell cols="2">point cloud features</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Semantic cues On top of the geometric features just discussed that just use the spatial coordinates of the bounding boxes, an important type of information RGB can provide is features that convey a semantic understanding of what's inside the box. This information often complements what can be learned from 3D point clouds and can help to distinguish between classes that are geometrically very similar (such as table vs. desk or nightstand vs. dresser).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, we highlight detection results of both the original VOTENET<ref type="bibr" target="#b30">[31]</ref> (with only point cloud input) and our IMVOTENET with point cloud plus image input, to show how image information can help 3D detection in various ways. The first example shows how 2D object localization</figDesc><table><row><cell>Ours 2D detection</cell><cell>Ours 3D detection</cell><cell cols="2">VoteNet</cell><cell></cell><cell>Ground truth</cell></row><row><cell></cell><cell>sofa</cell><cell>bookshelf</cell><cell>chair</cell><cell>table</cell><cell>desk</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2c</head><label>2c</label><figDesc>shows how different low-level image features (texture features) affect the end detection performance. It is clear that the raw RGB features are already effective while the more sophisticated per-pixel CNN features (from feature pyramids<ref type="bibr" target="#b20">[21]</ref> of the Faster R-CNN detector) actually hurts probably due to over-fitting. More details are in the supplementary material.</figDesc><table><row><cell>geometric cues 2D vote ray angle</cell><cell>mAP</cell><cell>semantic cues region feature</cell><cell># dims</cell><cell>mAP</cell><cell>texture cues pixel feature</cell><cell># dims</cell><cell>mAP</cell></row><row><cell></cell><cell>63.4</cell><cell>one-hot score</cell><cell>10</cell><cell>63.4</cell><cell>RGB</cell><cell>3</cell><cell>63.4</cell></row><row><cell></cell><cell>62.2 61.2</cell><cell>RoI [36]</cell><cell>64 1024</cell><cell>62.4 59.5</cell><cell>FPN-P 2 [21] FPN-P 3</cell><cell>256 256</cell><cell>62.0 62.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>58.9</cell><cell></cell><cell>-</cell><cell>62.4</cell></row><row><cell cols="2">(a) Ablation studies on 2D geometric cues. 2D</cell><cell cols="3">(b) Ablation studies on 2D semantic cues. Dif-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">vote means the lifted 2D vote (2-dim) as in Eq. 6 and ray angle means the direction of # » OC (3-</cell><cell cols="3">ferent region features are experimented. This includes simple one-hot class score vector and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dim). Both geometric cues helped our model.</cell><cell cols="3">rich RoI features. The former (default) works</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>best.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Ablation analysis on 2D cues. We provide detailed analysis on all types of features from 2D (see Sec. 3.2 for detailed descriptions).</figDesc><table><row><cell></cell><cell>tower weights</cell><cell></cell><cell></cell><cell>mAP</cell><cell></cell></row><row><cell>w img</cell><cell>w point</cell><cell>w joint</cell><cell>image</cell><cell>point cloud</cell><cell>joint</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.8</cell><cell>57.4</cell><cell>62.1</cell></row><row><cell>0.1</cell><cell>0.8</cell><cell>0.1</cell><cell>46.9</cell><cell>57.8</cell><cell>62.7</cell></row><row><cell>0.8</cell><cell>0.1</cell><cell>0.1</cell><cell>46.8</cell><cell>58.2</cell><cell>63.3</cell></row><row><cell>0.1</cell><cell>0.1</cell><cell>0.8</cell><cell>46.1</cell><cell>56.8</cell><cell>62.7</cell></row><row><cell>0.3</cell><cell>0.3</cell><cell>0.4</cell><cell>46.6</cell><cell>57.9</cell><cell>63.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Analysis on multi-tower training. In the first block we show performance without blending in gray. Then we show the setting where each of the tower dominates (0.8) the overall training. Finally we show our default setting where weights are more balanced.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See supplementary for more details on how to deal with a general camera model and camera-to-world transformations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">If the coordinates after projection is fractional, bi-linear interpolation is used.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We do not evaluate on the ScanNet dataset<ref type="bibr" target="#b4">[5]</ref> as in VOTENET because ScanNet involves multiple 2D views for each reconstructed scene -thus requires extra handling to merge multi-view features.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this supplementary, we provide more details on the IMVOTENET architecture in Sec. B, including point cloud network architecture, 2D detector, 2D votes and image votes lifting. We also show visualizations of the sparse point clouds in Sec. C.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scan2cad: Learning cad model alignment in rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Dahnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalizing the hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08755</idno>
		<title level="m">Minkowski convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>4d spatio-temporal convnets</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d ones in rgbdepth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Deng And Longin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2.5 d visual sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine analysis of bubble chamber pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Proc</title>
		<imprint>
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A category-level 3d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer depth cameras for computer vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Affine structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="377" to="385" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS. IEEE</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Database-assisted object retrieval for real-time 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Asist: automatic semantically invariant scene transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lior Shapira, Alex Bronstein, and Ran Gal</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="284" to="299" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A search-classify approach for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Sharf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Naroditsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visually indicated sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Edward H Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04244</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Data-driven structural priors for shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">What makes training multi-modal networks hard?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12681</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPr</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03320</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The sound of pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
