<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilateral Attention Network for RGB-D Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Ping</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
						</author>
						<title level="a" type="main">Bilateral Attention Network for RGB-D Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Bilateral attention</term>
					<term>salient object detection</term>
					<term>RGB-D image</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing RGB-D salient object detection (SOD) methods focus on the foreground region when utilizing the depth images. However, the background also provides important information in traditional SOD methods for promising performance. To better explore salient information in both foreground and background regions, this paper proposes a Bilateral Attention Network (BiANet) for the RGB-D SOD task. Specifically, we introduce a Bilateral Attention Module (BAM) with a complementary attention mechanism: foreground-first (FF) attention and background-first (BF) attention. The FF attention focuses on the foreground region with a gradual refinement style, while the BF one recovers potentially useful salient information in the background region. Benefitted from the proposed BAM module, our BiANet can capture more meaningful foreground and background cues, and shift more attention to refining the uncertain details between foreground and background regions. Additionally, we extend our BAM by leveraging the multi-scale techniques for better SOD performance. Extensive experiments on six benchmark datasets demonstrate that our BiANet outperforms other state-of-the-art RGB-D SOD methods in terms of objective metrics and subjective visual comparison. Our BiANet can run up to 80fps on 224×224 RGB-D images, with an NVIDIA GeForce RTX 2080Ti GPU. Comprehensive ablation studies also validate our contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S ALIENT object detection (SOD) aims to segment the most attractive objects in an image. As an fundamental computer vision task, SOD has been widely applied into many vision applications, such as visual tracking <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b31">[32]</ref>, image segmentation <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b42">[43]</ref>, and video analysis <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b46">[47]</ref>, etc. Most of existing SOD methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b50">[51]</ref> mainly deal with RGB images. However, they usually produce inaccurate SOD results on the scenarios of similar texture, complex background, or homogeneous objects <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b51">[52]</ref>. With the popularity of depth sensors in smartphones, the depth information, e.g., 3D layout and spatial cues, is crucial for reducing the ambiguity in the RGB images, and serves as important supplements to improve the SOD performance <ref type="bibr" target="#b24">[25]</ref>.</p><p>Recently, RGB-D SOD has received increasing research attention <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Early RGB-D SOD works <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref> introduced the depth contrast as an important prior for the SOD task. The recent work of CPFP <ref type="bibr" target="#b54">[55]</ref> utilized the depth contrast prior to design an effectiveness loss. These methods essentially explore depth information to shift more priority on the <ref type="bibr">Zhao</ref> Zhang (e-mail: zzhang@mail.nankai.edu.cn), Zheng Lin, Jun Xu, Shao-Ping Lu, and Deng-Ping Fan are with the TKLNDST, College of Computer Science, Nankai University. Wenda Jin is with Tianjin University. Shao-Ping Lu is the corresponding author (e-mail: slu@nankai.edu.cn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>Depth GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background-First Attention</head><p>Foreground-First Attention Bilateral Attention <ref type="figure">Fig. 1</ref>.</p><p>Comparison of RGB-D SOD results by Foreground-First, Background-First, and our Bilateral attention mechanisms. Depth information provides rich foreground and background relationships. Paying more attention to foreground helps to predict high-confidence foreground objects, but may produce incomplete results. Focusing more on background finds more complete objects, but may introduce unexpected noise. Our BiANet jointly explores foreground and background cues, and achieves complete foreground prediction with little background noise. foreground region <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>. However, as demonstrated in <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, understanding what background is can also promote the SOD performance. Several traditional methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b49">[50]</ref> predict salient objects jointly from the complementary foreground and background information, which is largely ignored by current RGB-D SOD networks.</p><p>In this paper, we propose a Bilateral Attention Network (BiANet) to collaboratively learn complementary foreground and background features from both RGB and depth streams for better RGB-D SOD performance. As shown in <ref type="figure">Figure 2</ref>, our BiANet employs a two-stream architecture, and the side outputs from the RGB and depth streams are concatenated in multiple stages. Firstly, we use the high-level semantic features F 6 to locate the foreground and background regions S 6 . However, the initial saliency map S 6 is coarse and in lowresolution. To enhance the coarse saliency map, we design a Bilateral Attention Module (BAM), which is composed of the complementary foreground-first (FF) attention and background-first (BF) attention mechanisms. The FF shifts attention on the foreground region to gradually refine its saliency prediction, while the BF focuses on the background region to recover the potential salient regions around the boundaries. By bilaterally exploring the foreground and background cues, the model helps predict more accurately as shown in <ref type="figure">Figure 1</ref>. Secondly, we propose a multi-scale extension of BAM (MBAM) to effectively learn multi-scale contextual information, and capture both local and global saliency information to further improve the SOD performance. Extensive experiments on six  </p><formula xml:id="formula_0">i } 6 i=1</formula><p>. We take the top feature F 6 to predicate a coarse salient map S 6 . To obtain the accurate and high-resolution result, we up-sample the initial salient map and compensate the details by BAMs in a top-down manner. BAMs receive the higher-level prediction S i+1 and current level feature F i as inputs. In a BAM, the foreground-first attention map A F i and the background-first attention map A B i can be calculated according to S i+1 . We apply the duel complementary attention maps to explore the foreground and background cues bilaterally, and jointly infer the residual for refining the up-sampled saliency map.</p><p>benchmark datasets demonstrate that our BiANet achieves better performance than previous state-of-the-arts on RGB-D SOD, and is very fast owing to our simple architecture.</p><p>In summary, our main contributions are three-fold:</p><p>• We propose a simple yet effective Bilateral Attention Module (BAM) to explore the foreground and background cues collaboratively with the rich foreground and background information from the depth images. • Our BiANet achieves better performance on six popular RGB-D SOD datasets under nine standard metrics, and presents better visual effects (e.g., contains more details and sharp edges) than the state-of-the-art methods. • Our BiANet runs at 34fps∼80fps on an NVIDIA GeForce RTX2080Ti GPU under different settings, and is a feasible solution for real-world applications. The remainder of this paper is organized as follows. In §II, we briefly survey the related work. In §III, we present the proposed Bilateral Attention Network (BiANet) for RGB-D Salient Object Detection. Extensive experiments are conducted in §IV to evaluate its performance when compared with stateof-the-art RGB-D SOD methods on six benchmark datasets. The conclusion is given in §V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. RGB-D Salient Object Detection</head><p>RGB-D salient object detection (SOD) aims to segment the most attractive object(s) in a pair of cross-modal RGB and depth images. Early methods mainly focus on extracting lowlevel saliency cues from RGB and depth images, exploring object distance <ref type="bibr" target="#b24">[25]</ref>, difference of Gaussian <ref type="bibr" target="#b21">[22]</ref>, graph knowledge <ref type="bibr" target="#b8">[9]</ref>, multi-level discriminative saliency fusion <ref type="bibr" target="#b41">[42]</ref>, multicontextual contrast <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b34">[35]</ref>, and background enclosure <ref type="bibr" target="#b12">[13]</ref>, etc. However, these methods often produce inaccurate saliency predictions, due to the lack of high-level feature representation.</p><p>Recently, deep neural networks (DNNs) <ref type="bibr" target="#b17">[18]</ref> have been employed to investigate high-level representations of crossmodal fusion of RGB and depth images, with much better SOD performance. Most of these DNNs <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b43">[44]</ref> first extract the RGB and depth features separately and then fuse them in the shallow, middle, or deep layers of the network. The methods of <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b36">[37]</ref> further improved the SOD performance by fusing cross-modal features in multi-level stages instead of as a one-off integration. Zhao et al. <ref type="bibr" target="#b54">[55]</ref> also took the enhanced depth image as attention maps to boost RGB features in multiple stages with better SOD performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Foreground and Background Cues</head><p>There are great differences in the distribution of foreground and background, so it is necessary to explore their respective cues. In traditional methods, some works focus on reasoning salient areas in foreground and background jointly. Yang et al. <ref type="bibr" target="#b49">[50]</ref> proposed a two-stage method for SOD. It first takes the four boundaries of the inputs as background seeds to infer foreground queries via a graph-based manifold ranking. Then, it ranks the graph depending on the foreground seeds in the same manner for final detection. This method is enlightening, but it has obvious limitations: 1) It is inappropriate to use the four boundaries directly as background, because the foreground is likely to be connected to the boundaries. 2) Aggregation at the super-pixel level also results in rough outputs. For the limitation 1), Liang et al. <ref type="bibr" target="#b28">[29]</ref> introduce the depth map to distinguish foreground and background regions instead of only assuming the boundaries as background. The depth map shows clear disparity in most senses; thus, it can support more precise locating. For the limitaion 2), Li et al. <ref type="bibr" target="#b25">[26]</ref> further used the regularized random walks ranking to formulate pixel-wised saliency maps, which improves the scaling effect caused by super-pixel aggregation. Nevertheless, only depending on these</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>Depth GT Pred</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreground-First Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background-First Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhanced Features Residuals</head><p>Level-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Level-2</head><p>Level-3</p><p>Level-4</p><p>Level-5 <ref type="figure">Fig. 3</ref>. Visualizing the working mechanism of bilateral attention. The original features are the averaged side-output features in each levels. We show the original features directly multiplied by foreground-and background-first attention maps in left columns of yellow and blue boxes. The right columns of the two boxes are the further convoluted features in two branches. As can be seen, the foreground-first features focus on foreground region to explore the saliency cues; while the background-first features shift more attention to the background regions to mine the potentially significant objects. No matter in the features of foreground-or background-first features, more priority is shifted to the uncertain areas caused by the up sampling. When fusing the two branches and jointly inferring, we can see the bilaterally enhanced features have a more accurate understanding where the foreground or background is. Due to obtaining more attention, the uncertain areas are reassigned to the right attribution by the residual with strong contrast. 'Pred' is the prediction of the model. low-level priors, traditional methods cannot accurately locate the initial region of foreground and background.</p><p>Recently, Chen et al. <ref type="bibr" target="#b5">[6]</ref> proposed to gradually explore saliency regions from the background using reverse attention, but they ignored the contribution of foreground cues to the final detection. As far as we know, how to jointly refine the salient objects from the foreground and background regions is still an open problem in deep RGB-D SOD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED BIANET FOR RGB-D SOD</head><p>In this section, we first introduce the overall architecture of our BiANet, and then present the bilateral attention module (BAM) as well as its multi-scaled extension (MBAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Overview</head><p>As shown in <ref type="figure">Figure 2</ref>, our Bilateral Attention Network (BiANet) contains three main steps: feature extracting, prediction up-sampling, and bilateral attention residual compensation. We extract the multi-level features from the RGB and depth streams. With increasing network depth, the high-level features (e.g., F 4 ) will be more potent for capturing global context, while it loses the object details. When we up-sample the high-level predictions, the saliency maps (e.g., S 5 ) will be blurred, e.g.the edges will become uncertain. Thus, we use the proposed Bilateral Attention Module (BAM) to distinguish foreground and background regions.</p><p>1) Feature extracting: We encode RGB and depth information with two streams. Specifically, both the RGB and depth streams employ five convolutional blocks from VGG-16 <ref type="bibr" target="#b40">[41]</ref> as the standard backbone and attach an additional convolution group with three convolutional layers to predict the saliency maps, respectively. Unlike previous works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b4">[5]</ref>, we explore the cross-modal fusion of RGB and depth features at multiple stages, rather than fusing them once in low or high stage. The i-th side output f rgb i from the RGB stream and f d i from the depth stream are concatenated as a feature tensor F i . Note that, F 6 is concatenated by M( f rgb 5 ) and M( f d 5 ), where M(·) denotes the max-pooling operation. The coarse saliency map S 6 is derived from F 6 , and {F 1 , F 2 , · · · , F 5 } are prepared for the BAMs in our BiANet to further refine the upsampled saliency maps, by distinguishing the uncertain regions as foreground or background in a top-down manner.</p><p>2) Prediction up-sampling: The initial saliency map predicted from the high-level features is coarse in low-resolution, but useful to predict the initial position of the foreground and background, since it contains rich semantic information. To refine the basic saliency map S 6 , a lower-level feature F 5 with more details is used to predict the residual component between the higher-level prediction and the ground-true (GT) with the help of BAM. We add the predicted residual component R 5 to the up-sampled higher-level prediction S 6 , and obtain a refined prediction S 5 , etc., that is,</p><formula xml:id="formula_1">S i = R i +U(S i+1 ), i ∈ {1, . . . , 5},<label>(1)</label></formula><p>where U(·) means up-sampling. Finally, our BiANet obtains a saliency map by S = σ (S 1 ), where σ (·) is a sigmoid function. The second row is the averaged foreground-first features from the model where the MBAMs are applied in the top three levels (marked with red numbers). The thrid row is the averaged foreground-first features obtained from the model in which all levels are armed with BAMs. We can see that, compared with applying the BAMs, MBAMs in higher levels capture more complete information, which is conducive to the object locating as shown in the first row.</p><p>to enable our BiANet to discriminate the foreground and background. In our BAM, the higher-level prediction serves as a foreground-first attention (FF) map, and the reversed prediction serves as background-first (BF) attention map to combine the bilateral attention on foreground and background.</p><p>In <ref type="figure">Figure 3</ref>, one can see that the residual generated by BAM possesses high contrast at the object boundaries. More details are described in Sections III-B and III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Loss function:</head><p>Deep supervision is widely used in the SOD task <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>. It clarifies the optimization goals for each step of the network, and accelerates the convergence of training. For quick convergence, we also apply deep supervision in the depth stream output S d , RGB stream output S rgb , and each top-down side output {S 1 , S 2 , · · · , S 6 }. The total loss function of our BiANet is</p><formula xml:id="formula_2">L = 6 i=1 w i L ce (σ (S i ) , GT) + w d L ce (σ (S d ) , GT) + w rgb L ce σ S rgb , GT ,<label>(2)</label></formula><p>in which w i , w d , and w rgb are the weight coefficients and simply set to 1 in our experiments. L ce (·) is the binary cross entropy loss, which is formulated as</p><formula xml:id="formula_3">L ce (X, Y) = − 1 N N i=1 y i log(x i ) + (1 − y i )log(1 − x i ) . (3)</formula><p>In the above equation, x i ∈ X and y i ∈ Y, and N denotes the total pixel number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bilateral Attention Module (BAM)</head><p>Given the initial foreground and background, how to refine the prediction using higher-resolution cross-modal features is the focus of this paper. Considering that the distribution of foreground and background are quite different, we design a bilateral attention module using a pair of reversed attention components to learn features from the foreground and background respectively, and then jointly refine the prediction. As can be seen in <ref type="figure">Figure 2</ref>, to focus more on the foreground, we use the up-sampled prediction from the higher-level as foreground-first attention (FF) maps {A F } 5 i=1 after they are activated by sigmoid, and the background-first attention (BF) maps {A B } 5 i=1 are generated by subtracting FF maps from matrix E, in which all the elements are 1.</p><formula xml:id="formula_4">   A F i = σ U(S i+1 ) , A B i = E − σ U(S i+1 ) , i ∈ {1, 2, 3, 4, 5}.<label>(4)</label></formula><p>Then, as shown in <ref type="figure">Figure 2</ref>, we apply FF and BF to weight the side-output features in two branches, respectively, and further predict the residual component jointly.</p><formula xml:id="formula_5">R i = P R P F i A F i , P F i A B i ,<label>(5)</label></formula><p>whereF i is the channel-reduced feature of F i using 32 1 × 1 convolutions to reduce the computational cost. P represents the feature extraction operation consisting of 32 convolution kernels with a size of 3 × 3 and a ReLU layer. The two branches do not share parameters.</p><p>[·, ·] means concatenation. P R is the prediction layer to output a single channel residual map via a 3 × 3 kernel after the same feature extraction operation with P. Once the R i is obtained, the refined prediction S i is obtained via Equation 1.</p><p>To better understand the working mechanism of BAM, in <ref type="figure">Figure 3</ref>, we visualize the channel-wise averaged features from BAMs in different levels. In BAM, the original features will be first fed into two branches by multiply the FF and BF attention maps, respectively. The result of the direct multiplication is illustrated in the left half of the yellow (FF features) and blue (BF features) boxes. We can see that FF branch shifts attention to the foreground area predicted from its higher level to explore foreground saliency cues. After a convolution layer, more priority is given to the uncertain area. Complementarily, BF branch focuses on the background area to explore the background cues, looking for possible salient objects within it. In our BiANet, the top-down prediction up-sampling is a process in which the resolution of salient objects is gradually increased. It will result in uncertain coarse edges. We can see that both of FF and BF features focus on the uncertain area (such as object boundaries). The low-level and high-resolution FF branch will eliminate the overflow of the uncertain area, while the BF branch will eliminate the uncertain area which does not belong to the background. That is an important reason why BiANet performs better on detail and is prone to predicting sharp edges. After the joint inferring, we can see the bilaterally enhanced features contain more discriminative spatial information of foreground and background. The generated residual components are with sharp contrast on the edges, and then suppress the background area and strengthen the foreground regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-Scale Extension of BAM (MBAM)</head><p>Salient objects in a scene are various in location, size, and shape. Thus, exploring the multi-scaled context in high-level layers benefits for understanding the scene <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b55">[56]</ref>. To this end, we extend our BAM with a multi-scale version, in which groups of dilated convolutions are used to extract pyramid representations from the undetermined foreground and background areas. Specifically, the module can be described as</p><formula xml:id="formula_6">R i = P R 4 i=1 D i F i A F i , 4 i=1 D i F i A B i ,<label>(6)</label></formula><p>where means a concatenate operation. D 1 consists of 1 × 1 kernels with 32 channels and a ReLU layer. {D i } 4 i=2 is a group of dilated convolutions, with rates of 3, 5, and 7. They all consist of 3 × 3 kernels with 32 channels and a ReLU layer.</p><p>We recommend applying the MBAM in high-level crossmodal features, such as {F 3 , F 4 , F 5 }, which need different sizes of receptive fields to explore multi-scale context. MBAM effectively improves the detection performance but introduces a certain computational cost. Thus, the number of MBAM should be a trade-off in practical applications. In Section IV-C3, we discuss in detail how the number of MBAM changes the detection effect and calculation cost.</p><p>In order to intuitively observe the gain effect brought by MBAM, we visualize the averaged foreground-first feature maps from MBAMs and BAMs in <ref type="figure">Figure 4</ref>. In the second row, the feature maps are obtained from the model with three MBAMs in its top three levels, while in the last row, all the feature maps are collected from BAMs. We can see the target object (horse) account for a large proportion of the scene. Without the ability to perceive multi-scale information effectively, the BAM fails to capture the accurate global salient regions in high levels and leads to incomplete prediction finally. When introducing the multi-scale extension, we can see higher-level features achieve stronger spatial representation, which supports to locate more complete salient object.</p><p>D. Implementation Details 1) Settings: We apply the MBAM in the high-level side outputs {F 3 , F 4 , F 5 } during implementation, and use bilinear interpolation in all interpolation operations. The initial parameters of our backbone are loaded from a VGG-16 network pretrained on ImageNet. Our BiANet is based on PyTorch <ref type="bibr" target="#b33">[34]</ref>.</p><p>2) Training: Following D3Net <ref type="bibr" target="#b11">[12]</ref>, we use the training set containing 1485 and 700 image pairs from the NJU2K <ref type="bibr" target="#b21">[22]</ref> and NLPR <ref type="bibr" target="#b34">[35]</ref> datasets, respectively. We employ the Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with an initial learning rate of 0.0001, β 1 = 0.9, and β 2 = 0.99. The batch size is set to 8, and we train our BiANet for 25 epochs in total. The training images are resized to 224 × 224, also during the test. The output saliency maps are resized back to the original size for evaluation. Accelerated by an NVIDIA GeForce RTX 2080Ti, our BiANet takes about 2 hours for training, and runs at 34∼80fps (with different numbers of MBAMs) for the inputs with 224 × 224 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Protocols</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Evaluation datasets:</head><p>We conduct experiments on six widely used RGB-D based SOD datasets. NJU2K <ref type="bibr" target="#b21">[22]</ref> and NLPR <ref type="bibr" target="#b34">[35]</ref> are two popular large-scale RGB-D SOD datasets containing 1985 and 1000 images, respectively. DES [7] contains 135 indoor images with fine structures collected with Microsoft Kinect <ref type="bibr" target="#b53">[54]</ref>. STERE <ref type="bibr" target="#b32">[33]</ref> contains 1000 internet images, and the corresponding depth maps are generated by stereo images using a sift flow algorithm <ref type="bibr" target="#b29">[30]</ref>. SSD <ref type="bibr" target="#b57">[58]</ref> is  2) Evaluation metrics: We employ 9 metrics to comprehensively evaluate these methods. Precision-Recall (PR) curve <ref type="bibr" target="#b37">[38]</ref> shows the precision and recall performances of the predicted saliency map at different binary thresholds. Fmeasure <ref type="bibr" target="#b0">[1]</ref> is computed by the weighted harmonic mean of the thresholded precision and recall. We employ maximum Fmeasure (max F β ), mean F-measure (mean F β ), and adaptive F-measure (adp F β ). Mean Absolute Error (MAE, M) <ref type="bibr" target="#b35">[36]</ref> directly estimates the average pixel-wise absolute difference between the prediction and the binary ground-truth map. Smeasure (S α ) <ref type="bibr" target="#b9">[10]</ref> is an advanced metric, which takes the region-aware and object-aware structural similarity into consideration. E-measure <ref type="bibr" target="#b10">[11]</ref> is the recent proposed Enhanced alignment measure in the binary map evaluation field, which combines local pixel values with the image level mean value in one term, jointly capturing image-level statistics and local pixel matching information. Similar to F β , we employ the maximum E-measure (max E ξ ), mean E-measure (mean E ξ ), and adaptive E-measure (adp E ξ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>Depth GT BiANet D3Net <ref type="bibr" target="#b11">[12]</ref> DMRA <ref type="bibr" target="#b36">[37]</ref> CPFP <ref type="bibr" target="#b54">[55]</ref> TANet <ref type="bibr" target="#b3">[4]</ref> PCF <ref type="bibr" target="#b2">[3]</ref>  B. Comparison with State-of-the-Arts 1) Comparison methods: We compared with 14 state-ofthe-art RGB-D SOD methods, including 5 traditional methods: ACSD <ref type="bibr" target="#b21">[22]</ref>, LBE <ref type="bibr" target="#b12">[13]</ref>, DCMC <ref type="bibr" target="#b8">[9]</ref>, MDSF <ref type="bibr" target="#b41">[42]</ref>, and SE <ref type="bibr" target="#b15">[16]</ref>, and 9 DNN-based methods: DF <ref type="bibr" target="#b38">[39]</ref>, AFNet <ref type="bibr" target="#b43">[44]</ref>, CTMF <ref type="bibr" target="#b16">[17]</ref>, MMCI <ref type="bibr" target="#b4">[5]</ref>, PCF <ref type="bibr" target="#b2">[3]</ref>, TANet <ref type="bibr" target="#b3">[4]</ref>, CPFP <ref type="bibr" target="#b54">[55]</ref>, DMRA <ref type="bibr" target="#b36">[37]</ref>, and D3Net <ref type="bibr" target="#b11">[12]</ref>. The codes and saliency maps of these methods are provided by the authors.    2) Quantitative evaluation: The complete quantitative evaluation results are listed in <ref type="table" target="#tab_1">Table I</ref>. The comparison methods are presented from right to left according to the comprehensive performance of these metrics, where the lower the value of MAE (M), the better the effect of the model. The other metrics are the opposite. We also plot the PR curves of these methods in <ref type="figure" target="#fig_2">Figure 5</ref>. One can see that our BiANet achieves remarkable advantages over the comparison methods. DMRA <ref type="bibr" target="#b36">[37]</ref> and D3Net <ref type="bibr" target="#b11">[12]</ref> are well-matched in these datasets. On the largescaled NJU2K <ref type="bibr" target="#b21">[22]</ref> and NLPR <ref type="bibr" target="#b34">[35]</ref> datasets, our BiANet outperforms the second best with ∼3% improvement on max F β . On the DES <ref type="bibr" target="#b6">[7]</ref> dataset, Compared to methods which are heavily dependent on depth information, our proposed BiANet also has a 3.8% improvement on max F β . This indicates that our BiANet can make more efficient use of depth information. Although the SSD <ref type="bibr" target="#b57">[58]</ref> dataset is high-resolution, the quality of the depth map is poor. Our BiANet still exceeds D3Net <ref type="bibr" target="#b11">[12]</ref>, which is specifically designed for robustness to low-quality depth maps. Our BiANet also performs the best on the SIP <ref type="bibr" target="#b11">[12]</ref>, which is a challenging dataset with complex scenes and multiple objects.</p><p>3) Qualitative results: To further demonstrate the effectiveness of our BiANet, we visualized the saliency maps of our BiANet and other top 5 methods in <ref type="figure" target="#fig_4">Figure 6</ref>. One can see that the target object in the 1st column is tiny, and its white shoes and hat are hard to distinguish from the background. Our BiANet effectively utilizes the depth information, while the others are disturbed by RGB background clutter. The inputs in the 2nd column are challenging because the depth map is mislabeled, and the RGB image was taken in a dark environment with low contrast. Our BiANet successfully detects the target sculpture and eliminates the interference of flowers and the base of the sculpture, while D3Net mistakenly detects a closer rosette, and DMRA loses the part of the object that is similar to the background. The 3rd column shows the ability of our BiANet to detect complex structures of salient objects. Among these methods, only our BiANet completely discover the chairs, including the fine legs. The 4th column is a multi-object scene. Because there are no depth differences between the three salient windows below and the wall, they are not reflected on the depth map, but the three windows above are clearly observed on the depth map. In this case, the depth map will mislead subsequent segmentation. Our BiANet detects multiple objects from RGB images with less noise. The 5th column is also a multi-object scene. The bottom half of depth map is confused with the interference from the ground. Thus, detecting the legs of these persons in the image is very difficult. However, our BiANet successfully detected all the legs. The last row is a large-scale object whose color and depth map are not distinguished. Large scale, low color contrast and lack of discriminative depth information make the scene very challenging. Fortunately, our BiANet is robust on this scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In this section, we mainly investigate: 1) the benefits of bilateral attention mechanism to our BiANet; 2) the effectiveness of BAM in different levels to our BiANet for RGB-D SOD; 3) the further improvements of MBAM in different levels to our BiANet; 4) the benefits of combining BAM and MBAM for RGB-D SOD; and 5) the impact of different backbones to our BiANet for RGB-D SOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effectiveness of bilateral attention:</head><p>We conduct ablation studies on the large-scaled NJU2K and STERE datasets to investigate the contributions of different mechanisms in the proposed method. The baseline model used here contains a VGG-16 backbones and a residual refine structure. It takes RGB images as input without depth information. The performance of our basic network without any additional mechanisms is illustrated in <ref type="table" target="#tab_1">Table II</ref> No. 1. Based on the network, we gradually add different mechanisms and test various combinations. These candidates are depth information (Dep), foregroundfirst attention (FF), background-first attention (BF), and multiscale extension (ME). In <ref type="table" target="#tab_1">Table II</ref> No. 3, by applying FF, the performance is improved to some extent, It benefits from the foreground cues being learned effectively by shifting the attention to the foreground objects. This is also reflected in <ref type="figure" target="#fig_5">Figure 7</ref>. The foreground objects are detected more accurately; however, without good understanding on background cues, it tend to mistake some background objects, such as the red house in the third row, or cannot find complete foreground objects as lack of exploration on background regions. We get a similar accuracy when using the BF only, as shown in No. 4. It excels at distinguishing between salient areas and non-salient areas in the background, and can help to find more complete regions of the salient object in the uncertain background; however, too much attention focusing on the background and without a good understanding of the foreground cues, it leads that sometimes background noise is introduced. When we combine FF together with BF to form our BAM and apply it in all side outputs, the performance boosts. We can see that BAM increases S-measure by 0.9% and max F-measure by 1.2% compared with No. 2. When we replace the top three levels BAMs with MBAMs, the performance further improved. In <ref type="figure" target="#fig_5">Figure 7</ref>, compared to the performance of No. 2 without BAM, the detected salient objects of No. 6 possess higher confidence, sharper edges, and less background noise.</p><p>2) Effectiveness of BAM with different levels: In order to verify that our BAM module is effective at each feature level, we apply BAM to each side output of the No. 2 model's feature extractor, respectively. That is, in each experiment, BAM is applied to one side output, while the others undergo general convolutions. From <ref type="table" target="#tab_1">Table III</ref>, we can see that the BAMs in every layer facilitate a universal improvement on detection performance. In addition, we find that BAM applied in the lower levels contributes more to the results.</p><p>3) Effectiveness of MBAM in different levels: In <ref type="table" target="#tab_1">Table II</ref>  <ref type="table" target="#tab_1">Table IV</ref>, where different levels of MBAM bring different degrees of improvement to the results. <ref type="table" target="#tab_1">Comparing Table III</ref> and <ref type="table" target="#tab_1">Table IV</ref>, we can see a more interesting phenomenon that the BAM applied in the lower level brings more improvement while the MBAM applied in the higher level is more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Cooperation between BAM and MBAM:</head><p>The observation above guides us that when using BAM and MBAM in cooperation, we should give priority to multi-scale expansion of higher-level BAM. Therefore, we expand BAM from top to bottom until all BAMs are converted into MBAMs. We record the final detection performance and calculation cost during the gradual expansion in <ref type="table" target="#tab_5">Table V</ref>. We start from the highest level, and gradually increase the number of MBAMs to three. We can see that the effect on the model is a steady improvement, but the computing cost is also increased. At the lower levels, adding MBAM has no obvious effect. This phenomenon is in line with our expectation. Besides, due to the high resolution, the extension of lower-level BAM will increase the calculation cost and reduce the robustness. The selection of the number of MBAM needs to balance the accuracy and speed requirements of the application scenario. In scenarios with higher speed requirements, we recommend not to use MBAM. Our most lightweight model can achieve ∼80fps while ensuring significant performance advantages. The parameter size and FLOPs are superior to the SOTA methods D3Net <ref type="bibr" target="#b11">[12]</ref> and DMRA <ref type="bibr" target="#b36">[37]</ref>. In scenarios where high accuracy is required, we suggest applying less than three MBAMs on higher-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Performances under different backbones:</head><p>We implement the BiANet based on some other widely-used backbones to demonstrate the effectiveness of the proposed bilateral attention mechanism on different feature extractors. Specifically, in addition to VGG-16 <ref type="bibr" target="#b40">[41]</ref>, we provide the results of BiANet on VGG-11 <ref type="bibr" target="#b40">[41]</ref>, ResNet-50 <ref type="bibr" target="#b17">[18]</ref>, and Res2Net-50 <ref type="bibr" target="#b14">[15]</ref>. Compared with VGG-16, VGG-11 is a lighter backbone. As shown in <ref type="table" target="#tab_1">Table VI</ref>, although the accuracy is slightly lower than VGG-16, it still reaches SOTA with a faster speed. BiANet with stronger backbones will bring more remarkable improvements. For example, when we employ ResNet-50 like D3Net <ref type="bibr" target="#b11">[12]</ref> as backbone, our BiANet brings 1.5% improvement on NJU2K <ref type="bibr" target="#b21">[22]</ref> in terms of the MAE compared with the D3Net <ref type="bibr" target="#b11">[12]</ref>. When armed with Res2Net-50 <ref type="bibr" target="#b14">[15]</ref>, BiANet achieves 3.8% improvement on NJU2K <ref type="bibr" target="#b21">[22]</ref> in terms of the max F-measure compared with the SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Failure Case Analysis</head><p>In <ref type="figure">Figure 8</ref>, we illustrate some failure cases when our BiANet works in some extreme environments. BiANet ex- <ref type="figure">Fig. 8</ref>. Failure cases of BiANet in extreme environments. In the first two columns, as the objects closer to the observer are not the targets, the depth maps provide misleading information. In the last two columns, the BiANet fails lead by the confusing RGB information and coarse depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Depth GT Prediction</head><p>plores the saliency cues bilaterally in the foreground and background regions with the relationship provided by depth information. However, when the foreground regions indicated by depth information do not belong to the salient object, it is likely to mislead the prediction. The first two columns in <ref type="figure">Figure 8</ref> are typical examples, where our BiANet mistakenly takes the object close to the observer as the target, and gives the wrong prediction. The other situation that may cause failure is when BiANet encounters coarse depth maps in complex scenarios ( see the last two columns). In the third column, the depth map provides inaccurate spatial information, which affects the detection of details. In the last column, the inaccurate depth map and the confusing RGB information make BiNet fail to locate the target object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a fast yet effective bilateral attention network (BiANet) for RGB-D saliency object detection (SOD) task. To better utilize the foreground and background information, we propose a bilateral attention module (BAM) to comprise the dual complementary of foreground-first attention and background-first attention mechanisms. To fully exploit the multi-scale techniques, we extend our BAM module to its multi-scale version (MBAM), capturing better global information. Extensive experiments on six benchmark datasets demonstrated that our BiANet, benefited by our BAM and MBAM modules, outperforms previous state-of-the-art methods on RGB-D SOD, in terms of quantitative and qualitative performance. The proposed BiANet runs at real-time speed on a single GPU, making it a potential solution for various real-world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 )Fig. 4 .</head><label>34</label><figDesc>Bilateral attention residual compensation: To get better residuals and distinguish up-sampled foreground and background regions, we design a bilateral attention module (BAM) Comparison of the high-level features capured by MBAM and BAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>PR curves of our BiANet and other 14 state-of-the-art methods across 6 datasets. The node on each curve denotes the precision and recall value used for calculating max F-measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>S α ↑ 0 050 SIP [ 12 ]</head><label>005012</label><figDesc>mean F β ↑ 0.469 0.489 0.572 0.470 0.564 0.624 0.672 0.689 0.721 0.777 0.773 0.747 0.828 0.815 0.832 adp F β ↑ 0.656 0.613 0.679 0.674 0.693 0.724 0.694 0.710 0.748 0.791 0.767 0.726 0.821 0.790 0.821 max E ξ ↑ 0.785 0.736 0.786 0.779 0.800 0.828 0.807 0.865 0.882 0.894 0.897 0.852 0.906 0.907 0.916 mean E ξ ↑ 0.566 0.574 0.646 0.576 0.631 0.690 0.762 0.796 0.796 0.856 0.861 0.839 0.897 0.886 0.896 adp E ξ ↑ 0.765 0.729 0.786 0.772 0.778 0.812 0.803 0.838 0.860 0.886 0.879 0.832 0.892 0.885 0.902 M ↓ 0.203 0.278 0.169 0.192 0.165 0.142 0.118 0.099 0.082 0.062 0.063 0.082 0.058 0.059 0.S α ↑ 0.732 0.727 0.683 0.717 0.628 0.653 0.720 0.716 0.833 0.842 0.835 0.850 0.806 0.864 0.883 max F β ↑ 0.763 0.751 0.618 0.698 0.661 0.657 0.712 0.694 0scale but high-resolution dataset with 400 images in 960 × 1080 resolution. SIP [12] is a high-quality RGB-D SOD dataset with 929 person images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Visual comparison of BiANet with other top 5 methods. The inputs include difficult scenes of tiny objects (column 1), complex background (column 1 and 2), complex texture (column 3), low contrast (column 2 and 6), low-quality or confusing depth (column 2, 4, and 6), and multiple objects (column 4 and 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Visual comparison in the ablation studies. The candidate mechanisms are deep information (Dep), foreground-first attention (FF), background-first attention (BF), and multi-scale extension (ME). No. 6: (Dep + FF + BF + ME). No. 5: (Dep + FF + BF). No. 4: (Dep + BF). No. 3: (Dep + FF). No. 2: Dep. No. 1: Baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISONS OF OUR BIANET WITH NINE DEEP-LEARNING-BASED METHODS AND FIVE TRADITIONAL METHODS ON SIX POPULARDATASETS IN TERM OF S-MEASURE (S α ), MAXIMUM F-MEASURE (MAX F β ), MEAN F-MEASURE (MEAN F β ), ADAPTIVE F-MEASURE (ADP F β ), MAXIMUM E-MEASURE (MAX E ξ ), MEAN E-MEASURE (MEAN E ξ ), ADAPTIVE E-MEASURE (ADP E ξ ), AND MEAN ABSOLUTE ERROR (MAE, M). F β AND E ξ REPRESENT MAX F β AND MAX E ξ BY DEFAULT. ↑ MEANSTHAT THE LARGER THE NUMERICAL VALUE, THE BETTER THE MODEL, WHILE ↓ MEANS THE OPPOSITE. FOR TRADITIONAL METHODS, THE STATISTICS ARE BASED ON OVERALL DATASETS RATHER ON THE TEST SET.</figDesc><table><row><cell></cell><cell cols="4">ACSD LBE DCMC MDSF</cell><cell>SE</cell><cell cols="4">DF AFNet CTMF MMCI</cell><cell>PCF</cell><cell cols="5">TANet CPFP DMRA D3Net BiANet</cell></row><row><cell></cell><cell cols="14">Metric ICIP14 CVPR16 SPL16 TIP17 ICME16 TIP17 arXiv19 TOC18 PR19 CVPR18 TIP19 CVPR19 ICCV19 arXiv19</cell><cell>2020</cell></row><row><cell></cell><cell>[22]</cell><cell>[13]</cell><cell>[9]</cell><cell>[42]</cell><cell>[16]</cell><cell>[39]</cell><cell>[44]</cell><cell>[17]</cell><cell>[5]</cell><cell>[3]</cell><cell>[4]</cell><cell>[55]</cell><cell>[37]</cell><cell>[12]</cell><cell>Ours</cell></row><row><cell>[22]</cell><cell cols="2">S α ↑ 0.699 0.695 max F β ↑ 0.711 0.748 mean F β ↑ 0.512 0.606</cell><cell cols="7">0.686 0.748 0.664 0.763 0.772 0.849 0.858 0.715 0.775 0.748 0.804 0.775 0.845 0.852 0.556 0.628 0.583 0.650 0.764 0.779 0.793</cell><cell cols="3">0.877 0.878 0.879 0.872 0.874 0.877 0.840 0.841 0.850</cell><cell cols="2">0.886 0.893 0.886 0.887 0.873 0.859</cell><cell>0.915 0.920 0.903</cell></row><row><cell>NJU2K</cell><cell cols="2">adp F β ↑ 0.696 0.740 max E ξ ↑ 0.803 0.803 mean E ξ ↑ 0.593 0.655</cell><cell cols="7">0.717 0.757 0.734 0.784 0.768 0.788 0.812 0.799 0.838 0.813 0.864 0.853 0.913 0.915 0.619 0.677 0.624 0.696 0.826 0.846 0.851</cell><cell cols="3">0.844 0.844 0.837 0.924 0.925 0.926 0.895 0.895 0.910</cell><cell cols="2">0.872 0.840 0.927 0.930 0.920 0.910</cell><cell>0.892 0.948 0.934</cell></row><row><cell></cell><cell cols="2">adp E ξ ↑ 0.786 0.791</cell><cell cols="7">0.791 0.812 0.772 0.835 0.846 0.864 0.878</cell><cell cols="3">0.896 0.893 0.895</cell><cell cols="2">0.908 0.894</cell><cell>0.926</cell></row><row><cell></cell><cell cols="2">M ↓ 0.202 0.153</cell><cell cols="7">0.172 0.157 0.169 0.141 0.100 0.085 0.079</cell><cell cols="3">0.059 0.060 0.053</cell><cell cols="2">0.051 0.051</cell><cell>0.039</cell></row><row><cell>[33]</cell><cell cols="2">S α ↑ 0.692 0.660 max F β ↑ 0.669 0.633 mean F β ↑ 0.478 0.501</cell><cell cols="7">0.731 0.728 0.708 0.757 0.825 0.848 0.873 0.740 0.719 0.755 0.757 0.823 0.831 0.863 0.590 0.527 0.610 0.617 0.806 0.758 0.813</cell><cell cols="3">0.875 0.871 0.879 0.860 0.861 0.874 0.818 0.828 0.841</cell><cell cols="2">0.835 0.889 0.847 0.878 0.837 0.841</cell><cell>0.904 0.898 0.879</cell></row><row><cell>STERE</cell><cell cols="2">adp F β ↑ 0.661 0.595 max E ξ ↑ 0.806 0.787 mean E ξ ↑ 0.592 0.601</cell><cell cols="7">0.742 0.744 0.748 0.742 0.807 0.771 0.829 0.819 0.809 0.846 0.847 0.887 0.912 0.927 0.655 0.614 0.665 0.691 0.872 0.841 0.873</cell><cell cols="3">0.826 0.835 0.830 0.925 0.923 0.925 0.887 0.893 0.912</cell><cell cols="2">0.844 0.829 0.911 0.929 0.879 0.906</cell><cell>0.873 0.942 0.926</cell></row><row><cell></cell><cell cols="2">adp E ξ ↑ 0.793 0.749</cell><cell cols="7">0.831 0.830 0.825 0.838 0.886 0.864 0.901</cell><cell cols="3">0.897 0.906 0.903</cell><cell cols="2">0.900 0.902</cell><cell>0.926</cell></row><row><cell></cell><cell cols="2">M ↓ 0.200 0.250</cell><cell cols="7">0.148 0.176 0.143 0.141 0.075 0.086 0.068</cell><cell cols="3">0.064 0.060 0.051</cell><cell cols="2">0.066 0.054</cell><cell>0.043</cell></row><row><cell></cell><cell cols="2">S α ↑ 0.728 0.703 max F β ↑ 0.756 0.788</cell><cell cols="7">0.707 0.741 0.741 0.752 0.770 0.863 0.848 0.666 0.746 0.741 0.766 0.728 0.844 0.822</cell><cell cols="3">0.842 0.858 0.872 0.804 0.827 0.846</cell><cell cols="2">0.900 0.898 0.888 0.880</cell><cell>0.931 0.926</cell></row><row><cell>[7] DES</cell><cell cols="2">mean F β ↑ 0.513 0.576 adp F β ↑ 0.717 0.796 max E ξ ↑ 0.850 0.890 mean E ξ ↑ 0.612 0.649</cell><cell cols="7">0.542 0.523 0.617 0.604 0.713 0.756 0.735 0.702 0.744 0.726 0.753 0.730 0.778 0.762 0.773 0.851 0.856 0.870 0.881 0.932 0.928 0.632 0.621 0.707 0.684 0.810 0.826 0.825</cell><cell cols="3">0.765 0.790 0.824 0.782 0.795 0.829 0.893 0.910 0.923 0.838 0.863 0.889</cell><cell cols="2">0.873 0.851 0.866 0.863 0.943 0.935 0.933 0.902</cell><cell>0.910 0.915 0.971 0.948</cell></row><row><cell></cell><cell cols="2">adp E ξ ↑ 0.855 0.911</cell><cell cols="7">0.849 0.869 0.852 0.877 0.874 0.911 0.904</cell><cell cols="3">0.912 0.919 0.927</cell><cell cols="2">0.944 0.946</cell><cell>0.975</cell></row><row><cell></cell><cell cols="2">M ↓ 0.169 0.208</cell><cell cols="7">0.111 0.122 0.090 0.093 0.068 0.055 0.065</cell><cell cols="3">0.049 0.046 0.038</cell><cell cols="2">0.030 0.033</cell><cell>0.021</cell></row><row><cell>[35]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NLPR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II ABLATION</head><label>II</label><figDesc>ANALYSIS FOR THE PROPOSED ARCHITECTURE ON THE NJU2K AND STERE DATASETS. THE CANDIDATE MECHANISMS ARE DEEP INFORMATION (DEP), FOREGROUND-FIRST ATTENTION (FF), BACKGROUND-FIRST ATTENTION (BF), AND MULTI-SCALE EXTENSION (ME). ME IS APPLIED ON THE TOP THREE LEVEL FEATURES.</figDesc><table><row><cell>#</cell><cell cols="4">Candidates Dep FF BF ME F β ↑ NJU2K [22] S α ↑</cell><cell cols="2">STERE [33] F β ↑ S α ↑</cell></row><row><cell>No. 1</cell><cell></cell><cell></cell><cell cols="4">0.881 0.885 0.882 0.893</cell></row><row><cell>No. 2</cell><cell></cell><cell></cell><cell cols="4">0.903 0.904 0.887 0.894</cell></row><row><cell>No. 3</cell><cell></cell><cell></cell><cell cols="4">0.908 0.908 0.895 0.901</cell></row><row><cell>No. 4</cell><cell></cell><cell></cell><cell cols="4">0.910 0.908 0.892 0.900</cell></row><row><cell>No. 5</cell><cell></cell><cell></cell><cell cols="4">0.915 0.913 0.897 0.903</cell></row><row><cell>No. 6</cell><cell></cell><cell></cell><cell cols="4">0.920 0.915 0.898 0.904</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">IMPROVEMENTS OF ACCURACY BY OUR BAM IN EACH SIDE OUTPUTS</cell></row><row><cell></cell><cell cols="5">COMPARED WITH NO. 2 (WITHOUT BAM &amp; MBA).</cell><cell></cell></row><row><cell cols="7">BAM Level-1 Level-2 Level-3 Level-4 Level-5 No. 2</cell></row><row><cell>S α ↑</cell><cell>0.908</cell><cell>0.909</cell><cell>0.908</cell><cell>0.906</cell><cell>0.904</cell><cell>0.904</cell></row><row><cell>F β ↑</cell><cell>0.910</cell><cell>0.911</cell><cell>0.909</cell><cell>0.905</cell><cell>0.904</cell><cell>0.903</cell></row><row><cell>E ξ ↑</cell><cell>0.944</cell><cell>0.945</cell><cell>0.943</cell><cell>0.943</cell><cell>0.941</cell><cell>0.942</cell></row><row><cell>M ↓</cell><cell>0.043</cell><cell>0.043</cell><cell>0.044</cell><cell>0.044</cell><cell>0.045</cell><cell>0.046</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV IMPROVEMENTS</head><label>IV</label><figDesc>OF ACCURACY BY OUR MBAM IN EACH SIDE OUTPUTS COMPARED WITH NO. 2 (WITHOUT BAM &amp; MBAM).</figDesc><table><row><cell cols="6">MBAM Level-1 Level-2 Level-3 Level-4 Level-5 No. 2</cell></row><row><cell>S α ↑</cell><cell>0.908</cell><cell>0.909</cell><cell>0.910</cell><cell>0.910</cell><cell>0.910 0.904</cell></row><row><cell>F β ↑</cell><cell>0.909</cell><cell>0.912</cell><cell>0.909</cell><cell>0.911</cell><cell>0.911 0.903</cell></row><row><cell>E ξ ↑</cell><cell>0.944</cell><cell>0.945</cell><cell>0.945</cell><cell>0.946</cell><cell>0.947 0.942</cell></row><row><cell>M ↓</cell><cell>0.044</cell><cell>0.043</cell><cell>0.042</cell><cell>0.042</cell><cell>0.042 0.046</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc>ACCURACY AND CALCULATION COST ANALYSIS FOR MBAM. ×0 ∼ ×5 MEANS THE NUMBER OF MBAMS, WHICH ARE APPLIED FROM HIGH LEVELS TO LOW LEVELS. FPS DENOTES FRAMES PER SECOND. PARAMS MEANS THE SIZE OF PARAMETERS. FLOPS = FLOATING POINT OPERATIONS. THE ACCURACY METRICS F β AND M ARE EVALUATED ON THE NJU2K DATASET. THE CALCULATION COST METRICS FPS AND FLOPS ARE TESTED AT 224 × 224 RESOLUTION. NOTE THAT, ×3 IS THE DEFAULT SETTING IN SECTION IV-B.</figDesc><table><row><cell></cell><cell></cell><cell>×0</cell><cell>×1</cell><cell>×2</cell><cell></cell><cell>×3</cell><cell>×4</cell><cell>×5</cell><cell>D3Net [12]</cell><cell>DMRA [37]</cell></row><row><cell></cell><cell>F β ↑</cell><cell>0.914</cell><cell>0.917</cell><cell cols="2">0.918</cell><cell>0.920</cell><cell>0.920</cell><cell>0.921</cell><cell>0.887</cell><cell>0.886</cell></row><row><cell></cell><cell>M ↓</cell><cell>0.041</cell><cell>0.040</cell><cell cols="2">0.040</cell><cell>0.039</cell><cell>0.038</cell><cell>0.039</cell><cell>0.051</cell><cell>0.051</cell></row><row><cell></cell><cell>FPS↑</cell><cell>∼80</cell><cell>∼65</cell><cell>∼55</cell><cell></cell><cell>∼50</cell><cell>∼42</cell><cell>∼34</cell><cell>∼55</cell><cell>∼40</cell></row><row><cell cols="2">Params ↓</cell><cell>45.0M</cell><cell>46.9M</cell><cell cols="2">48.7M</cell><cell>49.6M</cell><cell>50.1M</cell><cell>50.4M</cell><cell>145.9M</cell><cell>59.7M</cell></row><row><cell cols="2">FLOPs ↓</cell><cell>34.4G</cell><cell>35.0G</cell><cell cols="2">36.2G</cell><cell>39.1G</cell><cell>45.2G</cell><cell>58.4G</cell><cell>55.7G</cell><cell>121.0G</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">PERFORMANCES OF OUR BIANET BASED ON DIFFERENT BACKBONES.</cell><cell></cell><cell></cell></row><row><cell cols="7">VGG-11 AND VGG-16 IS THE VGG NETWORK PROPOSED IN [41].</cell><cell></cell><cell></cell></row><row><cell cols="7">RESNET-50 IS PROPOSED IN [18]. RES2NET-50 IS PROPOSED IN [15].</cell><cell></cell><cell></cell></row><row><cell cols="7">Backbone VGG-11 VGG-16 ResNet-50 Res2Net-50</cell><cell></cell><cell></cell></row><row><cell></cell><cell>FPS</cell><cell>60</cell><cell>50</cell><cell>25</cell><cell>23</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[22] NJU2K</cell><cell>S α ↑ F β ↑ E ξ ↑ M ↓</cell><cell>0.912 0.913 0.947 0.040</cell><cell>0.915 0.920 0.948 0.039</cell><cell>0.917 0.920 0.949 0.036</cell><cell>0.923 0.925 0.952 0.034</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[33] STERE</cell><cell>S α ↑ F β ↑ E ξ ↑ M ↓</cell><cell>0.899 0.892 0.941 0.045</cell><cell>0.904 0.898 0.942 0.043</cell><cell>0.905 0.899 0.943 0.040</cell><cell>0.908 0.904 0.942 0.039</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[7] DES</cell><cell>S α ↑ F β ↑ E ξ ↑</cell><cell>0.943 0.938 0.979</cell><cell>0.931 0.926 0.971</cell><cell>0.930 0.927 0.968</cell><cell>0.942 0.942 0.978</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>M ↓</cell><cell>0.019</cell><cell>0.021</cell><cell>0.021</cell><cell>0.017</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[35] NLPR</cell><cell>S α ↑ F β ↑ E ξ ↑ M ↓</cell><cell>0.927 0.914 0.951 0.024</cell><cell>0.925 0.914 0.961 0.024</cell><cell>0.926 0.917 0.962 0.023</cell><cell>0.929 0.919 0.963 0.023</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[58] SSD</cell><cell>S α ↑ F β ↑ E ξ ↑ M ↓</cell><cell>0.861 0.839 0.899 0.054</cell><cell>0.867 0.849 0.916 0.050</cell><cell>0.863 0.843 0.911 0.048</cell><cell>0.863 0.843 0.901 0.050</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[12]</cell><cell>S α ↑ F β ↑</cell><cell>0.877 0.882</cell><cell>0.883 0.890</cell><cell>0.887 0.890</cell><cell>0.889 0.893</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIP</cell><cell>E ξ ↑</cell><cell>0.924</cell><cell>0.925</cell><cell>0.926</cell><cell>0.928</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>M ↓</cell><cell>0.054</cell><cell>0.052</cell><cell>0.047</cell><cell>0.047</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, compared with No. 5, No. 6 carry out multi-scaled extension on its higher three levels {F 3 , F 4 , F 5 }. This extension effectively improves the performance of the model. In order to better show the gain of MBAM in each level features, similar to Table III, we apply MBAM to each side output of the No. 2 model, respectively. The experimental results are recorded in</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved saliency detection in rgb-d images using twophase depth estimation and selective deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglizhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4296" to="4307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjian</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Internet Multimedia Computing and Service</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Going from rgb to rgbd saliency: A depth-guided transformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaochun Cao, and Chunping Hou</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06781</idno>
		<title level="m">Qibin Hou, Menglong Zhu, and Ming-Ming Cheng. Rethinking rgb-d salient object detection: Models, datasets, and large-scale benchmarks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1623" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CNNs-based RGB-D saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selferasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Super diffusion for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified spectral-domain approach for saliency detection and its application to automatic object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1272" to="1283" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Depth matters: influence of depth cues on visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congyan</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="101" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust saliency detection via regularized random walks ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Dagan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2710" to="2717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Icnet: Information conversion network for rgb-d based salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4873" to="4884" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GradNet: Gradient-guided network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stereoscopic saliency model using contrast and depthguided-background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhua</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyun</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="2227" to="2238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep salient object detection with contextual information guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="360" to="374" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency-based discriminant tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1007" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhen</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RGBD salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depthinduced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluation: from Precision, Recall and F-measure to ROC, informedness, markedness and correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqiong</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploiting global priors for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangke</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangling</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image co-saliency detection and co-segmentation via progressive joint optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adaptive fusion for RGB-D salient object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Focal boundary guided salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2813" to="2824" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">What is and what is not a salient object? learning salient object detector by ensembling linear exemplar regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anlin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rgb-dsaliency detection with pseudo depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Jiao</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2126" to="2139" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A multistage refinement network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohua</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3534" to="3545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Salient object detection with lossless feature reflection and weighted structural loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3048" to="3060" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Study of saliency in objective video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1275" to="1288" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for RGBD salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangqian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3085" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">PDNet: Prior-model guided depth-enhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Three-pathway Psychobiological Framework of Salient Object Detection Using Stereoscopic Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3008" to="3014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
