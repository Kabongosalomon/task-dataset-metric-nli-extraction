<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A probabilistic constrained clustering for transfer learning and image category discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-06-28">28 Jun 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Schlosser</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tech Research Institute</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Zsolt</roleName><forename type="first">Phillip</forename><surname>Odom</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tech Research Institute</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Georgia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tech Research Institute</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A probabilistic constrained clustering for transfer learning and image category discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-06-28">28 Jun 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network-based clustering has recently gained popularity, and in particular a constrained clustering formulation has been proposed to perform transfer learning and image category discovery using deep learning. The core idea is to formulate a clustering objective with pairwise constraints that can be used to train a deep clustering network; therefore the cluster assignments and their underlying feature representations are jointly optimized end-toend. In this work, we provide a novel clustering formulation to address scalability issues of previous work in terms of optimizing deeper networks and larger amounts of categories. The proposed objective directly minimizes the negative log-likelihood of cluster assignment with respect to the pairwise constraints, has no hyper-parameters, and demonstrates improved scalability and performance on both supervised learning and unsupervised transfer learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised clustering algorithms have found a number of applications ranging from data analysis, visualization, and importantly transfer learning and unsupervised image category discovery <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10]</ref>. Traditional methods that group data with a predefined metric, however, are very sensitive to the resulting choices or do not optimize the feature space jointly with clustering. If there is a complex nonlinear mapping between the input data and the desired semantic meaning of the clusters, the resulting clustering often cannot capture such meaning. In contrast, constrained clustering methods explicitly define the desired pairwise relationships as side information to the clustering algorithm. When paired with deep learning, the algorithm can learn the metric or feature representation to fit the desired relationships as well as optimize the assignments themselves.</p><p>Within this class of methods, the constraints often represent must-link/cannot-link or similar/dissimilar pairs (we use the latter in this paper). If a pair of data should be assigned to the same cluster, then it is a similar pair; otherwise it is dissimilar. The constraints could be regarded as an in-terface to inject prior knowledge for clustering. Such information can be obtained via human labeling (supervised or semi-supervised setting), spatial or temporal relationships e.g. in videos (unsupervised setting) <ref type="bibr" target="#b14">[15]</ref>, or through a similarity metric learned on a different domain (transfer learning) <ref type="bibr" target="#b10">[11]</ref>. Hence the constrained clustering formulation is a powerful concept that can bridge (semi-)supervised learning, unsupervised learning, and transfer learning.</p><p>A recent work <ref type="bibr" target="#b10">[11]</ref> proposed a neural network-based constrained clustering objective in the form of a contrastive loss with KL-divergence. It demonstrated the transfer of learned pairwise similarity across tasks by clustering. Such a strategy can perform novel vision tasks, e.g. image category discovery with unlabeled examples of unseen categories. However, there are two critical challenges for this method: there is a degradation in the clustering performance when the chosen backbone network exceeds certain depth or when the number of clusters in the objective is larger (than a size of 10). These challenges prevent it from being applied to many real-world problems, which requires scalable solutions. Therefore, in this work we propose a new objective in the form of clustering likelihood to mitigate the mentioned issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Constrained Clustering Likelihood</head><p>This section considers the constrained clustering problem. Suppose X = {x 1 , .., x n } denotes a dataset of n instances, where each instance belongs to a set of unknown clusters c ∈ {1, 2, .., k}. The set of constraints S = S + ∪ S − contains two types of pairwise relationships, where S + is the set of tuples (i, j) where (x i , x j ) is a similar pair, and S − contains the same for dissimilar pairs. Similar to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>, we define a neural network f θ (x i ) = [p i,1 , p i,2 , .., p i,k ] T that maps each sample x i to a categorical distribution, i.e. k c=1 p i,c = 1, which is the probability of data x i belonging to cluster c. This is done by reinterpreting the softmax output of a neural network as outputting a probability distribution over cluster assignments, and a specialized loss function that utilizes the constraints is used to update θ, the set of parameters in the neural network.</p><p>We introduce y i to represent x i 's assigned cluster with probability P (y i = c|x i ) = p i,c . For a pair of data (x i , x j ), the probability of assigning both to the same cluster c is:</p><formula xml:id="formula_0">P (y i = c, y j = c|x i , x j ) = P (y i = c|x i )P (y j = c|x j ).</formula><p>In the above formulation we impose an assumption that y i and y j are independent given their source data x i and x j . For the simplicity of the representation, we omit the notation of conditioning since all of the following probabilities are conditioned on X.</p><p>Now we describe the probability of (x i , x j ) being a similar pair by marginalizing their joint distribution along the cluster index c:</p><formula xml:id="formula_1">P (y i = y j ) = k c=1 P (y i = c, y j = c) = f θ (x i ) T f θ (x j ).</formula><p>(1) Then the probability of (x i , x j ) being a dissimilar pair becomes straightforward:</p><formula xml:id="formula_2">P (y i = y j ) = 1 − P (y i = y j ).<label>(2)</label></formula><p>The constrained clustering likelihood L now can be defined as the product of all the probabilities of similar and dissimilar pairs:</p><formula xml:id="formula_3">L(θ|S + , S − ) = (i,j)∈S + P (y i = y j ) (i,j)∈S − P (y i = y j ).</formula><p>(3) For finding θ, we minimize the negative log-likelihood of equation <ref type="formula">(3)</ref>, which is referred to as CCL and optimize this objective using stochastic gradient descent. For collecting the final cluster assignment, we only feed the data forward into f θ again after the likelihood converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We evaluate the proposed CCL and compare it with KLdivergence based constrained clustering loss (KCL) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> in two settings using image datasets.</p><p>In the supervised setting, we construct S from groundtruth category labels. The motivation is to evaluate the upper-bound performance and explore scalability regarding the number of clusters and depth of the neural network, ablating the effect of noise. Tables 1 and 2 demonstrate the superiority of CCL over KCL in terms of scalability.</p><p>In the unsupervised transfer learning (i.e. image category discovery) setting, we follow the procedure for transferring across tasks <ref type="bibr" target="#b10">[11]</ref>, which uses a learned similarity function as the constraint provider. The predicted constraints are noisy and therefore demonstrate the performance of our algorithm in a real-world setting. The scenario of unknown number of clusters is also evaluated by    <ref type="table" target="#tab_2">Table 3</ref> shows the results of discovering 30 held-out categories in ImageNet <ref type="bibr" target="#b6">[7]</ref>. In the case of unknown number of clusters, CCL (71.5%) demonstrates a significant advantage over KCL (65.2%). A similar trend also happens in the experiments of discovering characters in the Omniglot dataset <ref type="table">(supplementary table 4</ref>) which includes comprehensive comparisons. Therefore we empirically conclude that CCL is a better clustering objective for these types of category discovery tasks.</p><p>Network Architecture: We use convolution neural networks with a varied number of layers: LeNet <ref type="bibr" target="#b11">[12]</ref>; VGG <ref type="bibr" target="#b16">[17]</ref> and the PreActResNet <ref type="bibr" target="#b7">[8]</ref>. We also add a VGG8, which only has one convolution layer before each pooling layer, as the supplement between LeNet and VGG11. The number of output nodes in the last fully connected layer is the maximum number of clusters. We set it to the true number of categories for this section. Since the clustering objectives KCL and CCL both work on pairs of inputs, we insert a pairwise enumeration layer <ref type="bibr" target="#b10">[11]</ref> between the network outputs and the loss function. Therefore the dense pairs in a mini-batch are all subject to the clustering loss.</p><p>Training configurations: All networks in this section are trained from scratch with randomly initialized weights. On the MNIST dataset, we use mini-batch size 100 with initial learning rate 0.1, which was dropped every 10 epochs by a factor of 0.1. We trained 30 epochs in total. On CI-FAR10 we use the same setting except that the learning rate is dropped every 30 epochs and it is trained for 70 epochs in total. We use SGD for the optimization on MNIST and CI-FAR10. For CIFAR100, the mini-batch size was 1000 and Adam was used as the optimizer with initial learning rate 0.001, which dropped every 70 epochs by a factor of 0.1. The training lasted 160 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Setup for image category discovery</head><p>We follow the unsupervised transfer learning procedures described in <ref type="bibr" target="#b10">[11]</ref> to evaluate the clustering performance with noisy constraints. We use the original implementation and replace KCL by our CCL. We use the same training configurations, including the same similarity prediction function to make sure performance gain only by adopting CCL. We follow <ref type="bibr" target="#b10">[11]</ref> to use CCN for Constrained Clustering Network, and use CCN-KCL and CCN-CCL to differentiate the two approaches in the tables. Noted that the clustering accuracy is directly calculated on the target dataset <ref type="table">Table 4</ref>: Unsupervised cross-task transfer from Omniglot bg to Omniglot eval for discovering the characters in Omniglot eval . The performance is averaged across 20 alphabets which have 20 to 47 letters. The ACC and NMI without brackets have the number of clusters equal to ground-truth. The "(100)" means the algorithms use k = 100, i.e. one hundred outputs from the network. The characteristics of how each algorithm utilizes the pairwise constraints are marked in the "Constraints in" column, where metric stands for the metric learning of feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Constraints in ACC ACC (100) NMI NMI (100) Metric Clustering K-means <ref type="bibr" target="#b12">[13]</ref> 21.7% 18.9% 0.353 0.464 LPNMF <ref type="bibr" target="#b3">[4]</ref> 22.2% 16.3% 0.372 0.498 LSC <ref type="bibr" target="#b4">[5]</ref> 23 instead of a holdout set since it is an unsupervised clustering setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The accuracy on CIFAR10 with different loss functions and different neural network architectures. All training configurations, except the loss functions, are the same. The performance for Cross Entropy (CE) is the classification accuracy on test set and is regarded as the upper bound performance of supervised learning. For KCL and CCL, we give the clustering accuracy<ref type="bibr" target="#b18">[19]</ref> on the test set.</figDesc><table><row><cell>Networks</cell><cell cols="2">CE KCL CCL (ours)</cell></row><row><cell>LeNet</cell><cell>84.4 83.6</cell><cell>83.9</cell></row><row><cell>VGG8</cell><cell>89.4 89.6</cell><cell>89.5</cell></row><row><cell>VGG11</cell><cell>90.0 85.6</cell><cell>90.4</cell></row><row><cell>VGG16</cell><cell>91.1 21.9</cell><cell>91.3</cell></row><row><cell>PReActResNet-18</cell><cell>92.6 81.7</cell><cell>92.7</cell></row><row><cell cols="2">PReActResNet-101 93.0 21.9</cell><cell>93.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The accuracy of supervised clustering with different number of clusters in the image datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#class Network CE KCL CCL</cell></row><row><cell>MNIST</cell><cell>10</cell><cell>LeNet</cell><cell>99.4 99.5 99.3</cell></row><row><cell>CIFAR10</cell><cell>10</cell><cell>VGG8</cell><cell>89.4 89.6 89.5</cell></row><row><cell>CIFAR100</cell><cell>100</cell><cell>VGG8</cell><cell>64.1 44.3 64.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Image category discovery on ImageNet. The values are the average of three random subsets in ImageNet 118 .</figDesc><table><row><cell cols="5">Each subset has 30 classes. The "ACC" (clustering accu-</cell></row><row><cell cols="5">racy) has K = 30 while the "ACC (100)" sets K = 100.</cell></row><row><cell cols="5">All methods use the features (outputs of average pooling)</cell></row><row><cell cols="5">from Resnet-18 pre-trained with ImageNet 882 classification.</cell></row><row><cell cols="4">NMI stands for normalized mutual information.</cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell cols="3">ACC(100) NMI NMI(100)</cell></row><row><cell>K-means</cell><cell>71.9%</cell><cell>34.5%</cell><cell>0.713</cell><cell>0.671</cell></row><row><cell>LSC</cell><cell>73.3%</cell><cell>33.5%</cell><cell>0.733</cell><cell>0.655</cell></row><row><cell>LPNMF</cell><cell>43.0%</cell><cell>21.8%</cell><cell>0.526</cell><cell>0.500</cell></row><row><cell cols="2">CCN-KCL 73.8%</cell><cell>65.2%</cell><cell>0.750</cell><cell>0.715</cell></row><row><cell cols="2">CCN-CCL 74.4%</cell><cell>71.5%</cell><cell>0.762</cell><cell>0.765</cell></row><row><cell cols="5">setting a large k (e.g. 100) in the network output.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Science Foundation and National Robotics Initiative (grant # IIS-1426998).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head><p>A. Setup for supervised clustering</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi-supervised kernel metric learning using relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ukkonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised kernel mean shift clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1201" to="1215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrating constraints and metric learning in semi-supervised clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Locality preserving nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1010" to="1015" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering with landmark-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural network-based clustering using pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ICLR workshop</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep image category discovery using a transferred similarity function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01253</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to cluster in order to transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large-scale object discovery and detector adaptation from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08832</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large-scale object discovery and detector adaptation from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1712.08832</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unseen class discovery in openworld classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05609</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On constrained spectral clustering and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image clustering using local discriminant models and global integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2761" to="2773" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
