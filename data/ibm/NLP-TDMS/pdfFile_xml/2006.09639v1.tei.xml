<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iterative Edit-Based Unsupervised Sentence Simplification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Alberta Alberta Machine Intelligence Institute (Amii)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Golab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
							<email>ovechtomova@uwaterloo.cadoublepower.mou@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Iterative Edit-Based Unsupervised Sentence Simplification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence simplification is the task of rewriting text to make it easier to read, while preserving its main meaning and important information. Sentence simplification is relevant in various real-world and downstream applications. For instance, it can benefit people with autism <ref type="bibr" target="#b6">(Evans et al., 2014)</ref>, dyslexia <ref type="bibr" target="#b28">(Rello et al., 2013)</ref>, and low-literacy skills <ref type="bibr" target="#b35">(Watanabe et al., 2009)</ref>. It can also serve as a preprocessing step to improve parsers <ref type="bibr" target="#b3">(Chandrasekar et al., 1996)</ref> and summarization systems <ref type="bibr" target="#b11">(Klebanov et al., 2004)</ref>.</p><p>Recent efforts in sentence simplification have been influenced by the success of machine translation. In fact, the simplification task is often treated as monolingual translation, where a complex sentence is translated to a simple one. Such simplification systems are typically trained in a supervised way by either phrase-based machine translation <ref type="bibr">(PBMT, Wubben et al., 2012;</ref><ref type="bibr" target="#b21">Narayan and Gardent, 2014;</ref><ref type="bibr" target="#b38">Xu et al., 2016)</ref> or neural machine translation (NMT, <ref type="bibr" target="#b39">Zhang and Lapata, 2017;</ref><ref type="bibr" target="#b7">Guo et al., 2018;</ref><ref type="bibr" target="#b12">Kriz et al., 2019)</ref>. Recently, sequence-to-sequence (Seq2Seq)-based NMT systems are shown to be more successful and serve as the state of the art.</p><p>However, supervised Seq2Seq models have two shortcomings. First, they give little insight into the simplification operations, and provide little control or adaptability to different aspects of simplification (e.g., lexical vs. syntactical simplification). Second, they require a large number of complexsimple aligned sentence pairs, which in turn require considerable human effort to obtain.</p><p>In previous work, researchers have addressed some of the above issues. For example, <ref type="bibr" target="#b0">Alva-Manchego et al. (2017)</ref> and <ref type="bibr" target="#b5">Dong et al. (2019)</ref> explicitly model simplification operators such as word insertion and deletion. Although these approaches are more controllable and interpretable than standard Seq2Seq models, they still require large volumes of aligned data to learn these operations. To deal with the second issue, <ref type="bibr" target="#b33">Surya et al. (2019)</ref> recently proposed an unsupervised neural text simplification approach based on the paradigm of style transfer. However, their model is hard to interpret and control, like other neural network-based models. <ref type="bibr" target="#b22">Narayan and Gardent (2016)</ref> attempted to address both issues using a pipeline of lexical substitution, sentence splitting, and word/phrase deletion. However, these operations can only be executed in a fixed order.</p><p>In this paper, we propose an iterative, editbased unsupervised sentence simplification approach, motivated by the shortcomings of existing work. We first design a scoring function that measures the quality of a candidate sentence based on the key characteristics of the simplification task, namely, fluency, simplicity, and meaning preservation. Then, we generate simplified candidate sentences by iteratively editing the given complex sentence using three simplification operations (lexical simplification, phrase extraction, deletion and reordering). Our model seeks the best simplified <ref type="figure">Figure 1</ref>: An example of three edit operations on a given sentence. Note that dropping clauses or phrases is common in text simplification datasets.</p><p>candidate sentence according to the scoring function. Compared with <ref type="bibr" target="#b22">Narayan and Gardent (2016)</ref>, the order of our simplification operations is not fixed and is decided by the model. <ref type="figure">Figure 1</ref> illustrates an example in which our model first chooses to delete a sentence fragment, followed by reordering the remaining fragments and replacing a word with a simpler synonym.</p><p>We evaluate our approach on the Newsela <ref type="bibr" target="#b37">(Xu et al., 2015)</ref> and WikiLarge <ref type="bibr" target="#b39">(Zhang and Lapata, 2017)</ref> corpora. Experiments show that our approach outperforms previous unsupervised methods and even performs competitively with state-ofthe-art supervised ones, in both automatic metrics and human evaluations. We also demonstrate the interpretability and controllability of our approach, even without parallel training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early work used handcrafted rules for text simplification, at both the syntactic level <ref type="bibr" target="#b31">(Siddharthan, 2002)</ref> and the lexicon level <ref type="bibr" target="#b2">(Carroll et al., 1999)</ref>. Later, researchers adopted machine learning methods for text simplification, modeling it as monolingual phrase-based machine translation <ref type="bibr" target="#b36">(Wubben et al., 2012;</ref><ref type="bibr" target="#b38">Xu et al., 2016)</ref>. Further, syntactic information was also considered in the PBMT framework, for example, constituency trees <ref type="bibr" target="#b42">(Zhu et al., 2010)</ref> and dependency trees <ref type="bibr" target="#b1">(Bingel and SÃ¸gaard, 2016)</ref>. <ref type="bibr" target="#b21">Narayan and Gardent (2014)</ref> performed probabilistic sentence splitting and deletion, followed by MT-based paraphrasing. <ref type="bibr" target="#b23">Nisioi et al. (2017)</ref> employed neural machine translation (NMT) for text simplification, using a sequence-to-sequence (Seq2Seq) model <ref type="bibr" target="#b34">(Sutskever et al., 2014)</ref>. <ref type="bibr" target="#b39">Zhang and Lapata (2017)</ref> used reinforcement learning to optimize a reward based on simplicity, fluency, and relevance. <ref type="bibr" target="#b40">Zhao et al. (2018a)</ref> integrated the transformer architecture and paraphrasing rules to guide simplification learning. <ref type="bibr" target="#b12">Kriz et al. (2019)</ref> produced diverse simplifications by generating and re-ranking candidates by fluency, adequacy, and simplicity. <ref type="bibr" target="#b7">Guo et al. (2018)</ref> showed that simplification benefits from multi-task learning with paraphrase and entailment generation. <ref type="bibr" target="#b17">Martin et al. (2019)</ref> enhanced the transformer architecture with conditioning parameters such as length, lexical and syntactic complexity.</p><p>Recently, edit-based techniques have been developed for text simplification. <ref type="bibr" target="#b0">Alva-Manchego et al. (2017)</ref> trained a model to predict three simplification operators (keep, replace, and delete) from aligned pairs. <ref type="bibr" target="#b5">Dong et al. (2019)</ref> employed a similar approach but in an end-to-end trainable manner with neural networks. However, these approaches are supervised and require large volumes of parallel training data; also, their edits are only at the word level. By contrast, our method works at both word and phrase levels in an unsupervised manner.</p><p>For unsupervised sentence simplification, <ref type="bibr" target="#b33">Surya et al. (2019)</ref> adopted style-transfer techniques, using adversarial and denoising auxiliary losses for content reduction and lexical simplification. However, their model is based on a Seq2Seq network, which is less interpretable and controllable. They cannot perform syntactic simplification since syntax typically does not change in style-transfer tasks. <ref type="bibr" target="#b22">Narayan and Gardent (2016)</ref> built a pipeline-based unsupervised framework with lexical simplification, sentence splitting, and phrase deletion. However, these operations are separate components in the pipeline, and can only be executed in a fixed order.</p><p>Unsupervised edit-based approaches have recently been explored for natural language generation tasks, such as style transfer, paraphrasing, and sentence error correction. <ref type="bibr" target="#b14">Li et al. (2018)</ref> proposed edit-based style transfer without parallel supervision. They replaced style-specific phrases with those in the target style, which are retrieved from the training corpus. <ref type="bibr" target="#b18">Miao et al. (2019)</ref> used Metropolis-Hastings sampling for constrained sentence generation. In this paper, we model text generation as a search algorithm, and design search objective and search actions specifically for text simplification. Concurrent work further shows the success of search-based unsupervised text generation for paraphrasing <ref type="bibr" target="#b15">(Liu et al., 2020)</ref> and summa-rization <ref type="bibr">(Schumann et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we first provide an overview of our approach, followed by a detailed description of each component, namely, the scoring function, the edit operations, and the stopping criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We first define a scoring function as our search objective. It allows us to impose both hard and soft constraints, balancing the fluency, simplicity, and adequacy of candidate simplified sentences (Section 3.2).</p><p>Our approach iteratively generates multiple candidate sentences by performing a sequence of lexical and syntactic operations. It starts from the input sentence; in each iteration, it performs phrase and word edits to generate simplified candidate sentences (Section 3.3).</p><p>Then, a candidate sentence is selected according to certain criteria. This process is repeated until none of the candidates improve the score of the source sentence by a threshold value. The last candidate is returned as the simplified sentence (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scoring Function</head><p>Our scoring function is the product of several individual scores that evaluate various aspects of a candidate simplified sentence. This is also known as the product-of-experts model <ref type="bibr" target="#b8">(Hinton, 2002)</ref>.</p><p>SLOR score from a syntax-aware language model (f eslor ). This measures the language fluency and structural simplicity of a candidate sentence. A probabilistic language model (LM) is often used as an estimate of sentence fluency <ref type="bibr" target="#b18">(Miao et al., 2019)</ref>. In our work, we make two important modifications to a plain LM.</p><p>First, we replace an LM's estimated sentence probability with the syntactic log-odds ratio (SLOR, <ref type="bibr" target="#b25">Pauls and Klein, 2012)</ref>, to better measure fluency and human acceptability. According to <ref type="bibr" target="#b13">Lau et al. (2017)</ref>, SLOR shows the best correlation to human acceptability of a sentence, among many sentence probability-based scoring functions. SLOR was also shown to be effective in unsupervised text compression <ref type="bibr" target="#b9">(Kann et al., 2018)</ref>.</p><p>Given a trained language model (LM) and a sentence s, SLOR is defined as</p><formula xml:id="formula_0">SLOR(s) = 1 |s| (ln(P LM (s)) â ln(P U (s))) (1)</formula><p>where P LM is the sentence probability given by the language model, P U (s) = wâs P (w) is the product of the unigram probability of a word w in the sentence, and |s| is the sentence length. SLOR essentially penalizes a plain LM's probability by unigram likelihood and the length. It ensures that the fluency score of a sentence is not penalized by the presence of rare words. Consider two sentences, "I went to England for vacation" and "I went to Senegal for vacation." Even though both sentences are equally fluent, a standard LM will give a higher score to the former, since the word "England" is more likely to occur than "Senegal." In simplification, SLOR is preferred for preserving rare words such as named entities. <ref type="bibr">2</ref> Second, we use a syntax-aware LM, i.e., in addition to words, we use part-of-speech (POS) and dependency tags as inputs to the LM <ref type="bibr" target="#b41">(Zhao et al., 2018b)</ref>. For a word w i , the input to the syntax-</p><formula xml:id="formula_1">aware LM is [e(w i ); p(w i ); d(w i )], where e(w i ) is the word embedding, p(w i ) is the POS tag embed- ding, and d(w i ) is the dependency tag embedding.</formula><p>Note that our LM is trained on simple sentences. Thus, the syntax-aware LM prefers a syntactically simple sentence. It also helps to identify sentences that are structurally ungrammatical.</p><p>Cosine Similarity (f cos ). Cosine similarity is an important measure of meaning preservation. We compute the cosine value between sentence embeddings of the original complex sentence (c) and the generated candidate sentence (s), where our sentence embeddings are calculated as the idf weighted average of individual word embeddings. Our sentence similarity measure acts as a hard filter, i.e., f cos (s) = 1 if cos(c, s) &gt; Ï , or f cos (s) = 0 otherwise, for some threshold Ï .</p><p>Entity Score (f entity ). Entities help identify the key information of a sentence and therefore are also useful in measuring meaning preservation. Thus, we count the number of entities in the sentence as part of the scoring function, where entities are detected by a third-party tagger.</p><p>Length (f len ). This score is proportional to the inverse of the sentence length. It forces the model to generate shorter and simpler sentences. However, we reject sentences shorter than a specified length (â¤6 tokens) to prevent over-shortening. FRE (f fre ). The Flesch Reading Ease (FRE) score <ref type="bibr" target="#b10">(Kincaid et al., 1975)</ref> measures the ease of readability in text. It is based on text features such as the average sentence length and the average number of syllables per word. A higher scores indicate that the text is simpler to read.</p><p>We compute the overall scoring function as the product of individual scores.</p><formula xml:id="formula_2">f (s) = f eslor (s) Î± Â· f fre (s) Î² Â· (1/f len (s)) Î³ Â·f entity (s) Î´ Â· f cos (s)<label>(2)</label></formula><p>where the weights Î±, Î², Î³, and Î´ balance the relative importance of the different scores. Recall that the cosine similarity measure does not require a weight since it is a hard indicator function. In Section 4.5, we will experimentally show that the weights defined for different scores affect different characteristics of simplification and thus provide more adaptability and controllability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating Candidate Sentences</head><p>We generate candidate sentences by editing words and phrases. We use a third-party parser to obtain the constituency tree of a source sentence. Each clause-and phrase-level constituent (e.g., S, VP, and NP) is considered as a phrase. Since a constituent can occur at any depth in the parse tree, we can deal with both long and short phrases at different granularities. In <ref type="figure" target="#fig_0">Figure 2</ref>, for example, both "good" (ADJP) and "tasted good" (VP) are constituents and thus considered as phrases, whereas "tasted" is considered as a single word. For each phrase, we generate a candidate sentence using the edit operations explained below, with <ref type="figure">Figure 1</ref> being a running example.</p><p>Removal. For each phrase detected by the parser, this operation generates a new candidate sentence by removing that phrase from the source sentence. In <ref type="figure">Figure 1</ref>, our algorithm can drop the phrase "according to a Seattle based reporter," which is not the main clause of the sentence. The removal operation allows us to remove peripheral information in a sentence for content reduction.</p><p>Extraction. This operation simply extracts a selected phrase (including a clause) as the candidate sentence. This allows us to select the main clause in a sentence and remove remaining peripheral information.</p><p>Reordering. For each phrase in a sentence, we generate candidate sentences by moving the phrase before or after another phrase (identified by clauseand phrase-level constituent tags). In the running example, the phrase "In 2016 alone" is moved between the phrases "12 billion dollars" and "on constructing theme parks." As seen, the reordering operation is able to perform syntactic simplification.</p><p>Substitution. In each phrase, we identify the most complex word as the rarest one according to the idf score. For the selected complex word, we generate possible substitutes using a two-step strategy.</p><p>First, we obtain candidate synonyms by taking the union of the WordNet synonym set <ref type="bibr" target="#b20">(Miller, 1995)</ref> and the closest words from GloVe <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref> and Word2Vec <ref type="bibr" target="#b19">(Mikolov et al., 2013</ref>) embeddings (where embedding closeness is measured by Euclidean distance). Second, a candidate synonym is determined to be an appropriate simple substitute if it satisfies the following conditions: a) it has a lower idf score than the complex word, where the scores are computed from the target simple sentences, b) it is not a morphological inflection of the complex word, c) its word embedding exceeds a cosine similarity threshold to the complex word, and, d) it is has the same partof-speech and dependency tags in the sentence as the complex word. We then generate candidate sentences by replacing the complex word with all qualified lexical substitutes. Notably, we do not replace entity words identified by entity taggers.</p><p>In our example sentence, consider the phrase "constructing theme parks." The word "constructing" is chosen as the word to be simplified, and is replaced with "building." As seen, this operation performs lexical simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Iterative Algorithm</head><p>Given an input complex sentence, our algorithm iteratively performs edits to search for a higher-scoring candidate.</p><p>In each iteration, we consider all the operations (i.e., removal, extraction, reordering, and substitution). Each operation may generate multiple candidates (e.g., multiple words for substitution); we filter out a candidate sentence if the improvement does not pass an operation-specific threshold. We choose the highest-scoring sentence from those that are not filtered out. Our algorithm terminates if no edit passes the threshold, and the final candidate is our generated simplified sentence.</p><p>Our algorithm includes a filtering step for each operation. We only keep a candidate sentence if it is better than the previous one by a multiplicative factor, i.e.,</p><formula xml:id="formula_3">f (c)/f (s) &gt; r op<label>(3)</label></formula><p>where s is the sentence given by the previous iteration, and c is a candidate generated by operator op from s. Notably, we allow different thresholds for each operation. This provides control over different aspects of simplification, namely, lexicon simplification, syntactic simplification, and content reduction. A lower threshold for substitution, for example, encourages the model to perform more lexical simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We use the Newsela <ref type="bibr" target="#b37">(Xu et al., 2015)</ref> and the Wiki-Large datasets <ref type="bibr" target="#b39">(Zhang and Lapata, 2017)</ref> for evaluating our model.</p><p>Newsela is a collection of 1,840 news articles written by professional editors at 5 reading levels for children. We use the standard split and exclude simple-complex sentence pairs that are one reading level apart, following <ref type="bibr" target="#b39">Zhang and Lapata (2017)</ref>. This gives 95,208 training, 1,129 validation, and 1,077 test sentences.</p><p>The WikiLarge dataset is currently the largest text simplification corpus. It contains 296,402, 2,000, and 359 complex-simple sentence pairs for training, validation, and testing, respectively. The training set of WikiLarge consists of automatically aligned sentence pairs from the normal and simple Wikipedia versions. The validation and test sets contain multiple human-written references, against which we evaluate our algorithm.</p><p>For each corpus, we only use its training set to learn a language model of simplified sentences. For the WikiLarge dataset, we also train a Word2Vec embedding model from scratch on its source and target training sentences. These embeddings are used to obtain candidate synonyms in the substitution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>For the LM, we use a two-layer, 256-dimensional recurrent neural network (RNN) with the gated recurrent unit (GRU, <ref type="bibr" target="#b4">Chung et al., 2014)</ref>. We initialize word embeddings using 300-dimensional GloVe <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref>; out-of-vocabulary words are treated as UNK, initialized uniformly in the range of Â±0.05. Embeddings for POS tags and dependency tags are 150-dimensional, also initialized randomly. We fine-tune all embeddings during training.</p><p>We use the Averaged Stochastic Gradient Descent (ASGD) algorithm <ref type="bibr" target="#b27">(Polyak and Juditsky, 1992)</ref> to train the LM, with 0.4 as the dropout and 32 as the batch size. For the Newsela dataset, the thresholds r op in the scoring function are set to 1.25 for all the edit operations. All the weights in our scoring function (Î±, Î², Î³, Î´) are set to 1. For the WikiLarge dataset, the thresholds are set as 1.25 for the removal and reordering operations, 0.8 for substitution, and 5.0 for extraction. The weights in the scoring function (Î±, Î², Î³, Î´) are set to 0.5, 1.0, 0.25 and 1.0, respectively.</p><p>We use CoreNLP  to construct the constituency tree and Spacy 3 to generate part-of-speech and dependency tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Competing Methods</head><p>We first consider the reference to obtain an upperbound for a given evaluation metric. We also consider the complex sentence itself as a trivial baseline, denoted by Complex.</p><p>Next, we develop a simple heuristic that removes rare words occurring â¤ 250 times in the simple sentences of the training corpus, denoted by Reduce-250. As discussed in Section 4.4, this simple heuristic demonstrates the importance of balancing different automatic evaluation metrics.</p><p>For unsupervised competing methods, we compare with <ref type="bibr" target="#b33">Surya et al. (2019)</ref>, which is inspired by unsupervised neural machine translation. They proposed two variants, UNMT and UNTS, but their results are only available for WikiLarge. We also compare our model with supervised methods. First, we consider non-neural phrasebased machine translation (PBMT) methods: <ref type="bibr">PBMT-R (Wubben et al., 2012)</ref>, which re-ranks sentences generated by PBMT for diverse simplifications; SBMT-SARI <ref type="bibr" target="#b38">(Xu et al., 2016)</ref>, which uses an external paraphrasing database; and Hybrid <ref type="bibr" target="#b21">(Narayan and Gardent, 2014)</ref>, which uses a combination of PBMT and discourse representation structures. Next, we compare our method with neural machine translation (NMT) systems: EncDecA, which is a vanilla Seq2Seq model with attention <ref type="bibr" target="#b23">(Nisioi et al., 2017)</ref>; Dress and Dress-Ls, which are based on deep reinforcement learning <ref type="bibr" target="#b39">(Zhang and Lapata, 2017)</ref>; DMass <ref type="bibr" target="#b40">(Zhao et al., 2018a)</ref>, which is a transformer-based model with external simplification rules; EncDecP, which is an encoder-decoder model with a pointermechanism; EntPar, which is based on multi-task learning <ref type="bibr" target="#b7">(Guo et al., 2018)</ref>; S2S-All-FA, which a reranking based model focussing on lexical simplification <ref type="bibr" target="#b12">(Kriz et al., 2019)</ref>; and Access, which is based on the transformer architecture <ref type="bibr" target="#b17">(Martin et al., 2019)</ref>. Finally, we compare with a supervised edit-based neural model, Edit-NTS <ref type="bibr" target="#b5">(Dong et al., 2019)</ref>. We evaluate our model with a different subset of operations, i.e., removal (RM), extraction (EX), reordering (RO), and lexical substitution (LS). In our experiments, we test the following variants: RM+EX, RM+EX+LS, RM+EX+RO, and RM+EX+LS+RO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Automatic Evaluation</head><p>Tables 1 and 2 present the results of the automatic evaluation on the Newsela and WikiLarge datasets, respectively.</p><p>We use the SARI metric <ref type="bibr" target="#b38">(Xu et al., 2016)</ref> to measure the simplicity of the generated sentences. SARI computes the arithmetic mean of the n-gram F1 scores of three rewrite operations: adding, deleting, and keeping. The individual F1-scores of these operations are reported in the columns "Add," "Delete," and "Keep."</p><p>We also compute the BLEU score <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref> to measure the closeness between a candidate and a reference. <ref type="bibr" target="#b38">Xu et al. (2016)</ref> and <ref type="bibr" target="#b32">Sulem et al. (2018)</ref> show that BLEU correlates with human judgement on fluency and meaning preservation for text simplification. 4  In addition, we include a few intrinsic measures (without reference) to evaluate the quality of a candidate sentence: the Flesch-Kincaid grade level (FKGL) evaluating the ease of reading, as well as the average length of the sentence.</p><p>A few recent text simplification studies <ref type="bibr" target="#b5">(Dong et al., 2019;</ref><ref type="bibr" target="#b12">Kriz et al., 2019)</ref> did not use BLEU for evaluation, noticing that the complex sentence itself achieves a high BLEU score (albeit a low SARI score), since the complex sentence is indeed fluent and preserves meaning. This is also shown by our Complex baseline.</p><p>For the Newsela dataset, however, we notice that the major contribution to the SARI score is from the deletion operation. By analyzing previous work such as EntPar, we find that it reduces the sentence length to a large extent, and achieves high SARI due to the extremely high F1 score of "Delete." However, its BLEU score is low, showing the lack of fluency and meaning. This is also seen from the high SARI of (Reduce-250) in <ref type="table">Table 1</ref>. Ideally, we want both high SARI and high BLEU, and thus, we calculate the geometric mean (GM) of them as the main evaluation metric for the Newsela dataset.</p><p>On the other hand, this is not the case for Wiki-Large, since none of the models can achieve high SARI by using only one operation among "Add," "Delete," and "Keep." Moreover, the complex sentence itself yields an almost perfect BLEU score (partially due to the multi-reference nature of Wik-iLarge). Thus, we do not use GM, and for this dataset, SARI is our main evaluation metric.</p><p>Overall results on Newsela. <ref type="table">Table 1</ref> shows the results on Newsela. By default (without â ), validation is performed using the GM score. Still, our unsupervised text simplification achieves a SARI score around 26-27, outperforming quite a few supervised methods. Further, we experiment with SARI-based validation (denoted by â ), following the setting of most previous work <ref type="bibr" target="#b5">(Dong et al., 2019;</ref><ref type="bibr" target="#b7">Guo et al., 2018)</ref>. We achieve 30.44 SARI, which is competitive with state-of-the-art supervised methods.</p><p>Our model also achieves high BLEU scores. As seen, all our variants, if validated by GM (without â ), outperform competing methods in BLEU. One of the reasons is that our model performs text simplification by making edits on the original sentence instead of rewriting it from scratch.</p><p>In terms of the geometric mean (GM), our unsupervised approach outperforms all previous work, showing a good balance between simplicity and content preservation. The readability of our generated sentences is further confirmed by the intrinsic FKGL score.   Overall results on WikiLarge. For the Wikilarge experiments in <ref type="table" target="#tab_2">Table 2</ref>, we perform validation on SARI, which is the main metric in this experiment. Our model outperforms existing unsupervised methods, and is also competitive with state-of-the-art supervised methods.</p><p>We observe that lexical simplification (LS) is important in this dataset, as its improvement is large compared with the Newsela experiment in <ref type="table">Table 1</ref>. Additionally, reordering (RO) does not improve performance, as it is known that WikiLarge does not focus on syntactic simplification <ref type="bibr" target="#b38">(Xu et al., 2016)</ref>. The best performance for this experiment is obtained by the RM+EX+LS model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Controllability</head><p>We now perform a detailed analysis of the scoring function described in Section 3.2 to understand the effect on different aspects of simplification. We use the RM+EX+LS+RO variant and the Newsela corpus as the testbed.</p><p>The SLOR score with syntax-aware LM. We analyze our syntax-aware SLOR score in the search objective. First, we remove the SLOR score and use the standard sentence probability. We observe that SLOR helps preserve rare words, which may be entities. As a result, the readability score (FKGL) becomes better (i.e., lower), but the BLEU score decreases. We then evaluate the importance of using a structural LM instead of a standard LM. We see a decrease in both SARI and BLEU scores. In both cases, the GM score decreases.</p><p>Threshold values and relative weights. Table 4 analyzes the effect of the hyperparameters of our model, namely, the threshold in the stopping criteria and the relative weights in the scoring function.</p><p>As discussed in Section 3.4, we use a threshold as the stopping criteria for our iterative search algorithm. For each operation, we require that a new candidate should be better than the previous iteration by a multiplicative threshold r op in Equation (3). In this analysis, we set the same threshold for all operations for simplicity. As seen in <ref type="table" target="#tab_5">Table 4</ref>, increasing the threshold leads to better meaning preservation since the model is more conservative (making fewer edits). This is shown by the higher BLEU and lower SARI scores.</p><p>Regarding the weights for each individual scoring function, we find that increasing the weight Î² for the FRE readability score makes sentences shorter, more readable, and thus simpler. This is also indicated by higher SARI values. When sentences are rewarded for being short (with large Î³), SARI increases but BLEU decreases, showing less meaning preservation. The readability scores initially increase with the reduction in length, but then decrease. Finally, if we increase the weight Î´ for the entity score, the sentences become longer and more complex since the model is penalized more for deleting entities.</p><p>In summary, the above analysis shows the controllability of our approach in terms of different simplification aspects, such as simplicity, meaning preservation, and readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Human Evaluation</head><p>We conducted a human evaluation on the Newsela dataset since automated metrics may be insufficient for evaluating text generation. We chose 30 sentences from the test set for annotation and considered a subset of baselines. For our model variants, we chose RM+EX+LS+RO, considering both validation settings (GM and SARI).</p><p>We followed the evaluation setup in <ref type="bibr" target="#b5">Dong et al. (2019)</ref>, and measure the adequacy (How much meaning from the original sentence is preserved?), simplicity (Is the output simper than the original sentence?), and fluency (Is the output grammatical?) on a five-point Likert scale. We recruited three volunteers, one native English speaker and two non-native fluent English speakers. Each of the volunteer was given 30 sentences from different models (and references) in a randomized order. Additionally, we asked the volunteers to measure the number of instances where models produce incorrect details or generate text that is not implied by the original sentence. We did this because neural models are known to hallucinate information <ref type="bibr" target="#b29">(Rohrbach et al., 2018)</ref>. We report the average count of false information per sentence, denoted as FI.</p><p>We observe that our model RM+EX+LS+RO (when validated by GM) performs better than Hybrid, a combination of PBMT and discourse representation structures, in all aspects. It also performs competitively with remaining supervised NMT models.</p><p>For adequacy and fluency, Dress-Ls performs the best since it produces relatively longer sentences. For simplicity, S2S-All-FA performs the best since it produces shorter sentences. Thus, a balance is needed between these three measures. As seen, RM+EX+LS+RO ranks second in terms of the average score in the list (reference excluded). The human evaluation confirms the effectiveness of our unsupervised text simplification, even when compared with supervised methods.</p><p>We also compare our model variants RM+EX+LS+RO (validated by GM) and RM+EX+LS+RO â  (validated by SARI). As expected, the latter generates shorter sentences, performing better in simplicity but worse in adequacy and fluency.</p><p>Regarding false information (FI), we observe that previous neural models tend to generate more false information, possibly due to the vagueness in  <ref type="table">Table 5</ref>: Human evaluation on Newsela, where we measure adequacy (A), simplicity (S), fluency (F), and their average score (Avg), based on 1-5 Likert scale. We also count average instances of false information per sentence (FI). the continuous space. By contrast, our approach only uses neural networks in the scoring function, but performs discrete edits of words and phrases. Thus, we achieve high fidelity (low FI) similar to the non-neural Hybrid model, which also performs editing on discourse parsing structures with PBMT.</p><p>In summary, our model takes advantage of both neural networks (achieving high adequacy, simplicity, and fluency) and traditional phrase-based approaches (achieving high fidelity).</p><p>Interestingly, the reference of Newsela has a poor (high) FI score, because the editors wrote simplifications at the document level, rather than the sentence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed an iterative, edit-based approach to text simplification. Our approach works in an unsupervised manner that does not require a parallel corpus for training. In future work, we plan to add paraphrase generation to generate diverse simple sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Constituency parse tree is used for detecting phrases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MethodSARI â Add â Delete â Keep â BLEU â GM â FKGL â Len</figDesc><table><row><cell>Reference</cell><cell>70.13</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>100</cell><cell>83.74</cell><cell>3.20</cell><cell>12.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Complex</cell><cell>2.82</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>21.30</cell><cell>7.75</cell><cell>8.62</cell><cell>23.06</cell></row><row><cell>Reduce-250</cell><cell>28.39</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.79</cell><cell>18.29</cell><cell>-0.23</cell><cell>14.48</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Supervised Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PBMT-R</cell><cell>15.77</cell><cell>3.07</cell><cell>38.34</cell><cell>5.90</cell><cell>18.1</cell><cell>16.89</cell><cell>7.59</cell><cell>23.06</cell></row><row><cell>Hybrid</cell><cell cols="4">28.61* 0.95* 78.86* 6.01*</cell><cell>14.46</cell><cell>20.34</cell><cell>4.03</cell><cell>12.41</cell></row><row><cell>EncDecA</cell><cell>24.12</cell><cell>2.73</cell><cell>62.66</cell><cell>6.98</cell><cell>21.68</cell><cell>22.87</cell><cell>5.11</cell><cell>16.96</cell></row><row><cell>Dress</cell><cell>27.37</cell><cell>3.08</cell><cell>71.61</cell><cell>7.43</cell><cell>23.2</cell><cell>25.2</cell><cell>4.11</cell><cell>14.2</cell></row><row><cell>Dress-Ls</cell><cell>26.63</cell><cell>3.21</cell><cell>69.28</cell><cell>7.4</cell><cell>24.25</cell><cell>25.41</cell><cell>4.21</cell><cell>14.37</cell></row><row><cell>DMass</cell><cell>31.06</cell><cell>1.25</cell><cell>84.12</cell><cell>7.82</cell><cell>11.92</cell><cell>19.24</cell><cell>3.60</cell><cell>15.07</cell></row><row><cell>S2S-All-FA</cell><cell>30.73</cell><cell>2.64</cell><cell>81.6</cell><cell>7.97</cell><cell>19.55</cell><cell>24.51</cell><cell>2.60</cell><cell>10.81</cell></row><row><cell>Edit-NTS</cell><cell cols="4">30.27* 2.71* 80.34* 7.76*</cell><cell>19.85</cell><cell>24.51</cell><cell>3.41</cell><cell>10.92</cell></row><row><cell>EncDecP</cell><cell>28.31</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.72</cell><cell>25.91</cell><cell>-</cell><cell>-</cell></row><row><cell>EntPar</cell><cell>33.22</cell><cell>2.42</cell><cell>89.32</cell><cell>7.92</cell><cell>11.14</cell><cell>19.24</cell><cell>1.34</cell><cell>7.88</cell></row><row><cell></cell><cell></cell><cell cols="4">Unsupervised Methods (Ours)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RM+EX</cell><cell>26.07</cell><cell>2.35</cell><cell>68.35</cell><cell>7.5</cell><cell>27.22</cell><cell>26.64</cell><cell>2.95</cell><cell>12.9</cell></row><row><cell>RM+EX+LS</cell><cell>26.26</cell><cell>2.28</cell><cell>68.94</cell><cell>7.57</cell><cell>27.17</cell><cell>26.71</cell><cell>2.93</cell><cell>12.88</cell></row><row><cell>RM+EX+RO</cell><cell>26.99</cell><cell>2.47</cell><cell>70.88</cell><cell>7.63</cell><cell>26.31</cell><cell>26.64</cell><cell>3.14</cell><cell>12.81</cell></row><row><cell>RM+EX+LS+RO</cell><cell>27.11</cell><cell>2.40</cell><cell>71.26</cell><cell>7.67</cell><cell>26.21</cell><cell>26.66</cell><cell>3.12</cell><cell>12.81</cell></row><row><cell cols="2">RM+EX+LS+RO  â  30.44</cell><cell>2.05</cell><cell>81.77</cell><cell>7.49</cell><cell>17.36</cell><cell>22.99</cell><cell>2.24</cell><cell>9.61</cell></row><row><cell cols="3">Table 1: Results on the Newsela dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>â  denotes the model with parameters tuned by SARI; other variants are tuned by the geometric mean (GM).â The higher, the better.â The lower, the better. * indicates a number that is different from that reported in the original paper. This is due to a mistreatment of capitalization in the previous work (confirmed by personal correspondence).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MethodSARI â Add â Delete â Keep â BLEU â FKGL â Len</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Complex</cell><cell>27.87</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.39</cell><cell>-</cell><cell>22.61</cell></row><row><cell></cell><cell></cell><cell cols="3">Supervised Methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PBMT-R</cell><cell>38.56</cell><cell>5.73</cell><cell>36.93</cell><cell>73.02</cell><cell>81.09</cell><cell>8.33</cell><cell>22.35</cell></row><row><cell>Hybrid</cell><cell>31.40</cell><cell>1.84</cell><cell>45.48</cell><cell>46.87</cell><cell>48.67</cell><cell>4.56</cell><cell>13.38</cell></row><row><cell>EncDecA</cell><cell>35.66</cell><cell>2.99</cell><cell>28.96</cell><cell>75.02</cell><cell>89.03</cell><cell>8.42</cell><cell>21.26</cell></row><row><cell>Dress</cell><cell>37.08</cell><cell>2.94</cell><cell>43.15</cell><cell>65.15</cell><cell>77.41</cell><cell>6.59</cell><cell>16.14</cell></row><row><cell>Dress-Ls</cell><cell>37.27</cell><cell>2.81</cell><cell>42.22</cell><cell>66.77</cell><cell>80.44</cell><cell>6.62</cell><cell>16.39</cell></row><row><cell>Edit-NTS</cell><cell>38.23</cell><cell>3.36</cell><cell>39.15</cell><cell>72.13</cell><cell>86.69</cell><cell>7.30</cell><cell>18.87</cell></row><row><cell>EntPar</cell><cell>37.45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.49</cell><cell>7.41</cell><cell>-</cell></row><row><cell>Access</cell><cell>41.87</cell><cell>7.28</cell><cell>45.79</cell><cell>72.53</cell><cell>75.46</cell><cell>7.22</cell><cell>22.27</cell></row><row><cell></cell><cell cols="5">Models using external knowledge base</cell><cell></cell><cell></cell></row><row><cell>SBMT-SARI</cell><cell>39.96</cell><cell>5.96</cell><cell>41.42</cell><cell>72.52</cell><cell>73.03</cell><cell>7.29</cell><cell>23.44</cell></row><row><cell>DMass</cell><cell>40.45</cell><cell>5.72</cell><cell>42.23</cell><cell>73.41</cell><cell>-</cell><cell>7.79</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">Unsupervised Methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNMT</cell><cell>35.89</cell><cell>1.94</cell><cell>37.68</cell><cell>68.04</cell><cell>70.61</cell><cell>8.23</cell><cell>21.85</cell></row><row><cell>UNTS</cell><cell>37.20</cell><cell>1.50</cell><cell>41.27</cell><cell>68.81</cell><cell>74.02</cell><cell>7.84</cell><cell>19.05</cell></row><row><cell>RM+EX</cell><cell>36.46</cell><cell>1.68</cell><cell>35.17</cell><cell>72.54</cell><cell>88.90</cell><cell>6.47</cell><cell>18.62</cell></row><row><cell>RM+EX+LS</cell><cell>37.85</cell><cell>2.31</cell><cell>43.65</cell><cell>67.59</cell><cell>73.62</cell><cell>6.30</cell><cell>18.45</cell></row><row><cell>RM+EX+RO</cell><cell>36.54</cell><cell>1.73</cell><cell>36.10</cell><cell>71.79</cell><cell>85.07</cell><cell>6.89</cell><cell>19.24</cell></row><row><cell cols="2">RM+EX+LS+RO 37.58</cell><cell>2.30</cell><cell>43.97</cell><cell>66.46</cell><cell>70.15</cell><cell>6.69</cell><cell>19.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the WikiLarge dataset.</figDesc><table /><note>â The higher, the better.â The lower, the better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MethodSARI â Add â Delete â Keep â BLEU â GM â FKGL â Len</figDesc><table><row><cell>RM+EX+LS+RO</cell><cell>27.11</cell><cell>2.40</cell><cell>71.26</cell><cell>7.67</cell><cell>26.21</cell><cell>26.66</cell><cell>3.12</cell><cell>12.81</cell></row><row><cell>â SLOR</cell><cell>27.63</cell><cell>2.22</cell><cell>73.20</cell><cell>7.49</cell><cell>24.14</cell><cell>25.83</cell><cell>2.61</cell><cell>12.37</cell></row><row><cell cols="2">â syntax-awareness 26.91</cell><cell>2.16</cell><cell>71.19</cell><cell>7.39</cell><cell>24.98</cell><cell>25.93</cell><cell>3.65</cell><cell>12.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation test of the SLOR score based on syntax-aware language modeling. SARI â BLEU â GM â FRE â Len</figDesc><table><row><cell cols="4">Value Effect of threshold r op</cell></row><row><cell>1.0</cell><cell>29.20</cell><cell>21.69</cell><cell>25.17 83.75 11.75</cell></row><row><cell>1.1</cell><cell>28.38</cell><cell>23.59</cell><cell>25.87 82.83 12.17</cell></row><row><cell>1.2</cell><cell>27.45</cell><cell>25.54</cell><cell>26.48 81.98 12.62</cell></row><row><cell>1.3</cell><cell>26.60</cell><cell>26.47</cell><cell>26.53 81.47 13.07</cell></row><row><cell></cell><cell cols="3">Effect of weight Î± for f eslor</cell></row><row><cell>0.75</cell><cell>27.04</cell><cell>25.75</cell><cell>26.39 83.46 12.46</cell></row><row><cell>1.25</cell><cell>26.91</cell><cell>25.96</cell><cell>26.43 81.26 12.96</cell></row><row><cell>1.50</cell><cell>26.74</cell><cell>25.20</cell><cell>25.96 80.94 13.06</cell></row><row><cell>2.0</cell><cell>26.83</cell><cell>24.29</cell><cell>25.53 80.11 13.15</cell></row><row><cell></cell><cell cols="3">Effect of weight Î² for f fre</cell></row><row><cell>0.5</cell><cell>26.42</cell><cell>25.53</cell><cell>25.97 78.61 13.20</cell></row><row><cell>1.5</cell><cell>27.38</cell><cell>26.04</cell><cell>26.70 84.31 12.58</cell></row><row><cell>2.0</cell><cell>27.83</cell><cell>25.27</cell><cell>26.52 87.03 12.26</cell></row><row><cell>3.0</cell><cell>28.29</cell><cell>23.69</cell><cell>26.52 90.34 11.91</cell></row><row><cell></cell><cell cols="3">Effect of weight Î³ for 1/f len</cell></row><row><cell>0.5</cell><cell>24.54</cell><cell>25.06</cell><cell>24.80 80.49 14.55</cell></row><row><cell>2.0</cell><cell>29.00</cell><cell>21.65</cell><cell>25.06 82.69 10.93</cell></row><row><cell>3.0</cell><cell>29.93</cell><cell>19.05</cell><cell>23.88 82.20 10.09</cell></row><row><cell>4.0</cell><cell>30.44</cell><cell>17.36</cell><cell>22.99 80.86 9.61</cell></row><row><cell></cell><cell cols="3">Effect of weight Î´ for f entity</cell></row><row><cell>0.5</cell><cell>27.81</cell><cell>24.68</cell><cell>26.20 83.6 12.01</cell></row><row><cell>2.0</cell><cell>25.44</cell><cell>24.63</cell><cell>25.03 79.36 14.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Analysis of the threshold value of the stopping criteria and relative weights in the scoring function.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Avg â FI â Hybrid 2.63 2.74 2.39 2.59 0.03 Dress-Ls 3.29 3.05 4.11 3.48 0.2 EntPar 1.92 2.97 3.16 2.68 0.47 S2S-All-FA 2.25 3.24 3.90 3.13 0.3 Edit-NTS 2.37 3.17 3.73 3.09 0.23 RM+EX+LS+RO 2.97 3.09 3.78 3.28 0.03 RM+EX+LS+RO â  2.58 3.21 3.33 3.04 0.07 Reference 2.91 3.49 4.46 3.62 0.77</figDesc><table><row><cell>Method</cell><cell>A â</cell><cell>S â</cell><cell>F â</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is released at https://github.com/ ddhruvkr/Edit-Unsup-TS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we do not use SLOR to evaluate lexicon simplicity, which will later be evaluated by the Flesch reading ease (FRE) score. The SLOR score, in fact, preserves rare words, so that we can better design dictionary-based word substitution for lexical simplification (Section 3.3).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://spacy.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This does not hold when sentence splitting is involved. In our datasets, however, sentence splitting is rare, for example, 0.18% in the Newsela validation set).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), under grant Nos. RGPIN-2019-04897, RGPIN-2020-04465, and the Canada Research Chair Program. Lili Mou is also supported by Al-taML, the Amii Fellow Program, and the Canadian CIFAR AI Chair Program. This research was supported in part by Compute Canada (www. computecanada.ca).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning how to simplify from explicit labeling of complex-simplified text pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Alva-Manchego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing (IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Text simplification as tree labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Short Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Short Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="337" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simplifying text for language-impaired readers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Canning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siobhan</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Motivations and methods for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1331</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3393" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An evaluation of syntactic simplification rules for people with autism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>OrÈsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-1215</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations</title>
		<meeting>the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic multi-level multi-task learning for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 27th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="462" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">https:/www.mitpressjournals.org/doi/pdfplus/10.1162/089976602760128018?casa_token=fsB6IQcroc0AAAAA%3AWhj7HpchYQtTyNGOhuNoAs0MLLci7l3z0i5TYLw91VKOHHCP-VxvHPKIeo9E62zZgNuZPB5k25tgpjk&amp;</idno>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentence-level fluency evaluation: References help, but can be spared!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert P Fishburne</forename><surname>Peter Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text simplification for informationseeking applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OTM Confederated International Conferences&quot; On the Move to Meaningful Internet Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="735" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Complexity-weighted loss and diverse reranking for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reno</forename><surname>Kriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JoÃ£o</forename><surname>Sedoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3137" to="3147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Grammaticality, acceptability, and probability: A probabilistic view of linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lappin</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12414</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1202" to="1241" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delete, retrieve, generate: a simple approach to sentiment and style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies NAACL-HLT</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1865" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised paraphrasing by simulated annealing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ãric de la Clergerie, and Antoine Bordes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BenoÃ®t</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02677</idno>
	</analytic>
	<monogr>
		<title level="m">Controllable sentence simplification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CGMH: Constrained sentence generation by Metropolis-Hastings sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6834" to="6842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/219717.219748</idno>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hybrid simplification using deep semantics and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised sentence simplification using deep semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-6620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Natural Language Generation conference (INLG)</title>
		<meeting>the 9th International Natural Language Generation conference (INLG)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring neural text simplification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Nisioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>SanjaÅ¡tajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liviu P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dinu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale syntactic language modeling with treelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="959" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
		<idno type="DOI">https:/epubs.siam.org/doi/pdf/10.1137/0330046?casa_token=-UvVKImhPCAAAAAA%3ANGPnDnTRtIWAwuZv3btI9ascIgCqlUH2VpgqDMTNh2pxHs4-64rRdMmDHkR6WbEEuC7H-MhjLLhQ&amp;</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dyswebxia 2.0!: more accessible text for people with dyslexia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luz</forename><surname>Rello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Bayarri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuki</forename><surname>GÃ²rriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurang</forename><surname>Kanvinde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Topac</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/pdf/10.1145/2461121.2461150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility</title>
		<meeting>the 10th International Cross-Disciplinary Conference on Web Accessibility</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object hallucination in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4035" to="4045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Olga Vechtomova, and Katja Markert. 2020. Discrete optimization for unsupervised sentence summarization with word-level extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An architecture for a text simplification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Engineering Conference, 2002. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BLEU is not suitable for the evaluation of text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elior</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="738" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised neural text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2058" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">VinÃ­cius Rodriguez UzÃªda, Renata Pontin de Mattos Fortes, Thiago Alexandre Salgueiro Pardo, and Sandra Maria AluÃ­sio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaldo Candido</forename><surname>Willian Massami Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junior</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/pdf/10.1145/1621995.1622002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on Design of communication</title>
		<meeting>the 27th ACM international conference on Design of communication</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
	<note>Facilita: reading assistance for low-literacy readers</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Problems in current text simplification research: New data can help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="283" to="297" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00107</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="584" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Integrating transformer and paraphrase rules for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Saptono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bambang</forename><surname>Parmanto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3164" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A language model based evaluator for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), Short Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (ACL), Short Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A monolingual tree-based translation model for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 23rd International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1353" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
