<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TripleNet: Triple Attention Network for Multi-Turn Response Selection in Retrieval-based Chatbots</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">iFLYTEK Research</orgName>
								<orgName type="department" key="dep2">China ‡ Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="department" key="dep3">§ iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">iFLYTEK Research</orgName>
								<orgName type="department" key="dep2">China ‡ Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="department" key="dep3">§ iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">iFLYTEK Research</orgName>
								<orgName type="department" key="dep2">China ‡ Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="department" key="dep3">§ iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">iFLYTEK Research</orgName>
								<orgName type="department" key="dep2">China ‡ Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="department" key="dep3">§ iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Nan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">iFLYTEK Research</orgName>
								<orgName type="department" key="dep2">China ‡ Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="department" key="dep3">§ iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">iFLYTEK Research</orgName>
								<orgName type="department" key="dep2">China ‡ Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="department" key="dep3">§ iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">iFLYTEK Research</orgName>
								<orgName type="department" key="dep2">China ‡ Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="department" key="dep3">§ iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
							<email>gphu@iflytek.com‡ymcui</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">iFLYTEK Research</orgName>
								<orgName type="department" key="dep2">China ‡ Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="department" key="dep3">§ iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TripleNet: Triple Attention Network for Multi-Turn Response Selection in Retrieval-based Chatbots</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the importance of different utterances in the context for selecting the response usually depends on the current query. 1 In this paper, we propose the model TripleNet to fully model the task with the triple context, query, response instead of context, response in previous works. The heart of TripleNet is a novel attention mechanism named triple attention to model the relationships within the triple at four levels. The new mechanism updates the representation for each element based on the attention with the other two concurrently and symmetrically. We match the triple C, Q, R centered on the response from char to context level for prediction. Experimental results on two large-scale multi-turn response selection datasets show that the proposed model can significantly outperform the state-of-the-art methods. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To establish a human-machine dialogue system is one of the most challenging tasks in Artificial Intelligence (AI). Existing works on building dialogue systems are mainly divided into two categories: retrieval-based method <ref type="bibr" target="#b26">Zhou et al., 2016)</ref>, and generation-based method <ref type="bibr" target="#b17">(Vinyals and Le, 2015)</ref>. The retrievalbased method retrieves multiple candidate responses from the massive repository and selects the best one as the system's response, while the generation-based method uses the encoderdecoder framework to generate the response, which is similar to machine translation.</p><p>A: i downloaded angry ip scanner and now it doesn't work and i can't uninstall it B: you installed it via package or via some ::::: binary ::::::</p><p>installer A: i installed from ubuntu soft center B: hm i do n't know what package it is but it should let you remove it the same way A: ah makes sense then ... hm :::</p><p>was : it : a :::: deb :: file True Response: i think : it :::: was :::::: another ::::: format mayge sth starting with r False Response: thanks i appreciate it try sudo apt-get install libxine-extracodecs <ref type="figure">Figure 1</ref>: A real example in the Ubuntu Corpus. The upper part is the conversation between speaker A and B. The speaker A want to uninstall the ip scanner and the current query is about the format of the package, so the true response is about the format, but the existing conversation model can be easily misled by the high frequency term 'install' as they deal with the query and other utterances in the same way.</p><p>In this paper, we are focusing on the retrievalbased method because it is more practical in applications. Selecting a response from a set of candidates is an important and challenging task for the retrieval-based method. Many of the previous approaches are based on Deep Neural Network (DNN) to select the response for single-turn conversation <ref type="bibr" target="#b12">(Lu and Li, 2013)</ref>. We study multi-turn response selection in this paper, which is rather difficult because it not only requires identification of the important information such as keywords, phrases, and sentences, but also the latent dependencies between the context, query, and candidate response.</p><p>Previous works <ref type="bibr" target="#b27">(Zhou et al., 2018;</ref><ref type="bibr" target="#b22">Wu et al., 2017)</ref> show that representing the context at different granularities is vital for multi-turn response selection. However, it is not enough for multi-turn response selection. <ref type="figure">Figure 1</ref> illustrates the problem with a real example in Ubuntu Corpus. As demonstrated, the following two points should be modeled to solve the problem: (1) the importance of current query should be highlighted, because it has great impact on the importance of different utterances in the context. For example, the query in the case is about the format of the file ('deb file'), which leads the last two utterances (including the query) are more important than the previous ones. If we only match the response with the context, the model may be misled by the high frequency word 'install' and choose the false candidate. (2) the information of different granularities is important, which includes not only the word, utterance, and context level, but also the char level. For example, the different tenses ('install,' 'installed') and the misspelling word ('angry') appear constantly in the conversation. Similar to the role of question for the task of machine reading comprehension <ref type="bibr" target="#b16">(Seo et al., 2016;</ref><ref type="bibr" target="#b3">Cui et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2019)</ref>, the query in this task is also the key to selecting the response. In this paper, we propose a model named TripleNet to excavate the role of query. The main contributions of our work are listed as follows.</p><p>• we use a novel triple attention mechanism to model the relationships within C, Q, R instead of C, R ;</p><p>• we propose a hierarchical representation module to fully model the conversation from char to context level;</p><p>• The experimental results on Ubuntu and Douban corpus show that TripleNet significantly outperform the state-of-the-art result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Earlier works on building the conversation systems are generally based on rules or templates <ref type="bibr" target="#b18">(Walker et al., 2001)</ref>, which are designed for the specific domain and need much human effort to collect the rules and domain knowledge. As the portability and coverage of such systems are far from satisfaction, people pay more attention to the data-driven approaches for the opendomain conversation system <ref type="bibr" target="#b15">(Ritter et al., 2011;</ref><ref type="bibr" target="#b4">Higashinaka et al., 2014)</ref>. The main challenge for open-domain conversation is to produce a corresponding response based on the current context. As mentioned previously, the retrieval-based and generation-based methods are the mainstream approaches for conversational response generation.</p><p>In this paper, we focus on the task response selection which belongs to retrieval-based approach.</p><p>The early studies of response selection generally focus on the single-turn conversation, which use only the current query to select the response <ref type="bibr" target="#b12">(Lu and Li, 2013;</ref><ref type="bibr" target="#b6">Ji et al., 2014;</ref><ref type="bibr" target="#b20">Wang et al., 2015)</ref>. Since it is hard to get the topic and intention of the conversation by single-turn, researchers turn their attention to multi-turn conversation and model the context instead of the current query to predict the response. First, <ref type="bibr" target="#b10">Lowe et al. (2015)</ref> released the Ubuntu Dialogue dataset and proposed a neural model which matches the context and response with corresponding representations via RNNs and LSTMs. <ref type="bibr" target="#b7">Kadlec et al. (2015)</ref> evaluate the performances of various models on the dataset, such as LSTMs, Bi-LSTMs, and CNNs. Later,  concatenated utterances with the reformulated query and various features in a deep neural network. <ref type="bibr" target="#b0">Baudiš et al. (2016)</ref> regarded the task as sentence pair scoring and implemented an RNN-CNN neural network model with attention. <ref type="bibr" target="#b26">Zhou et al. (2016)</ref> proposed a multiview model with CNN and RNN, modeling the context in both word and utterance view. Further, <ref type="bibr" target="#b23">Xu et al. (2017)</ref> proposed a deep neural network to incorporate background knowledge for conversation by LSTM with a specially designed recall gate. <ref type="bibr" target="#b22">Wu et al. (2017)</ref> proposed matching the context and response by their word and phrase representations, which had significant improvement from previous work. <ref type="bibr" target="#b25">Zhang et al. (2018)</ref> introduced a self-matching attention to route the vital information in each utterance, and used RNN to fuse the matching result. <ref type="bibr" target="#b27">Zhou et al. (2018)</ref> used self-attention and cross-attention to construct the representations at different granularities, achieving a state-of-the-art result.</p><p>Our model is different from the previous methods: first we model the task with the triple C, Q, R instead of C, R in the early works, and use a novel triple attention matching mechanism to model the relationships within the triple. Then we represent the context from low (character) to high (context) level, which constructs the representations for the context more comprehensively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we will give a detailed introduction of the proposed model TripleNet. We first formal- Triple Attention ize the problem of the response selection for multiturn conversation. Then we briefly introduce the overall architecture of the proposed model. Finally, the details of each part of our model will be illustrated.</p><formula xml:id="formula_0">Input Input Input C Q C Q C Q C Q R R R C Q R BAF BAF BAF C' R' Q' C Q R BAF BAF BAF C' R' Q' C Q R BAF BAF BAF C' R' Q' C Q R BAF BAF BAF C' R' Q' Output Match C 1 C 2 R 2 R 1 Q 2 Q 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>For the response selection, we define the task as given the context C, current query Q and candidate response R, which is different from almost all the previous works <ref type="bibr" target="#b27">(Zhou et al., 2018;</ref><ref type="bibr" target="#b22">Wu et al., 2017)</ref>. We aim to build a model function g(C, Q, R) to predict the possibility of the candidate response to be the correct response.</p><formula xml:id="formula_1">score = g(C, Q, R)<label>(1)</label></formula><p>The information in context is composed of four levels: context, utterances, words and characters, which can be formulated as C = (u 1 , u 2 , ..., u i , ..., u n ), where u i represents the ith utterance, and n is the maximum utterance number. The last utterance in the context is query Q = U n ; we still use query as the end of context to maintain the integrity of the information in context. Each utterance can be formulated as u i = (w 1 , ..., w j , .., w m ), where w j is the jth word in the utterance and m is the maximum word number in the utterance. Each word can be represented by multiply characters w j = (ch 1 , ..., ch k , .., ch l ), where ch k is the kth char and l is the length of the word in char-level. The latter two levels are similar in the query and response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><p>The overall architecture of the model TripleNet is displayed in <ref type="figure" target="#fig_0">Figure 2</ref>. The model has a bottomup architecture that organizes the calculation from char to context level. In each level, we first uses the hierarchical representation module to construct the representations of context, response and query. Then the triple attention mechanism is applied to update the representations. At last, the model matches them while focused on the response and fuses the result for prediction.</p><p>In the hierarchical representation module, we represent the conversation in four perspectives including char, word, utterance, and context. In the char-level, a convolutional neural network (CNN) is applied to the embedding matrix of each word and produces the embedding of the word by convolution and maxpooling operations as the charlevel representation. In word-level, we use a shared LSTM layer to obtain the word-level embedding for each word. After that, we use selfattention to encode the representation of each utterance into a vector which is the utterance-level representation. At last, the utterance-level representation of each utterance is fed into another LSTM layer to further model the information among different utterances, forming the contextlevel representation.</p><p>The structure of the triple attention mechanism can be seen in the right part of <ref type="figure" target="#fig_0">Figure 2</ref>. We first design a bi-directional attention function (BAF) to calculate the attention between two sequences and output their new representations. To model the relationship of the triple C, Q, R , we apply BAF to each pair within the triple and get two new representations for each one element, and then we add them together as its final attention-based representation. In the triple attention mechanism, we can update the representation of each one based on the attention result with the other two simultaneously, and each element participates in the whole calculation in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Representation</head><p>Char-level Representation. At first, we embed the characters in each word into fixed size vectors and use a CNN followed by max-pooling to get character-derived embeddings for each word, which can be formulated by</p><formula xml:id="formula_2">ch j,t = tanh(W j 1 * x t:t+s j −1 + b j 1 ) (2) ch j = M axP ooling L t=0 [ch j,t ]<label>(3)</label></formula><p>where W j 1 , b j 1 are parameters, x t:t+s j −1 refers to the concatenation of the embedding of (x t ,...,x t+s j −1 ), s j is the window size of jth filter, and the ch is the representation of the word in char-level. Word-level Representation. Furthermore, we embed word x by pre-trained word vectors, and we also introduce a word matching (MF) feature to the embedding to make the model more sensitive to concurrent words. If the word appears in the response and context or query simultaneously, we set the feature to 1, otherwise to 0.</p><formula xml:id="formula_3">e(x) = [W e · x; ch(x); M F ]<label>(4)</label></formula><p>where e(x) to denotes the embedding representation, W e is the pre-trained word embedding, and ch(x) is the character embedding function. We use a shared bi-directional LSTM to get contextual word representations in each utterance, query, and the response. The representation of each word is formed by concatenating the forward and backward LSTM hidden output.</p><formula xml:id="formula_4">← − − h(x) = ← −−− − LSTM(e(x)) (5) − − → h(x) = − −−− → LSTM(e(x)) (6) h(x) = [ ← − − h(x); − − → h(x)]<label>(7)</label></formula><p>where h(x) is the representation of the word. We denote the word-level representation of the context as h u ∈ R m * dw and the response as h r ∈ R m * dw , where d w is the dimension of Bi-LSTMs. Until now, we have constructed the representations of context, query, and response in char and word level, and we only represent the latter two in these two levels because they don't have such rich contextual information as the context.</p><formula xml:id="formula_5">Utterance-level Representation. Given the k th utterance u k = [h i u k ] m i=1</formula><p>, we construct the utterance-level representation by self-attention <ref type="bibr" target="#b9">(Lin et al., 2017)</ref>:</p><formula xml:id="formula_6">α k i = sof tmax(W 3 tanh(W 2 h u k (i) T )) (8) u k = m i=1 h i u k α k i<label>(9)</label></formula><p>where W 2 ∈ R d * dw , W 3 ∈ R d are trainable weights, d is a hyperparameter, u k is the utterancelevel representation, and α k i is the attention weight for the ith word in the kth utterance, which signifies the importance of the word in the utterance. Context-level Representation. To further model the continuity and contextual information among the utterances, we fed the utterance-level representations into another bi-directional LSTM layer to obtain the representation for each utterance in context perspective.</p><formula xml:id="formula_7">c k = Bi-LSTM([u k ] n k=1 )<label>(10)</label></formula><p>where c k ∈ R dc is the context-level representation for the kth utterance in the context and d c is the output size of the Bi-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Triple Attention</head><p>In this part, we update the representations of context, query, and response in each level by triple attention, the motivation of which is to model the latent relationships within context, query, response .</p><p>Given the triple C, Q, R , we fed each of its pairs into bi-directional attention function (BAF).</p><formula xml:id="formula_8">C 1 , Q 1 = BAF (C, Q) (11) C 2 , R 1 = BAF (C, Q) (12) Q 2 , R 2 = BAF (C, R) (13) C = BN (C 1 + C 2 ) (14) Q = BN (Q 1 + Q 2 ) (15) R = BN (R 1 + R 2 )<label>(16)</label></formula><p>where BN denotes the batch normalization layer <ref type="bibr" target="#b5">(Ioffe and Szegedy, 2015)</ref> which is conducive to preventing vanishing or exploding of gradients. BAF produces the new representations for two sequences (P, Q) by the attention from two directions, which is inspired by <ref type="bibr" target="#b16">Seo et al. (2016)</ref>. We can formulate it by</p><formula xml:id="formula_9">M pq = P T tanh(W 3 Q) (17) Att pq = sof tmax(M pq )<label>(18)</label></formula><formula xml:id="formula_10">Att qp = sof tmax(M T pq )<label>(19)</label></formula><formula xml:id="formula_11">P = P −Q;Q = QAtt pq ;<label>(20)</label></formula><p>Q = Q −P ;P = P Att qp ;</p><p>where Att pq , Att qp are the attention between P and Q in two directions, P , Q are the new representations the two sequences (P, Q), and we apply a batch normalization layer upon them too. We find that the triple attention has some interesting features: (1) triple, the representation for each element in the triple C, Q, R is updated based on the attention to the other two concurrently; (2) symmetrical, which means each element in the triple plays the same role in the structure because their contents are similar in the whole conversation; (3) unchanged dimension, all the outputs of triple attention has the same dimensions as the inputs, so we can stack multiple layers as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Triple Matching and Prediction</head><p>Triple Matching. We match the triple C, Q, R in each level with the cosine distance using new representations produced by triple attention. This process focuses on the response because it is our target. For example, in the char-level, we match the triple bỹ</p><formula xml:id="formula_13">M 1 rc (i, k, j) = cosine(ch r (i), ch u k (j)) (22) M 1 rc (i, k) = max 0&lt;j&lt;mM 1 (i, j, k) (23) M 1 rq (i, j) = cosine(ch r (i), ch q (j)) (24) M 1 = [M 1 rc (i, k); M 1 rq (i, j)]<label>(25)</label></formula><p>where ch is the representation updated by triple attention, M 1 ∈ R m * (n+m) is the char-level matching result, the word-level matches the triple in the same way, and the utterance and the context level match the triple without the maxpooling operation. We use M 2 , M 3 , M 4 as the matching results in the word, utterance and context levels. Fusion. After obtaining the four-level matching matrix, we use hierarchical RNN to get highly abstract features. Firstly, we concatenate the four matrices to form a 3D cube M ∈ R m * (n+m) * 4 and we use m as one of the matrix in M , which denotes the matching result for one word in response in four levels.</p><formula xml:id="formula_14">M = [M 1 ; M 2 ; M 3 ; M 4 ] (26) m = M axP ooling n+m i=0 [Bi-LSTM(m i )] (27) v = M axP ooling m j=0 [Bi-LSTM(m j )] (28)</formula><p>Where m i andm j are the ith, jth row in the matrix m andm. We merge the results from different time steps in the outputs of LSTM by max-pooling operation. Until now, we encode the matching result into a single feature vector v.</p><p>Final Prediction. For the final prediction, we fed the vector V into a full-connected layer with sigmoid output activation.</p><formula xml:id="formula_15">g(C, Q, R) = sigmoid(W 4 · v + b 4 )<label>(29)</label></formula><p>where W 4 , b 4 are trainable weights. Our purpose is to predict the matching score between the context, query and candidate response, which can be seen as a binary classification task. To train our model, we minimize the cross entropy loss between the prediction and ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We first evaluate our model on Ubuntu Dialogue Corpus <ref type="bibr" target="#b10">(Lowe et al., 2015)</ref> because it is the largest public multi-turn dialogue corpus which consists of about one million conversations in the specific domain. To reduce the number of unknown words, we use the shared copy of the Ubuntu corpus by <ref type="bibr" target="#b23">Xu et al. (2017)</ref> which replaces the numbers, paths, and URLs by specific symbols. 3 Furthermore, to verify the generalization of our model, we also carry out experiments on Douban Conversation Corpus <ref type="bibr" target="#b22">(Wu et al., 2017)</ref>, which shares similar format with the Ubuntu corpus but is opendomain and in the Chinese language. For the Ubuntu corpus, we use the recall at position k in n candidate responses (R n @k) as evaluation metrics, and we use MAP (Mean Average Precision), MRR (Mean Reciprocal Rank), and Precision-at-one as the additional metrics for Douban corpus, following the previous work <ref type="bibr" target="#b22">(Wu et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Setup</head><p>We implement our model by Keras <ref type="bibr" target="#b2">(Chollet et al., 2015)</ref> with TensorFlow backend. In the Embedding Layer, the word embeddings are pre-trained using the training set via GloVe <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref>, the weights of which are trainable. For char embedding, we set the kernel shape as 3 and filter number as 200 in the CNN layer. For all the Bidirectional LSTM layers, we set their hidden size to 200. We use Adamax <ref type="bibr" target="#b8">(Kingma and Ba, 2014)</ref> for weight updating with an initial learning rate of 0.002. For ensemble models, we generate 6 models for each corpus using different random seeds and merge the result by voting.</p><p>For better comparison with the baseline models, the main super parameters in TripleNet, such</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Models</head><p>We basically divided baseline models into two categories for comparisons. Non-Attention Models. The majority of the previous works on this task are designed without attention mechanisms, including the Sequential Matching Network (SMN) <ref type="bibr" target="#b22">(Wu et al., 2017)</ref>, Multi-View model <ref type="bibr" target="#b26">(Zhou et al., 2016)</ref>, Deep Learning to Respond (DL2R) , Match-LSTM <ref type="bibr" target="#b21">(Wang and Jiang, 2016)</ref>, <ref type="bibr">MV-LSTM (Wan et al., 2016)</ref>, and DualEncoder <ref type="bibr" target="#b10">(Lowe et al., 2015)</ref>. Attention-based Models. The attention-based models typically match the context and the candidate response based on the attention among them, including DAM <ref type="bibr" target="#b27">(Zhou et al., 2018)</ref>, DUA <ref type="bibr" target="#b25">(Zhang et al., 2018)</ref>, and RNN-CNN <ref type="bibr" target="#b0">(Baudiš et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Overall Results</head><p>The overall results on two datasets are depicted in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Our results are obviously better on the two datasets compared with recently attention-based model DAM, which exceeds 2.3% in R 10 @1 of Ubuntu and 2.6% in P @1 of Douban. Furthermore, our score is significantly exceeding in almost all metrics except the R 10 @5 in Douban when compared with DUA, which may be be-cause the metric is not very stable as the test set in Douban is very small (1000).</p><p>To further improve the performance, we utilize pre-trained ELMo <ref type="bibr" target="#b14">(Peters et al., 2018)</ref> and finetune it on the training set in the Ubuntu condition while we train ELMo from scratch using the Douban training set. As the baseline of Douban corpus is relatively lower, we observe much bigger improvements in the corpus using ELMo. The model ensemble has further improvements based on the single model with ELMo; the score of R 10 @1 in Ubuntu is close to the average performance of human experts at 83.8 <ref type="bibr" target="#b11">(Lowe et al., 2016)</ref>.</p><p>Compared to non-attention models such as the SMN and Multi-view, which match the context and response at two levels, TripleNet shows substantial improvements. On R 10 @1 for Ubuntu corpus, there is a 6.3% absolute improvement from SMN and 12.8% from Multi-view, showing the effectiveness of triple attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Ablation</head><p>To better demonstrate the effectiveness of TripleNet, we conduct the ablations on the model under the Ubuntu corpus for its larger data size.</p><p>We first remove the triple attention and matching parts (-TAM); the result shows a marked decline (2.4% in R 10 @1), which is in the second part of <ref type="table" target="#tab_3">Table 2</ref>. The performance of the model is similar to the baseline model DAM. This indicates that our four-level hierarchical representation may play a similar role to the five stacks Transformer in DAM. We then remove the triple attention part, which means we match the pairs C, R and Q, R with their original representation in each level; the score of R 10 @1 drops 1.4%, which shows the effect of triple attention. We also have tried to remove all the parts related to the query (-Query). That means the attention and matching parts are only calculated within the pair C, R . It is worth mentioning that the information of the query is still contained at the end of the context. The performance also has a marked drop (1.6% in R 10 @1), which shows that it is necessary to model the query separately. To find out which subsection in those parts is more important, we remove each one of them. Triple attention matching ablation. As we can see in the third part of <ref type="table" target="#tab_3">Table 2</ref>, when attention between context and response is removed (-A CR ),  the largest decrease (0.6% in R 10 @1) appears, which indicates that the relationship between context and response is most important in the triple. The attentions in the other two pairs C, Q and Q, R all lead to a slight performance drop (0.3 and 0.5 in R 10 @1), which may be because they overlap with each other for updating the representation of the triple. When we remove the matching between context and response, we find that the performance of the model has a marked drop (2.1 in R 10 @1), which shows that the relationship within C, R is the base for selecting the response. The query and response matching part also leads to a significant decline. This shows that we should pay more attention to query within the whole context. Hierarchical representation ablation. To find out the calculation of which level is most important, we also tried to remove each level calculation from the hierarchical representation module, which can be seen in the fourth part of Table 2. To our surprise, when we remove char (char) and context level calculation (-context), we observe that the reduction (0.5 in R 10 @1) is more significant than the other two, indicating that we should pay more attention to the lowest and highest level information. Also by removing the other two levels, there is also a significant reduction from TripleNet, which means each level of the three is indispensable for our TripleNet .</p><p>From the experiments in this part, we find that each subsection of the hierarchical representation module only leads to a slight performance drop. Maybe it's because the representation from each level represent the conversation from a unique and indispensable perspective, and the information conveyed by different representations may have some overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visualization</head><p>By decoding our model for the case in <ref type="figure">Figure 1</ref>, we find that our model TripleNet can choose the true response. To analyze in detail how triple attention works, we get the attention in word-level as the example and visualize it in <ref type="figure" target="#fig_1">Figure 3</ref>. As there are so many words in the context, we only use the second utterance in the upper part of <ref type="figure">Figure 1</ref> for its relatively rich semantics.</p><p>In the query-context attention, the query mainly pays attention to the keyword 'package.' This is helpful to get the topic of the conversation. While the attention of context focuses on the word 'a' which is near the key phrase 'deb file,' which may be because the representation of the word catches some information from the words nearby by Bi-LSTM. In the query-response attention, the result shows that the attention of the query mainly focuses on the word 'format,' which is the most important word in the response. But we can also find that the response does not catch the important words in the query. In the response-context attention, the response pays more attention to the word 'binary,' which is another important word in the context.</p><p>From the three maps, we find that each attention can catch some important information but miss some useful information too. If we join the information in query-context and response-context attention, we can catch the most import information in the context. Furthermore, the query-response attention can help us catch the most important word in the response. So it is natural for TripleNet to select the right response because the model can integrate the three attentions together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion</head><p>In this section, we will discuss the importance of different utterances in the context. To find out the importance of different utterances in the context, we conduct an experiment by removing each one of them with the model (-Query) in the ablation experiment part because the model deals all the utterances include the query in the same way. For each experiment in this part, we remove the ith (0 &lt; i &lt; 13 and Q = U 12 ) utterance in the context both in training and evaluation processes and report the decrease of performance in <ref type="figure" target="#fig_2">Figure 4</ref>. We find that the removing of the query leads the most significant decline (more than 6% in R 10 @1), that indicates the query is much more important than any other utterances. Furthermore, the decrease is stable before the 9th utterances and raises rapidly in the last 3 utterances. We can deduce that the last three utterances are more important than the other ones.</p><p>From the whole result, we can conclude that it's better to model the query separately than deal all of the utterances in the same way for their significantly different importance; we also find that we should pay more attention to the utterances near the query because they are more important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a model TripleNet for multi-turn response selection. We model the context from low (character) to high (context) level, update the representation by triple attention within C, Q, R , match the triple focused on response, and fuse the matching results with hierarchical LSTM for prediction. Experimental results show that the proposed model achieves state-of-the-art results on both Ubuntu and Douban corpus, which ranges from a specific domain to open domain, and English to Chinese language, demonstrating the effectiveness and generalization of our model. In the future, we will apply the proposed triple attention mechanism to other NLP tasks to further testify its extensibility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The neural architecture of the model TripleNet. (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The attention visualization among the query, context, and response in word-level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The decrease of the performance when the utterance i is removed in Ubuntu Corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on two public dialogue datasets. The table is segmented into three sections: Non-Attention models, Attention-based models and our models. The italics denotes the previous best results, and the scores in bold express the new state-of-the-art result of single model without any pre-training layer.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Ubuntu Dialogue Corpus</cell><cell></cell><cell cols="4">Douban Conversation Corpus</cell><cell></cell></row><row><cell></cell><cell cols="10">R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5</cell></row><row><cell>DualEncoder</cell><cell>90.1</cell><cell>63.8</cell><cell>78.4</cell><cell>94.9</cell><cell>48.5</cell><cell>52.7</cell><cell>32.0</cell><cell>18.7</cell><cell>34.3</cell><cell>72.0</cell></row><row><cell>MV-LSTM</cell><cell>90.6</cell><cell>65.3</cell><cell>80.4</cell><cell>94.6</cell><cell>49.8</cell><cell>53.8</cell><cell>34.8</cell><cell>20.2</cell><cell>35.1</cell><cell>71.6</cell></row><row><cell>Match-LSTM</cell><cell>90.4</cell><cell>65.3</cell><cell>80.4</cell><cell>94.6</cell><cell>49.8</cell><cell>53.8</cell><cell>34.8</cell><cell>20.2</cell><cell>34.8</cell><cell>71.0</cell></row><row><cell>DL2R</cell><cell>89.9</cell><cell>62.6</cell><cell>78.3</cell><cell>94.4</cell><cell>48.8</cell><cell>52.7</cell><cell>33.0</cell><cell>19.3</cell><cell>34.2</cell><cell>70.5</cell></row><row><cell>Multi-View</cell><cell>90.8</cell><cell>66.2</cell><cell>80.1</cell><cell>95.1</cell><cell>50.5</cell><cell>54.3</cell><cell>34.2</cell><cell>20.2</cell><cell>35.0</cell><cell>72.9</cell></row><row><cell>SMN</cell><cell>92.6</cell><cell>72.6</cell><cell>84.7</cell><cell>96.1</cell><cell>52.9</cell><cell>56.9</cell><cell>39.7</cell><cell>23.3</cell><cell>39.6</cell><cell>72.4</cell></row><row><cell>RNN-CNN</cell><cell>91.1</cell><cell>67.2</cell><cell>80.9</cell><cell>95.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DUA</cell><cell>-</cell><cell>75.2</cell><cell>86.8</cell><cell>96.2</cell><cell>55.1</cell><cell>59.9</cell><cell>42.1</cell><cell>24.3</cell><cell>42.1</cell><cell>78.0</cell></row><row><cell>DAM</cell><cell>93.8</cell><cell>76.7</cell><cell>87.4</cell><cell>96.9</cell><cell>55.0</cell><cell>60.1</cell><cell>42.7</cell><cell>25.4</cell><cell>41.0</cell><cell>75.7</cell></row><row><cell>TripleNet</cell><cell>94.3</cell><cell>79.0</cell><cell>88.5</cell><cell>97.0</cell><cell>56.4</cell><cell>61.8</cell><cell>44.7</cell><cell>26.8</cell><cell>42.6</cell><cell>77.8</cell></row><row><cell>TripleNet elmo</cell><cell>95.1</cell><cell>80.5</cell><cell>89.7</cell><cell>97.6</cell><cell>60.9</cell><cell>65.0</cell><cell>47.0</cell><cell>27.8</cell><cell>48.7</cell><cell>81.4</cell></row><row><cell>TripleNet ensemble</cell><cell>95.6</cell><cell>82.1</cell><cell>90.9</cell><cell>98.0</cell><cell>63.2</cell><cell>67.8</cell><cell>51.5</cell><cell>31.3</cell><cell>49.4</cell><cell>83.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on Ubuntu Dialogue Corpus.</figDesc><table><row><cell>The letter 'A' stands for the subsection in triple atten-</cell></row><row><cell>tion, and 'M' the is triple matching part.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, we define the last message which is waiting for a response as the 'query,' the conversation history including the query as 'context,' and each message in the context as an 'utterance.' 2 TripleNet source code is available at https:// github.com/wtma/TripleNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip as the embedding size, max length of each turn, and the vocabularies, are the same as those of the baseline models. The maximum number of conversation turns, which changes with the models, is 12 in our model, 9 in DAM<ref type="bibr" target="#b22">(Wu et al., 2017)</ref>, and 10 in SMN<ref type="bibr" target="#b22">(Wu et al., 2017)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank all anonymous reviewers for their hard work on reviewing and providing valuable comments on our paper. We also would like to thank Yunyi Anderson for proofreading our paper thoroughly. This work is supported by National Key R&amp;D Program of China via grant 2018YFC0832100.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Baudiš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Pichl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Vyskočil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Sedivỳ</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06127</idno>
		<title level="m">Sentence pair scoring: Towards unified framework for text comprehension</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional spatial attention model for reading comprehension with multiple-choice questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1055</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of ACL</title>
		<meeting>the 55th Annual Meeting of ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards an open-domain conversational system fully based on natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichiro</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toyomi</forename><surname>Meguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiaki</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nozomi</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiro</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="928" to="939" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An information retrieval approach to short text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.6988</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03753</idno>
		<title level="m">Improved deep learning baselines for ubuntu corpus dialogs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4640</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the evaluation of dialogue systems with next utterance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05414</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A deep architecture for matching short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1367" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP 2011</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantitative and qualitative evaluation of darpa communicator spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">E</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2001</title>
		<meeting>ACL 2001</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="515" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04378</idno>
		<title level="m">Match-srnn: Modeling the recursive matching structure with spatial rnn</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02427</idno>
		<title level="m">Syntax-based deep matching of short texts</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incorporating loosestructured knowledge into conversation modeling via recall-gate lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3506" to="3513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling multiturn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-view response selection for human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1036</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<meeting>ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
