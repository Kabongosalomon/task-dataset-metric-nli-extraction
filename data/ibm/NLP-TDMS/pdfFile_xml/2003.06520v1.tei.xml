<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symmetry Detection of Occluded Point Cloud Using Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhelun</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Telecom Tech Division Hunan</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyun</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Symmetry Detection of Occluded Point Cloud Using Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>3D Symmetry Detection</term>
					<term>Computer Graphics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Symmetry detection has been a classical problem in computer graphics, many of which using traditional geometric methods. In recent years, however, we have witnessed the arising deep learning changed the landscape of computer graphics. In this paper, we aim to solve the symmetry detection of the occluded point cloud in a deep-learning fashion. To the best of our knowledge, we are the first to utilize deep learning to tackle such a problem. In such a deep learning framework, double supervisions: points on the symmetry plane and normal vectors are employed to help us pinpoint the symmetry plane. We conducted experiments on the YCBvideo dataset and demonstrate the efficacy of our method. Our code will be available soon after on GitHub.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the human visual system, our brains are very skilled at finding and utilizing symmetry of objects as abstraction entails finding intrinsic logic of shape. Such an assumption is legitimate, given the ubiquity of symmetry in humanmade merchandise.</p><p>Additionally, symmetry offers a critical hint when it comes to object completion in occlusion. Completion of such is conducive to object detection and makes it more robust in cluttered scenarios. Despite all these advantages in mind, the status quo is far from satisfactory. Most works on symmetry are built on top of classical methodology, including curvature, ICP (Iterative Closest Point), and others. None had dealt with deep-learning oriented symmetry detection on occluded point cloud before, save for PRS-Net, which aimed to predict symmetry plane for the mesh-based 3D objects with deep-learning techniques.</p><p>Unlike PRS-Net with exact three symmetry predictions, our algorithm can deal with an indefinite amount of symmetries as we base on double supervisions and RANSAC (random sample consensus). In our work, we are trying to locate the symmetry plane by two elements: the points on the plane and the normal vector of the plane. These are the very two elements that determine where a plane is.</p><p>Two stages of training are employed in order to get our model well-trained. In the first stage, the primary goal is to let the model learn the whereabouts of those points. The second stage begins when we ascertain the stability of point predictions. In the second stage, the model learns to predict normal vectors concerning those points. After these two stages, we can extrapolate the location of the symmetry plane using RANSAC.</p><p>Actually, by introducing RANSAC into the algorithm, we successfully do away with the restriction of fixation on the number of predictions. Of course, we are not saying our algorithm is robust enough. For one, our method relies on the presence of points on symmetry planes. Further works on more robust algorithms may well become available after this paper.</p><p>This paper mainly has three contributions:</p><p>symmetry as long as points on the symmetry plane are present in the observer's view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Attributed to a two-step framework and RANSAC, our algorithm allows flexible numbers of symmetry to be learned and detected under the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A new accuracy measurement for symmetry detection is designed, and we elaborate on how PR-curve can be drawn in our scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Symmetry Detection</head><p>Both 2D and 3D geometry have a long history of symmetry detection. Generally, there are four types of symmetry problems, two types on two dimensions: extrinsic and intrinsic, global and partial.</p><p>Global and partial symmetry differ on the scale of symmetry; the former requires the symmetry to map the entire object to itself while the latter only requires part of the object. Extrinsic and intrinsic symmetry differ on metrics of the symmetry; the previous measures symmetry on natural Euclidean space and the other measures on different metric spaces. In this paper, we are aiming to solve extrinsic global symmetry detection, especially for reflective symmetry detection.</p><p>As far as extrinsic global symmetry concerns, various works have made progress. Some researchers have made adaptions to the original problem. Zabrodsky et al. <ref type="bibr" target="#b0">1</ref> introduce the concept of approximate symmetry to tackle the problem. <ref type="bibr">Raviv et al. 2</ref> adapt the symmetries for non-rigid shapes and Kazhdan et al. <ref type="bibr" target="#b2">3</ref> offer to solve n-fold rotational symmetries. Others use mapping to solve the problem. <ref type="bibr">Podolak et al. 4</ref> use PRST (planar reflective symmetry transform) to detect symmetry. <ref type="bibr">Mitra et al. 6</ref> also use transformation to extract constant symmetry features and clustering to detect symmetries. However, most of these works are aimed to tackle 2D detection.</p><p>Other works can handle 3D symmetry detections. One of the closest is the work by Aleksandrs <ref type="bibr">Ecins et al. 5</ref> where cluttered objects are segmented by help of recursively detecting symmetries and segmenting. We will compare the effect of this paper with our paper in the experiments. PRS-Net 7 is the first one to deal with 3D data with deep learning methods; however, unlike our paper, it is not designed to adapt to cluttered objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Point Cloud Deep Representation</head><p>3D cases are quite different from 2D images. The geometry and depth information need to be absorbed in conquering symmetry detection. The complexity of 3D geometry representation probably explains the diversity when it comes to dealing with symmetry detections. PRS-Net 7 deals with mesh-based 3D models. Paper 5 by Ecins uses point clouds; however, it also requires occlusion map 9 . Our paper does not require extra information other than RGB-D images. The framework is heavily dependent on the DenseFusion 8 framework. DenseFusion extracts point cloud information from multiple scales by leveraging both color and geometry information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Symmetry Detection Framework Using Double Supervision</head><p>Customarily, when we predict reflectional symmetry, we are anticipating a point on the plane and the normal vector. In this method, we conquer the reflection symmetry detection of occluded point cloud by leveraging two point-wise predictions. First, we learn to predict the on-plane confidence, whether a point is on the symmetry plane. Meanwhile, point-wise normal vectors are predicted for on-plane points. Since the location of each on-plane point combined with its normal vector prediction uniquely decides the symmetry plane, we should fuse all these symmetry plane predictions of on-plane points into our final symmetry predictions. In this case, we employ RANSAC to fuse point-wise predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>In our framework, as shown in <ref type="figure">Fig. 1</ref>, we adopted the DenseFusion framework as our backbone in geometric and color embedding extraction. In the first stage, we use the YCB ground-truth segmentation label to segment the object out on the photo. The segmented object is then projected onto the 3D space. Apparently, these projections may be occluded, even heavily occluded. These projected points are passed into the network, and CNN (convolutional neural network) is employed to extract color embeddings while resorting to PointNet 10 for geometric embeddings. These pixel-wise features are then fused and concatenated together, leveraged to generate on-plane confidence and pointwise normal vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>To successfully train the model, we first let the point learn if it is on the symmetry plane. Those points that are closer than ϵ " to the symmetry plane are regarded as on-plane points. Those on-plane points should be labeled as 1, while off-plane points should be labeled as 0. We adopt cross-entropy loss here:</p><p>where ( ) is the on-plane confidence prediction for the point , ( ) is the on-plane point sets for symmetry plane , i.e., those points within distance " of and are all ground-truth symmetry planes for an occluded point cloud. After some steps, some points start to have higher on-plane confidence prediction, then we are letting those points learn normal vectors directions. Compared to on-plane prediction, we loosen up the distance from " to -. The loss for learning normal vectors is 1 loss between the prediction and the ground truth, where ( ) is the normal vector prediction for point , 12 (S) is the ground-truth normal vector of plane , ′( ) are those points within distanceof and are all ground-truth symmetry planes for an occluded point cloud. So the training loss, in conclusion, is the following, and P 56 = 0.6, = 10 in our training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Testing</head><p>Now that we are done with the training, now we have to fuse all symmetry plane predictions into final predictions as we briefed at the beginning of the section. We devise a three-step RANSAC in our inference stage:</p><p>1.</p><p>We pick ten random points out of the point cloud and settle on the one point with the proposal sweeping across the maximum number of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>According to the proposal of point , we sift through those points with similar proposals ( 1 distance " ) and take them out to prepare a unanimous vote. We denote the set as ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Initialized with the proposal of point , an iterative optimizer is employed to generate a proposal that covers the maximum number of points. Besides the proposal, we also provide a confidence score calculated from the percentage of points covered in ( ). After getting the proposal and the confidence score, points with similar proposals ( 1 distance -, we usually set -&gt; " ) in the point cloud are scrubbed.</p><p>The three-step RANSAC is executed until there are insufficient number of points to feed into. Specifically, " = 0.6 and -= 1.3. Through the procedure, we are able to generate some symmetry predictions and confidence scores, indicating how sure the algorithm is about the correctness of these predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first conduct quantitative experiments comparing our performance on objects suffering from different degrees of occlusion with the state-of-the-art work from Ecins 5 . Next up, we show some of the galleries demonstrating prediction accuracies visually. All our experiments are conducted on the YCB-video dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Accuracy Experiments</head><p>We start by introducing our way of accuracy measurement. In our experiments, we use PR(precision-recall) curve to demonstrate the efficacy of our algorithm. Recall that precision = TP/(TP + FP) and recall = TP/(TP + FN). First off, let us expound the definition of true positives, false positives, and false negatives. We say a prediction (c " , " WWWW⃗) is within the correctness threshold of a ground-truth symmetry plane (c -, -WWWW⃗) if and only if two planes are close enough: (c -− c " ) -WWWW⃗ ≤ d 56 and have similar directions: ∠( " WWWW⃗, -WWWW⃗) ≤ angle 56 . The variable here for the PR curve is the confidence threshold, separating open predictions (those above) and suppressed predictions (those under).</p><p>True positives(TP) are those predictions within the correctness threshold and are confident enough (above the confidence threshold). False positives(FP) are predictions out of the correctness threshold but still have higher confidence than the confidence threshold. As for predictions within the correctness threshold but below the confidence threshold, they are regarded as false negatives(FN). If there is a TP for a ground truth symmetry plane, then other predictions are ignored and are not considered as either FP or FN.</p><p>With the above definition in mind, we are going to break down accuracy performance according to two standards: intersection circumference and degree of occlusion. The degree of occlusion is the percentage of the occluded surface as opposed to the entire surface, including self-occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. PR curve comparison between ours(top) and Ecins 5 (bottom) under different intersection circumferences</head><p>In <ref type="figure">Fig. 2</ref>, we show how our algorithm fares compared to Ecins 5 in different intersection circumferences. The thresholds are set as d 56 = 0.01,0.02, … , 0.05 and angle 56 = 20°. Intersection circumferences refer to the intersection between the symmetry plane and the object surface, i.e., the line on the surface that cut the object in half. Intuitively, we find for Ecins 5 , which based on existing pixel correspondence, longer intersection circumferences mean we are likely able to find more correspondence, as shown in the bottom line in <ref type="figure" target="#fig_1">Fig. 4</ref>. That may explain why Ecins 5 has a sudden boost in performance in cases when intersection circumferences are longer than 15cm. As for cases where intersection circumferences are shorter than 15cm, the accuracy of Ecins 5 suffers considerably as opposed to our method.</p><p>The high accuracy of our algorithm could be led by the fact that in heavy occlusion, Ecins 5 , which heavily relies on geometry information, is more susceptible to shape changes. Our algorithm, using deep learning, has acquired the capability to distinguish symmetry planes from the clutter.  In <ref type="table" target="#tab_0">Table 1</ref>, we see how our algorithm performs under dist 56 = 0.01 and angle 56 = 20°. It concretely demonstrates that our algorithm has gained robustness towards reflective symmetry detection in occlusion in all categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Prediction Visualization</head><p>Now let us see how well our algorithm has dealt with each degree of occlusion split: under 60%, from 60% to 80% and above 80%. To get a clearer view of what happens in prediction, we need to take a look at <ref type="figure" target="#fig_1">Fig. 4</ref>. As we explained above, an RGB-D image is first segmented from a YCB-video frame and projected into 3D space as occluded point clouds. Through our later deep-learning framework, we are able to predict normal vectors for those on-plane points. Those highlighted points on the leftmost column are normal vector predictions by our algorithm. We have seen highlighted points align with ground truth planes, and the color of those points, normal vectors indicating different directions, is following unanimous predictions in each direction.</p><p>It is worth noted that for those symmetry planes with longer intersection circumferences like the bottom row, it is more likely the points on the symmetry plane are missing out, causing lower accuracy, as we demonstrated in <ref type="figure">Figure  2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a deep-learning framework to settle symmetry detection in a 3D scenario, specifically on point clouds. Objects may be cluttered, some heavily occluded, which are quite tricky for correspondence-based algorithms like Ecins5. Our framework has been proven by far superior to Ecins. When we divide the YCB-video dataset by degrees of occlusion, we found our framework excels both in recall rate and precision rate, meaning we can retrieve more symmetry planes and more accurately.</p><p>As far as we know, we are the first to leverage deep-learning techniques to tackle reflective symmetry on occluded 3D objects. Although our algorithm does require that at least part of the symmetry plane not occluded, it could inspire fellow researchers to drive toward a path where more robust and efficient deep-learning algorithms lie.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>PR curve comparison between ours(top) and Ecins 5 (bottom) under different degrees of occlusion Fig. 3 offers us another angle to compare our algorithm and Ecins. Under whatever degrees of occlusion, our algorithm surpasses Ecins in both recall rate and precision. Especially when it comes to large occlusions, our algorithm shows persistent accuracy and robustness in detecting symmetries from tiny unoccluded pieces. Thus, it is proper to say that deep learning could serve as a great aid in detecting symmetries in occluded scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of our predictions under different degrees of occlusion (Better viewed in PDF and zoomed in)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Precision boost when dist 56 = 0.01. (Values are arg max (prec × recall))</figDesc><table><row><cell>Degree of Occ.</cell><cell>Precision(Ecins)</cell><cell>Precision(Ours)</cell><cell>Recall(Ecins)</cell><cell>Recall(Ours)</cell><cell cols="2">P × R(Ecins) P × R(Ours)</cell></row><row><cell>Under 60%</cell><cell>0.16</cell><cell>0.91</cell><cell>0.08</cell><cell>0.38</cell><cell>0.013</cell><cell>0.346</cell></row><row><cell>60% to 80%</cell><cell>0.09</cell><cell>0.80</cell><cell>0.04</cell><cell>0.32</cell><cell>0.004</cell><cell>0.256</cell></row><row><cell>Above 80%</cell><cell>0.06</cell><cell>0.86</cell><cell>0.04</cell><cell>0.60</cell><cell>0.002</cell><cell>0.516</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. We propose a novel deep-learning symmetry detection framework for 3D models. Our model is the first effective model to deal with the occluded point cloud. By using double supervisions, we can detect the</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Symmetry as a continuous feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zabrodsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Avnir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1154" to="1166" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Full and partial symmetries of non-rigid shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">D</forename><surname>Raviv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="39" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A reflective symmetry descriptor for 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="201" to="225" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A planar-reflective symmetry transform for 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">J</forename><surname>Podolak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shilane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="549" to="559" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting reflectional symmetries in 3d data through symmetrical fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ecins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fermuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1779" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Partial and approximate symmetry detection for 3D geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<idno type="arXiv">arXiv:1910.06511</idno>
		<title level="m">PRS-Net: Planar Reflective Symmetry Detection Net for 3D Models</title>
		<imprint/>
	</monogr>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">OctoMap: An efficient probabilistic 3D mapping framework based on octrees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Wurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous robots</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
