<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-12">12 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
							<email>andor@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
							<email>luheng@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
							<email>kentonl@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
							<email>epitler@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-12">12 Sep 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33% absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end reading comprehension models have been increasingly successful at extractive question answering. For example, performance on the SQuAD 2.0 <ref type="bibr" target="#b4">(Rajpurkar et al., 2018)</ref> benchmark has improved from 66.3 F1 to 89.5 1 in a single year. However, the Discrete Reasoning Over Passages (DROP) <ref type="bibr" target="#b1">(Dua et al., 2019)</ref> dataset demonstrates that as long as there is quantitative reasoning involved, there are plenty of relatively straightforward questions that current extractive QA systems find difficult to answer. Other recent work has shown that even state-of-the-art neural models struggle with numerical operations and quantitative reasoning when trained in an end-to-end manner <ref type="bibr">(Saxton et al., 2019;</ref><ref type="bibr" target="#b5">Ravichander et al., 2019)</ref>. In other words, even BERT <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref> is not very good at doing simple calculations. The correct answer is not explicitly stated in the passage and instead must be computed. The NAQANet model 2 <ref type="bibr" target="#b1">(Dua et al., 2019)</ref> predicts a negative number of people, whereas our model predicts that an operation Diff should be taken and identifies the two arguments.</p><p>In this work, we extend an extractive QA system with numerical reasoning abilities. We do so by asking the neural network to synthesize small programs that can be executed. The model picks among simple programs of the form Operation(args, ...), where the possible operations include span extraction, answering yes or no, and arithmetic. For math operations, the arguments are pointers to numbers in the text and, in the case of composition, other operations. In this way, the burden of actually doing the computation is offloaded from the neural network to a calculator tool. The program additionally provides a thin layer of interpretability that mirrors some of the reasoning required for the answer. For example, in <ref type="table" target="#tab_0">Table 1</ref>, the model predicts subtraction (Diff) over two numbers in the passage, and executes it to produce the final answer.</p><p>We start with a simple extractive question answering model based on BERT <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref>, and show the following:</p><p>1. Predicting unary and binary math operations with arguments resulted in significant improvements on the DROP dataset.  <ref type="bibr" target="#b4">(Rajpurkar et al., 2018)</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the system needs to perform fuzzy matching between "from Europe" and "European nationals" in order to identify the arguments.</p><p>Numerically-aware QANet (NAQANet) <ref type="bibr" target="#b1">(Dua et al., 2019)</ref> is the current state-of-the-art 3 system for DROP. It extends the QANet model <ref type="bibr">(Yu et al., 2018)</ref> with predictions for numbers (0-9) and summation operations. For the latter, it performs a 3-way classification (plus, minus, and zero) on all the numbers in the passage.</p><p>While certain binary operations are expressible efficiently with flat sign prediction, it is difficult to generalize the architecture. Moreover, each number is tagged independently, which can cause global inconsistencies; for instance, in <ref type="table" target="#tab_0">Table 1</ref> it assigns a single minus label and no plus labels, leading to a prediction of negative people.</p><p>Mathematical Word Problems have been addressed with a wide variety of datasets and approaches; see <ref type="bibr">Zhang et al. (2018)</ref> for an overview. One such dataset of arithmetic problems is the Illinois dataset . The problems are posed in simple natural language that has a specific, narrow domain, For example: "If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how many bottle caps are in the box?". Unlike DROP, the problems are typically 1-3 sentences long and do not require read-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We extend a BERT-based extractive reading comprehension model with a lightweight extraction and composition layer. For details of the BERT architecture see <ref type="bibr" target="#b0">Devlin et al. (2019)</ref>. We only rely on the representation of individual tokens that are jointly conditioned on the given question Q and passage P . Our model predicts an answer by selecting the top-scoring derivation (i.e. program) and executing it.</p><p>Derivations We define the space of possible derivations D as follows:</p><p>• Literals: {YES, NO, UNKNOWN, 0, . . . 9}.</p><p>• Numerical operations: including various types of numerical compositions of numbers 4 , such as Sum or Diff. • Text spans: composition of tokens into text spans up to a pre-specified length. • Composition of compositions: we only consider two-step compositions, including merging text spans and nested summations. The full set of operations are listed in <ref type="table" target="#tab_3">Table 2</ref>. For example, Sum is a numerical operation that adds two numbers and produces a new number. While we could recursively search for compositions with deep derivations, here we are guided by what is required in the DROP data and simplify inference by heavily restricting multi-step composition. Specifically, spans can be composed into a pair of merged spans (Merge), and the sum of two numbers (Sum) can subsequently be summed with a third (Sum3). The results in <ref type="table" target="#tab_5">Table 3</ref> show the dev set oracle performance using these shallow derivations, by answer type.</p><p>Representation and Scoring For each derivation d ∈ D, we compute a vector representation h d and a scalar score ρ(d, P, Q) using the BERT output vectors. The scores ρ are used for computing the probability P (d | P, Q) as well as for pruning. For brevity, we will drop the dependence on P and Q in this section.</p><p>Literals are scored as "Japanese"</p><formula xml:id="formula_0">ρ(d) = w ⊺ d MLP lit (h CLS ),</formula><formula xml:id="formula_1">Compositions Merge : s0, s1 → {s0, s1}</formula><p>What languages are spoken by more than 1%, but fewer than 2% of Richmond's residents?</p><p>"Hmong-Mien languages", "Laotian" Sum3 : n0, n1, n2 → (n0 + n1) + n2 How many residents, in terms of percentage, speak either English, Spanish, or Tagalog?</p><p>Sum(64.56, 23.13)+ 2.11 = 89.8 where h CLS is the output vector at the [CLS] token of the BERT model <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref>.</p><p>Numeric operations use the vector representations h i of the first token of each numeric argument. Binary operations are represented as</p><formula xml:id="formula_2">h d = MLP binary (h i , h j , h i • h j )<label>(1)</label></formula><p>and scored as ρ(d) = w ⊺ op h d , where h d represents the binary arguments and op is the operation type.</p><p>• is the Hadamard product. Unary operations such as Diff100 are scored as w ⊺ op MLP unary (h i ). Text spans are scored as if they were another binary operation taking as arguments the start and end indices i and j of the span <ref type="bibr" target="#b2">(Lee et al., 2017)</ref>:</p><formula xml:id="formula_3">h d = MLP span (h i , h j )<label>(2)</label></formula><p>and scored as ρ(d) = w ⊺ span h d . Compositions of compositions are scored with the vector representations of its children. For example, the ternary Sum3, comprising a Sum and a number, is scored with w ⊺ Sum3 MLP Sum3 (h d0 , h k ), where h d0 corresponds to the representation from the first Sum, and h k is the representation of the third number. The composition of two spans is scored as</p><formula xml:id="formula_4">w ⊺ Merge MLP Merge (h d0 , h d1 , h d0 •h d1 ),</formula><p>where h d0 and h d1 are span representations from (2). The intuition for including h d0 • h d1 is that it encodes span similarity, and spans with similar types are more likely to be merged.</p><p>This strategy differs from the NAQANet baseline in a few ways. One straightforward difference is that we use BERT as the base encoder rather than QANet. A more meaningful difference is that we model all derivations in the unified op scoring framework described above, which allows generalizing to new operations, whereas NAQANet would require more large-scale changes to go beyond addition and subtraction. Generalizing the model to new ops is a case of extending the derivations and scoring functions. In Section 4, we will show the impact of incrementally adding Diff100, Sum3, and Merge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training</head><p>We used exhaustive pre-computed oracle derivations D * following <ref type="bibr" target="#b1">Dua et al. (2019)</ref>.</p><p>We marginalized out all derivations d * that lead to the answer 5 and minimized:</p><formula xml:id="formula_5">J (P, Q, D * ) = − log d * ∈D * P (d * | P, Q) P (d | P, Q) = exp ρ(d, P, Q) d ′ exp ρ(d ′ , P, Q)</formula><p>If no derivation lead to the gold answer (D * is empty), we skipped the example.  had two or more alternatives. During training, the model became effective at resolving many of these ambiguities. We monitored the entropy of P (d * | P, Q) for the ambiguous examples as training progressed. At the start, the entropy was 2.5 bits, which matches the average ambiguous oracle length of ∼ 6 alternatives. By the end of 4 epochs, the average entropy had dropped to &lt; 0.2 bits, comparable to a typical certainty of 95-99% that one of the derivations is the correct one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our main experiments pertain to DROP <ref type="bibr" target="#b1">(Dua et al., 2019)</ref>, using DROP and, optionally, CoQA <ref type="bibr" target="#b6">(Reddy et al., 2018)</ref> data for training. Pre-processing and hyperparameter details are given in the supplementary material. In addition to full DROP results, we performed ablation experiments for the incremental addition of the Diff100, Sum3, and Merge operations, and finally the CoQA training data. We ran on the CoQA dev set, to show that the model co-trained on CoQA can still perform traditional reading comprehension. To investigate our model's ability to do symbolic reasoning at the other extreme, we performed few-shot learning experiments on the Illinois dataset of math problems .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DROP Results</head><p>As shown in  <ref type="table" target="#tab_5">Table 3</ref>. After fine-tuning on DROP, the model forgot how to do CoQA, with an overall F1 score of 52.2 on the CoQA dev set. If one prefers a model competent in both types of input, then the forgetting can be prevented by fine-tuning on both CoQA and DROP datasets simultaneously. This resulted in dev set F1 scores of 82.2 on CoQA and 81.1 on DROP. The CoQA performance is decent and compares well with the pre-trained model performance of 82.5. The 0.5% drop in DROP performance is likely attributable to the difference between pre-training versus fine-tuning on CoQA.</p><p>We ensembled 6 models (3 seeds × 2 learning rates) for an additional 1% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Math Word Problems</head><p>We trained our model on the Illinois math word problems dataset , which contains answers requiring multiplication and division-operations not present in DROP-as    well as addition and subtraction, in roughly equal proportion. Given the small (N = 562) dataset size, training and evaluation is done with five-fold cross-validation on a standardized set of splits. As shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We proposed using BERT for reading comprehension combined with lightweight neural modules for computation in order to smoothly handle both traditional factoid question answering and questions requiring symbolic reasoning in a single unified model. On the DROP dataset, which includes a mix of reading comprehension and numerical reasoning, our model achieves a 33% absolute improvement over the previous best. The same model can also do standard reading comprehension on CoQA, and focused numerical reasoning on math word problems. We plan to generalize this model to more complex and compositional answers, with better searching and pruning strategies of the derivations. After processing the input with the standard BERT tokenizer, we extracted the locations and values of numbers. We allowed for up to 128 numbers per document, with typical documents having 10-20. The maximum length of spans is set to 32. Documents longer than 512 tokens are split up.</p><p>We use a whole-word masked version of BERT similar to <ref type="bibr" target="#b9">Sun et al. (2019)</ref>. Unless otherwise indicated, we fine-tuned BERT LARGE with a batch size of 32 over a small grid of hyperparameters: We varied only the learning rate in the range 2e−5 to 5e−5 and the number of epochs between 1-5. We did random restarts with 2-4 random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Examples of Wins and Losses</head><p>We selected a few examples of the wins and losses of our model in <ref type="table" target="#tab_0">Table 1</ref>, 2, and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diff100 Question</head><p>How many percent of people were not Hispanic? Passage</p><p>The <ref type="formula" target="#formula_2">2010</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sum</head><p>Question How many millions of people in all did Germany and France have as residents that were born outside the EU? Passage</p><p>In 2010, 47.3 million people who lived in the EU were born outside their resident country. This corresponds to 9.4% of the total EU population. Of these, 31.4 million (6.3%) were born outside the EU and 16.0 million (3.2%) were born in another EU member state. The largest absolute numbers of people born outside the EU were in Germany (6.4 million), France (5.1 million), the United Kingdom (4.7 million), Spain (4.1 million), Italy (3.2 million), and the Netherlands (1.4 million).</p><p>Answer 11.5 Prediction Sum(6.4, 5.1) = 6.4 + 5.1 = 11.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sum3</head><p>Question How many people, households, and families reside in the county according to the 2000 census? Passage</p><p>As of the census of 2000, there were 40,543 people, 15,416 households, and 11,068 families residing in the county. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diff</head><p>Question How many years after the king's brothers raised a rebellion did Ava cede all northern Avan territory down to present-day Shwebo to Mohnyin? Passage</p><p>Ava's authority deteriorated further in Shwenankyawshin's reign . Three of the king's own brothers openly raised a rebellion in 1501. Mohnyin, Ava's former vassal, now began to raid its territory. In 1507, Ava ceded to Mohnyin all northern Avan territory down to present-day Shwebo in the vain hope that the raids would stop. It did not. Ava desperately tried to retain Toungoo's loyalty by ceding the key Kyaukse granary to Toungoo but it too failed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merge Question</head><p>What professionals paid higher rates than the advocates? Passage</p><p>The poll tax was resurrected during the 17th century, usually related to a military emergency. It was imposed by Charles I of England in 1641 to finance the raising of the army against the Scottish and Irish uprisings. With the Restoration (England) of Charles II of England in 1660, the Convention Parliament (1660) instituted a poll tax to finance the disbanding of the New Model Army (pay arrears, etc.) (12 Charles II c.9). The poll tax was assessed according to "rank", e.g. dukes paid 100, earls 60, knights 20, esquires 10. Eldest sons paid 2/3rds of their fathers rank, widows paid a third of their late husbands rank. The members of the livery companies paid according to companys rank (e.g. masters of first-tier guilds like the Mercers paid 10, whereas masters of fifth-tier guilds, like the Clerks, paid 5 shillings). Professionals also paid differing rates, e.g. physicians (10), judges (20), advocates (5), attorneys (3), and so on. Anyone with property (land, etc.) paid 40 shillings per 100 earned, anyone over the age of 16 and unmarried paid 12-pence and everyone else over 16 paid 6-pence.</p><p>Answer "physicians", "judges" Prediction Merge(physicians, judges) = "physicians", "judges" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>How many yards was the shortest touchdown run?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Passage</head><p>The Steelers went back home for another showdown with the Patriots. This game is notable for being the very first game that QB Ben Roethlisberger would miss out on against the Patriots. In the first quarter, The Patriots scored first when Tom Brady found James White on a 19-yard touchdown pass for a 7-0 lead for the only score of the period. In the second quarter, they increased their lead when LaGarrette Blount ran for a 3-yard touchdown to make it 14-0. The Steelers got on the board later on in the quarter when Landry Jones found Darrius Heyward-Bey on a 14-yard touchdown pass for a 14-7 game. The Steelers closed out the scoring of the first half when Chris Boswell kicked a 32-yard field goal for a 14-10 game at halftime. In the third quarter, the Steelers went back to work as Boswell kicked another field goal to get his team within 1, 14-13 from 46 yards out. The Pats pulled away later on when Brady found Rob Gronkowski on a 36-yard touchdown pass (with a failed PAT) for a 20-13 game. In the fourth quarter, the Steelers came within 4 again when Boswell made a 44-yard field goal for a 20-16 game. But the Pats sealed the game after Blount ran for a 5-yard touchdown and the eventual final score of 27-16. With the loss, the Steelers went into their bye week at 4-3. Regardless, due to the Ravens' loss to the Jets, they still remain in first place in the AFC North. The team dropped to 0-1 on the season without Roethlisberger as a starter and their seven-game home winning streak was snapped.</p><p>Answer "3" (Span) Prediction Sum(3, 0) = 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrong type</head><p>Question How many points did the Lions score in the first half? Passage</p><p>For their annual Thanksgiving Day game, the Lions hosted a rematch with their divisional rival, the Minnesota Vikings. The Vikings scored 13 points in the first quarter via a oneyard touchdown pass from Case Keenum to Kyle Rudolph, and a nine-yard touchdown run from Keenum. The Lions responded with 10 points in the second quarter via a 32yard field goal from Matt Prater and a six-yard touchdown pass from Matthew Stafford to Marvin Jones Jr. The Vikings extended their lead in the second quarter via a 22-yard touchdown pass from Keenum to Rudolph to make the score 20-10 in favor of Minnesota at half-time. The Vikings opened the scoring in the second half via a two-yard touchdown run from Latavius Murray. The Lions responded with two field goals from Prater in the third quarter from 32-yards, and 50-yards, respectively. The Lions reduced the Vikings lead to four points in the fourth via a 43-yard touchdown pass from Stafford to Jones. The Vikings extended their lead in the fourth quarter via a 36-yard field goal from Kai Forbath. The Lions' attempted comeback failed when Stafford's pass intended for Jones was intercepted by Xavier Rhodes. On the Vikings' ensuing drive, Forbath's 25-yard field goal attempt was blocked by Darius Slay and recovered by Nevin Lawson and returned for a 77-yard touchdown, which was then nullified due to an offside penalty on Slay, making the final score 30-23 in favor of Minnesota, snapping the Lions' three-game winning streak.</p><p>Answer "10" (Span) Prediction 7 (Literal) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example from the DROP development set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2. Our model can smoothly handle more traditional reading comprehension inputs as well as math problems with new operations. Cotraining with the CoQA<ref type="bibr" target="#b6">(Reddy et al., 2018)</ref> dataset improved performance on DROP.</figDesc><table><row><cell>The</cell></row><row><cell>DROP+CoQA trained model had never seen</cell></row><row><cell>multiplication or division examples, but can</cell></row><row><cell>learn to predict these two ops when ap-</cell></row><row><cell>propriate in a math word problem setting</cell></row><row><cell>(Roy and Roth, 2015) with very few training</cell></row><row><cell>examples.</cell></row><row><cell>2 Background and Related Work</cell></row><row><cell>Discrete Reasoning over Paragraphs (DROP)</cell></row><row><cell>(Dua et al., 2019) is a reading comprehension task</cell></row><row><cell>that requires discrete reasoning. Inspired by se-</cell></row><row><cell>mantic parsing tasks where models need to pro-</cell></row><row><cell>duce executable 'programs', it keeps the open-</cell></row><row><cell>domain nature of reading comprehension tasks</cell></row><row><cell>such as SQuAD 2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Operations supported by the model. s, n refer to arguments of type span and number, respectively. i, j are the start and end indices of span s. The omitted definitions of Diff, Mul, and Div are analogous to Sum.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Pruning During inference, the Merge and Sum3 operations are composed from the results of Span and Sum operations, respectively. The space of possible results of Merge is quadratic in the number |S| of possible spans. With |S| ∼ 10 4 , the complete set of Merge instances becomes overwhelming. Similarly, with |N | ∼ 100 numbers in each passage, there are millions of possible Sum3 derivations. To do training and inference efficiently, we kept only the top 128 Span and Sum results when computing Merge and Sum3. 6Spurious ambiguities Of the answers for which we could find at least one oracle derivation, 36%</figDesc><table><row><cell></cell><cell>Oracle</cell><cell cols="2">Overall Dev</cell><cell cols="2">Overall Test</cell><cell cols="8">Date (1.6%) Number (62%) Span (32%) Spans (4.4%)</cell></row><row><cell></cell><cell>Dev EM</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>NAQANet</cell><cell></cell><cell cols="7">46.75 50.39 44.24 47.77 32.0 39.6 44.9</cell><cell>45.0</cell><cell cols="2">58.2 64.8</cell><cell>0.0</cell><cell>27.3</cell></row><row><cell>Our basic 7</cell><cell>80.03</cell><cell cols="2">66.50 69.91</cell><cell>-</cell><cell>-</cell><cell cols="3">57.0 65.1 65.8</cell><cell>66.1</cell><cell cols="2">78.0 82.6</cell><cell>0.0</cell><cell>35.7</cell></row><row><cell>+Diff100</cell><cell>88.75</cell><cell cols="2">75.52 78.82</cell><cell>-</cell><cell>-</cell><cell cols="3">53.6 61.3 80.3</cell><cell>80.5</cell><cell cols="2">78.4 82.8</cell><cell>0.0</cell><cell>35.8</cell></row><row><cell>+Sum3</cell><cell>90.16</cell><cell cols="2">76.70 80.06</cell><cell>-</cell><cell>-</cell><cell cols="3">58.0 64.6 81.9</cell><cell>82.1</cell><cell cols="2">78.9 83.4</cell><cell>0.0</cell><cell>36.0</cell></row><row><cell>+Merge</cell><cell>93.01</cell><cell cols="2">76.95 80.48</cell><cell>-</cell><cell>-</cell><cell cols="3">58.1 61.8 82.0</cell><cell>82.1</cell><cell cols="2">78.8 83.4</cell><cell>5.1</cell><cell>45.0</cell></row><row><cell>+CoQA</cell><cell>93.01</cell><cell cols="7">78.09 81.65 76.96 80.53 59.5 66.4 83.1</cell><cell>83.3</cell><cell cols="2">79.8 84.3</cell><cell>6.2</cell><cell>47.0</cell></row><row><cell>+Ensemble</cell><cell>93.01</cell><cell cols="7">78.97 82.56 78.14 81.78 59.7 67.7 83.9</cell><cell>84.1</cell><cell cols="2">81.1 85.4</cell><cell>6.0</cell><cell>47.0</cell></row><row><cell>Oracle</cell><cell></cell><cell>93.01</cell><cell></cell><cell></cell><cell></cell><cell>71.6</cell><cell></cell><cell>94.5</cell><cell></cell><cell>95.8</cell><cell></cell><cell>60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Accuracies on the DROP dev and test set in terms of exact match (EM) and token-level F1. The righthand columns show the performance breakdown with different answer types on the development set. The largest improvements come from Date, Number, and Spans (answers with multiple spans). Oracle rows and columns indicate the performance that could be achieved by perfect selections of derivations. The ensemble used 6 models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table /><note>, our model achieves over 50% relative improvement (over 33% absolute) over the previous state-of-the-art NAQANet sys- tem. The ablations indicate that the improvements due to the addition of extra ops (Diff100, Sum3, Merge) are roughly consistent with their propor- tion in the data. Specifically, the Diff100 and Sum3 derivations increase the oracle performance by 8.7% and 1.4% respectively, corresponding to model improvements of roughly 9% and 1.1%, re- spectively. Answers requiring two spans occur about 2.8% of the time, which is a 60.4% propor- tion of the Spans answer type. Merge only im- proves the Spans answer type by 9%, which we think is due to the significant 11:1 class imbalance between competing single and multiple spans. As a result, multiple spans are under-predicted, leav- ing considerable headroom there. Pre-training on CoQA then fine-tuning on DROP lead to our best results on DROP, reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on the Illinois (IL) dataset 8 of 562 single-step word problems, using the five crossvalidation folds of. Standard deviations were computed from the five folds. Roughly half the questions require the use of Sum and Diff, and half require Mul and Div.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>, when we added Mul and Div to our basic DROP operations, the model was able to learn to use them. Transferring from the DROP dataset further improved performance beyond that of Liang et al. (2016), a model spe- cific to math word problems that uses rules over dependency trees. Compared to other more gen- eral systems, our model outperforms the deep rein- forcement learning based approach of Wang et al. (2018).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Dongxiang Zhang, Lei Wang, Nuo Xu, Bing Tian Dai, and Heng Tao Shen. 2018. The gap of semantic parsing: A survey on automatic math word problem solvers. IEEE transactions on pattern analysis and machine intelligence.</figDesc><table><row><cell>APPENDIX for: Giving BERT a Calculator: Finding Operations and</cell></row><row><cell>Arguments with Reading Comprehension</cell></row><row><cell>Daniel Andor, Luheng He, Kenton Lee, Emily Pitler</cell></row><row><cell>Google Research</cell></row><row><cell>{andor, luheng, kentonl, epitler}@google.com</cell></row><row><cell>A Pre-processing and Hyper-parameters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>United States Census reported that Lassen County had a population of 34,895. The racial makeup of Lassen County was 25,532 (73.2%) White (U.S. Census), 2,834 (8.1%) African American (U.S. Census), 1,234 (3.5%) Native American (U.S.</figDesc><table><row><cell></cell><cell>Census), 356 (1.0%)</cell></row><row><cell></cell><cell>Asian (U.S. Census), 165 (0.5%) Pacific Islander (U.S. Census), 3,562 (10.2%) from Race</cell></row><row><cell></cell><cell>(United States Census), and 1,212 (3.5%) from two or more races. Hispanic (U.S. Census) or</cell></row><row><cell></cell><cell>Latino (U.S. Census) of any race were 6,117 persons (17.5%).</cell></row><row><cell>Answer</cell><cell>82.5</cell></row><row><cell cols="2">Prediction Diff100(17.5) = 100 − 17.5 = 82.5</cell></row><row><cell>Question</cell><cell>How many percent were not from 18 to 24?</cell></row><row><cell>Passage</cell><cell>In the city, the population was spread out with 12.0% under the age of 18, 55.2% from 18 to</cell></row><row><cell></cell><cell>24, 15.3% from 25 to 44, 10.3% from 45 to 64, and 7.1% who were 65 years of age or older.</cell></row><row><cell></cell><cell>The median age was 22 years. For every 100 females, there were 160.7 males. For every 100</cell></row><row><cell></cell><cell>females age 18 and over, there were 173.2 males.</cell></row><row><cell>Answer</cell><cell>44.8</cell></row><row><cell cols="2">Prediction Diff100(55.2) = 100 − 55.2 = 44.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>66% of the population were Race (United States Census) or Race (United States Census) of any race. 29.3% were of united states, 22.2% germans, 12.1% english people and 10.9% irish people ancestry according to 2000 United States Census.</figDesc><table><row><cell>The population density was 99 people per square mile (38/km 2 ).</cell></row><row><cell>There were 16,577 housing units at an average density of 40 per square mile (16/km 2 ). The</cell></row><row><cell>racial makeup of the county was 95.99% Race (United States Census), 2.19% Race (United</cell></row><row><cell>States Census) or Race (United States Census), 0.26% Race (United States Census), 0.38%</cell></row><row><cell>Race (United States Census), 0.20% from Race (United States Census), and 0.97% from two or</cell></row><row><cell>more races. 0.Answer 67027</cell></row><row><cell>Prediction Sum3(Sum(40543, 15416), 11068) = 40543 + 15416 + 11068 = 67027</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Toungoo took the region but formally broke away in 1510. Ava's only ally was the Shan state of Thibaw , which too was fighting Mohnyin's raids on its territory. Mohnyin was attacking other Shan states when it was not raiding Ava. It seized Bhamo from Thibaw in 1512 in the east, and raiding Kale in the west. The Ava-Thibaw alliance was able to retake Shwebo for a time but Mohnyin proved too strong. By the early 1520s, Chief Sawlon of Mohnyin had assembled a confederation of Shan states under his leadership. Prome had also joined the confederation. The confederation wiped out Ava's defences in Shwebo in 1524. Finally on 25 March 1527, the forces of the confederation and Prome took Ava. The Confederation later sacked Prome in 1533 because Sawlon felt that Prome had not given sufficient help.</figDesc><table><row><cell>Answer</cell><cell>6</cell></row><row><cell cols="2">Prediction Diff(1501, 1507) = 1507 − 1501 = 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 1 :</head><label>1</label><figDesc>Correct predictions of our model. Examples are from the DROP development set.</figDesc><table><row><cell>Div</cell><cell>Question</cell><cell>Eric has 9306 erasers. If he shares them among 99 friends, how many erasers does each friend get?</cell></row><row><cell></cell><cell>Answer</cell><cell>94.0</cell></row><row><cell></cell><cell cols="2">Prediction Div(9306, 99) = 9306/99 = 94</cell></row><row><cell>Mul</cell><cell>Question</cell><cell>It took Katherine 3 hours to run to Louis's house at 8 miles per hour. How far is it between</cell></row><row><cell></cell><cell></cell><cell>Katherine's house and Louis's house?</cell></row><row><cell></cell><cell>Answer</cell><cell>24</cell></row><row><cell></cell><cell cols="2">Prediction Mul(3, 8) = 3  *  8 = 24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 2 :</head><label>2</label><figDesc>Correct predictions from our model. Examples are from the DROP and IL development sets.QuestionHow many touchdown passes were there during the second half? PassageComing off their home win over the Lions, the 49ers flew to the Louisiana Superdome for a Week 4 duel with the New Orleans Saints. In the first quarter, the Niners struck first as kicker Joe Nedney got a 47-yard field goal. In the second quarter, the Saints took the lead with QB Drew Brees completing a 5-yard and a 33-yard TD pass to WR Lance Moore.San Francisco would answer with Nedney's 49-yard field goal, yet New Orleans replied with Brees' 47-yard TD pass to WR Robert Meachem. In the third quarter, the 49ers tried to rally as Nedney kicked a 38-yard field goal. However, in the fourth quarter, the Saints continued to pull away as RB Deuce McAllister got a 1-yard TD run. The Niners tried to rally as QB J.T. O'Sullivan completed a 5-yard TD pass to WR Isaac Bruce, yet New Orleans sealed the win with kicker Martn Gramtica nailing a 31-yard field goal.</figDesc><table><row><cell>Counting error</cell><cell></cell></row><row><cell>Answer</cell><cell>1 (Literal)</cell></row><row><cell cols="2">Prediction 3 (Literal)</cell></row><row><cell>Correct for the</cell><cell></cell></row><row><cell>wrong reason</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 3 :</head><label>3</label><figDesc>Examples of wrong predictions from our model on the DROP development set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://leaderboard.allenai.org/drop/submissions/public ing complex passages. Instead, the main challenge is mathematical reasoning. According toZhang et al. (2018), the current state of the art uses syntactic parses and deterministic rules to convert the input to logical forms<ref type="bibr" target="#b3">(Liang et al., 2016)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Numbers are heuristically extracted from the text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In practice we capped the number of derivations at 64, which covers 98.7% of the training examples.6  During training, the pruned arguments had recall of 80-90% after 1 epoch and plateaued at 95-98%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The "basic" model includes all Ddirect, all S, and the simple binary operations Sum and Diff.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://cogcomp.org/page/resource view/98</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Chris Alberti, Livio Baldini Soares, and Yoon Kim for tremendously helpful discussions, and we are grateful to all members of the Google Research Language team.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno>abs/1611.01436</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A tag-based statistical english math word problem solver with understanding, reasoning and explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Yi</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Tsung</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen-Yu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Know what you dont know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Equate: A benchmark evaluation framework for quantitative reasoning in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">Penstein</forename><surname>Rosé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno>abs/1901.03735</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">CoQA: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1808.07042</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<title level="m">Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ernie: Enhanced representation through knowledge integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno>abs/1904.09223</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
