<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Index Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Index Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>MANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Upsampling Operators</term>
					<term>Dynamic Networks</term>
					<term>Image Denoising</term>
					<term>Semantic Segmentation</term>
					<term>Image Matting</term>
					<term>Depth Estimation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that existing upsampling operators can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature map, we introduce the concept of 'learning to index', and present a novel index-guided encoder-decoder framework where indices are learned adaptively from data and are used to guide downsampling and upsampling stages, without extra training supervision. At the core of this framework is a new learnable module, termed Index Network (IndexNet), which dynamically generates indices conditioned on the feature map. IndexNet can be used as a plug-in applicable to almost all convolutional networks that have coupled downsampling and upsampling stages, enabling the networks to dynamically capture variations of local patterns. In particular, we instantiate, investigate five families of IndexNet, highlight their superiority in delivering spatial information over other upsampling operators with experiments on synthetic data, and demonstrate their effectiveness on four dense prediction tasks, including image matting, image denoising, semantic segmentation, and monocular depth estimation. Code and models are available at: https://git.io/IndexNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>U PSAMPLING is an essential stage for dense prediction tasks using deep convolutional neural networks (CNNs). The frequently used upsampling operators include transposed convolution <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, unpooling <ref type="bibr" target="#b2">[3]</ref>, periodic shuffling <ref type="bibr" target="#b3">[4]</ref> (a.k.a. depth-to-space), and naive interpolation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> followed by convolution. These operators, however, are not general-purpose designs and often exhibit different behaviors in different tasks.</p><p>The widely-adopted upsampling operator in semantic segmentation and depth estimation is bilinear interpolation, while unpooling is less popular. A reason might be that the feature map generated by max unpooling is sparse, while the bilinearly interpolated feature map has dense and consistent representations for local regions (compared to the feature map before interpolation). This is particularly true for semantic segmentation and depth estimation where pixels in a region often share the same class label or have similar depth. However, we observe that bilinear interpolation can perform significantly worse than unpooling in boundary-sensitive tasks such as image matting. A fact is that the leading deep image matting model <ref type="bibr" target="#b6">[7]</ref> largely borrows the design from the SegNet method <ref type="bibr" target="#b2">[3]</ref>, where unpooling was first introduced. When adapting other stateof-the-art segmentation models, such as DeepLabv3+ <ref type="bibr" target="#b5">[6]</ref> and RefineNet <ref type="bibr" target="#b4">[5]</ref>, to this task, we observe that they tend to fail to recover boundary details ( <ref type="figure">Fig. 1)</ref>. A plausible explanation is that, compared to the bilinearly upsampled feature map, unpooling uses max-pooling indices to guide upsampling. Since boundaries in the shallow layers usually have the maximum responses, indices extracted from these responses record the boundary locations. The feature map <ref type="figure">Fig. 1</ref>. Alpha mattes of different models for the task of image matting. From left to right, Deeplabv3+ <ref type="bibr" target="#b5">[6]</ref>, RefineNet <ref type="bibr" target="#b4">[5]</ref>, Deep Matting <ref type="bibr" target="#b6">[7]</ref> and IndexNet (Ours). Bilinear upsampling tends to fail to recover subtle details, while unpooling and our learned upsampling operator can produce much clear mattes with good local contrast.</p><p>projected by the indices thus shows improved boundary delineation.</p><p>We thus believe that different upsampling operators may exhibit different characteristics, and we expect a specific behavior of the upsampling operator when dealing with specific image content for a particular vision task. A question of interest is: Can we design a generic operator to upsample feature maps that better predict boundaries and regions simultaneously? A key observation of this work is that unpooling, bilinear interpolation or other upsampling operators are some forms of index functions. For example, the nearest neighbor interpolation of a point is equivalent to allocating indices of one to its neighbor and then map the value of the point. In this sense, indices are models <ref type="bibr" target="#b7">[8]</ref>, therefore indices can be modeled and learned.</p><p>In this work, we model indices as a function of the local feature map and learn index functions to implement upsampling within deep CNNs. In particular, we present a novel indexguided encoder-decoder framework, which naturally generalizes models like SegNet. Instead of using max-pooling and unpooling, we introduce indexed pooling and indexed upsampling where downsampling and upsampling are guided arXiv:1908.09895v2 [cs.CV] 5 Apr 2020 by learned indices. The indices are generated dynamically conditioned on the feature map and are learned using a fully convolutional network, termed IndexNet, without extra supervision needed. IndexNet is a highly flexible module. It can be applied to almost all convolutional networks that have coupled downsampling and upsampling stages. Compared to the fixed max function or bilinear interpolation, learned index functions show potentials for simultaneous boundary and region delineation.</p><p>IndexNet is a high-level concept and represents a broad family of networks modeling the so-called index function. In this work, we instantiate and investigate five families of IndexNet. Different designs correspond to different assumptions. We compare the behavior of IndexNet with existing upsampling operators and demonstrate its superiority in delivering spatial information. We show that In-dexNet can be incorporated into many CNNs to benefit a number of visual tasks, for instance: i) image matting: our MobileNetv2-based <ref type="bibr" target="#b8">[9]</ref> model with IndexNet exhibits at least 16.1% improvement against the VGG-16-based DeepMatting baseline <ref type="bibr" target="#b6">[7]</ref> on the Composition-1k matting dataset; by visualizing learned indices, the indices automatically learn to capture the boundaries and textural patterns; ii) image denoising: a modified DnCNN model with IndexNet can achieve performance comparable to the baseline DnCNN <ref type="bibr" target="#b9">[10]</ref> that has no downsampling stage on the BSD68 and Set12 datasets <ref type="bibr" target="#b10">[11]</ref>, thus reducing the computational cost and memory consumption significantly; iii) semantic segmentation: consistently improved performance is observed when SegNet <ref type="bibr" target="#b2">[3]</ref> is equipped with IndexNet on the SUN RGB-D dataset <ref type="bibr" target="#b11">[12]</ref>; and iv) monocular depth estimation: IndexNet also improves the performance of a recent lightweight FastDepth model on the NYUDv2 dataset <ref type="bibr" target="#b12">[13]</ref>, with negligible extra computation cost.</p><p>We make the following main contributions:</p><p>â€¢ We present a unified perspective of existing upsampling operators with the notion of the index function; â€¢ We introduce Index Networks-a novel family of networks that can be included into standard CNNs to provide dynamic, adaptive downsampling and upsampling capabilities; to the best of our knowledge, In-dexNet is one of the first attempts towards the design of generic upsampling operators; â€¢ We instantiate, and investigate five designs of IndexNet and demonstrate their effectiveness on four vision tasks.</p><p>A preliminary conference version of this work appeared in <ref type="bibr" target="#b13">[14]</ref>. We extend <ref type="bibr" target="#b13">[14]</ref> by i) further investigating two lightweight IndexNets; ii) comparing properties and computational complexity of IndexNets; iii) presenting a taxonomy of upsampling operators with the notion of guided upsampling and blind upsampling; and iv) designing image reconstruction experiments on synthetic data to compare two upsampling paradigms in recovering spatial information. Besides image matting in <ref type="bibr" target="#b13">[14]</ref>, we now v) apply IndexNets to a few more vision tasks including image denoising, semantic segmentation and monocular depth estimation and report extensive experimental results, not only between index networks but also across different vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LITERATURE REVIEW</head><p>We review upsampling operators and a closely-related group of networks: dynamic networks.</p><p>Upsampling in Deep Networks. Compared with other components in the design of deep networks, downsampling and upsampling of feature maps are relatively less studied. Since learning a CNN without sacrificing the spatial resolution is computationally expensive and memory intensive, and suffers from limited receptive fields, downsampling operators are common choices, such as strided convolution and max/average pooling. To recover the resolution, upsampling is thus an essential stage for almost all dense prediction tasks. This poses a fundamental question: What is the principal approach to recover the resolution of a downsampled feature map (decoding). A few upsampling operators are proposed. The deconvolution operator, a.k.a. transposed convolution, was initially used in <ref type="bibr" target="#b0">[1]</ref> to visualize convolutional activations and introduced to semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, but this operator sometimes can be harmful due to its behavior in producing checkerboard artifacts <ref type="bibr" target="#b14">[15]</ref>. To avoid this, a suggestion is the "resize+convolution" paradigm, which has currently become the standard configuration in state-of-the-art semantic segmentation models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Apart from these, perforate <ref type="bibr" target="#b15">[16]</ref> and unpooling <ref type="bibr" target="#b2">[3]</ref> generate sparse indices to guide upsampling. The indices are able to capture and keep boundary information, but one issue is that the two operators can induce much sparsity after upsampling. Convolutional layers with large filter sizes must follow for densification. In addition, periodic shuffling (PS) was introduced in [4] as a fast and memory-efficient upsampling operator for image super-resolution. PS recovers resolution by rearranging the feature map of size H Ã— W Ã— Cr 2 to rH Ã— rW Ã— C. It is also used in some segmentation models <ref type="bibr" target="#b16">[17]</ref>.</p><p>Our work is primarily inspired by the unpooling operator <ref type="bibr" target="#b2">[3]</ref>. We remark that, it is important to extract spatial information before its loss during downsampling, and more importantly, to use stored information during upsampling. Unpooling shows a simple and effective use case, while we believe that there is much room to improve. Here we show that unpooling is a special form of index function, and we can learn an index function beyond unpooling.</p><p>We notice that concurrent work of <ref type="bibr" target="#b17">[18]</ref> also pursues the idea of data-dependent upsampling and proposes an universal upsampling operator termed CARAFE. Although the idea is similar, IndexNet is different from CARAFE in several aspects. First, CARAFE does not associate upsampling with the notion of the index function. Second, the kernels used in CARAFE are generated conditioned on decoder features, while IndexNet builds upon encoder features, so the generated indices can also be used to guide downsampling. Third, CARAFE can be viewed as one of our investigated index networks-holistic index networks, but with different upsampling kernels and normalization strategies. We further compare IndexNet with CARAFE in Section 5.2 and in experiments.</p><p>Dynamic Networks. If considering the dynamic property of IndexNet, IndexNet shares similarity with an interesting group of networks-dynamic networks. Dynamic networks are often implemented with adaptive modules to extend the modeling capabilities of CNNs. These networks share the following characteristics. The output is dynamic, conditioned on the input feature map. Since dynamic networks are learnable modules, they are generic in the sense that they can be used as building blocks in many network architectures. They are also flexible to allow modifications according to target tasks.</p><p>Spatial Transformer Networks (STNs) <ref type="bibr" target="#b18">[19]</ref>. STN allows explicit manipulation of spatial transformation within the network. It achieves this by regressing transformation parameters Î¸ with a side-branch network. A spatially-transformed output is then produced by a sampler parameterized by Î¸. This results in a holistic transformation of the feature map. The dynamic nature of STN is reflected by the fact that, given different inputs, the inferred Î¸ is different, allowing to learn some forms of invariance to translation, scale, rotation, etc.</p><p>Dynamic Filter Networks (DFNs) <ref type="bibr" target="#b19">[20]</ref>. DFN implements a filter generating network to dynamically generate kernel filter parameters Compared to conventional filter parameters that stay fixed during inference, filter parameters in DFN are dynamic and sample-specific.</p><p>Deformable Convolutional Networks (DCNs) <ref type="bibr" target="#b20">[21]</ref>. DCNs introduce deformable transformation into convolution. The key idea is to predict offsets for convolutional kernels. With offsets, convolution can be executed on irregular sampling grids, enabling adaptive manipulation of the receptive field.</p><p>Attention Networks <ref type="bibr" target="#b21">[22]</ref>. Attention networks are a broad family of networks that use attention mechanisms. The mechanisms introduce multiplicative interactions between the inferred attention map and the feature map. In computer vision, attention mechanisms are usually referred to spatial attention <ref type="bibr" target="#b22">[23]</ref>, channel attention <ref type="bibr" target="#b23">[24]</ref> or both <ref type="bibr" target="#b24">[25]</ref>. These network modules are widely applied in CNNs to force the network focusing on specific regions and therefore to refine feature maps. Essentially, attention is about feature selection. No attentional module has been designed to deal with the downsampling/upsampling stage.</p><p>In contrast to above dynamic networks, IndexNet focuses on upsampling, rather than manipulating filters or refining features. Akin to dynamic networks above, the dynamics in IndexNet also has a physical definition-indices. Such a definition also closely relates to attention networks. Later we show that the downsampling and upsampling operators used with IndexNet can, to some extent, be viewed as attentional operators. Indeed, max-pooling indices are a form of hard attention. It is worth noting that, despite that IndexNet in its current implementation may closely relate to attention, it focuses on upsampling rather than refining feature maps. IndexNet also shares the other characteristics mentioned above. It is implemented in a convolutional sidebranch network, is trained without extra supervision and is generic and flexible. We demonstrate its effectiveness on four dense prediction tasks and five variants of IndexNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AN INDEXING PERSPECTIVE OF UPSAMPLING</head><p>With the argument that upsampling operators are index functions, here we offer a unified indexing perspective of upsampling operators. The unpooling operator is straight-forward. We can define its index function in a k Ã— k local region as an indicator function</p><formula xml:id="formula_0">I max (x) = 1(x = max(X)) , x âˆˆ X ,<label>(1)</label></formula><p>where X âˆˆ R kÃ—k . 1(Â·) is the indicator function with output being a binary matrix. Similarly, if one extracts indices from average pooling, its index function takes the form</p><formula xml:id="formula_1">I avg (x) = 1(x âˆˆ X) .<label>(2)</label></formula><p>If further using I avg (x) during upsampling, it is equivalent to the nearest neighbor interpolation. As for the bilinear interpolation and deconvolution operators, their index functions have an identical form</p><formula xml:id="formula_2">I bilinear/dconv (x) = W âŠ— 1(x âˆˆ X) ,<label>(3)</label></formula><p>where W is the weight/filter of the same size as X, and âŠ— denotes the element-wise multiplication. The difference is that, W is learned in deconvolution but predefined in bilinear interpolation. Indeed bilinear interpolation has been shown to be a special case of deconvolution <ref type="bibr" target="#b1">[2]</ref>. Note that, in this case, the index function generates soft indices. The sense of index for the PS operator <ref type="bibr" target="#b3">[4]</ref> is also clear, because the rearrangement of the feature map is an indexing process. Considering PS a tensor Z of size 1 Ã— 1 Ã— r 2 to a matrix Z of size r Ã— r, the index function can be expressed by the one-hot encoding</p><formula xml:id="formula_3">I l ps (x) = 1(x = Z l ) , l = 1, ..., r 2 ,<label>(4)</label></formula><p>such that Z m,n = Z[I l ps (x)], where m = 1, ..., r, n = 1, ..., r, and l = (r âˆ’ 1) Â· m + n. Z l denotes the l-th element of Z. Similar notation applies to Z m,n .</p><p>Since upsampling operators can be unified by the notion of the index function, it is plausible to ask whether one can learn an index function to dynamically capture local spatial patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LEARNING TO INDEX, TO POOL, AND TO UPSAM-PLE</head><p>Before introducing the designs of IndexNet, we first present the general idea about how learned indices may be used in downsampling and upsampling with a new index-guided encoder-decoder framework. Our framework is a generalization of SegNet, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. For ease of exposition, let us assume the downsampling and upsampling rates to be 2, and the pooling operator to use a kernel size of 2 Ã— 2. The IndexNet module dynamically generates indices given the feature map. The proposed indexed pooling and indexed upsampling operators further receive generated indices to guide downsampling and upsampling, respectively. In practice, multiple such modules can be combined and used analogous to the max pooling layers for every downsampling and upsampling stage.</p><p>IndexNet models the index as a function of the feature map X âˆˆ R HÃ—W Ã—C . Given X, it generates two index maps for downsampling and upsampling, respectively. An important concept for the index is that an index can either be represented in a natural order, e.g., 1, 2, 3, ..., or be represented in a logical form, i.e., 0, 1, 0, ..., meaning that an index map can be used as a mask. This is exactly how   we use the index map in downsampling/upsampling. The predicted index shares the same definition of the index in computer science, except that we generate soft indices for smooth optimization, i.e., for any index i, i âˆˆ [0, 1]. IndexNet consists of a predefined index block and two index normalization layers. An index block can simply be a heuristically defined function, e.g., a max function, or more generally, a parameterized function such as neural network. In this work, we use a fully convolutional network to be the index block. More details are presented in Sections 4.2 and 4.3. Note that the index maps sent to the encoder and decoder are normalized differently. The decoder index map only goes through a sigmoid function such that for any predicted index i âˆˆ (0, 1). As for the encoder index map, indices of each local region L are further normalized by a softmax function such that iâˆˆL i = 1. The second normalization guarantees the magnitude consistency of the feature map after downsampling.</p><p>Indexed Pooling (IP) performs downsampling using generated indices. Given a local region E âˆˆ R kÃ—k , IP calculates a weighted sum of activations and corresponding indices over E as IP(E) = xâˆˆE I(x)x, where I(x) is the index of x. It is easy to see that max pooling and average pooling are special cases of IP. In practice, this operator can be easily implemented with an element-wise multiplication between the feature map and the index map, an average pooling layer, and a multiplication of a constant used to compensate the effect of averaging, as instantiated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The current implementation is equivalent to 2 Ã— 2 stride-2 convolution with dynamic kernels, but is more efficient than explicit onthe-fly kernel generation.</p><p>Indexed Upsampling (IU) is the inverse operator of IP. IU upsamples d âˆˆ R 1Ã—1 that spatially corresponds to E taking the same indices into account. Let I âˆˆ R kÃ—k be the local index map formed by I(x)s, IU upsamples d as IU(d) = I âŠ— D, where âŠ— denotes the element-wise multiplication, and D is of the same size as I and is upsampled from d with the nearest neighbor interpolation. IU also relates to deconvolution, but an important difference between IU and deconvolution is that, deconvolution applies a fixed kernel to all local regions (even if the kernel is learned), while IU upsamples different regions with different kernels (indices).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Holistic Index</head><p>Depthwise Index</p><formula xml:id="formula_4">2x2xC 1 x1x4 2x2xC 1 x1x4C</formula><p>HxWxC H xWx1 HxWxC HxWxC <ref type="figure" target="#fig_6">Fig. 3</ref>. Conceptual differences between holistic and depthwise index.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Index Networks</head><p>Here we present a taxonomy of proposed index networks. According to the shape of the output index map, index networks can be first categorized into two branches: holistic index networks (HINs) and depthwise (separable) index networks (DINs). Their conceptual differences are shown in <ref type="figure" target="#fig_6">Fig. 3</ref>. HINs learn an index function I(X) : R HÃ—W Ã—C â†’ R HÃ—W Ã—1 . In this case, all channels of the feature map share a holistic index map. By contrast, DINs learn an index function I(X) : R HÃ—W Ã—C â†’ R HÃ—W Ã—C , where the index map is of the same size as the feature map.</p><p>Since the index map generated by DINs can correspond to individual slices of the feature map, we can incorporate further assumptions into DINs to simplify the designs. If assuming that each slice of the index map only relates to its corresponding slice of the feature map, this is the One-to-One (O2O) assumption and O2O DINs. If each slice of the index map relates to all channels of the feature map, this leads to Many-to-One (M2O) assumption and M2O DINs. In O2O DINs, one can further consider sharing IndexNet. In the most simplified case, the same IndexNet can be applied to every slice of the feature map and can be shared across different downsampling/upsampling stages, like the max function. We name this IndexNet Modelwise O2O DINs. If IndexNet is stage-specific, i.e., only sharing indices in individual stages, we call this IndexNet Shared Stagewise O2O DINs. Finally, without sharing any parameter (each feature slice has its specific index function), we obtain the standard design, termed Unshared Stagewise O2O DINs. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the tree diagram of these index networks. The difference between modelwise IndexNet and stagewise IndexNet is also shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Notice that, HINs and M2O DINs are both stagewise.</p><p>With the taxonomy, we investigate five families of In-dexNet. Each family can be designed to have either linear mappings or nonlinear mappings, as we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Holistic Index Networks</head><p>Recall that HINs learn an index function I(X) : R HÃ—W Ã—C â†’ R HÃ—W Ã—1 . A naive design choice is to assume a linear mapping between the feature map and the index map.</p><p>Linear HINs. An example is shown in <ref type="figure" target="#fig_4">Fig. 6</ref>(a). The network is implemented in a fully convolutional network. It first applies stride-2 2 Ã— 2 convolution (assuming that the downsampling rate is 2) to the feature map of size H Ã— W Ã— C, generating a concatenated index map of size H/2Ã—W/2Ã—4. Each slice of the index map (H/2Ã—W/2Ã—1) is designed to correspond to the indices of a certain position of all local regions, e.g., the top-left corner of all 2Ã—2 regions. The network finally applies a PS-like shuffling operator to rearrange the index map to the size of H Ã— W Ã— 1.</p><p>In many situations, a linear relationship is not sufficient. For example, a linear function even cannot approximate the max function. Thus, the second design choice is to introduce nonlinearity into the network.</p><p>Nonlinear HINs. <ref type="figure" target="#fig_4">Fig. 6</ref>(b) illustrates a nonlinear HIN where the feature map is first projected to a map of size H/2 Ã— W/2 Ã— 2C, followed by a batch normalization layer and a ReLU function for nonlinear mappings. We then use point-wise convolution to reduce the channel dimension to an indices-compatible size. The remaining transformations follow its linear counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Depthwise Index Networks</head><p>In DINs, we seek I(X) : R HÃ—W Ã—C â†’ R HÃ—W Ã—C , i.e., each spatial index corresponds to each spatial activation. As aforementioned, this type of networks further has two different high-level design strategies that correspond to two different assumptions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">One-to-One Depthwise Index Networks</head><p>O2O assumption assumes that each slice of the index map only relates to its corresponding slice of the feature map. It can be denoted by a local index function l(X) : R kÃ—kÃ—1 â†’ R kÃ—kÃ—1 , where k denotes the size of the local region. Since the local index function operates on individual feature slices, we can design whether different feature slices share the same local index function. Such a weight sharing strategy can be applied at a modelwise level or at a stagewise level, which leads to the following designs of O2O DINs: 1) Modelwise O2O DINs: the model only has a unique index function that is shared by all feature slices, even in different downsampling and upsampling stages. This is the most light-weight design; 2) Shared Stagewise O2O DINs: the index function is also shared by feature slices, but every stage has stagespecific IndexNet. This design is also light-weight; 3) Unshared Stagewise O2O DINs: even in the same stage, different feature slices have distinct index functions.</p><p>Similar to HINs, DINs can also be designed to have linear/nonlinear modeling ability. <ref type="figure" target="#fig_5">Fig. 7</ref> shows an example when k = 2. Note that, in contrast to HINs, DINs follow a multi-column architecture. Each column is responsible for predicting indices specific to a certain spatial location of all local regions. We implement DINs with group convolutions.</p><p>Linear O2O DINs. According to <ref type="figure" target="#fig_5">Fig. 7</ref>, the feature map first goes through four parallel convolutional layers with the same kernel size. Modelwise O2O DINs and Shared Stagewise O2O DINs only use a kernel size of 2 Ã— 2 Ã— 1, a stride of 2, and 1 group, while Unshared Stagewise O2O DINs has a kernel size of 2 Ã— 2 Ã— C, a stride of 2, and C groups. One can simply reshape the feature map, i.e., reshaping  </p><formula xml:id="formula_5">H Ã— W Ã— C to be C Ã— H Ã— W Ã— 1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Many-to-One Depthwise Index Networks</head><p>M2O assumption assumes that all feature slices have contributions to each index slice. The local index function is defined by l(X) : R kÃ—kÃ—C â†’ R kÃ—kÃ—1 . Compared to O2O DINs, the only difference in implementation is the use of standard convolution instead of group convolution, i.e., M = C, N = 1 in <ref type="figure" target="#fig_5">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Property and Model Complexity</head><p>Both HINs and DINs have merits and drawbacks. Here we discuss some important properties of IndexNet. We also present an analysis of computational complexity.</p><p>Remark 1. Index maps generated by HINs and used by the IP and IU operators are related to spatial attention.</p><p>The holistic index map is shared by all feature slices, which means that the index map is required to be expanded to the size of H Ã— W Ã— C when feeding into IP and IU. This index map can be thought as a collection of local attention maps <ref type="bibr" target="#b21">[22]</ref> applied to individual local spatial regions. In this case, the IP and IU operators can also be referred to as "attentional pooling" and "attentional upsampling". However, it should be noted that spatial attention has no pooling or upsampling operators like IP and IU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.</head><p>HINs are more flexible than DINs and more friendly for decoder design.</p><p>Since the holistic index map is expandable, the decoder feature map does not need to forcibly increase/reduce its dimensionality to fit the shape of the index map during upsampling. This gives much flexibility for decoder design, while it is not the case for DINs. No matter how large the model capacity is or how wide the feature channels are, the number of parameters in Modelwise O2O DINs remains at a constant level, and that in Shared Stagewise O2O DINs is only proportional to the number of downsampling/upsampling stages. This is desirable as the number of parameters introduced by IndexNet is not significant. However, these two types of IndexNet may be limited to capture sophisticated local patterns. HINs</p><formula xml:id="formula_6">L K Ã— K Ã— C Ã— 4 NL K Ã— K Ã— C Ã— 2C + 2C Ã— 4 NL+C 2K Ã— 2K Ã— C Ã— 2C + 2C Ã— 4</formula><p>Modelwise O2O DINs</p><formula xml:id="formula_7">L (K Ã— K) Ã— 4 NL (K Ã— K Ã— 2 + 2) Ã— 4 NL+C (2K Ã— 2K Ã— 2 + 2) Ã— 4 Shared Stagewise O2O DINs L (K Ã— K) Ã— 4 NL (K Ã— K Ã— 2 + 2) Ã— 4 NL+C (2K Ã— 2K Ã— 2 + 2) Ã— 4 Unshared Stagewise O2O DINs L (K Ã— K Ã— C) Ã— 4 NL (K Ã— K Ã— 2C + 2C Ã— C) Ã— 4 NL+C (2K Ã— 2K Ã— 2C + 2C Ã— C) Ã— 4 M2O DINs L (K Ã— K Ã— C Ã— C) Ã— 4 NL (K Ã— K Ã— C Ã— 2C + 2C Ã— C) Ã— 4 NL+C (2K Ã— 2K Ã— C Ã— 2C + 2C Ã— C) Ã— 4</formula><p>L: Linear; NL: Nonlinear; C: Context. Remark 4. M2O DINs have the most powerful modeling capability among IndexNet variants, but also introduce many extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M2O DINs have higher capacity than HINs and O2O</head><p>DINs due to the use of standard convolution.</p><p>Another desirable property of IndexNet is that they may be able to predict the indices from a large local feature map, e.g., l(X) : R 2kÃ—2kÃ—C â†’ R kÃ—kÃ—1 . An intuition behind this idea is that, if one identifies a local maximum point from a k Ã— k region, its surrounding 2k Ã— 2k region can further support whether this point is a part of a boundary or only an isolated noise point. This idea can be easily implemented by enlarging the convolutional kernel size and with appropriate padding.</p><p>In <ref type="table" target="#tab_4">Table 1</ref>, we summarize the model complexity of different index networks used at a single downsampling and upsampling stage. We assume the convolution kernel has a size of K Ã— K applied on a C-channel feature map. The number of parameters in BN layers is excluded. When considering weak context, we assume the kernel size is 2K Ã— 2K. Since C K, generally we have the model complexity M2O DINs&gt;HINs&gt;Unshared Stagewise O2O DINs&gt;Shared Stagewise O2O DINs&gt;Modelwise O2O DINs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GUIDED UPSAMPLING OR BLIND UPSAMPLING: A RECONSTRUCTION-BASED JUSTIFICATION</head><p>Here we introduce the concept of guided upsampling and blind upsampling to summarize existing upsampling operators. Particularly, here we present a comparison between two data-dependent upsampling operators-IndexNet and CARAFE <ref type="bibr" target="#b17">[18]</ref>. In addition, we show results of an image reconstruction task on synthetic data, highlighting the difference between guided upsampling and blind upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Guided Upsampling vs. Blind Upsampling</head><p>By blind upsampling, we mean that an upsampling operator that is pre-defined with fixed parameters. Guided upsampling, instead, is guided with the involvement of auxiliary information.</p><p>Thus, most widely-used upsampling operators peform blind upsampling. These operators include nearestneighbor (NN) interpolation, bilinear interpolation, spaceto-depth and deconvolution. It is worth noting that the recent data-dependent upsampling operator CARAFE is also a A taxonomy of commonly-used upsampling operators and their corresponding downsampling operators is summarized in <ref type="table" target="#tab_5">Table 2</ref>. An upsampling operator should generally have an corresponding downsampling operator, and vice versa.</p><p>The main difference here is that guided upsampling is made possible to exploit extra information to better recover the spatial information during upsampling. Thus, it is important that the spatial information is properly encoded during downsampling and is transferred to unsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">IndexNet vs. CARAFE</head><p>Both IndexNet and CARAFE are one of the few attempts pursuing the idea of data-dependent upsampling. The similarities include: i) They both are related to dynamic networks. ii) Both are parametric upsampling operators; iii) CARAFE and HINs both perform holistic upsampling. iv) CARAFE also learns an index function. The index function has an identical form to Eq. (3), but with dynamic and normalized W . In this sense, CARAFE may be considered as a single-input version of IU, where the index map is generated internally. The differences are: i) CARAFE is a blind upsampling operator, while In-dexNet implements guided upsampling; ii) The reassembly kernels in CARAFE are generated conditioned on the low-resolution decoder feature map. The index maps predicted by IndexNet, however, build upon the high-resolution encoder feature map, before spatial information is lost; iii) In IndexNet, each upsampled feature point only associates with a single point in the low-resolution feature map. From low resolution to high resolution, it is a one-to-many mapping. In CARAFE, each upsampled point is a weighted sum of a local region from the lowresolution feature map. This is a many-to-one mapping; iv) Compared to CARAFE which is presented as a single upsampling operator, IndexNet is a more general framework. In particular, the key difference lies in the intermediate path that allows spatial information to be visible to upsampling. To further demonstrate the benefit of this intermediate path, we present an image reconstruction experiment on synthetic data, namely, the Fashion-MNIST dataset <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Fashion-MNIST Image Reconstruction</head><p>The idea is that, if an upsampling operator can recover spatial information well from downsampled feature maps, the reconstructed output should be visually closer to the input image. The quality of reconstruction results can be a good indicator how well spatial information is recovered by an upsampling operator.</p><p>Network Architecture and Baselines. We use a standard encoder-decoder architecture. Let C(k) denote a 2D convolutional layer with k-channel 3 Ã— 3 filters, followed by BN and ReLU. D represents a downsampling operator with a downsampling ratio of 2, and U an upsampling operator with an upsampling ratio of 2. The reconstruction network can therefore be defined by Training Details. The Fashion-MNIST dataset <ref type="bibr" target="#b25">[26]</ref> includes 60, 000 training images and 10, 000 testing images. The input images are resized to 32 Ã— 32. 1 loss is used in training. The initial learning rate is set to 0.01. We train the network for 100 epochs with a batch size of 100. The learning rate is decreased by Ã—10 at the 50-th, 70-th and 85th epoch, respectively. We report Peak Signal-to-Noise Ratio (PSNR), Structural SIMilarity index (SSIM), Mean Absolute Error (MAE) and root Mean Square Error (MSE).</p><p>Discussions. Quantitative and qualitative results are shown in <ref type="table" target="#tab_6">Table 3</ref> and <ref type="figure" target="#fig_8">Fig. 8</ref>, respectively. We observe in <ref type="figure" target="#fig_8">Fig. 8</ref> that, all baselines that exploit blind upsampling fail to reconstruct the input images. While in some cases they may produce reasonable reconstructions, images of trousers and high-heeled shoes for instance, in most cases these baselines generate blurred results when complex textural patterns appear, tops and wallets for example. By contrast, other baselines that leverage guided upsampling produce visually pleasing reconstructions, in all circumstances. In particular, by comparing MaxPool-MaxUnpool with IP-IU, the former tends to yield jittering artifacts. This suggests index maps extract and encode richer spatial information than maxpooling indices. In addition, by disabling the intermediate path, IP-Bilinear leads to significantly poor reconstructions, which means that the intermediate path matters. The differences between two upsampling paradigms are also supported by the numerical results in <ref type="table" target="#tab_6">Table 3</ref> where guided upsampling exhibits significantly better PSNR and lower errors than blind upsampling. Indeed, the intermediate path distinguishes guided upsampling from blind upsampling, and also IndexNet from CARAFE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICATIONS</head><p>In this section, we show several applications of IndexNet on the tasks of image matting, image denoising, semantic segmentation, and monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Image Matting</head><p>We first evaluate IndexNet on the task of image matting. Image matting is defined as a problem of estimating soft foreground from images. This problem is ill-posed due to the fact that solving a linear system of 7 unknown variables with only 3 known inputs: given the RGB color at pixel i, I i , one needs to estimate the corresponding foreground color F i , background color B i , and matte Î± i , such that</p><formula xml:id="formula_8">I i = Î± i F i + (1 âˆ’ Î± i )B i , for any Î± i âˆˆ [0, 1].</formula><p>Previous methods have extensively studied this problem from a low-level view <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>; and particularly, they have been designed to solve the above matting equation. Despite being theoretically elegant, these methods heavily rely on the color cues, rendering failures of matting in general natural scenes where colors are not reliable. With the tremendous success of deep CNNs in high-level vision tasks <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, deep matting methods are emerging. Recently deep image matting was proposed <ref type="bibr" target="#b6">[7]</ref>. In <ref type="bibr" target="#b6">[7]</ref> the authors presented the first deep image matting approach (DeepMatting) based on SegNet <ref type="bibr" target="#b2">[3]</ref> and significantly outperformed other competitors. In this application, we use DeepMatting as our baseline. Image matting is particularly suitable for evaluating the effectiveness of IndexNet, because the quality of learned indices can be visually observed from inferred alpha mattes. We conduct experiments on the Adobe Image Matting dataset <ref type="bibr" target="#b6">[7]</ref>. This is so far the largest publicly available matting dataset. The training set has 431 foreground objects and ground-truth alpha mattes. Each foreground is composited with 100 background images randomly chosen from MS COCO <ref type="bibr" target="#b32">[33]</ref>. The validation set termed Composition-1k includes 100 unique objects. Each of them is composited with 10 background images chosen from Pascal VOC <ref type="bibr" target="#b33">[34]</ref>. Overall, we have 43, 100 training images and 1, 000 testing images. We evaluate the results using widely-used Sum of Absolute Differences (SAD), root Mean Square Error (MSE), and perceptually-motivated Gradient (Grad) and Connectivity (Conn) errors <ref type="bibr" target="#b34">[35]</ref>. The evaluation code implemented by <ref type="bibr" target="#b6">[7]</ref> is used. In what follows, we first describe our modified MobileNetv2-based architecture and training details. We then perform extensive ablation studies to justify choices of model design, make comparisons of different index networks, and visualize learned indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Network Architecture and Implementation Details</head><p>Here we describe the network architecture and training details.</p><p>Network Architecture. We build our model based on Mo-bileNetv2 <ref type="bibr" target="#b8">[9]</ref> with only slight modifications to the backbone. We choose MobileNetv2 for its lightweight model and fast inference. The basic network configuration is shown in <ref type="figure" target="#fig_9">Fig. 9</ref>. It also follows the encoder-decoder paradigm same as SegNet. We simply change all 2-stride convolution to be 1-stride and attach 2-stride 2 Ã— 2 max pooling after each encoding stage for downsampling, which allows us to extract indices. If applying the IndexNet idea, max pooling and unpooling layers can be replaced with IP and IU, respectively. We also investigate alternative ways for low-level feature fusion and whether encoding context (Section 6.1.2). Note that, the matting refinement stage <ref type="bibr" target="#b6">[7]</ref> is not applied here.</p><p>Training Details. To enable a direct comparison with deep matting <ref type="bibr" target="#b6">[7]</ref>, we follow the same training configurations used in <ref type="bibr" target="#b6">[7]</ref>. The 4-channel input concatenates the RGB image and its trimap. We follow exactly the same data augmentation strategies, including 320 Ã— 320 random cropping, random flipping, random scaling, and random trimap dilation. We use a combination of the alpha prediction loss and the composition loss during training as in <ref type="bibr" target="#b6">[7]</ref>. Only losses from the unknown region of the trimap are calculated. Encoder parameters are pretrained on ImageNet <ref type="bibr" target="#b35">[36]</ref>. The parameters of the 4-th input channel are initialized with zeros. The Adam optimizer <ref type="bibr" target="#b36">[37]</ref> is used. We update parameters with 30 epochs (around 90, 000 iterations). The learning rate is initially set to 0.01 and reduced by 10Ã— at the 20-th and 26th epoch respectively. We use a batch size of 16 and fix the BN layers of the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Results on the Adobe Image Matting Dataset</head><p>Ablation Study on Model Design. To establish a better baseline comparable to DeepMatting, here we first investigate strategies for fusing low-level features (no fusion, skip fusion as in ResNet <ref type="bibr" target="#b37">[38]</ref> or concatenation as in UNet <ref type="bibr" target="#b38">[39]</ref>) and whether encoding context for image matting. 11 baselines are consequently built to justify model design. Results on the Composition-1k testing set are reported in <ref type="table" target="#tab_7">Table 4</ref>. B3 is cited from <ref type="bibr" target="#b6">[7]</ref>. We can make the following observations: i) Indices are of great importance. Matting can significantly benefit from only indices (B3 vs. B4, B5 vs. B6); ii) State-of-the-art semantic segmentation models cannot be directly applied to image matting (B1/B2 vs. B3); iii) Fusing low-level features help, and concatenation is better than skip connection but at a cost of increased computation (B6 vs. B8 vs. B10 or B7 vs. B9 vs. B11); iv) Modules such as ASPP may improve the results (e.g., B6 vs. B7 or B8). v) A MobileNetv2-based matting model can work as well as a VGG-16-based one (B3 vs. B11).   <ref type="bibr" target="#b4">[5]</ref>; ASPP: atrous spatial pyramid pooling <ref type="bibr" target="#b5">[6]</ref>; OS: output stride. The lowest errors are boldfaced. For the following experiments, we now mainly use B11. Ablation Study on Index Networks. Here we compare different index networks and justify their effectiveness. The configurations of index networks used in the experiments follow Figs. 6 and 7. We primarily investigate the 2Ã—2 kernel with a stride of 2. Whenever the weak context is considered, we use a 4 Ã— 4 kernel in the first convolutional layer of index networks. To highlight the effectiveness of HINs, we further build a baseline called holistic max index (HMI) where max-pooling indices are extracted from a squeezed feature map X âˆˆ R HÃ—W Ã—1 . X is generated by applying the max function along the channel dimension of X âˆˆ R HÃ—W Ã—C . Furthermore, since IndexNet increases extra parameters, we introduce another baseline B11-1.4 where the width multiplier of MobilieNetV2 is adjusted to be 1.4 to increase the model capacity. In addition, to compare IndexNet against CARAFE in this task, we build an additional baseline B11carafe where the unpooling operator in B11 is replaced with CARAFE. Results on the Composition-1k testing dataset are listed in <ref type="table" target="#tab_8">Table 5</ref>. We observe that, most index networks reduce the errors notably, except for some low-capacity IndexNet modules (due to limited modeling capabilities). In particular, nonlinearity and the context generally have a positive effect on deep image matting, but they do not work effectively in O2O DINs. A possible reason may be that the limited dimensionality of the intermediate feature map is not sufficient to model complex patterns in matting. Compared to holistic max index, the direct baseline of HINs, the best HIN ("Nonlinearity+Context") has at <ref type="figure">Fig. 10</ref>. Qualitative results on the Composition-1k testing set. From left to right, the original image, trimap, ground-truth alpha matte, Closed-form Matting <ref type="bibr" target="#b29">[30]</ref>, DeepMatting <ref type="bibr" target="#b29">[30]</ref>, and ours (M2O DIN with 'Nonlinearity+Context').  <ref type="figure">Fig. 10</ref>. Our predicted mattes show improved delineation for edges and textures like hair and water drops.</p><p>Ablation Study on Index Normalization. Index normalization is important for the final performance. Here we justify this by evaluating different normalization choices. Apart from the sigmoid function used for the decoder and the sigmoid+softmax function for the encoder, we compare other three different combinations of normalization strategies listed in <ref type="table" target="#tab_9">Table 6</ref>. The experiment is conducted based The lowest errors are boldfaced. on M2O DIN with "Nonlinearity+Context". It is clear that keeping the magnitude consistency during downsampling matters. In fact, both max pooling and average pooling satisfy this property naturally, and our normalization design is inspired from this fact.</p><p>Index Map Visualization. It is interesting to see what indices are learned by IndexNet. For the holistic index, the index map itself is a 2D matrix and can be easily visualized. Regarding the depthwise index, we squeeze the index map along the channel dimension and calculate the average responses. Two examples of learned index maps are visualized in <ref type="figure" target="#fig_10">Fig. 11</ref>. We observe that, initial random indices have poor delineation for edges, while learned indices automatically capture the complex structural and textual patterns, e.g., the fur of the dog, and even air bubbles in the water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Image Denoising</head><p>The goal of image denoising is to recover a clean image x from a corrupted observation y following an image  </p><formula xml:id="formula_9">degradation model y = x + v,</formula><p>where v is commonly assumed to be additive white Gaussian noise (AWGN) parameterized by Ïƒ. While such an assumption has been challenged in recent real-image denoising <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, we still follow the AWGN paradigm in evaluation because our focus is not to improve image denoising. When deep CNNs are widely accepted, the data-driven paradigm now becomes the first-class choice for image denoising <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Most deep denoising models are designed with the same highlevel idea-processing the feature map without decreasing its spatial resolution. Indeed, it has been observed that, when the feature map is downsampled, the performance drops remarkably <ref type="bibr" target="#b41">[42]</ref>. For such networks, although the model parameters largely reduce, computational complexity of training and inference becomes much heavier. We show that, by inserting IndexNet into a denoising model, it can effectively compensate the loss of spatial information, achieving performance comparable to or even better than the network without downsampling. Thus, despite the number of parameters increases, computation us much reduced. We choose DnCNN <ref type="bibr" target="#b9">[10]</ref> as our baseline to demonstrate this on standard benchmarks. We follow the experimental setting of <ref type="bibr" target="#b42">[43]</ref> that uses a 400-image training set. The performance is reported on a 68-image Berkeley segmentation dataset (BSD68) and the other 12-image test set (Set12). The networks are trained for Gaussian denoising, with three noise levels, i.e., Ïƒ = 15, 25 and 50. PSNR and SSIM are used as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Network Architecture and Implementation Details</head><p>Network Architecture. We use the 17-layer DnCNN model <ref type="bibr" target="#b9">[10]</ref>, implemented by PyTorch. To enable the use of IndexNet, we modify DnCNN to a SegNet-like architecture with 3 downsampling and upsampling stages (the input image size is 40 Ã— 40). The number of layers remains the same to ensure a relatively fair comparison. <ref type="figure" target="#fig_1">Fig. 12</ref> illustrates the original DnCNN and our modified architecture. The first 9 layers follow VGG-16 except that the first layer is a singlechannel input, and the rest are 7 decoding layers formed by unpooling and convolution and the final prediction layer. All convolutional operations use 3 Ã— 3 kernels. To incorpo-rate IndexNet, it is straightforward to replace max pooling and unpooling with IP and IU.</p><p>Training Details. We follow the same experimental configurations used in <ref type="bibr" target="#b9">[10]</ref>. At each epoch, 40 Ã— 40 image patches are cropped from multiple scales (0.7, 0.8, 0.9, 1) with a stride of 10 and are added with Gaussian noise of a certain noise level (Ïƒ = 15, 25, or 50); image patches are further augmented with random flipping and random rotation. This results in around 240, 000 training samples.</p><p>2 loss is used. All networks are trained from scratch with a batch size of 128. Model parameters are initialized with the improved Xavier <ref type="bibr" target="#b43">[44]</ref>. The Adam optimizer is also used. Parameters are updated with 60 epochs. The learning rate is initially set to 0.001 and reduced by 10Ã— at the 45-th and 55-th epoch, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Results on the BSD68 and Set12 Datasets</head><p>Apart from the DnCNN baseline, we also report the performance of our modified DnCNN-SegNet with max pooling and unpooling. Furthermore, to compare IndexNet against CARAFE, we build three additional baselines where CARAFE is combined with different downsampling strategies, including max pooling, average pooling, and stride-2 convolutions, denoted by DnCNN-max-carafe, DnCNNavg-carafe, and DnCNN-conv-carafe, respectively. Results are shown in <ref type="table" target="#tab_11">Table 7</ref>. It can be observed that, simply downsampling with max pooling and upsampling by unpooling as in DnCNN-SegNet lead to significant drops in both PSNR (generally &gt; 1dB) and SSIM (&gt; 0.1). This suggests that spatial information plays an important role in image denoising. Denoising is content-irrelevant (the model is unaware of regions coming from the foreground or the background). Downsampling without recording sufficient spatial information (only the boundary information is not sufficient) impedes the model from recovering the appearance and the structure in the original image. This is particularly true for baselines adopting CARAFE. Since CARAFE applies blind upsampling, no spatial information is transferred during upsampling, which may lead to inferior results. Interestingly, after IndexNet is inserted into downsampled DnCNN, the loss of PSNR and SSIM is effectively compensated. The compensation behaviors can be observed from almost all types of IndexNet, except the two cases in Modelwise O2O DINs with nonlinearity. The poor performance of Modelwise O2O DINs may attribute to the insufficient modeling ability, particularly when Ïƒ = 50. Nonlinearity and weak context generally have a positive effect on image denoising, and the effectiveness of different IndexNets is similar. Hence, Shared Stagewise O2O DINs appear to be a preferred choice due to slightly increased parameters and negligible extra computation costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Semantic Segmentation</head><p>Here we further evaluate IndexNet on semantic segmentation. Semantic segmentation aims to predict a dense labeling map for each image where each pixel is labeled into one category. Since the FCNs were introduced <ref type="bibr" target="#b1">[2]</ref>, FCNbased encoder-decoder architectures have been studied extensively <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Efforts have been spent on how to encode contextual information, We use SegNet <ref type="bibr" target="#b2">[3]</ref> as  <ref type="figure" target="#fig_6">Fig. 13</ref>. IndexNet-guided feature pyramid network and multi-level feature fusion.</p><p>our baseline because IndexNet is primarily inspired by the unpooling operator in SegNet. We follow the experimental setting in <ref type="bibr" target="#b2">[3]</ref> and report performance on the SUN RGB-D <ref type="bibr" target="#b11">[12]</ref> dataset. We use RGB as the input (depth is not used).</p><p>The standard mean Intersection-over-Union (mIoU) is used as the evaluation metric. In addition, we also compare against the recent UperNet <ref type="bibr" target="#b45">[46]</ref>. We evaluate UperNet on the ADE20K dataset <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Network Architecture and Implementation Details</head><p>Network Architecture. The architecture of SegNet employs the first 13 layers of the VGG-16 model pretrained on ImageNet as the encoder. The decoder uses unpooling for upsampling. Each unpooling layer is followed by the same number of convolutional layers as in the corresponding encoder stage.</p><p>Overall, SegNet has 5 downsampling and 5 upsampling stages. Convolutional layers in the decoding stage mainly play a role to smooth the feature maps generated by unpooling. To insert IndexNet, the only modification is to replace max pooling and unpooling layers with IP and IU, respectively, which is straightforward.</p><p>UperNet builds upon the idea of Pyramid Pooling Module (PPM) <ref type="bibr" target="#b47">[48]</ref> and Feature Pyramid Network (FPN) <ref type="bibr" target="#b48">[49]</ref>. UperNet also implements a Multi-level Feature Fusion (MFF) module that fuses multi-resolution feature maps by concatenation. In FPN, downsampling is implemented by 2-stride convolution, and upsampling uses bilinear interpolation. It produces four feature levels {D 2 , D 3 , D 4 , D 5 } with output strides of {4, 8, 16, 32}, conditioned on the encoder features {E 2 , E 3 , E 4 , E 5 }. MFF further fuses four levels of features and generates the output M 2 with an output stride of 4. To insert IndexNet, three IndexNet blocks can be inserted into the encoder to generate index maps to guide upsampling. The same index maps can also be used in MFF in a sequential upsampling manner to fuse features, as shown in <ref type="figure" target="#fig_6">Fig. 13</ref>. Note that, in theory IndexNet can also be applied to PPM, because PPM itself has internal downsampling and upsampling stages. However, we discourage the use of IndexNet in PPM, because it will significantly increase parameters (due to mixed downsampling/upsampling rates). In this case blind upsampling such as NN/bilinear interpolation may be a better choice.</p><p>Training Details. On the SUN RGB-D dataset, the VGG-16 model pretrained on ImageNet with BN layers is used. We employ the standard data augmentation strategies: random scaling, random cropping 320 Ã— 320 sub-images, and random horizontal flipping. We learn the model with the standard softmax loss. Encoder parameters are pretrained on ImageNet. All other parameters are initialized with the improved Xavier <ref type="bibr" target="#b43">[44]</ref>. The SGD optimizer <ref type="bibr" target="#b36">[37]</ref> is used with a momentum of 0.9 and a weight decay of 0.0001. We train the model with a batch size of 16 for 300 epochs (around 90, 000 iterations). The learning rate is initially set to 0.01 and reduced by 10Ã— at the 250-th and 280-th epoch, respectively. The BN layers of the encoder are fixed. On the ADE20K benchmark, we use the MobileNetV2 pretrained on ImageNet as the encoder and UperNet the decoder. Due to limited computational resources, only this setting enables us to train a model on 4 GPUs with a batch size of 16 following the official implementation of UperNet and provided experimental settings. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Results on the SUN RGB-D Dataset</head><p>We report the results in <ref type="table" target="#tab_12">Table 8</ref>. All index networks show improvements over the baseline, among which Modelwise and Shared Stagewise O2O DINs improve the baseline with few extra parameters and GFLOPs. Compared with other types of IndexNet, M2O DINs and HINs (particularly under the setting of "Nonlinearity+Context") increase many parameters and GFLOPs but do not exhibit clear advantages.</p><p>From the qualitative results shown in <ref type="figure" target="#fig_2">Fig. 14,</ref> we can see that the improvement comes from the ability of IndexNet suppressing fractured predictions that frequently appears in the baseline SegNet. IndexNet seemingly does better in producing predictions at boundaries.</p><p>Notice that, in contrast to the behaviour in matting and denoising, CARAFE significantly enhances the performance in segmentation (32.47 â†’ 36.30), outperforming IndexNet. We observe that CARAFE tends to produce consistent region-wise predictions. A plausible explanation is that, CARAFE is designed in a way to tackle region-sensitive tasks such as semantic segmentation where region-wise matching between predictions and ground truths matters, while IndexNet prefers detail-sensitive tasks like image matting where errors come from detail-rich regions. <ref type="bibr" target="#b0">1</ref>. https://github.com/CSAILVision/semantic-segmentation-pytorch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Results on the ADE20K Dataset</head><p>Here we conduct ablative studies to highlight the role of upsampling in UperNet. Both IndexNet and CARAFE are considered. In addition to the full replacement of upsampling operators following <ref type="figure" target="#fig_6">Fig. 13</ref>, we also replace bilinear upsampling either in FPN or in MFF with IndexNet/CARAFE. Results are shown in <ref type="table" target="#tab_13">Table 9</ref>. It can be observed that, IndexNet improves UperNet (37.08 â†’ 37.62) only when bilinear upsampling in FPN and MFF is simultaneously replaced. However, when only one component is modified, IndexNet even leads to negative results. This suggests that the guided information should be used consistently in the decoder. In addition, CARAFE also works better than In-dexNet, showing that spatial information may not play a critical role in semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Monocular Depth Estimation</head><p>Estimating per-pixel depth from a single image is challenging because one needs to recover 3D information from a 2D plane. With deep learning, significant progress has been witnessed <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. We use the recent FastDepth <ref type="bibr" target="#b51">[52]</ref> as our baseline. We compare the performance with/without In-dexNet on the NYUv2 dataset <ref type="bibr" target="#b12">[13]</ref> with the official train/test split. To be in consistent with <ref type="bibr" target="#b51">[52]</ref>, the following metrics are used to quantify the performance:</p><p>â€¢ root mean square error (rms):</p><formula xml:id="formula_10">1 T T i=1 (d i âˆ’ g i ) 2 ;</formula><p>â€¢ accuracy with threshold th: percentage (%) of d 1 , s.t. max d1 g1 , g1 d = Î´ 1 &lt; th.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Network Architecture and Implementation Details</head><p>Network Architecture. FastDepth is an encoder-decoder architecture, with MobileNet as its backbone. Here we choose the best upsampling option suggested by the authors <ref type="bibr" target="#b51">[52]</ref> where upsampling is implemented by Ã—2 NN interpolation and 5 Ã— 5 convolution. Hence, our baseline is FastDepth-NNConv5: downsampling with 2-stride convolution and upsampling via NN interpolation. We also modify this baseline by changing the stride-2 convolution to be stride-1 followed by max-pooling, named as FastDepth-P-NNConv5. <ref type="figure" target="#fig_3">Fig. 15</ref> shows how we insert IndexNet into FastDepth. Similar to the modifications applied to the matting network, stride-2 convolution layers in the encoder are changed to be stride-1, followed by IP, and the NN interpolation in the decoder is replaced with IU. To compare IndexNet with CARAFE, we build two additional baselines: FastDepthcarafe and FastDepth-P-carafe, where NNConv5 is modi- fied to CARAFE in FastDepth-NNConv5 and FastDepth-P-NNConv5. Training Details. We follow similar training settings used by FastDepth <ref type="bibr" target="#b51">[52]</ref>. 1 loss is used. Random rotation, random scaling and random horizontal flipping are used for data augmentation. The initial learning rate is set to 0.01 and reduced by Ã—10 every 5 epochs. The SGD optimizer is used with a momentum of 0.9 and a weight decay of 0.0001. Encoder weights are pretrained on ImageNet <ref type="bibr" target="#b35">[36]</ref>. A batch size of 16 is used to train the network for 30 epochs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Results on the NYUDv2 Dataset</head><p>We report the results in <ref type="table" target="#tab_4">Table 10</ref>. We observe that almost all types of IndexNet improve the performance compared to the baselines except for the most light-weight designlinear Modelwise O2O DIN. It may be because only 16 parameters are not sufficient to model local variations of highdimensional feature maps. Note that, Unshared Stagewise O2O DINs (with only linear mappings) shows clear improvements with only slightly increased parameters. HINs and M2O DINs increase a large amount of parameters and floating-point calculations because of the high dimensionality of feature maps, while the improved performance is not proportional to such a high cost. Some qualitative results are further illustrated in <ref type="figure" target="#fig_4">Fig. 16</ref>. We observe that IndexNet exhibits better boundary delineation than the baseline, e.g., the edge of the desk, and the contour of the woman. Moreover, CARAFE achieves comparable performance against IndexNet in this task. In addition, we report the results of applying IndexNet to a state-of-the-art model <ref type="bibr" target="#b52">[53]</ref> in <ref type="table" target="#tab_4">Table 11</ref>. We modify the usage of IndexNet here by taking the same feature fusion strategies shown in <ref type="figure" target="#fig_6">Fig. 13</ref>. Other implementation details and evaluations are kept consistent with <ref type="bibr" target="#b13">[14]</ref>. Compared Only HIN ('Nonlinear+Context') is evaluated due to varied feature dimensionality of decoder and multi-level feature fusion. rel and log10 denote the average relative error and average log 10 error <ref type="bibr" target="#b50">[51]</ref>, respectively. The best performance is boldfaced.</p><p>with the baseline, IndexNet shows improvement in the first four metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Insights Towards Good Practices</head><p>As a summary of our evaluations, here we provide some guidelines for using guided/blind upsampling: 1) In detail-sensitive tasks, such as image matting, image restoration, and edge detection, the spatial information is important. Thus guided upsampling may be preferred. 2) Blind upsampling may be used in the situation when computational budget is limited because most blind upsampling operators are non-parametric and computationally efficient. 3) In image matting, the best IndexNet configuration is "M2O DINs+Nonlinearity+Context". This configuration is also true for the image reconstruction experiment and image denoising, where M2O DINs exhibit the best performance and the most stable behavior, respectively. Hence, the capacity of IndexNet is closely related to the complexity of local patterns. M2O DINs is preferred in a detail-or boundary-sensitive task, but one should also be aware of the increased model parameters and computation costs, especially when the feature maps are high-dimensional. 4) If one prefers a flexible decoder design, e.g., squeezing/enlarging the dimensionality of the decoder feature map, HINs are good choices, because DINs only generate index maps whose dimensionality is identical to the input feature map. 5) For real-time applications, Shared Stagewise O2O DINs are the first choices. Model parameters increased by Shared Stagewise O2O DINs are comparable to Modelwise O2O DINs, and the extra GFLOPs are also negelectable. Shared Stagewise O2O DINs, however, always work better than Modelwise O2O DINs for applications considered in this work. It implies that each upsampling stage should learn a stage-specific index function; 6) It is worth noting that, the current implementation of IndexNet has some limitations. Currently IndexNet only implements single-point upsampling-each upsampled feature point is only associated with a single point. In this sense, we may not simulate the behavior of bilinear interpolation where each upsampled point is affected by multiple points of a local region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Inspired by an observation in image matting, we examine the role of indices and present a unified view of upsampling operators using the notion of index functions. We show that an index function can be learned within a proposed index-guided encoder-decoder framework. In this framework, indices are learned with a flexible network module termed IndexNet, and are used to guide downsampling and upsampling using IP and IU. IndexNet itself is also a subframework that can be designed depending on the task at hand. We investigate five index networks, and demonstrate their effectiveness on four dense prediction tasks. We believe that IndexNet is an important step towards generic upsampling operators for deep networks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Indexed</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The index-guided encoder-decoder framework. The proposed IndexNet dynamically predicts indices for individual local regions, conditioned on the input local feature map itself. The predicted indices are further used to guide the downsampling in the encoding stage and the upsampling in the corresponding decoding stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>A taxonomy of proposed index networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Modelwise IndexNet vs. stagewise IndexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Holistic index networks. (a) a linear index network; (b) a nonlinear index network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Depthwise index networks. M = 1, N = 1 for Modelwise O2O DINs and Shared Stagewise O2O DINs; M = C, N = C for Unshared Stagewise O2O DINs; and M = C, N = 1 for the M2O DINs. The masked modules are invisible to linear networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Remark 3 .</head><label>3</label><figDesc>The number of parameters in Modelwise O2O DINs and Shared Stagewise O2O DINs is independent of the dimensionality of feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(32)-D-C(64)-D-C(128)-D-C(256)-C(128)-U -C(64)-U -C(32)-U -C(1). Note that BN and ReLU are not included in the last C(1). We build the following baselines: i) Average Pooling-NN interpolation (AvgPool-NN); ii) stride-2 Convolution-Bilinear interpolation (Conv /2 -Bilinear); iii) Space-to-Depth-Depth-to-Space (S2D-D2S); iv) stride-2 Convolution-2-stride Deconvolution (Conv /2 -Deconv /2 ); v) stride-2 Convolution-CARAFE (Conv /2 -CARAFE); vi) Max Pooling-Max Unpooling (MaxPool-MaxUnpool); vii) Indexed Pooling-Indexed Upsampling (IP-IU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Image reconstruction results on the Fashion-MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Customized MobileNetv2-based encoder-decoder network architecture. Our modifications are boldfaced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Visualization of the randomly initialized index map (left) and the learned index map (right) of HINs (top) and DINs (bottom). Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>The DnCNN architecture and our modified SegNet-like DnCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Semantic segmentation results on the SUNRGB-D dataset. From left to right, the original image, ground-truth, SegNet, and ours (Shared Stagewise O2O DIN with 'Nonlinearity'). Our modified FastDepth [52] architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Qualitative results on the NYUDv2 dataset. From left to right, the original image, ground-truth, FastDepth-NNConv5, and ours (Unshared Stagewise O2O DIN with 'Linear').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>to enable a 2 Ã— 2 Ã— 1 kernel operating on each H Ã— W Ã— 1 feature slice, respectively. All O2O DINs lead to four downsampled feature maps of size H/2 Ã— W/2 Ã— C. The final index map of size H Ã— W Ã— C is composed from the four feature maps by shuffling and rearrangement. Note that the parameters of four columns are not shared. Nonlinear O2O DINs. Nonlinear DINs can be easily modified from linear DINs by inserting four extra convolutional layers. Each of them is followed by a batch normalization (BN) layer and a ReLU unit, as shown inFig. 7. The rest remains the same as the linear DINs.</figDesc><table><row><cell></cell><cell></cell><cell>BN: Batch Normalization</cell></row><row><cell>BN+ReLU</cell><cell>Group Conv 1x1xM</cell></row><row><cell></cell><cell>group N</cell></row><row><cell>BN+ReLU</cell><cell></cell></row><row><cell>BN+ReLU</cell><cell></cell></row><row><cell>HxWxC</cell><cell></cell><cell>HxWxC</cell></row><row><cell>BN+ReLU</cell><cell></cell></row><row><cell></cell><cell>H/2xW/2x2C</cell><cell>H/2xW/2xC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 A</head><label>1</label><figDesc>Comparison of Model Complexity of Different Index Networks</figDesc><table><row><cell>IndexNet</cell><cell>Type</cell><cell># Param.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 Blind</head><label>2</label><figDesc>Upsampling and Guided Upsampling Operators By contrast, guided upsampling operators are rare in literature. Max unpooling, albeit being simple, is a guided upsampling operator. The auxiliary information used in upsampling comes from the max-pooling indices. Thefore, our proposed IndexNet clearly implements guided upsampling, with inferred dynamic indices as the auxiliary information.</figDesc><table><row><cell></cell><cell>Upsampling</cell><cell>Downsampling</cell></row><row><cell></cell><cell>NN Interpolation</cell><cell>Average Pooling</cell></row><row><cell>Blind Upsampling</cell><cell>Bilinear Interpolation Deconvolution Space-to-Depth</cell><cell>Convolution Convolution Depth-to-Space</cell></row><row><cell></cell><cell>CARAFE</cell><cell>Convolution</cell></row><row><cell>Guided</cell><cell>Max Unpooling</cell><cell>Max Pooling</cell></row><row><cell>Upsampling</cell><cell cols="2">Indexed Upsampling Indexed Pooling</cell></row><row><cell cols="2">blind upsampling operator.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3</head><label>3</label><figDesc>Performance of Image Reconstruction on the Fashion-MNIST Dataset denotes Modelwise O2O DIN; â€  indicates HIN; â€¡ refers to M2O DIN. All IndexNets are with nonlinearity and weak context. The best performance is boldfaced.</figDesc><table><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>MAE</cell><cell>MSE</cell></row><row><cell>AvgPool-NN</cell><cell>25.88</cell><cell>0.9811</cell><cell>0.0259</cell><cell>0.0509</cell></row><row><cell>Conv /2 -Bilinear</cell><cell>24.45</cell><cell>0.9726</cell><cell>0.0320</cell><cell>0.0600</cell></row><row><cell>S2D-D2S</cell><cell>28.93</cell><cell>0.9901</cell><cell>0.0204</cell><cell>0.0358</cell></row><row><cell>Conv /2 -Deconv /2</cell><cell>28.75</cell><cell>0.9903</cell><cell>0.0187</cell><cell>0.0366</cell></row><row><cell>Conv /2 -CARAFE</cell><cell>25.55</cell><cell>0.9798</cell><cell>0.0277</cell><cell>0.0529</cell></row><row><cell>MaxPool-MaxUnpool</cell><cell>29.33</cell><cell>0.9920</cell><cell>0.0202</cell><cell>0.0342</cell></row><row><cell>IP-IU *</cell><cell>37.83</cell><cell>0.9989</cell><cell>0.0089</cell><cell>0.0128</cell></row><row><cell>IP-IU  â€ </cell><cell>45.93</cell><cell>0.9998</cell><cell>0.0032</cell><cell>0.0051</cell></row><row><cell>IP-IU  â€¡</cell><cell>48.37</cell><cell>0.9999</cell><cell>0.0026</cell><cell>0.0038</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Ablation Study of Design Choices</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No.</cell><cell>Architecture</cell><cell>Backbone</cell><cell>Fusion</cell><cell>Indices</cell><cell>Context</cell><cell>OS</cell><cell>SAD</cell><cell>MSE</cell><cell>Grad</cell><cell>Conn</cell></row><row><cell>B1</cell><cell>DeepLabv3+ [6]</cell><cell>MobileNetv2</cell><cell>Concat</cell><cell>No</cell><cell>ASPP</cell><cell>16</cell><cell>60.0</cell><cell>0.020</cell><cell>39.9</cell><cell>61.3</cell></row><row><cell>B2</cell><cell>RefineNet [5]</cell><cell>MobileNetv2</cell><cell>Skip</cell><cell>No</cell><cell>CRP</cell><cell>32</cell><cell>60.2</cell><cell>0.020</cell><cell>41.6</cell><cell>61.4</cell></row><row><cell>B3</cell><cell>SegNet [7]</cell><cell>VGG16</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>32</cell><cell>54.6</cell><cell>0.017</cell><cell>36.7</cell><cell>55.3</cell></row><row><cell>B4</cell><cell>SegNet</cell><cell>VGG16</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>32</cell><cell>122.4</cell><cell>0.100</cell><cell>161.2</cell><cell>130.1</cell></row><row><cell>B5</cell><cell>SegNet</cell><cell>MobileNetv2</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>32</cell><cell>60.7</cell><cell>0.021</cell><cell>40.0</cell><cell>61.9</cell></row><row><cell>B6</cell><cell>SegNet</cell><cell>MobileNetv2</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>32</cell><cell>78.6</cell><cell>0.031</cell><cell>101.6</cell><cell>82.5</cell></row><row><cell>B7</cell><cell>SegNet</cell><cell>MobileNetv2</cell><cell>No</cell><cell>Yes</cell><cell>ASPP</cell><cell>32</cell><cell>58.0</cell><cell>0.021</cell><cell>39.0</cell><cell>59.5</cell></row><row><cell>B8</cell><cell>SegNet</cell><cell>MobileNetv2</cell><cell>Skip</cell><cell>Yes</cell><cell>No</cell><cell>32</cell><cell>57.1</cell><cell>0.019</cell><cell>36.7</cell><cell>57.0</cell></row><row><cell>B9</cell><cell>SegNet</cell><cell>MobileNetv2</cell><cell>Skip</cell><cell>Yes</cell><cell>ASPP</cell><cell>32</cell><cell>56.0</cell><cell>0.017</cell><cell>38.9</cell><cell>55.9</cell></row><row><cell>B10</cell><cell>UNet</cell><cell>MobileNetv2</cell><cell>Concat</cell><cell>Yes</cell><cell>No</cell><cell>32</cell><cell>54.7</cell><cell>0.017</cell><cell>34.3</cell><cell>54.7</cell></row><row><cell>B11</cell><cell>UNet</cell><cell>MobileNetv2</cell><cell>Concat</cell><cell>Yes</cell><cell>ASPP</cell><cell>32</cell><cell>54.9</cell><cell>0.017</cell><cell>33.8</cell><cell>55.2</cell></row></table><note>Fusion: fuse encoder features; Indices: max-pooling indices (where Indices is 'No', bilinear interpolation is used for upsampling); CRP: chained residual pooling</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 Results</head><label>5</label><figDesc>indicates increased parameters compared to B11. GFLOPs are measured on a 224 Ã— 224 Ã— 4 input. The lowest errors are boldfaced.least 12.3% relative improvement. Compared to B11, the baseline of DINs, M2O DIN with "Nonlinearity+Context" exhibits at least 16.5% relative improvement. Notice that, our best model outperforms the DeepMatting approach<ref type="bibr" target="#b6">[7]</ref> that even has the refinement stage. In addition, according to the results of B11-1.4, the performance improvement does not come from increased parameters. Moreover, CARAFE also enhances matting performance, but it falls behind M2O DIN. Some qualitative results are shown in</figDesc><table><row><cell></cell><cell></cell><cell cols="5">on the Composition-1k Testing Set</cell></row><row><cell cols="2">Method</cell><cell cols="6">#Param. GFLOPs SAD MSE Grad Conn</cell></row><row><cell cols="2">B3 [7]</cell><cell>130.55M</cell><cell>32.34</cell><cell>54.6</cell><cell>0.017</cell><cell>36.7</cell><cell>55.3</cell></row><row><cell>B11</cell><cell></cell><cell>3.75M</cell><cell>4.08</cell><cell>54.9</cell><cell>0.017</cell><cell>33.8</cell><cell>55.2</cell></row><row><cell cols="2">B11-1.4</cell><cell>8.86M</cell><cell>7.61</cell><cell>55.6</cell><cell>0.016</cell><cell>36.4</cell><cell>55.7</cell></row><row><cell cols="2">B11-carafe</cell><cell>4.06M</cell><cell>5.01</cell><cell>50.2</cell><cell>0.015</cell><cell>27.9</cell><cell>50.0</cell></row><row><cell>HMI</cell><cell></cell><cell>3.75M</cell><cell>4.08</cell><cell>56.5</cell><cell>0.021</cell><cell>33.0</cell><cell>56.4</cell></row><row><cell>NL</cell><cell>C</cell><cell>âˆ†</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>HINs</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>+4.99K</cell><cell>4.09</cell><cell>55.1</cell><cell>0.018</cell><cell>32.1</cell><cell>55.2</cell></row><row><cell></cell><cell></cell><cell>+0.26M</cell><cell>4.22</cell><cell>50.6</cell><cell>0.015</cell><cell>27.9</cell><cell>49.4</cell></row><row><cell></cell><cell></cell><cell>+1.04M</cell><cell>4.61</cell><cell>49.5</cell><cell>0.015</cell><cell>25.6</cell><cell>49.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Modelwise O2O DINs</cell><cell></cell></row><row><cell></cell><cell></cell><cell>+16</cell><cell>4.08</cell><cell>57.3</cell><cell>0.017</cell><cell>37.3</cell><cell>57.4</cell></row><row><cell></cell><cell></cell><cell>+56</cell><cell>4.08</cell><cell>52.4</cell><cell>0.016</cell><cell>30.1</cell><cell>52.2</cell></row><row><cell></cell><cell></cell><cell>+152</cell><cell>4.08</cell><cell>59.1</cell><cell>0.018</cell><cell>39.0</cell><cell>59.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Shared Stagewise O2O DINs</cell></row><row><cell></cell><cell></cell><cell>+80</cell><cell>4.08</cell><cell>48.9</cell><cell>0.014</cell><cell>26.2</cell><cell>48.0</cell></row><row><cell></cell><cell></cell><cell>+280</cell><cell>4.08</cell><cell>51.1</cell><cell>0.016</cell><cell>30.2</cell><cell>50.7</cell></row><row><cell></cell><cell></cell><cell>+760</cell><cell>4.08</cell><cell>56.0</cell><cell>0.016</cell><cell>37.5</cell><cell>55.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Unshared Stagewise O2O DINs</cell></row><row><cell></cell><cell></cell><cell>+4.99K</cell><cell>4.09</cell><cell>50.3</cell><cell>0.015</cell><cell>33.7</cell><cell>50.0</cell></row><row><cell></cell><cell></cell><cell>+17.47K</cell><cell>4.10</cell><cell>50.6</cell><cell>0.016</cell><cell>26.5</cell><cell>50.3</cell></row><row><cell></cell><cell></cell><cell>+47.42K</cell><cell>4.15</cell><cell>50.2</cell><cell>0.016</cell><cell>26.8</cell><cell>49.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">M2O DINs</cell><cell></cell></row><row><cell></cell><cell></cell><cell>+0.52M</cell><cell>4.34</cell><cell>51.0</cell><cell>0.015</cell><cell>33.7</cell><cell>50.5</cell></row><row><cell></cell><cell></cell><cell>+1.30M</cell><cell>4.73</cell><cell>48.9</cell><cell>0.015</cell><cell>32.1</cell><cell>47.9</cell></row><row><cell></cell><cell></cell><cell>+4.40M</cell><cell>6.30</cell><cell>45.8</cell><cell>0.013</cell><cell>25.9</cell><cell>43.7</cell></row><row><cell cols="4">DeepMatting w. Refinement [7]</cell><cell>50.4</cell><cell>0.014</cell><cell>31.0</cell><cell>50.8</cell></row></table><note>NL: Non-Linearity; C: Context.âˆ†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 Ablation.8 0.013 25.9 43.7</head><label>6</label><figDesc>Study of Different Normalization Choices on Index Maps</figDesc><table><row><cell>Encoder</cell><cell>Decoder SAD MSE Grad Conn</cell></row><row><cell>sigmoid</cell><cell>sigmoid 52.7 0.016 29.3 52.4</cell></row><row><cell>softmax</cell><cell>softmax 51.6 0.015 29.2 51.6</cell></row><row><cell cols="2">softmax+sigmoid softmax 57.3 0.016 43.5 57.3</cell></row><row><cell cols="2">sigmoid+softmax sigmoid 45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7</head><label>7</label><figDesc>Average PSNR (dB) and SSIM Results of Various Noise Levels on the BSD68 and Set12 Image Denoising Benchmarks</figDesc><table><row><cell></cell><cell cols="3">Method</cell><cell></cell><cell>#Param.</cell><cell>GFLOPs</cell><cell>BSD68</cell><cell>Set12</cell></row><row><cell></cell><cell cols="3">Noise Level</cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell>25</cell><cell>50</cell><cell>15</cell><cell>25</cell><cell>50</cell></row><row><cell></cell><cell cols="3">DnCNN [10]</cell><cell></cell><cell>0.56M</cell><cell>25.89</cell><cell>31.74/0.9410 29.22/0.9015 26.23/0.8269</cell><cell>32.87/0.9544 30.42/0.9296 27.17/0.8775</cell></row><row><cell></cell><cell cols="3">DnCNN-SegNet</cell><cell></cell><cell>7.09M</cell><cell>18.14</cell><cell>30.74/0.9278 28.27/0.8752 24.88/0.7437</cell><cell>31.91/0.9395 28.98/0.8881 24.99/0.7485</cell></row><row><cell cols="4">DnCNN-max-carafe</cell><cell></cell><cell>7.29M</cell><cell>19.11</cell><cell>25.70/0.7578 21.41/0.5670 15.40/0.2988</cell><cell>24.64/0.6997 20.19/0.4965 14.22/0.2489</cell></row><row><cell cols="4">DnCNN-avg-carafe</cell><cell></cell><cell>7.29M</cell><cell>19.11</cell><cell>25.70/0.7578 21.43/0.5673 15.56/0.2968</cell><cell>24.64/0.6997 20.20/0.4968 14.26/0.2460</cell></row><row><cell cols="4">DnCNN-conv-carafe</cell><cell></cell><cell>7.29M</cell><cell>15.23</cell><cell>25.70/0.7578 21.43/0.5672 15.50/0.2994</cell><cell>24.64/0.6997 20.20/0.4967 14.22/0.2496</cell></row><row><cell cols="2">NL</cell><cell></cell><cell>C</cell><cell></cell><cell>âˆ†</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HINs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+7.17K</cell><cell>18.16</cell><cell>31.13/0.9357 29.02/0.8997 26.29/0.8281</cell><cell>32.71/0.9536 30.28/0.9285 27.20/0.8789</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+0.69M</cell><cell>19.30</cell><cell>31.15/0.9356 29.01/0.8999 26.29/0.8301</cell><cell>32.77/0.9537 30.36/0.9295 27.18/0.8799</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+2.76M</cell><cell>22.75</cell><cell>31.20/0.9365 29.05/0.9004 26.30/0.8305</cell><cell>32.79/0.9541 30.37/0.9300 27.22/0.8804</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Modelwise O2O DINs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+16</cell><cell>18.14</cell><cell>31.22/0.9366 29.06/0.9002 25.84/0.8294</cell><cell>32.83/0.9545 30.42/0.9302 26.21/0.8782</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+56</cell><cell>18.14</cell><cell>30.64/0.9255 27.39/0.8391 24.15/0.6776</cell><cell>31.92/0.9386 27.97/0.8330 24.14/0.6747</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+152</cell><cell>18.14</cell><cell>30.87/0.9296 27.70/0.8617 24.09/0.6939</cell><cell>32.23/0.9432 28.31/0.8677 23.85/0.6634</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shared Stagewise O2O DINs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+48</cell><cell>18.14</cell><cell>31.14/0.9364 29.05/0.9002 26.32/0.8310</cell><cell>32.80/0.9542 30.41/0.9302 27.24/0.8807</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+168</cell><cell>18.14</cell><cell>31.20/0.9365 28.97/0.9000 26.18/0.8272</cell><cell>32.83/0.9545 30.43/0.9302 27.24/0.8801</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+456</cell><cell>18.14</cell><cell>31.22/0.9366 29.07/0.9004 26.31/0.8311</cell><cell>32.82/0.9543 30.41/0.9300 27.27/0.8814</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unshared Stagewise O2O DINs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+7.17K</cell><cell>18.16</cell><cell>31.17/0.9366 28.25/0.8944 25.02/0.8235</cell><cell>32.80/0.9544 30.23/0.9286 26.41/0.8675</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+25.1K</cell><cell>18.19</cell><cell>31.25/0.9368 29.06/0.9002 26.33/0.8306</cell><cell>32.77/0.9541 30.43/0.9303 27.29/0.8814</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+68.1K</cell><cell>18.32</cell><cell>31.21/0.9364 27.68/0.8740 26.33/0.8312</cell><cell>32.83/0.9544 30.32/0.9288 27.24/0.8807</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M2O DINs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+1.38M</cell><cell>20.44</cell><cell>31.22/0.9365 29.03/0.9005 26.33/0.8316</cell><cell>32.82/0.9544 30.42/0.9302 27.28/0.8812</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+3.45M</cell><cell>23.88</cell><cell>31.23/0.9368 29.07/0.9002 26.26/0.8278</cell><cell>32.84/0.9546 30.44/0.9304 27.28/0.8808</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+11.7M</cell><cell>37.67</cell><cell>31.23/0.9365 29.06/0.8996 26.34/0.8315</cell><cell>32.82/0.9545 30.43/0.9301 27.29/0.8803</cell></row><row><cell></cell><cell>E5</cell><cell></cell><cell></cell><cell></cell><cell>D5</cell><cell></cell><cell>IU</cell></row><row><cell></cell><cell>IP</cell><cell></cell><cell cols="3">IndexNet</cell><cell>IU</cell><cell>C</cell></row><row><cell></cell><cell>E4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>D4</cell><cell></cell></row><row><cell></cell><cell>IP</cell><cell></cell><cell cols="3">IndexNet</cell><cell>IU</cell><cell>C IU</cell></row><row><cell></cell><cell>E3</cell><cell></cell><cell></cell><cell></cell><cell>D3</cell><cell></cell><cell>M3</cell></row><row><cell></cell><cell>IP</cell><cell></cell><cell cols="3">IndexNet</cell><cell>IU</cell><cell>IU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C</cell></row><row><cell>E2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>D2</cell><cell></cell><cell>M2</cell></row><row><cell>IP</cell><cell>Indexed Pooling</cell><cell>IU</cell><cell>Indexed Upsampling</cell><cell>C</cell><cell>Concatenation</cell><cell></cell></row></table><note>NL: Non-Linearity; C: Context.âˆ† indicates increased parameters compared to the SegNet-DnCNN baseline. GFLOPs are measured on a 224 Ã— 224 Ã— 1 input.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8</head><label>8</label><figDesc>Performance on the SUN RGB-D Dataset.</figDesc><table><row><cell>Method</cell><cell></cell><cell>#Param.</cell><cell>GFLOPs</cell><cell>mIoU</cell></row><row><cell cols="2">SegNet [3]</cell><cell>24.96M</cell><cell>24.76</cell><cell>32.47</cell></row><row><cell cols="2">SegNet-carafe</cell><cell>25.35M</cell><cell>25.75</cell><cell>36.30</cell></row><row><cell>NL</cell><cell>C</cell><cell>âˆ†</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>HINs</cell></row><row><cell></cell><cell></cell><cell>+23.55K</cell><cell>24.79</cell><cell>33.25</cell></row><row><cell></cell><cell></cell><cell>+4.90M</cell><cell>26.40</cell><cell>33.11</cell></row><row><cell></cell><cell></cell><cell>+19.55M</cell><cell>31.28</cell><cell>33.31</cell></row><row><cell></cell><cell></cell><cell cols="2">Modelwise O2O DINs</cell></row><row><cell></cell><cell></cell><cell>+16</cell><cell>24.76</cell><cell>33.18</cell></row><row><cell></cell><cell></cell><cell>+56</cell><cell>24.76</cell><cell>33.70</cell></row><row><cell></cell><cell></cell><cell>+152</cell><cell>24.77</cell><cell>33.26</cell></row><row><cell></cell><cell></cell><cell cols="3">Shared Stagewise O2O DINs</cell></row><row><cell></cell><cell></cell><cell>+80</cell><cell>24.76</cell><cell>33.26</cell></row><row><cell></cell><cell></cell><cell>+280</cell><cell>24.76</cell><cell>33.97</cell></row><row><cell></cell><cell></cell><cell>+760</cell><cell>24.77</cell><cell>33.41</cell></row><row><cell></cell><cell></cell><cell cols="3">Unshared Stagewise O2O DINs</cell></row><row><cell></cell><cell></cell><cell>+0.02M</cell><cell>24.79</cell><cell>33.27</cell></row><row><cell></cell><cell></cell><cell>+0.08M</cell><cell>24.82</cell><cell>33.59</cell></row><row><cell></cell><cell></cell><cell>+0.22M</cell><cell>24.96</cell><cell>33.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell>M2O DINs</cell></row><row><cell></cell><cell></cell><cell>+9.76M</cell><cell>28.02</cell><cell>33.28</cell></row><row><cell></cell><cell></cell><cell>+24.44M</cell><cell>32.90</cell><cell>33.51</cell></row><row><cell></cell><cell></cell><cell>+83.02M</cell><cell>52.42</cell><cell>33.48</cell></row></table><note>NL: Non-Linearity; C: Context.âˆ† indicates increased parameters com- pared to the SegNet baseline. GFLOPs are measured on a 224Ã—224Ã—3 input.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 9 Performance</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">on the ADE-20K Dataset</cell></row><row><cell>FPN</cell><cell>MFF</cell><cell>mIoU</cell><cell>Pixel Accuracy (%)</cell></row><row><cell>Bilinear</cell><cell>Bilinear</cell><cell>37.08</cell><cell>78.29</cell></row><row><cell>IndexNet</cell><cell>Bilinear</cell><cell>36.25</cell><cell>78.23</cell></row><row><cell>Bilinear</cell><cell>IndexNet</cell><cell>36.90</cell><cell>78.27</cell></row><row><cell>IndexNet</cell><cell>IndexNet</cell><cell>37.62</cell><cell>78.29</cell></row><row><cell>CARAFE</cell><cell>Bilinear</cell><cell>37.76</cell><cell>78.81</cell></row><row><cell>Bilinear</cell><cell>CARAFE</cell><cell>38.03</cell><cell>78.51</cell></row><row><cell>CARAFE</cell><cell>CARAFE</cell><cell>38.31</cell><cell>78.90</cell></row><row><cell cols="4">MFF: Multi-level Feature Fusion. Only HINs ('Linear') are eval-</cell></row><row><cell cols="4">uated due to varied decoder feature dimensionality. The best</cell></row><row><cell cols="2">performance is boldfaced.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 10</head><label>10</label><figDesc>Performance of FastDepth [52] on the NYUDv2 Dataset. indicates increased parameters compared to the standard FastDepth baseline. GFLOPs are measured on a 224Ã—224Ã—3 input. The best performance is boldfaced.</figDesc><table><row><cell>Method</cell><cell></cell><cell>#Param.</cell><cell>GFLOPs</cell><cell>rms</cell><cell>Î´ 1 &lt; 1.25</cell></row><row><cell cols="2">FastDepth-NNConv5</cell><cell>3.96M</cell><cell>0.69</cell><cell>0.567</cell><cell>0.781</cell></row><row><cell cols="2">FastDepth-P-NNConv5</cell><cell>3.96M</cell><cell>1.01</cell><cell>0.577</cell><cell>0.778</cell></row><row><cell cols="2">FastDepth-carafe</cell><cell>4.31M</cell><cell>1.63</cell><cell>0.558</cell><cell>0.790</cell></row><row><cell cols="2">FastDepth-P-carafe</cell><cell>4.31M</cell><cell>1.96</cell><cell>0.571</cell><cell>0.782</cell></row><row><cell>NL</cell><cell>C</cell><cell>âˆ†</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>HINs</cell><cell></cell></row><row><cell></cell><cell></cell><cell>+31.23K</cell><cell>1.03</cell><cell>0.566</cell><cell>0.784</cell></row><row><cell></cell><cell></cell><cell>+11.17M</cell><cell>2.65</cell><cell>0.565</cell><cell>0.786</cell></row><row><cell></cell><cell></cell><cell>+44.62M</cell><cell>7.53</cell><cell>0.559</cell><cell>0.787</cell></row><row><cell></cell><cell></cell><cell cols="3">Modelwise O2O DINs</cell></row><row><cell></cell><cell></cell><cell>+16</cell><cell>1.02</cell><cell>0.569</cell><cell>0.778</cell></row><row><cell></cell><cell></cell><cell>+56</cell><cell>1.02</cell><cell>0.568</cell><cell>0.785</cell></row><row><cell></cell><cell></cell><cell>+152</cell><cell>1.02</cell><cell>0.564</cell><cell>0.786</cell></row><row><cell></cell><cell></cell><cell cols="4">Shared Stagewise O2O DINs</cell></row><row><cell></cell><cell></cell><cell>+80</cell><cell>1.02</cell><cell>0.562</cell><cell>0.783</cell></row><row><cell></cell><cell></cell><cell>+280</cell><cell>1.02</cell><cell>0.565</cell><cell>0.786</cell></row><row><cell></cell><cell></cell><cell>+760</cell><cell>1.02</cell><cell>0.567</cell><cell>0.783</cell></row><row><cell></cell><cell></cell><cell cols="4">Unshared Stagewise O2O DINs</cell></row><row><cell></cell><cell></cell><cell>+31.23K</cell><cell>1.03</cell><cell>0.556</cell><cell>0.789</cell></row><row><cell></cell><cell></cell><cell>+0.11M</cell><cell>1.06</cell><cell>0.564</cell><cell>0.786</cell></row><row><cell></cell><cell></cell><cell>+0.30M</cell><cell>1.16</cell><cell>0.562</cell><cell>0.788</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">M2O DINs</cell></row><row><cell></cell><cell></cell><cell>+22.30M</cell><cell>4.27</cell><cell>0.563</cell><cell>0.783</cell></row><row><cell></cell><cell></cell><cell>+55.78M</cell><cell>9.15</cell><cell>0.562</cell><cell>0.786</cell></row><row><cell></cell><cell></cell><cell>+189.57M</cell><cell>28.67</cell><cell>0.565</cell><cell>0.787</cell></row></table><note>NL: Non-Linearity; C: Context.âˆ†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 11</head><label>11</label><figDesc>Performance of Hu et al. [53] on the NYUDv2 dataset rms rel log10 Î´ &lt; 1.25 Î´ &lt; 1.25 2 Î´ &lt; 1.25 3 Hu et al.</figDesc><table><row><cell>[53]</cell><cell>0.558</cell><cell>0.129</cell><cell>0.055</cell><cell>0.837</cell><cell>0.968</cell><cell>0.992</cell></row><row><cell>Hu et al. + IndexNet</cell><cell>0.554</cell><cell>0.128</cell><cell>0.054</cell><cell>0.843</cell><cell>0.968</cell><cell>0.992</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1925" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR<address><addrLine>1, 2, 8, 9</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The case for learned index structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Management of Data</title>
		<meeting>International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fields of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image superresolution with fast approximate convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Neural Information Processing</title>
		<meeting>International Conference on Neural Information essing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">DeeperLab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CARAFE: Context-aware reassembly of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">KNN matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1826" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Im-ageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3291" to="3300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Generating training data for denoising real rgb images via camera pipeline simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jaroensri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biscarrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>2017. 12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Monocular relative depth perception with web stereo data supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fastdepth: Fast monocular depth estimation on embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wofk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
