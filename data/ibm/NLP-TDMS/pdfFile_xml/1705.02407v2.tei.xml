<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Zhihai</forename><surname>He</surname></persName>
						</author>
						<title level="a" type="main">Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human Pose Estimation</term>
					<term>Fractal Networks</term>
					<term>Knowledge-Guided Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose estimation using deep neural networks aims to map input images with large variations into multiple body keypoints which must satisfy a set of geometric constraints and inter-dependency imposed by the human body model. This is a very challenging nonlinear manifold learning process in a very high dimensional feature space. We believe that the deep neural network, which is inherently an algebraic computation system, is not the most effecient way to capture highly sophisticated human knowledge, for example those highly coupled geometric characteristics and interdependence between keypoints in human poses. In this work, we propose to explore how external knowledge can be effectively represented and injected into the deep neural networks to guide its training process using learned projections that impose proper prior. Specifically, we use the stacked hourglass design and inception-resnet module to construct a fractal network to regress human pose images into heatmaps with no explicit graphical modeling. We encode external knowledge with visual features which are able to characterize the constraints of human body models and evaluate the fitness of intermediate network output. We then inject these external features into the neural network using a projection matrix learned using an auxiliary cost function. The effectiveness of the proposed inception-resnet module and the benefit in guided learning with knowledge projection is evaluated on two widely used human pose estimation benchmarks. Our approach achieves state-of-the-art performance on both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation</head><p>Guanghan Ning, Student Member, IEEE, Zhi Zhang, Student Member, IEEE, and Zhihai He, Fellow, IEEE Abstract-Human pose estimation using deep neural networks aims to map input images with large variations into multiple body keypoints which must satisfy a set of geometric constraints and inter-dependency imposed by the human body model. This is a very challenging nonlinear manifold learning process in a very high dimensional feature space. We believe that the deep neural network, which is inherently an algebraic computation system, is not the most effecient way to capture highly sophisticated human knowledge, for example those highly coupled geometric characteristics and interdependence between keypoints in human poses. In this work, we propose to explore how external knowledge can be effectively represented and injected into the deep neural networks to guide its training process using learned projections that impose proper prior. Specifically, we use the stacked hourglass design and inception-resnet module to construct a fractal network to regress human pose images into heatmaps with no explicit graphical modeling. We encode external knowledge with visual features which are able to characterize the constraints of human body models and evaluate the fitness of intermediate network output. We then inject these external features into the neural network using a projection matrix learned using an auxiliary cost function. The effectiveness of the proposed inception-resnet module and the benefit in guided learning with knowledge projection is evaluated on two widely used human pose estimation benchmarks. Our approach achieves state-of-the-art performance on both datasets.</p><p>Index Terms-Human Pose Estimation, Fractal Networks, Knowledge-Guided Learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE task of human pose estimation is to determine the precise pixel locations of body keypoints from a single input image <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Closely-related tasks include 3D human pose estimation <ref type="bibr" target="#b7">[8]</ref> and human pose estimation in videos <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Human pose estimation is very important for many high-level computer vision tasks, including action and activity recognition <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, semantic content retrieval <ref type="bibr" target="#b13">[14]</ref>, humancomputer interaction, motion capture <ref type="bibr" target="#b14">[15]</ref>, and animation. Estimating human poses from still images is a challenging task. An effective human pose estimation system must be able to handle large pose variations, changes in clothing and lighting conditions, severe body deformations, heavy body occlusions <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. A key question for addressing these problems is how to extract strong low and mid-level appearance features capturing discriminative as well as relevant contextual information and how to model complex part relationships allowing for effective yet efficient pose inference. Traditional methods for pose estimation are mostly based on Pictorial Structure (PS) models <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b23">[24]</ref>, which models the spatial relations of rigid body parts using a tree model. A major drawback of such models is the need to hand-design the structure of the model  <ref type="figure">Fig. 1</ref>. Knowledge projection for guided learning. We encode external knowledge visual features which characterizes the constraints of human body models and then inject these external features into the neural network using a projection matrix learned using an auxiliary cost function, which is removed during testing, therefore not increasing network complexity. in order to capture important problem-specific dependencies amongst the different output variables and at the same time allow for tractable inference.</p><p>With Convolutional Neural Networks (ConvNets) and many assistive methods such as batch normalization <ref type="bibr" target="#b24">[25]</ref>, resnet <ref type="bibr" target="#b25">[26]</ref>, and inception design <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, human pose estimation has recently achieved significant progress. Even though deep neural networks are capable of fitting large training data through extensive training, the network often needs to be constructed deeper and wider to gain enough representation power <ref type="bibr" target="#b28">[29]</ref>. As the network becomes more complex, the learning and training processing become more sophisticated and challenging <ref type="bibr" target="#b29">[30]</ref>, especially for those applications with complicated loss functions.</p><p>Human pose estimation using deep neural networks requires us to map the input images with large variations into multiple body keypoints which must satisfy a set of geometric constraints and interdependence imposed by the human body model. This is a very challenging nonlinear manifold learning process in a very high dimensional feature space. We believe that the deep neural network, which is inherently an algebraic computation system, is not the most efficient way to capture highly sophisticated human knowledge, for example those highly coupled geometric characteristics and interdependence between keypoints in human poses.</p><p>In this work, we propose to explore how external knowledge can be effectively represented and injected into the deep neural networks to guide its training process using learned projections arXiv:1705.02407v2 [cs.CV] 8 Aug 2017</p><p>for more accurate and robust human pose estimation. Specifically, as illustrated in <ref type="figure">Fig. 3</ref>, we use inception-resnet module and the stacked hourglass structure to construct a fractal network to regress human pose images into heatmaps with no explicit graphical modeling. We encode external knowledge with visual features which characterize the constraints of human body models and evaluate the fitness of intermediate network output. We then inject these external features into the neural network using a projection matrix learned using an auxiliary cost function. The guidance from the external knowledge is only used during the training process, and is turned off during network inference for human pose estimation. The benefit of external knowledge is to guide the training of the neural network. Its effect is implicitly imposed on the tuning of the parameters, instead of explicit feature representation of the network. The injected features for pairs of limbs impose a strong prior during the training, preventing human part keypoint from connecting to noises, e.g., keypoint from other people in the background that is not cropped out for the target person.</p><p>The major contributions of this work are summarized as follows: <ref type="formula" target="#formula_1">(1)</ref> We develop a new framework to represent and project human knowledge to guide the training of deep neural networks for human pose estimation. This external knowledge project framework is generic and can be extended to other learning and training applications and deep neural network design. <ref type="bibr" target="#b1">(2)</ref> We propose an efficient network structure, called fractal networks, for human pose estimation to capture the multi-scale interdependence between body joints in the pose model. This fractal network uses an inception-resnet module as the building block.</p><p>The rest of the paper is organized as follows. In section II, we provide a brief review of recent works on human pose estimation. Section III introduces the concept of knowledge guided learning, the structure of fractal network, and the design of inception-resnet module. Section V presents our experimental results. Section VI concludes our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structured Prediction and Graphical Models</head><p>Prior to the advent of neural networks most previous work was based on pictorial structures <ref type="bibr" target="#b30">[31]</ref> which model the human body as a collection of rigid templates and a set of pairwise potentials taking the form of a tree structure, thus allowing for efficient and exact inference at test time. Higher knowledge of the human body is exploited by modeling humans with body parts that are connected via a skeleton structure. Pictorial structure model <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, models the spatial relations of rigid body parts using a tree model. A pre-defined kinematic body model is often used to assume that each body part is independent of all the others except for the ones it is attached to. A major drawback of such models is the need to handdesign the structure of the model in order to capture important problem-specific dependencies amongst the different output variables and at the same time allow for tractable inference.</p><p>Recent work includes sophisticated extensions like mixture, hierarchical, multimodal and strong appearance models <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, non-tree models <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> as well as cascaded/sequential prediction models like pose machines <ref type="bibr" target="#b34">[35]</ref>. While in <ref type="bibr" target="#b30">[31]</ref> each limb is represented by a single template that is parameterized by location, orientation, shape parameters, and an appearance model, Yang and Ramanan <ref type="bibr" target="#b32">[33]</ref> propose mixtures of part templates where body part is represented by a set of deformable part templates. Although this approach performs well in comparison to classical pictorial structure models for human pose estimation, it has some limitations. For instance, the used scanning-window templates trained with linear SVMs and HOG features <ref type="bibr" target="#b35">[36]</ref> are very sensitive to noise <ref type="bibr" target="#b36">[37]</ref>. Hierarchical models <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> represent the relationships between parts at different scales and sizes in a hierarchical tree structure. The underlying assumption of these models is that larger parts (that correspond to full limbs instead of joints) can often have discriminative image structure that can be easier to detect and consequently help reason about the location of smaller, harder-to-detect parts. On the other hand, there are non-tree models <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> to incorporate interactions that introduce loops to augment the tree structure with additional edges that capture symmetry, occlusion and long-range relationships. These methods usually have to rely on approximate inference during both learning and at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Neural Networks for Human Pose Regression</head><p>ConvNets have been shown to produce remarkable performance for a variety of difficult Computer Vision tasks including detection <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, recognition <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and semantic segmentation <ref type="bibr" target="#b40">[41]</ref>. A key feature of these approaches is that they integrate non-linear hierarchical feature extraction with the classification or regression task in hand being also able to capitalize on very large data sets that are now readily available.</p><p>Since the work of DeepPose by Toshev et al. <ref type="bibr" target="#b15">[16]</ref>, research on human pose estimation has shifted from traditional approaches to deep neural networks (DNN) due to their superior performance. In the context of human pose estimation, it is natural to formulate the problem as a regression one in which CNN features are regressed in order to provide joint predictions of the body parts <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b41">[42]</ref>. For the case of nonvisible parts, learning the complex mapping from occluded part appearances to part locations is hard and the network has to rely on contextual information provided by other visible parts to infer the occluded part locations. DeepPose uses a deep neural network to directly regress the coordinates of body joints. Tompson et al. <ref type="bibr" target="#b16">[17]</ref> argued that it is more efficient to use DNN to regress heatmap images at multiple scales. While body models are not a necessary component for effective part localization, constraints between parts allow us to assemble independent detections into a body configuration. Detectionbased methods are relying on powerful CNN-based part detectors which are then combined using a graphical model <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b42">[43]</ref> or refined using regression <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Regression-based methods try to learn a mapping from image and CNN features to part locations. <ref type="bibr" target="#b42">[43]</ref> achieved promising results by combining CNN-based body part detectors with a body model <ref type="bibr" target="#b32">[33]</ref>. Human pose estimation methods using deep neural networks have proven their significant advantages over traditional approaches. However, deeper and wider networks are often required to improve the feature representation power, which in turn leads to increased difficulty in training the neural networks. Recently, residual learning <ref type="bibr" target="#b25">[26]</ref> has been used to significantly improve the performance of human pose estimation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b45">[46]</ref>. It was used for part detection in the system of <ref type="bibr" target="#b45">[46]</ref>. stacked hourglass network of <ref type="bibr" target="#b17">[18]</ref> elegantly extends fully convolutional networks <ref type="bibr" target="#b46">[47]</ref> and deconvolution nets <ref type="bibr" target="#b47">[48]</ref> with residual learning.</p><p>Intermediate supervision <ref type="bibr" target="#b48">[49]</ref>, recursive prediction <ref type="bibr" target="#b49">[50]</ref>, and inception design <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> are among other successful techniques that have been applied by recent methods for human pose estimation. Recently, researchers recognize that successive predictions can boost the performance of pose estimation, where parts are sequentially refined <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. In these models an initial prediction is made of all the parts; in subsequent steps, all part predictions are refined based on the image and earlier part predictions. Tompson et al. <ref type="bibr" target="#b43">[44]</ref> use a cascade of networks for refined predictions to achieve significantly improved precision in joint localization. Carreira et al. <ref type="bibr" target="#b51">[52]</ref> introduce a so-called Iterative Error Feedback scheme, where a set of predictions is included in the input, and each pass through the network further refines these predictions. Their method requires multi-stage training and the weights are shared between iterations. Recently, adding supervision to intermediate layers of deep networks is also explored to assist the training process <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Methods in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> use intermediate supervision to add auxiliary supervision branches in the network to assist the training process for human pose estimation. These approaches all employ the inception design by concatenating heatmaps from different stages or abstract levels as the input for the next layers.</p><p>One direction for further improvement of human pose estimation is to design convolutional networks that can produce robust visual features. Multi-scale processing by repet-itive down-sampling and up-sampling has been introduced in Stacked Hourglass Networks <ref type="bibr" target="#b17">[18]</ref>. Another approach to improve human pose estimation performance is to use explicit part-based models <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref> or implicitly encode configuration model using its contexts <ref type="bibr" target="#b53">[54]</ref>. These methods involve additional sub-networks to detect parts, which increases the overall complexity. In this work, we leverage these ideas and approaches. We propose a fractal network structure using inception-resnet as building blocks to explore the multi-scale interdependence nature of human pose configuration and to capture these characteristics across different scales and resolutions. The network is fractal in that it reflects the co-occurrance of inception and residual design at both the highest and lowest levels of abstractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transfer Learning and Guided Training</head><p>Nevertheless, training such deep networks has proven to be challenging <ref type="bibr" target="#b54">[55]</ref>. Significant efforts has been devoted to alleviate this problem. For instance, there has been another line of work in which a student network is trained from scratch to mimic the behavior of a much larger teacher network. Staring from Bucila et al.s work <ref type="bibr" target="#b55">[56]</ref> and Hinton et al.s more general Knowledge Distillation (KD) <ref type="bibr" target="#b56">[57]</ref> approach, the knowledge transfer in learning process has gained a lot of research interest. In this paper, we consider a unique setting of the problem. Instead of transferring knowledge from teacher networks into a student network, we propose an external knowledge representation and projection framework to guide the training process of our deep neural network for human pose estimation. Specifically, we inject hand-designed features that are inferred from ground truth as external knowledge to aid the training of a highly complex network with deep structure and multiple loss functions. Inspired by <ref type="bibr" target="#b57">[58]</ref>, which proposed a locality principle to learn task-specific feature mapping for shape regression, we project the external knowledge with a learned feature mapping. The procedure involves domain adaptation and model training simultaneously. Since external GNet Predicted Heatmaps</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predict Poses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth Heatmaps</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Converting Poses</head><p>External Knowledge</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inferring Features</head><p>Learned Projections</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learn Linear Projections Compute Gradients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss 2 Compute Gradients</head><p>GNet Predict Poses</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D-NMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward</head><p>Gradient from Loss 1 Gradient from Loss 2 W <ref type="figure">Fig. 3</ref>. Framework of our proposed Guided Network (GNet). The projected knowledge affects the gradients propagated back to convolutional layers but they are not part of the network during deployment. By enforcing constraints with external knowledge injection, high-level information of longrange dependencies between image and multi-part cues that is hard to capture with implicit learning can be better learned under the guidance of mid-level knowledge projection.</p><p>knowledge is inferred from ground truth, it is inherently more reliable and effective than the outputs from a teacher network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Structure and Design</head><p>Human pose estimation methods using hand-crafted features or graphical structure models based on human knowledge lack the flexibility in learning and the potential to achieve great representation power. On the other hand, pure data-driven neural networks may not be able to capture sophisticated knowledge involved in human pose estimation. In this work, we propose to represent and inject external human knowledge to guide the learning of deep neural networks (DNN), as illustrated in <ref type="figure">Fig. 1</ref>. Our major idea is that, by enforcing constraints and guidance with external knowledge injection, high-level information of long-range dependencies between image and multi-part cues, that are hard to capture with implicit learning, can be better learned under the guidance of mid-level knowledge projection. As shown in <ref type="figure">Fig. 3</ref>, the projected knowledge affects the gradients propagated back to convolutional layers during training, but they are not part of the network during test.</p><p>We borrow the ideas from inception-residual networks <ref type="bibr" target="#b58">[59]</ref> and propose to construct a basic inception-resnet module in replacement of convolutional layers for more robust feature representation. Hourglass network is first introduced in <ref type="bibr" target="#b17">[18]</ref> where features are processed across all scales by repetitve down-sampling and up-sampling and then consolidated to best capture various spatial relationships associated with the body. We introduce a modified version of the hourglass network with the proposed inception-resnet module. As shown in <ref type="figure">Figure 2</ref>, we use proposed inception-resnet modules and improved hourglass sub-networks to construct a fractal network to regress human pose images into heatmaps with no explicit graphical modeling. The network is fractal in that it has the same network configuration at all levels of analysis and abstractions. This fractal network is designed to capture the multiscale interdependence nature of human pose configuration and to represent these characteristics across different scales and resolutions. In the inception network, we perform channelwise concatenation of two tensors from different sources. This enforces the information represented by the features stored in these tensors to be complementary to each other. It encourages and directs these two sources to work on different concepts to produce a more robust union representation <ref type="bibr" target="#b25">[26]</ref>. In the Resnet model, we perform pixel-wise addition of two tensors with the same number of channels. From our experiments, we find that this network design allows us to train the network more effectively, since it enforces two separate tensors to be simultaneously accurate in order to render the expected outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fractal Network with Inception-resnet Modules</head><p>Our motivation in the fractal network design is that, we need the network to focus on various scales across human parts, and at each scale, the network should also have an overall understanding of this receptive field. At higher levels, the network captures dependencies among various human parts. At lower levels, we use same fractal design to capture regional dependencies. It is essential to capture local dependencies in addition to local appearances. Because at a certain highlevel scale, the receptive field may involve a human part as well as noises from other parts. These adjacent parts may be from the same or other persons. Therefore, local dependencies are helpful in providing more reliable features to higher-level networks.</p><p>The construction of inception-resnet module is shown in <ref type="figure">Figure 4</ref>. Based on the hourglass design proposed in <ref type="bibr" target="#b17">[18]</ref> shown in <ref type="figure">Figure 5</ref>, an improved version of hourglass network is developed in this work as a mid-level sub-network which also uses inception-resnet modules as the basic units, as illustrated in <ref type="figure" target="#fig_0">Figure 6</ref>. To combine the advantages of both inception and resnet design, we introduce the inception-resnet module as the basic building block to analyze local fields, while using an improved hourglass network to capture the global information of different parts.</p><p>At the bottom level, we propose to use inception-resnet module as the basic structure unit of the network. It consists of convolutional layers, batch norm layers and relu units, with channel-wise concatenation and pixel-wise additions. Convolution layers are padded such that the resolution of output is the same as that of the input. Although the concatenation of two branches maintains different level of information, the concatenated features across different channels need to be transformed and normalized by the subsequent convolutional layers. In the proposed inception-resnet module, the concatenation layer is followed by another convolutional layer with 1 × 1 kernels. The benefit of this module is that the input and output have the same resolution while the depth of channels can be flexible.</p><p>At the sub-network level, we implement the recursive hourglass for 4 levels as shown in <ref type="figure">Figure 5</ref>. In other words, it will process the image at four scales. The hourglass network</p><formula xml:id="formula_0">Channels = M &lt;M&gt; k=1x1 &lt;N/2&gt; &lt;M&gt; k=1x1 &lt;N/2&gt; &lt;N/2&gt; k=3x3 &lt;N/2&gt; &lt;N/2&gt; k=3x3 &lt;N/2&gt; &lt;N/2&gt; k=3x3 &lt;N/2&gt; &lt;M&gt; k=1x1 &lt;N&gt; &lt;N&gt; k=1x1 &lt;N&gt; Channels = N Convolution</formula><p>Batch Norm Relu Unit Concatenate Pixel-wise Add <ref type="figure">Fig. 4</ref>. Basic module: Inception-resnet. Convolution layers are padded such that the resolution of output is the same as that of the input. The benefit of this module is that the input and output are of uniform resolution while the depth of channels can be changed. The function of this basic module is to interpret the input information from one form to another, extracting features for another abstraction level with little loss of information quantity. <ref type="figure">Fig. 5</ref>. An illustration of hourglass design proposed in <ref type="bibr" target="#b17">[18]</ref>. Pixel-wise addition fuses the information from two branches while keeping the input and output resolution uniform. The illustration gives an example of a 4-level hourglass.</p><p>is nested in itself. The first level of hourglass in our network is an inception-resnet module. As illustrated in <ref type="figure">Figure.</ref> 6, we also borrow the idea of hourglass design by down-sampling and then up-sampling the data while using inception-resnet as proposed common building block. Pixel-wise addition fuses the information from two branches while keeping the input and output resolution the same. At the top fractal network level, images of size 256 × 256 are down-sampled into the resolution of 64×64. Subsequently, inputs and outputs of all modules are of size 64×64, including the output heatmaps. The network captures and consolidates information across all scales of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. External Knowledge Representation</head><p>The fractal network is used to boost the data representation power of the deep neural network for human pose estimation. As the network grows deeper and more complicated, it requires careful attention to the training process. Furthermore, we recognize that the deep neural network is inherently an algebraic computing system, which might not be the most efficient way to capture the highly sophisticated human knowledge during pose estimation, for example those highly coupled geometric constraints and interdependence among body joints. To address these two issues, in this work, we propose to encode and inject external knowledge into the fractal network to guide the training process of the network using learned projections, enforcing a prior during the training process.</p><p>In this work, we propose to inject the geometric representation of knowledge into the heatmap layer of the network. Since the heatmaps to be predicted are correlated to each other as they largely share parameters on former layers, the constraint on one heatmap influences the parameters of these layers and therefore having an impact on the training of other heatmaps. We observe that intermediate layers in our network are low and mid-level visual features; higher-level semantic features are hard to locate and explicitly interpret. The predicted heatmaps are easier to enforce the external knowledge and constraints upon. During the training process, the external knowledge and its visual representations are projected into the background and keypoint heatmaps using a projection matrix. We find that this type of knowledge-guided learning inherently enforces longrange dependencies and configurations among human joints, while leaving the flexibility of representation to the depth of the network, the quality and quantity of training data. In the following, we explain the proposed method in more detail.</p><p>During the training process, the external knowledge representation module illustrated in <ref type="figure">Figure.</ref> 1 has access to the original training sample image and its ground-truth joint locations.</p><p>Specifically, during feature mapping which is denoted as Φ, we perform Hough Transform on each line traversing two separate joints denoted as (u i , v i ) and (u j , v j ). In Hough space, each line is represented by a coordinate (θ, ρ).</p><formula xml:id="formula_1">   θ = arctan( x i − x j y j − y i ) ρ = x j × cos(θ) + y j × sin(θ)<label>(1)</label></formula><p>In order to represent the information in a less crisp manner, we convert the coordinates into a normalized vector representation. To incorporate the inherent learning of geometrical features such as angles and distance, we also inject the joint locations alongside each line. Based on the visibility of each joint, the line traversing it is encoded with the number of visible joints.</p><p>In addition to encoding geometric features, we encode image descriptors such as Histogram of Gradients (HOG)   [36] around each pair of adjacent joints in order to capture visual features to compensate spatial dependencies. While we preserve the flexibility of deep convolutional features that automatically learn visual semantics, we use hand-crafted features as guidance of the learning by enforcing a strong prior during the training of the neural network. We noticed that human joints may connect to those from an adherent person, even though the ground truth joints are not self-occluded or objectoccluded. We believe HOG features are helpful in observing edges and therefore distinguishing real and false limbs. The injected features for pairs of limbs impose a strong prior during the training, preventing human part keypoint from connecting to noises, e.g., keypoint from other people in the background that is not cropped out for the target person, which is helpful in the learning of body part interdependencies. As illustrated in <ref type="figure" target="#fig_1">Figure 7</ref>, the features are concatenated and normalized for the external knowledge representation. For self-occluded and object-occluded joints, we mask corresponding features with zeros. Specifically, we follow the traditional HOG feature extraction schemes, applying filters D x = [−1 0 1] and D y = [1 0 − 1] T horizontally and vertically to generate gradient maps I x and I y . Instead of scanning a window for blocks and cells over the image which is done in traditional ways, we locate limbs based on meta-data from the training set and extract histogram of gradients for such regions. The magnitude and orientation of the gradient are respectively computed by:</p><formula xml:id="formula_2">|G| = I 2 x + I 2 y<label>(2)</label></formula><p>and ϕ = arctan</p><formula xml:id="formula_3">I y I x<label>(3)</label></formula><p>We use 8 bins for the pooling, followed by block normalization (L2-norm) to mitigate the effect of unbalanced area of regions:</p><formula xml:id="formula_4">f = v ||v|| 2 2 + e 2<label>(4)</label></formula><p>Where e is a very small number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Knowledge Projection into the Deep Neural Network</head><p>In favor of decoding the abstract external knowledge in higher-dimensional space, we afford 2 fully-connected (FC) layer and 3 convolutional layers for the for geometric features and edge features, between the projection representation and the injected knowledge to learn linear projection W , which will be removed during testing as it is undesirable to keep redundant layers.</p><p>We inject external features as knowledge K via global feature mapping function Φ and learn a global linear projection W by minimizing the loss from the knowledge projection layer:</p><formula xml:id="formula_5">L KP = ||K − W × H J || 2 2 + β × ||W || 2 2<label>(5)</label></formula><p>where the first term is the regression target, the second term is a L2 regularization on W , and β controls the regularization strength. Regularization is necessary because the dimensionality of the features is very high. Since the objective function is quadratic with respect to W , we can always reach its global optimum <ref type="bibr" target="#b57">[58]</ref>. Specifically, we enforce two loss functions, one for injected geometric features and one for limb-wise edge features. <ref type="bibr" target="#b0">(1)</ref> The ground truth heatmap is convolved by 1x1 kernels, outputting 8 channels of maps. It is padded such that the resolution does not change. A fully connected layer with an output of 224-dimensional geometric feature is added to the convolutional layer. We add L2 loss (weighted by 0.05) for the geometric features and the inferred features from the ground truth. <ref type="bibr" target="#b1">(2)</ref> We branch out the 3rd inception-residual module at the early stage and feed its output to a series of convolutional layers with 1x1 kernels. The numbers of output channels are scaled twice by a factor of 1/2 until it reaches 32 channels, followed by a fully connected layer. We add L2 loss (weighted by 0.05) for injected edge features and the inferred edge features.</p><p>We denote the pixel location of the j-th anatomical landmark (which we refer to as human joint), Y j ∈ Z ⊂ R 2 , where P is the set of all (u, v) pixel locations in image coordinate system. Our goal is to predict the image locations Y = (Y 1 , ...Y J ) for all J joints. The output heatmaps are of size J × 64 × 64, denoted by H = (H 1 , ..., H J ), which are predicted beliefs for assigning a pixel location to each joint Y j = p, ∀z ∈ P, producing belief scores S j for all pixels in the heatmap of joint j:</p><formula xml:id="formula_6">H j (p) ← S j (Y j = p)<label>(6)</label></formula><p>In our experiments, we regress RGB-channel images into a set of 15 heatmaps, 14 of which are human joints while the other one as the background. The heatmaps are then suppressed into joint locations Y with our proposed 3D-NMS algorithm specially designed for human pose estimation. During training, we provide ground truth heatmaps for each joint by creating Gaussian peaks at ground truth locations. The cost function L f we aim to minimize for the fractal network is given by:</p><formula xml:id="formula_7">L f = j∈J ||H j (p) − H * j (p)|| 2 2 (7)</formula><p>The overall loss for training is a weighted combination of heatmap cost and projection matrix fitness provided by knowledge-guided learning, with a control parameter on how much guidance should be imposed. The overall network is then trained to minimize the following joint loss function:</p><formula xml:id="formula_8">W * f ← arg min W f (λ × L KP + (1 − λ) × L f )<label>(8)</label></formula><p>where L KP and L f are loss from knowledge projection layer and the fractal network loss, λ is the weight parameter decaying during training, and W * f is the trained parameters in the fractal network.</p><p>The output of knowledge projection layer will guide the training of fractal network by generating a strong and explicit gradient applied to backward path to the injection layer in the following form:</p><formula xml:id="formula_9">∆W f,i = −λ · ∂L KP ∂W f,i<label>(9)</label></formula><p>Where W f,i is the weight matrix of injection layer in fractal network. Note that the network update only occurs during training. During testing, the knowledge representation and projection modules are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Cross-Heatmap Non-Maximum Suppression</head><p>In this work, we introduce a novel pose non-maximum suppression (NMS) algorithm specially designed for human pose estimation. Our experiments in Section V-D show that employing pose-NMS consistently render better predictions for all models across iterations on both MPII <ref type="bibr" target="#b59">[60]</ref> and LSP <ref type="bibr" target="#b60">[61]</ref> datasets. Instead of finding the maximum value at pixel-level to predict joint location as in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b53">[54]</ref>, we detect blobs with high responses in each heatmap. Basically, we gather blobs from all heatmaps for suppression. We first find the blob with maximum response, then suppress other blobs from the same heatmap, and blobs from other heatmaps very close to this blob in image coordinate system. We repeat this procedure until all blobs are removed. The suppression takes place in image coordinate system and channel-wise (u, v, c), therefore called cross-heatmap NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SUMMARY OF TRAINING AND TESTING PROCEDURES</head><p>We summarize our training and testing procedures in Algorithm 1 and 2, respectively. There exist around 250 convolutional layers in the original hourglass network, while the proposed network with inception-resnet modules consist of over 300 convolutional layers. The network for training the proposed network has an additional cost with 1 external feature extraction module, 2 fully connected layers, 3 convolutional layers and 2 additonal loss layers. In our implementation, it takes the hourglass network an average of 47ms to feed forward with a single Pascal TITAN X GPU. In comparison, the feed forward time of the proposed network with inceptionresnet modules during testing is 62ms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>For comprehensive experimental analysis, we will first introduce the datasets, evaluation criteria and implementation details. Then we will present quantitative evaluations on benchmark datasets. Finally, diagnostic experiments, algorithm performance analysis and dicussions are provided for further analysis.</p><p>A. Datasets and Criteria 1) Datasets: We evaluate the proposed method on two widely used benchmarks: MPII Human Pose <ref type="bibr" target="#b59">[60]</ref> and extended Leeds Sports Poses (LSP) <ref type="bibr" target="#b60">[61]</ref>. The MPII Human Pose dataset includes about 25K images with 40k annotated poses. The images are collected from YouTube videos covering daily human activities with highly articulated human poses. The LSP dataset with extended training data consists of 11K training images and 1K testing images from sports activities.</p><p>2) Criteria: There are three criteria used in the experiments to evaluate the performance of the proposed human pose estimation approach: Percentage of Corrected Parts (PCP) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, Percentage of Detected Joints (PDJ) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b32">[33]</ref>, and Percentage of Corrected Keypoints (PCK) <ref type="bibr" target="#b32">[33]</ref>. a) PCP: A widely-used criterion for human pose estimation is PCP which evaluates the localization accuracy of body parts (sticks of skeleton). It requires the estimated part end points must be within half of the part length from the ground truth part end points. As pointed by Yang and Ramanan <ref type="bibr" target="#b32">[33]</ref>, some previous work requires only the average of theendpoints of a part to be correct (PCP-average), rather than both endpoints (PCP-strict). Moreover, the early PCP implementation <ref type="bibr" target="#b61">[62]</ref> selects the best matched output without penalizing false positives. In all our experiments, we adopt the strictest measure, i.e., PCP-strict with single output, if not specially specified. For more detailed descriptions on PCP, it is recommented to refer to <ref type="bibr" target="#b61">[62]</ref> and <ref type="bibr" target="#b32">[33]</ref>. b) AUC: Though PCP is the initially preferred criterion for evaluation, it has the drawback of penalizing shorter limbs, such as lower arms. Thus PDJ is introduced <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref> to measure the detection rate of body joints, where a joint is considered to be detected if the distance between the detected joint and the true joint is less than a fraction of the torso diameter. The torso diameter is usually defined as the distance between opposing joints on the human torso, such as left shoulder and right hip <ref type="bibr" target="#b15">[16]</ref>. The Area Under Curve (AUC) can be used as the overall evaluation of the PDJ curve.</p><p>In the following experiments, we report AUC as our PDJ performance. c) PCK: The PCK measure is very similar to the PDJ criterion. The only difference is that the torso diameter is replaced with the maximum side length of the external rectangle of ground truth body joints. For full body images with extreme pose (especially when the torso becomes very small), the PCK may be more suitable to evaluate the accuracy of body part localization.</p><p>In our experiments, we follow the official benchmark evaluation protocals 1 . Official benchmark on MPII dataset adopts PCKh (using portion of head length as reference) at 0.5, while official benchmark on LSP dataset adopts both PCP and PCK at 0.2. LSP benchmark provide comparisons on both Observer-Centric (OC) and Person-Centric (PC) evaluations, of which the most widely adopted evaluation protocal is PCK-PC. In addition, both benchmarks adopt AUC scores.</p><p>B. Implementation Details 1) Data Augmentation: We crop the images with the target human centered at the images with roughly the same scale, and warp the image patch to the size 256 × 256. Then, we randomly rotate (±30 • ) and flip the images, perform random re-scaling (0.75 to 1.25) and color jittering to make the model more robust to scale and illumination changes.</p><p>2) Experimental Settings: We use a modified version of Caffe <ref type="bibr" target="#b63">[64]</ref> that produces three kinds of outputs from the data layer: the augmented image, the corresponding transformed ground truth heatmaps, and the injected knowledge for the augmented image. The knowledge projection is switched off during testing. We train our model using the initial learning rate of 2.5×10 −4 . The parameters are optimized by RMSprop <ref type="bibr" target="#b64">[65]</ref> algorithm. We divide the learning rate by 2 when the validation set hits plateaus. The minimum learning rate is set to 10 −6 . We use 4 Pascal TITAN GPUs to train the model on the merged dataset of MPII and extended LSP for over 300 epochs, and adopt Tompson's validation split for the MPII dataset used in <ref type="bibr" target="#b16">[17]</ref> to monitor the training process. The same model is used for the testing of both MPII and LSP test sets. According to <ref type="bibr" target="#b65">[66]</ref>, there is a prior towards the background  that forces the network to converge to zero. It is therefore important to weight the gradient responses so that there is an equal contribution to the parameter update between the foreground and background heatmap pixels. In our training process, we weight the foreground and background by 20 : 1. The neural network takes the cropped images patches or ROI of the images as inputs. However, there exists such situation where the cropped patches or ROI contains limbs from other persons. In this case, our ground truth simply ignores other limbs. For example, any region that is not from the keypoints of the target person is considered as background heatmap in the ground truth. Since the target person is always centered in the cropped image or ROI, it enforces a prior during training. Therefore, limbs from other persons are usually of lower response, reflected by the predicted heatmaps.</p><p>3) Inference: During testing, we follow the standard routine to crop image patches with the given rough position and the scale of the test human for MPII dataset. For the LSP dataset, we use image size as the rough scale, and image center as the rough position of the target human to crop the image patches. Before feeding into the neural network, we further pre-process images with normalization and pixel-wise subtraction by estimated mean value. All the experimental results are produced from the original and flipped image pyramids with 2 scales (1 and 0.75). Note that we swap heatmaps of left and right limbs before merging corresponding heatmaps for each joint. The merged heatmaps are transformed into joint coordinates by the proposed cross-heatmap nonmaximum suppression method. The feed forward time of the network during testing is 62ms with a single Pascal TITAN X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Benchmark Evaluation</head><p>We use the Percentage Correct Keypoints (PCK) <ref type="bibr" target="#b32">[33]</ref> metric for comparisons on the LSP dataset, and the PCKh measure <ref type="bibr" target="#b59">[60]</ref>, where the error tolerance is normalized with respect to head size, for comparisons on the MPII Human Pose dataset. We train our model by adding the MPII training set to the extended LSP training set with person-centric (PC) annotations, which is a standard routine <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>.</p><p>1) Results on the MPII Human Pose Dataset: a) AUC: The AUC score of our network for MPII dataset is 63.6. b) PCKh@0.5: <ref type="table" target="#tab_5">Table II</ref> reports the comparison of the PCKh performance of our method and previous state-of-theart at a normalized distance of 0.5. Our total PCKh-0.5 score achieves state of the art performance at 91.2%. We apply all techniques described in Section. V-D during testing. Note that we test at same multiple scales (1 and 0.75) as that used on LSP dataset, which may not be ideal. While cropping the images with the given scale of MPII dataset, for some images the feet are cropped out, therefore suffering a comparatively lower detection rate for ankles.</p><p>2) Results on the Leeds Sports Pose Dataset: a) AUC: The AUC score of our network for LSP dataset is 69.1.   <ref type="table" target="#tab_5">Table III</ref> reports the PCK at threshold of 0.2, and <ref type="figure" target="#fig_4">Fig. 9</ref> exhibits PCK over various thresholds. Our approach achieves state-of-the-art performance with PCK value of 93.9%, and outperforms all existing methods on each body part prediction. c) PCP: <ref type="table" target="#tab_5">Table IV</ref> reports the PCP at threshold of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Algorithm Performance Analysis and Ablation Study</head><p>Since the ground truth of MPII dataset is not publicly available and it is forbidden to frequently submit MPII test results to the official, we perform component analysis of our proposed method on the LSP dataset. We analyze the contribution of each component in <ref type="table" target="#tab_5">Table I.</ref> We compare the proposed inception-resnet module and the basic resnet module employed by stacked hourglass networks <ref type="bibr" target="#b17">[18]</ref>. Since their performance is not reported on LSP dataset, we implement their network within our system to render fair comparisons. Under identical settings, our network with  inception-resnet module achieves superior performance over that with basic resnet module by improving the accuracy by 1.1%. We also compare our network under standard training with the same network under knowledge projection and guided learning. Results show that better performance is achieved with knowledge guided training with an accuracy improvement of 1.3%. We then analyze contributions of other techniques employed mainly during testing, i.e., flipping the image, testing the image at multiple scales, and using proposed NMS algorithm for pose estimation. Testing on original and flipped images improves performance by 0.7%, while testing on both original and 0.75 scales further improves performance by another 0.7%. Cross-heatmap non-maximum suppression improves the PCK value by 0.5%.</p><p>It should be noted that our implementation 2 in PyCaffe <ref type="bibr" target="#b63">[64]</ref> may not fully reproduce equivalent performance on MPII dataset of the hourglass network <ref type="bibr" target="#b17">[18]</ref>, which is implemented in Torch <ref type="bibr" target="#b74">[75]</ref>. However, we discuss with performance analysis to show that our proposed knowledge guided training is able to improve the performance on top of existing deep neural network. We expect that the same performance gain can be achieved on other network structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we have proposed to encode and inject external human knowledge into deep neural networks to guide its training process with learned projections for more effective human  pose estimation. We adopt the stacked hourglass design and propose to use inception-resnet as the building block of our fractal network to regress human pose into heatmaps with no explicit graphical modeling. Utilizing a multi-resolution feature representation with guided learning, the network learns an empirical set of low and high-level features which are typically more tolerant to variations in the training set. Knowledgeguided learning is a generic scheme that can be potentially used to aid other deep neural network training tasks. The effectiveness of the proposed inception-resnet module and the benefit in guided learning with knowledge projection is evaluated on two widely used benchmarks.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 6 .</head><label>6</label><figDesc>Improved hourglass sub-network. While using inception-resnet as proposed common building block, we borrow the idea of hourglass design by down-sampling and then up-sampling the dataflow in one branch, maintaining the resolution of the other branch. The lowest level of the recursive hourglass in our network is an inception-resnet module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 7 .</head><label>7</label><figDesc>We encode image descripors around each pair of adjacent joints in order to capture visual features to compensate spatial dependencies. The features are concatenated and normalized for the external knowledge representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 8 .</head><label>8</label><figDesc>Example output produced by our network. On the top-left we see the final pose estimate provided by NMS across all heatmaps. Elsewhere we show sample heatmaps: (1) The first row shows the final part regression heatmap results; (2) the second row shows the preliminary part regression results from the intermediate supervision layer. The heatmaps from the first row have finer predictions than the second row, especially the heatmap for the right foot, where the preliminary prediction renders belief scores for the soccer ball as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Bulat et al., ECCV'16 Wei el al., CVPR'16 Insafutdinov et al., ECCV'16 . Pishchulin et al., CVPR'16 Lifshitz et al., ECCV'16 Belagiannis et al., FG'17 Yu et al., ECCV'16 Rafi et al., BMVC'16 Yang et al., CVPR'16 Chen&amp;Yuille, NIPS'14 Fan et al., CVPR'15 Tompson et al., NIPS'14 Pishchulin et al., ICCV'13 Wang&amp;Li, CVPR'13 Ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .</head><label>9</label><figDesc>Person-Centric (PC) PCK curves on the LSP test set. Ours is on top. b) PCK@0.2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative results on the MPII test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Qualitative results on the LSP test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Failure cases on LSP dataset: (a) Ambiguity caused by full occlusion of 2 or more adjacent body parts; (b) Regression mistake caused by the concurrence of body part noise from other persons and full occulusion of less than 2 body parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Overview of fractal network. The network is fractal in that it reflects the concurrence of inception and residual design at both the highest and lowest (inception-resnet module) levels of abstractions. At top level, images of size 256 × 256 are down-sampled into the resolution of 64 × 64. Subsequently, inputs and outputs of all modules are of size 64 × 64, including the output heatmaps. The numbers within brackets in each module denote the number of input and output channels, respectively.</figDesc><table><row><cell>&lt;3&gt; k=1x1 s=2 &lt;64&gt;</cell><cell>&lt;64&gt; &lt;128&gt;</cell><cell>/2</cell><cell>&lt;128&gt; &lt;128&gt;</cell><cell>&lt;128&gt; &lt;128&gt;</cell><cell>&lt;128&gt; &lt;256&gt;</cell><cell>&lt;256&gt; &lt;512&gt; 4</cell><cell>&lt;512&gt; k=1x1 &lt;512&gt;</cell><cell>&lt;512&gt; k=1x1 &lt;256&gt;</cell><cell>&lt;256&gt; k=1x1 &lt;Joints&gt;</cell></row><row><cell>Channels = 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&lt;384&gt;</cell><cell></cell><cell></cell><cell>&lt;Joints&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>k=1x1</cell><cell></cell><cell></cell><cell>k=1x1</cell></row><row><cell>Convolution</cell><cell>Batch Norm</cell><cell></cell><cell>Relu Unit</cell><cell cols="2">Sampling</cell><cell>&lt;384&gt;</cell><cell></cell><cell></cell><cell>&lt;384&gt;</cell></row><row><cell cols="2">Inception-resnet Module</cell><cell></cell><cell cols="3">Improved hourglass network</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Concatenate</cell><cell></cell><cell></cell><cell cols="2">Pixel-wise Add</cell><cell></cell><cell>&lt;384&gt;</cell><cell>&lt;512&gt;</cell><cell>&lt;512&gt;</cell><cell>&lt;256&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&lt;512&gt;</cell><cell>k=1x1</cell><cell>k=1x1</cell><cell>k=1x1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>&lt;512&gt;</cell><cell>&lt;256&gt;</cell><cell>&lt;Joints&gt;</cell></row><row><cell>Fig. 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Back-propagate w.r.t W f , W KP ;</figDesc><table><row><cell></cell><cell>Algorithm 1: Summary of Procedures: Training Phase</cell></row><row><cell></cell><cell>input : A set of RGB images I and corresponding</cell></row><row><cell></cell><cell>ground truth joint coordinates J</cell></row><row><cell></cell><cell>output: Trained weights W  *  f for the Fractal Network, W  *  KP for the knowledge projection layers</cell></row><row><cell cols="2">1 Initialize DNN with fractal network and knowledge</cell></row><row><cell></cell><cell>projection layers;</cell></row><row><cell cols="2">2 for k epoches do</cell></row><row><cell>3</cell><cell>for mini-batch in I do</cell></row><row><cell>4</cell><cell>Compute external knowledge representation:</cell></row><row><cell></cell><cell>K ←− {I n , J n } ;</cell></row><row><cell>7</cell><cell>end</cell></row><row><cell cols="2">8 end</cell></row><row><cell cols="2">9 return {W  *  f , W  *  KP }</cell></row><row><cell></cell><cell>Algorithm 2: Summary of Procedures: Testing Phase</cell></row><row><cell></cell><cell>input : A set of RGB images I and a fractal network</cell></row><row><cell></cell><cell>with trained weights W  *  f</cell></row><row><cell></cell><cell>output: A set of predicted joint coordinates J in the</cell></row><row><cell></cell><cell>same image coordinate system</cell></row><row><cell cols="2">1 initialize network only with fractal network layers W  *  f ;</cell></row><row><cell cols="2">2 while not at end of this image set do</cell></row><row><cell>3</cell><cell>Load image I i ;</cell></row><row><cell>4</cell><cell>Forward the network: J i ← {W  *  f , I i } ;</cell></row><row><cell cols="2">5 end</cell></row><row><cell cols="2">6 return J</cell></row></table><note>56 {Wf , W KP } ← arg min W f (λ × L KP + (1 − λ) × L f ) ;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE I</head><label>I</label><figDesc>COMPONENT ANALYSIS ON THE LSP DATASET OF PCK@0.2 SCORE. NOTE THAT NUMBERS IN BOLD INDICATE THE METHOD HAS EMPLOYED ALL TECHNIQUES DURING TESTING.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>SCORE ON THE MPII TEST SET.</figDesc><table><row><cell>Method</cell><cell cols="2">Head Sho. Elb. Wri.</cell><cell>Hip</cell><cell cols="3">Knee Ank. Total</cell></row><row><cell>Ours</cell><cell>98.1</cell><cell cols="2">96.3 92.2 87.8 90.6</cell><cell>87.6</cell><cell>82.7</cell><cell>91.2</cell></row><row><cell>Newell et al., ECCV'16 [18]</cell><cell>98.2</cell><cell cols="2">96.3 91.2 87.1 90.1</cell><cell>87.4</cell><cell>83.6</cell><cell>90.9</cell></row><row><cell>Bulat&amp;Tzimiropoulos, ECCV'16 [54]</cell><cell>97.9</cell><cell cols="2">95.1 89.9 85.3 89.4</cell><cell>85.7</cell><cell>81.7</cell><cell>89.7</cell></row><row><cell>Wei et al., CVPR'16 [51]</cell><cell>97.8</cell><cell cols="2">95.0 88.7 84.0 88.4</cell><cell>82.8</cell><cell>79.4</cell><cell>88.5</cell></row><row><cell>Insafutdinov et al., ECCV'16 [46]</cell><cell>96.8</cell><cell cols="2">95.2 89.3 84.4 88.4</cell><cell>83.4</cell><cell>78.0</cell><cell>88.5</cell></row><row><cell>Rafi et al., BMVC'16 [67]</cell><cell>97.2</cell><cell cols="2">93.9 86.4 81.3 86.8</cell><cell>80.6</cell><cell>73.4</cell><cell>86.3</cell></row><row><cell>Gkioxary et al., ECCV'16 [68]</cell><cell>96.2</cell><cell cols="2">93.1 86.7 82.1 85.2</cell><cell>81.4</cell><cell>74.1</cell><cell>86.1</cell></row><row><cell>Lifshitz et al., ECCV'16 [69]</cell><cell>97.8</cell><cell cols="2">93.3 85.7 80.4 85.3</cell><cell>76.6</cell><cell>70.2</cell><cell>85.0</cell></row><row><cell>Pishchulin et al., CVPR'16 [45]</cell><cell>94.1</cell><cell cols="2">90.2 83.4 77.3 82.6</cell><cell>75.7</cell><cell>68.6</cell><cell>82.4</cell></row><row><cell>Hu&amp;Ramanan, CVPR'16 [70]</cell><cell>95.0</cell><cell cols="2">91.6 83.0 76.6 81.9</cell><cell>74.5</cell><cell>69.5</cell><cell>82.4</cell></row><row><cell>Tompson et al., CVPR'15 [44]</cell><cell>96.1</cell><cell cols="2">91.9 83.9 77.8 80.9</cell><cell>72.3</cell><cell>64.8</cell><cell>82.0</cell></row><row><cell>Carreira et al., CVPR'16 [52]</cell><cell>95.7</cell><cell cols="2">91.7 81.7 72.4 82.8</cell><cell>73.2</cell><cell>66.4</cell><cell>81.3</cell></row><row><cell>Tompson et al., NIPS'14 [17]</cell><cell>95.8</cell><cell cols="2">90.3 80.5 74.3 77.6</cell><cell>69.7</cell><cell>62.8</cell><cell>79.6</cell></row><row><cell>Pishchulin et al., ICCV'13 [34]</cell><cell>74.3</cell><cell cols="2">49.0 40.8 34.1 36.5</cell><cell>34.4</cell><cell>35.2</cell><cell>44.1</cell></row><row><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">COMPARISONS OF PCKH@0.5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>TABLE III COMPARISONS OF PCK@0.2 SCORE ON THE LSP TEST SET.</figDesc><table><row><cell>Method</cell><cell cols="2">Head Sho. Elb. Wri.</cell><cell>Hip</cell><cell cols="3">Knee Ank. Total</cell></row><row><cell>Ours</cell><cell>98.2</cell><cell cols="2">94.4 91.8 89.3 94.7</cell><cell>95.0</cell><cell>93.5</cell><cell>93.9</cell></row><row><cell>Bulat&amp;Tzimiropoulos. ECCV'16 [54],</cell><cell>97.2</cell><cell cols="2">92.1 88.1 85.2 92.2</cell><cell>91.4</cell><cell>88.7</cell><cell>90.7</cell></row><row><cell>Wei et al. CVPR'16 [51],</cell><cell>97.8</cell><cell cols="2">92.5 87.0 83.9 91.5</cell><cell>90.8</cell><cell>89.9</cell><cell>90.5</cell></row><row><cell>Insafutdinov et al. ECCV'16 [46],</cell><cell>97.4</cell><cell cols="2">92.7 87.5 84.4 91.5</cell><cell>89.9</cell><cell>87.2</cell><cell>90.1</cell></row><row><cell>Pishchulin et al. CVPR'16 [45],</cell><cell>97.0</cell><cell cols="2">91.0 83.8 78.1 91.0</cell><cell>86.7</cell><cell>82.0</cell><cell>87.1</cell></row><row><cell>Lifshitz et al. ECCV'16 [69],</cell><cell>96.8</cell><cell cols="2">89.0 82.7 79.1 90.9</cell><cell>86.0</cell><cell>82.5</cell><cell>86.7</cell></row><row><cell>Belagiannis&amp;Zisserman FG'17 [50],</cell><cell>95.2</cell><cell cols="2">89.0 81.5 77.0 83.7</cell><cell>87.0</cell><cell>82.8</cell><cell>85.2</cell></row><row><cell>Yu et al. ECCV'16 [71],</cell><cell>87.2</cell><cell cols="2">88.2 82.4 76.3 91.4</cell><cell>85.8</cell><cell>78.7</cell><cell>84.3</cell></row><row><cell>Rafi et al. BMVC'16 [67],</cell><cell>95.8</cell><cell cols="2">86.2 79.3 75.0 86.6</cell><cell>83.8</cell><cell>79.8</cell><cell>83.8</cell></row><row><cell>Yang et al. CVPR'16 [72],</cell><cell>90.6</cell><cell cols="2">78.1 73.8 68.8 74.8</cell><cell>69.9</cell><cell>58.9</cell><cell>73.6</cell></row><row><cell>Chen&amp;Yuille NIPS'14 [43],</cell><cell>91.8</cell><cell cols="2">78.2 71.8 65.5 73.3</cell><cell>70.2</cell><cell>63.4</cell><cell>73.4</cell></row><row><cell>Fan et al. CVPR'15 [73],</cell><cell>92.4</cell><cell cols="2">75.2 65.3 64.0 76.7</cell><cell>68.3</cell><cell>70.4</cell><cell>73.0</cell></row><row><cell>Tompson et al. NIPS'14 [17],</cell><cell>90.6</cell><cell cols="2">79.2 67.9 63.4 69.5</cell><cell>71.0</cell><cell>64.2</cell><cell>72.3</cell></row><row><cell>Pishchulin et al. ICCV'13 [34],</cell><cell>87.2</cell><cell cols="2">56.7 46.7 38.0 61.0</cell><cell>57.5</cell><cell>52.7</cell><cell>57.1</cell></row><row><cell>Wang&amp;Li et al. CVPR'13 [74],</cell><cell>84.7</cell><cell cols="2">57.1 43.7 36.7 56.7</cell><cell>52.4</cell><cell>50.8</cell><cell>54.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>SCORE ON THE LSP TEST SET.</figDesc><table><row><cell>Method</cell><cell cols="7">Torso U.Leg L.Leg U.Arm Forearm Head Total</cell></row><row><cell>Ours</cell><cell>98.6</cell><cell>95.8</cell><cell>93.6</cell><cell>90.7</cell><cell>84.2</cell><cell>96.4</cell><cell>92.3</cell></row><row><cell>Bulat&amp;Tzimiropoulos. ECCV'16 [54],</cell><cell>97.7</cell><cell>92.4</cell><cell>89.3</cell><cell>86.7</cell><cell>79.7</cell><cell>95.2</cell><cell>88.9</cell></row><row><cell>Wei et al. CVPR'16 [51],</cell><cell>98.0</cell><cell>82.2</cell><cell>89.1</cell><cell>85.8</cell><cell>77.9</cell><cell>95.0</cell><cell>88.3</cell></row><row><cell>Insafutdinov et al. ECCV'16 [46],</cell><cell>97.0</cell><cell>90.6</cell><cell>86.9</cell><cell>86.1</cell><cell>79.5</cell><cell>95.4</cell><cell>87.8</cell></row><row><cell>Yu et al. ECCV'16 [71],</cell><cell>98.0</cell><cell>93.1</cell><cell>88.1</cell><cell>82.9</cell><cell>72.6</cell><cell>83.0</cell><cell>85.4</cell></row><row><cell>Pishchulin et al. CVPR'16 [45],</cell><cell>97.0</cell><cell>88.8</cell><cell>82.0</cell><cell>82.4</cell><cell>71.8</cell><cell>95.8</cell><cell>84.3</cell></row><row><cell>Lifshitz et al. ECCV'16 [69],</cell><cell>97.3</cell><cell>88.8</cell><cell>84.4</cell><cell>80.6</cell><cell>71.4</cell><cell>94.8</cell><cell>84.3</cell></row><row><cell>Belagiannis&amp;Zisserman FG'17 [50],</cell><cell>96.0</cell><cell>86.7</cell><cell>82.2</cell><cell>79.4</cell><cell>69.4</cell><cell>89.4</cell><cell>82.1</cell></row><row><cell>Rafi et al. BMVC'16 [67],</cell><cell>97.6</cell><cell>87.3</cell><cell>80.2</cell><cell>76.8</cell><cell>66.2</cell><cell>93.3</cell><cell>81.2</cell></row><row><cell>Yang et al. CVPR'16 [72],</cell><cell>95.6</cell><cell>78.5</cell><cell>71.8</cell><cell>72.2</cell><cell>61.8</cell><cell>83.9</cell><cell>74.8</cell></row><row><cell>Chen&amp;Yuille NIPS'14 [43],</cell><cell>96.0</cell><cell>77.2</cell><cell>72.2</cell><cell>69.7</cell><cell>58.1</cell><cell>85.6</cell><cell>73.6</cell></row><row><cell>Fan et al. CVPR'15 [73],</cell><cell>95.4</cell><cell>77.7</cell><cell>69.8</cell><cell>62.8</cell><cell>49.1</cell><cell>86.6</cell><cell>70.1</cell></row><row><cell>Tompson et al. NIPS'14 [17],</cell><cell>90.3</cell><cell>70.4</cell><cell>61.1</cell><cell>63.0</cell><cell>51.2</cell><cell>83.7</cell><cell>66.6</cell></row><row><cell>Pishchulin et al. ICCV'13 [34],</cell><cell>88.7</cell><cell>63.6</cell><cell>58.4</cell><cell>46.0</cell><cell>35.2</cell><cell>85.1</cell><cell>58.0</cell></row><row><cell>Wang&amp;Li et al. CVPR'13 [74],</cell><cell>87.5</cell><cell>56.0</cell><cell>55.8</cell><cell>43.1</cell><cell>32.1</cell><cell>79.1</cell><cell>54.1</cell></row><row><cell></cell><cell cols="2">TABLE IV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">COMPARISONS OF PCP@0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://human-pose.mpi-inf.mpg.de/#evaluation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code and models available at: http://github.com/Guanghan/GNet-pose</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Orgm: Occlusion relational graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="927" to="941" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Body parts dependent joint regressors for human pose estimation in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2131" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation and tracking via parsing a tree structure based human model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On Systems, Man, And Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="580" to="592" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation using consistent max covering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1911" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a tracking and estimation integrated graphical model for human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3176" to="3186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation by exploiting spatial and temporal constraints in body-part configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="443" to="454" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose co-estimation and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2282" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatio-temporal matching for human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1492" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1913" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Web-based classifiers for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1031" to="1045" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Let your body speak: Communicative cue extraction on natural interaction using rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcos-Ramiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marron-Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1721" to="1732" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective active skeleton representation for low latency human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="154" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual sentences for pose retrieval over low-resolution cross-media dance collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1652" to="1661" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic human mocap data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kadu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2191" to="2202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the spatial hierarchy of mixture models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using linking features in learning nonparametric part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Do we need more training data or better models for object detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust optimization for deep regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2830" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Training deeper convolutional networks with deep supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02496</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02914</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5" to="8" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="160" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">2d articulated human pose estimation and retrieval in (almost) unconstrained still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="214" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>COURSERA: Neural networks for machine learning</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep deformation network for object landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
