<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongling</forename><surname>Xiao</surname></persName>
							<email>xiaodongling@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<email>zhanghan17@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
							<email>liyukun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
							<email>tianhao@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<email>wanghaifeng@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current pre-training works in natural language generation pay little attention to the problem of exposure bias on downstream tasks. To address this issue, we propose an enhanced multi-flow sequence to sequence pre-training and fine-tuning framework named ERNIE-GEN, which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder. Experimental results demonstrate that ERNIE-GEN achieves state-of-the-art results with a much smaller amount of pre-training data and parameters on a range of language generation tasks, including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue response generation (Persona-Chat) and generative question answering (CoQA). The source codes and pretrained models have been released at https://github. com/PaddlePaddle/ERNIE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained on large-scale unlabeled text corpora and finetuned on downstream tasks, self-supervised representation models such as <ref type="bibr">GPT [Radford et al., 2018]</ref>, <ref type="bibr">BERT [Devlin et al., 2019]</ref> and XLNet <ref type="bibr" target="#b3">[Yang et al., 2019b]</ref> have achieved remarkable improvements in natural language understanding (NLU). Different from encoder-only pre-training like BERT or decoder-only pre-training like GPT, natural language generation (NLG) relies on the sequence to sequence generation framework (seq2seq) which consists of a bidirectional encoder and a unidirectional decoder. Current pre-training works in NLG such as MASS <ref type="bibr">[Song et al., 2019]</ref> and UNILM * indicates equal contribution.  <ref type="figure">Figure 1</ref>: Schematic of two generation mechanisms (left) and data strategies for pre-training (right). Blocks in green, orange and blue denote source texts, target texts and artificial symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Span-by-span flow</head><p>[ <ref type="bibr" target="#b0">Dong et al., 2019]</ref> mainly focus on jointly pre-training encoder and decoder on different self-supervised tasks. However, these works pay little attention to the exposure bias issue <ref type="bibr" target="#b1">[Ranzato et al., 2016]</ref>, a major drawback of teacher-forcing training. This issue is due to the fact that groundtruth words are used during training, while generated words, whether predicted correctly or not, are used for inference where mistakes tend to accumulate. To alleviate this issue, we present ERNIE-GEN, an enhanced multi-flow seq2seq training framework characterized by a carefully-designed Multi-Flow Attention architecture based on Transformer <ref type="bibr" target="#b1">[Vaswani et al., 2017]</ref>, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. ERNIE-GEN incorporates a novel infilling generation mechanism and a noiseaware generation method into pre-training and fine-tuning, which is proved to be effective through experiments in §4.3.</p><p>• Infilling generation. Instead of using last groundtruth word in training or last generated word in inference, we adopt an inserted artificial symbol [ATTN] along with its position to gather history contextual representations at each step in both training and inference, which diverts model's attention away from last word and coerces it into focusing on all former representations, thus alleviating negative influence of previous mistakes to subsequent generation, as shown in <ref type="figure">Figure 1</ref>(b).</p><p>• Noise-Aware generation. We corrupt the input target se-  quence by randomly replacing words to arbitrary words in the vocabulary. This setup, despite its simplicity, proves to be an effective way to make the model be aware of mistakes in training, so that the model is able to detect mistakes and ignore them during inference.</p><p>Moreover, in light of the fact that entities, phrases and sentences in human writing are organized in a coherent manner, we incorporate a span-by-span generation task into ERNIE-GEN as a new generation flow to train the model to predict semantically-complete spans consecutively rather than predicting word by word as traditional models do. This task is implemented through the infilling generation mechanism in parallel with an infilling-based word-by-word generation flow to facilitate convergence in training, as shown in <ref type="figure">Figure 1b</ref>.</p><p>In addition, as shown in <ref type="figure">Figure 1</ref>(c-d), recent pre-training works for NLG like UNILM and MASS only sample a single continuous segment as target sequence. However, this sampling method compromises the correlation between encoder and decoder when it comes to pre-training of long texts (typically 512 words), given that adjacent segments are often relevant semantically. ERNIE-GEN adopts a multi-granularity target fragments sampling strategy to force decoder to rely more on the encoder representations other than the previous generated words, thus enhancing the correlation between encoder and decoder, as shown in <ref type="figure">Figure 1e</ref>.</p><p>Empirically, ERNIE-GEN is particularly effective and achieves state-of-the-art results on a range of NLG tasks including abstractive summarization (Gigaword and CN-N/DailyMail), question generation (SQuAD), dialogue response generation (Persona-Chat) and generative question answering (CoQA), utilizing a much smaller amount of pretraining data and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pre-training for NLP Tasks. Recently, pre-training methods have achieved state-of-the-art results in multiple NLU tasks. ELMo <ref type="bibr" target="#b1">[Peters et al., 2018]</ref> pre-trains two unidirectional language models (LMs) with forward and backward direction respectively to feature downstream tasks. GPT utilizes an adjusted Transformer <ref type="bibr" target="#b1">[Vaswani et al., 2017]</ref> to learn a forward LM and then fine-tunes the forward LM on supervised datasets. BERT proposes a masked language modeling (MLM) task to learn deep bidirectional representations. Nevertheless, above methods are usually implemented by just one encoder or decoder, which is less effective in encoder-decoder based generation tasks, thus several works have preliminarily explored the pre-training towards NLG by incorporating BERT's MLM into the seq2seq framework and shown excellent performance on a range of generation tasks. MASS masks a consecutive fragment (50%) of the input sentence with [MASK] symbols to predict. UNILM masks several words in the input sequence which is a pair of segments for encoder and decoder, and then predicts the masked words in accordance with BERT's MLM. Exposure Bias Issue. NLG tasks suffer from the exposure bias which is caused by teacher-forcing training. To address such issue, RNN-based variational autoencoders (VAEs) are leveraged in <ref type="bibr" target="#b2">[Yang et al., 2019a;</ref>, whereas it requires inference for both posterior and prior distribution. Reinforcement learning is also adopted to text generation against exposure bias issue <ref type="bibr" target="#b1">[Ranzato et al., 2016;</ref><ref type="bibr" target="#b1">Wang et al., 2018]</ref>, which is, however, inefficient during training because of the word-by-word sampling procedure. These methods are inefficient and less practical for pretraining that relies on large-scale unlabeled text corpora. Span-level <ref type="bibr">Pre-training. [Sun et al., 2019;</ref><ref type="bibr" target="#b1">Sun et al., 2020;</ref> verify that predicting spans reaches substantially better performance on NLU tasks. Meanwhile, inspired by characteristics of human expression, we hope the model have the foresight to generate a semantically-complete span at each step rather than a word. Consequently, a spanby-span generating task is proposed to make the model capable of generating texts more human-like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Framework</head><p>Built on infilling generation mechanism, ERNIE-GEN adopts a Multi-Flow Attention architecture to train the model on word-by-word and span-by-span generation tasks in parallel. In this section, we describe ERNIE-GEN according to the training process shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Granularity Target Fragments</head><p>Given an input sequence S = {s 1 , ..., s n }, we first sample a length distribution D i from a distribution set D = {D 1 , ..., D |D| } with probability p i for target fragments, and then select fragments according to D i in S iteratively until the fragment budget has been spent (e.g. 25% of S). We denote S i j as the j-th fragment which is sampled in length distribution D i . Sampled fragments are then removed from S and stitched together to form target sequence</p><formula xml:id="formula_0">T = [T 1 , ..., T k ] = [S i 1 , ..., S i k ].</formula><p>We denote S as the left input sequence after removing sampled fragments. ERNIE-GEN performs pretraining by predicting the fragmented target sequence T and minimizing the negative log likelihood:</p><formula xml:id="formula_1">L(θ; S, D i ) = −logP (T |S , D i ; θ) = −log k j=1 P (T j |T &lt;j , S , D i ; θ).</formula><p>(1)</p><p>where the target sequence T is sorted by the positions of sampled fragments. For each fragment</p><formula xml:id="formula_2">T = {t 1 , ..., t |T | } in T , we have P (T ) = |T | j=1 P (t j |t &lt;j ). S [ATTN] [A][A][A] [A] Word-by-word Flow Feed Forward p T p S p T p T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YWord YSpan</head><p>Contextual Flow </p><formula xml:id="formula_3">L + + + + S t1 t2 t3 t4 S [A][A][A] [A] t1 t2 t3 t4 S t1 t1 t2 t3 t4 t2 t3 t4 [ATTN]</formula><p>[ATTN]</p><p>[ATTN]</p><p>[ATTN]</p><p>[ATTN]</p><p>[ATTN]</p><p>[ATTN]  Following preliminary trials, we set a hyperparameter γ = 0.25, which denotes the ratio of length of all fragments to that of the input sequence S. Besides, we introduce two uniform distributions D = {U (1, 4), U (4, 32)} with probability of 0.4 and 0.6 respectively to sample fragments, which aims to learn representations from different perspectives. On the one hand, short fragments benefit learning of semantic relation between words; on the other hand, longer fragments help to memorize sentence-level expressions.</p><formula xml:id="formula_4">[ATTN] t 1 ( ) l a 1 ( ) l a 2 ( ) l a 3 ( ) l a 4 ( ) l S S T A W ( ) l S Q K,V Q K,V t 2 ( ) l t 3 ( ) l t 4 ( ) l t 3 ( +1) l t 2 ( +1) l t 1 ( +1) l t 4 (l+1) a 3 (l+1) a 2 (l+1) a 1 (l+1) a 4 (l+1) ( +1) l Q,K,V</formula><formula xml:id="formula_5">... t 1 ( ) l a 1 ( ) l a 2 ( ) l a 3 ( ) l a 4 ( ) l S ( ) l Q K,V Q K,V t 2 ( ) l t 3 ( ) l t 4 ( ) l Q,K,V ... S t 3 ( +1) l t 2 ( +1) l t 1 ( +1) l t 4 (l+1) ( +1) l y1 y2 y3 y4 ... ... ... ... ... S t 3 ( +1) l t 2 ( +1) l t 1 ( +1) l t 4 (l+1) a 3 (l+1) a 2 (l+1) a 1 (l+1) a 4 (l+1) y1 y2 y3 y4 ... ... ... S t 3 ( +1) l t 2 t 1 ( +1) l t 4 (l+1) A A S T span M : C M : S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Noise-Aware Generation</head><p>To train a generation model which can detect the false prediction and mitigate its impact on subsequent generation, we corrupt the groundtruth sequence T with a procedure where words are being replaced randomly, and the corrupted T is represented as T . There are two hyperparameters, ρ p and ρ f , denoting the noising rate in pre-training and fine-tuning respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture: Multi-Flow Attention</head><p>Formally, given a source sequence S = {s 1 , ..., s n }, a noised target sequence T = {t 1 , ..., t m }, we denote the inference of seq2seq network based on shared Transformer as follows:</p><formula xml:id="formula_6">s (l+1) i ← MH-Attn(Q = s (l) i , KV = S (l) ). t (l+1) i ← MH-Attn(Q = t (l) i , KV = S (l) , t (l) ≤i ).</formula><p>(2)</p><p>where Q, K, V denote the query, key, and value in Multi-Head attention <ref type="bibr" target="#b1">[Vaswani et al., 2017]</ref>. s i indicate the i-th vector representations of the l-th layer of Multi-Head Attention for the encoder and the decoder respectively, [·] denotes the concatenation operation. In this work, we call the above procedure the Contextual Flow.</p><p>Word-by-word Generation Flow. Based on infilling generation mechanism, this flow utilizes an inserted [ATTN] symbol to gather history representations word by word (see <ref type="figure">Figure 1b</ref>). To facilitate this process, we place all inserted [ATTN] together to construct an artificial symbol sequence <ref type="bibr">[ATTN]</ref> m } which has the same length as T , as shown in <ref type="figure" target="#fig_3">Figure 3b</ref>. To be specific, the word-byword generation flow is updated as follow:</p><formula xml:id="formula_7">A W = {[ATTN] 1 , ...,</formula><formula xml:id="formula_8">a (l+1) i ← MH-Attn(Q = a (l) i , KV = S (l) , t (l) &lt;i , a (l) i ). (3)</formula><p>where a (l) i indicates the i-th vector representation of the l-th layer for the artificial symbol sequence A W . Span-by-span Generation Flow. Different from word-byword generation flow, span-by-span flow uses [ATTN] symbols to predict spans consecutively, as shown in <ref type="figure" target="#fig_3">Figure 3c</ref>. Formally, given a list of span boundaries B = {b 1 , ..., b |B| }, we conduct the span-by-span generation flow as:</p><formula xml:id="formula_9">a (l+1) j ← MH-Attn(Q = a (l) j , KV = S (l) , t (l) &lt;bi , a (l) j ). (4) where j ∈ [b i , b i+1 ), and a (l)</formula><p>j denotes the (j − b i )-th vector representation of the i-th span. Essentially, the model is trained to predict a whole span {t bi , ..., t bi+1−1 } with the same history context [S, t &lt;bi ]. Instead of randomly sampling spans, we prefer sampling spans with semantical information and knowledge. Specifically, we consider the following two steps to sample spans consecutively in T :</p><p>• Firstly, we implement a T-test to compute t-statistic scores of all bigrams and trigrams, which is based on an initial hypothesis H 0 : a random span of n arbitrary words w = {w 1 , ..., w n } with probability p (w) = n i=1 p(w i ) cannot be a statistical n-gram. The t-statistic score is calculated by</p><formula xml:id="formula_10">(p(w)−p (w)) √ σ 2 /N , where p(w) = Count(w)</formula><p>N and σ 2 = p(w)(1 − p(w)), indicating the statistic probability and the standard deviation of w respectively, N denotes the total number of n-grams appearing in the training data. According to the t-statistic scores, we select the top 200,000 bigrams, top 50,000 trigrams and all unigrams to construct a specific vocabulary of spans, which is represented as V span .</p><p>• Secondly, we search the trigram, bigram and unigram in order, starting with current word until a span (n-gram, n ≤ 3) is retrieved in V span .</p><p>Multi-Flow Attention. To integrate the word-by-word generation flow and span-by-span generation flow, we apply them in parallel with a shared contextual flow by leveraging the multi-flow attention architecture, as <ref type="figure" target="#fig_3">Figure 3a</ref> describes. The multi-flow attention is computed as:</p><formula xml:id="formula_11">       X (l+1) ← MH-Attn(Q = X (l) , KV = X (l) , M C ) A (l+1) W ← MH-Attn(Q = A (l) W , KV = X (l) , A (l) W , M W ) A (l+1) S ← MH-Attn(Q = A (l) S , KV = X (l) , A (l) S , M S )<label>(5)</label></formula><p>where X denotes the concatenation of S and T , X (l) is the vector sequence of the l-th layer for the contextual flow. A</p><formula xml:id="formula_12">(l) W , A<label>(l)</label></formula><p>S are vector sequences of the l-th layer for the wordby-word and span-by-span generation flow respectively. As shown in <ref type="figure" target="#fig_3">Figure 3d</ref>, attention mask matrix M determines whether query and key can attend to each other by modifying the attention weight W= softmax <ref type="bibr">et al., 2017]</ref> . Specifically, M is assigned as:</p><formula xml:id="formula_13">( QK T √ d k + M ) [Vaswani</formula><formula xml:id="formula_14">M ij = 0, can be attended −∞, prevent from attending<label>(6)</label></formula><p>While training, we add the loss of the word-by-word and span-by-span generation flow with an coefficient λ:</p><formula xml:id="formula_15">L(T ) = λL W ord (T ) + (1 − λ)L Span (T ) = −λlogP (T |A (L−1) W ) − (1 − λ)logP (T |A (L−1) S ).<label>(7)</label></formula><p>where T indicates the unnoised target sequence, and L(·) denotes the cross entropy loss function. In detail, we set λ = 0.5 and λ = 1.0 respectively in pre-training and fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference: Infilling Decoding</head><p>During inference, the target sequence T is unknown, we insert symbol [ATTN] step by step to gather the representation of history context instead of preparing an artificial symbol sequence A in advance. Meanwhile, for the purpose of efficiency, we need to drop the inserted [ATTN] after inference at each step, as detailed in <ref type="figure" target="#fig_5">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we compare our ERNIE-GEN with previous works and conduct several ablation experiments to assess the performance of proposed methods in §3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-training and Implementation</head><p>Analogous to BERT and UNILM, ERNIE-GEN is trained on English Wikipedia and BookCorpus <ref type="bibr" target="#b5">[Zhu et al., 2015]</ref>, totaling 16GB. We also pre-train ERNIE-GEN on larger scaled text corpora, which is specifically described in appendix A. The input sequence is lowercased and truncated to a maximum length of 512. We train a base model ERNIE-GEN BASE (L=12, H=768, A=12, Total Parameters=110M) 1 and a large model ERNIE-GEN LARGE (L=24, H=1024, A=16, Total Parameters=340M) with parameters initialized by BERT BASE and BERT LARGE respectively. Specifically, Adam optimizer with β 1 = 0.9, β 2 = 0.999, = 10 −9 is employed. The peak learning rate is 5e-5 with warmup over the first 4,000 steps and linear decay scheduling. The noising rate ρ p for pre-training is 0.05. Batches are organized by limiting the maximum number of tokens to 196,608. Pre-training experiments are carried out on PaddlePaddle platforms 2 and Nvidia Tesla V100 GPU. By virtue of float16 mixed precision training, it takes almost 4 days for 400,000 steps to train ERNIE-GEN BASE while almost 7 days for 450,000 steps to train ERNIE-GEN LARGE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fine-tuning on Downstream Tasks</head><p>Abstractive Summarization aims at generating fluent and concise summaries without being constrained to extracting sub-sequences from the input articles. We execute experiments on <ref type="bibr">Gigaword dataset [Rush et al., 2015]</ref>     The results on Gigaword task with two scales (10k and 3.8M) are presented in <ref type="table" target="#tab_3">Table 2</ref>, and the fine-tuning settings are shown in <ref type="table" target="#tab_4">Table 1</ref>. On the low-resource task (Gigaword 10k), ERNIE-GEN LARGE yields a gain of +1.94 ROUGE-L compared with UNILM LARGE . On the full training set, ERNIE-GEN LARGE creates the state-of-the-art results, outperforming various previous methods. Specifically, ERNIE-GEN BASE outperforms PEGASUS (568M and 750G) by using only 110M parameters and 16G training data. <ref type="table" target="#tab_5">Table 3</ref> shows the performance on CNN/DailyMail. With a similar amount of pre-training data and parameters, ERNIE-GEN BASE outperforms MASS by +0.67 ROUGE-L scores. Fairly compared with UNILM LARGE , ERNIE-GEN LARGE obtains substantial gain of +0.73 ROUGE-L scores. Meanwhile, in spite of small pre-training data and parameters, our large model also achieves state-of-the-art result on ROUGE-L and comparable performance on ROUGE-1/2. Question Generation is to generate a question according to a given input passage and a corresponding answer. We evaluate on the SQuAD 1.1 dataset <ref type="bibr" target="#b1">[Rajpurkar et al., 2016]</ref> for question generation task (called SQuAD QG). Following UNILM, we redistribute the original dataset into a new   Generative Question Answering / Dialogue Response in multi-turn conversations are challenging because of complex background knowledge and diverse utterances. We conduct an experiment on Persona-Chat dataset <ref type="bibr" target="#b4">[Zhang et al., 2018]</ref> to generate responses according to given multi-turn conversations and persona profile. <ref type="table" target="#tab_8">Table 5</ref> shows that ERNIE-GEN outperforms current task-specific pre-training model on dialogue generation. Beside, we also execute an experiment on CoQA dataset <ref type="bibr" target="#b1">[Reddy et al., 2019]</ref> to generate free-form answers for input questions and conversations. As shown in <ref type="table" target="#tab_10">Table 6</ref>, our generative question answering model works considerably better than early works by +2.0 F1-scores.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>To better understand the importance of each proposed generation methods, we conduct experiments concerning the following two aspects:</p><p>• The robustness of infilling generation mechanism and noise-aware generation method against the exposure bias.</p><p>• The effectiveness of span-by-span generation task and the complete ERNIE-GEN model.</p><p>In <ref type="table" target="#tab_13">Table 8</ref>, we compare two ERNIE-GEN BASE variants that are pre-trained with typical generation mechanism and infilling generation mechanism and that generate word by word. Row 1-3 shows that without noising groundtruth texts, infilling generation outperforms typical generation    across tasks. Furthermore, both variants achieve remarkable improvements by fine-tuning with noise-aware generation method (row 4-6). Specifically, <ref type="figure" target="#fig_6">Figure 5a</ref> shows the results with diverse choices of noising rate ρ f on two tasks, indicating that appropriate noising substantially benefits the training and alleviates the training-inference discrepancy. To further analyze the excellence of infilling generation mechanism with noising, we compute the average attention weights of source tokens, unnoised target tokens and noised target tokens in the last self-attention layer respectively on 1,000 samples. Average attention weights with diverse noising rate ρ f are shown in <ref type="figure" target="#fig_6">Figure 5b</ref>, which tells us that the model pays more attention on the decoder side to figure out noised points and assign them less attention weights as the noising rate ρ f increased in fine-tuning. Thereby, the model is able to detect and ignore the false predictions properly to alleviate accumulating mistakes while inference. In column 1 of <ref type="table" target="#tab_12">Table 7</ref>, we compare four base size variants on three tasks. We see that noise-aware generation method and span-by-span generation task (rows 2-3 of <ref type="table" target="#tab_12">Table 7</ref>) play an important role in ERNIE-GEN pre-training and significantly outperform the baseline model which is only pre-trained with word-by-word infilling generation flow (row 4 of <ref type="table" target="#tab_12">Table 7</ref>). After integrating noise-aware generation method and span-by-span generation task, ERNIE-GEN boosts the performance across all three tasks, as shown in row 1 of <ref type="table" target="#tab_12">Table  7</ref>. In addition, UNILM is fine-tuned by masking words in the encoder and decoder to predict, which is also a case of noising for generation. To verify the idea that fine-tuning with masking language modeling like UNILM is inefficient due to the coupling of masking (noising) and predicting that only the masked (noised) position will be learned, we also list the fine-tuning results obtained by predicting masked words with masking probability of 0.7, as shown in column 2 of <ref type="table" target="#tab_12">Table 7</ref>. We observe that our noise-aware generation method significantly outperforms the mask language modeling in seq2seq fine-tuning by predicting all words in the decoder side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present an enhanced multi-flow seq2seq pre-training and fine-tuning framework named ERNIE-GEN for language generation, which incorporates an infilling generation mechanism and a noise-aware generation method to alleviate the exposure bias. Besides, ERNIE-GEN integrates a new span-byspan generation task to train the model to generate texts like human writing, which further improves the performance on downstream tasks. Through extensive experiments, ERNIE-GEN achieves state-of-the-art results on a range of NLG tasks. Future work includes incorporating reinforcement learning into pre-training for exposure bias and applying ERNIE-GEN to more NLG tasks such as machine translation. <ref type="table">Table 9</ref>: Evaluation results on Gigaword and CNN/DailyMail for pre-trained models using large-scale text corpora. ERNIE-GEN with the ‡ mark is pre-trained with the 430GB text corpora.</p><p>We also fine-tune ERNIE-GEN on the SQuAD 1.1 dataset for question generation task, the results are presented in <ref type="table" target="#tab_4">Table  10</ref>. We observe that larger scaled pre-training corpora can slightly improve the Rouge-L score and BLEU-4 score for the SQuAD dataset.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of ERNIE-GEN framework. S, T and Y donate source, target, and generated texts, T is the noised version of T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the Multi-Flow Attention module. (a):Overview of multi-flow attention. The encoder and the decoder share the parameters of multi-layer Transformer. (b):Word-by-word generation flow with history contextual representations from Contextual Flow. (c):Spanby-span generation flow with shared Contextual Flow. (d):The attention mask matrixes of word-by-word generation flow (MW ), contextual flow (MC ) and span-by-span generation flow (MS). The i-th generated token yi is calculated by argmax(softmax(Fc(a (L−1) i ))).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Schematic of infilling decoding: the particular procedures in infilling decoding including dropping and inserting (left) and the attention mask matrixes at each step (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Results of ablation studies. (a):Comparisons between typical generation and infilling generation on Gigaword 10k and SQuAD QG with different fine-tuning noising rate ρ f . (b):Noising Analysis, average attention weights of source words, unnoised target words and noised target words for diverse fine-tuning noising rate ρ f . (c):Ablation study on Gigaword 10k, the x-axis shows fine-tuning epochs.# Fine-tuning method 1 Noising fine-tuning: Fine-tuning with noise-aware generation 2 Masking fine-tuning: Only updating the gradients of masked words RG-2 / RG-L RG-1 / RG-2 / RG-L Bleu-4 / MTR / RG-L RG-1 / RG-2 / RG-L RG-1 / RG-2 / RG-L Bleu-4 / MTR / RG-L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and CNN/D-ailyMail dataset [Hermann et al., 2015]. Gigaword dataset contains 3.8M articles extracted from the Gigaword corpus, while CNN/DailyMail dataset consists of 93k articles and 220k articles from the CNN and Daily Mail respectively.</figDesc><table><row><cell>Model</cell><cell>Data Params RG-1 / RG-2 / RG-L</cell></row><row><cell>* 10k training samples : Gigaword 10k</cell><cell></cell></row><row><cell>MASS [Song et al., 2019]</cell><cell>18G 160M 25.03 / 9.48 / 23.48</cell></row><row><cell>UNILM LARGE [Dong et al., 2019]</cell><cell>16G 340M 32.96 / 14.68 / 30.56</cell></row><row><cell>ERNIE-GEN BASE</cell><cell>16G 110M 33.75 / 15.23 / 31.35</cell></row><row><cell>ERNIE-GEN LARGE</cell><cell>16G 340M 35.05 / 16.10 / 32.50</cell></row><row><cell>* Fully 3.8M training samples</cell><cell></cell></row><row><cell>MASS [Song et al., 2019]</cell><cell>18G 160M 37.66 / 18.53 / 34.89</cell></row><row><cell>BERTSHARE [Rothe et al., 2019]</cell><cell>16G 110M 38.13 / 19.81 / 35.62</cell></row><row><cell>UNILM LARGE [Dong et al., 2019]</cell><cell>16G 340M 38.45 / 19.45 / 35.75</cell></row><row><cell>PEGASUS(C4) [Zhang et al., 2019]</cell><cell>750G 568M 38.75 / 19.96 / 36.14</cell></row><row><cell cols="2">PEGASUS(HugeNews) [Zhang et al., 2019] 3.8T 568M 39.12 / 19.86 / 36.24</cell></row><row><cell>ERNIE-GEN BASE</cell><cell>16G 110M 38.83 / 20.04 / 36.20</cell></row><row><cell>ERNIE-GEN LARGE</cell><cell>16G 340M 39.25 / 20.25 / 36.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison on Gigaword dataset with state-of-the-art results. Models in the upper block use 10k sample for fine-tuning. We also report the size of pre-training data and parameters utilized for each listed model (columns 2-3). RG is short for ROUGE.</figDesc><table><row><cell>Task</cell><cell cols="2">Epoch BASE LARGE</cell><cell cols="2">Learning Rate BASE LARGE</cell><cell cols="2">Noising Rate ρf BASE LARGE</cell><cell cols="2">Dropout Rate BASE LARGE</cell><cell>Batch Size</cell><cell>Label Smooth</cell><cell>Beam Size</cell><cell>Evaluation Metric</cell></row><row><cell>SQuAD QG</cell><cell>10</cell><cell>10</cell><cell cols="2">2.5e-5 1.5e-5</cell><cell>0.7</cell><cell>0.7</cell><cell>0.1</cell><cell>0.2</cell><cell>32</cell><cell>0.1</cell><cell>1</cell><cell>BLEU-4, METEOR (MTR), ROUGE-L (RG-L)</cell></row><row><cell>CNN/DailyMail</cell><cell>30</cell><cell>20</cell><cell>5e-5</cell><cell>4e-5</cell><cell>0.7</cell><cell>0.7</cell><cell>0.1</cell><cell>0.1</cell><cell>64</cell><cell>0.1</cell><cell>5</cell><cell>ROUGE-F1 scores:</cell></row><row><cell>Gigaword</cell><cell>10</cell><cell>5</cell><cell>3e-5</cell><cell>3e-5</cell><cell>0.5</cell><cell>0.6</cell><cell>0.1</cell><cell>0.2</cell><cell>128</cell><cell>0.1</cell><cell>5</cell><cell>ROUGE-1 (RG-1), ROUGE-2 (RG-2), ROUGE-L (RG-L)</cell></row><row><cell>Persona-Chat</cell><cell>-</cell><cell>30</cell><cell>-</cell><cell>1e-4</cell><cell>-</cell><cell>0.0</cell><cell>-</cell><cell>0.1</cell><cell>64</cell><cell>0.1</cell><cell>10</cell><cell>BLEU-1, BLEU-2, Distinct-1, Distinct-2</cell></row><row><cell>Generative CoQA</cell><cell>-</cell><cell>10</cell><cell>-</cell><cell>1e-5</cell><cell>-</cell><cell>0.5</cell><cell>-</cell><cell>0.1</cell><cell>32</cell><cell>0.1</cell><cell>3</cell><cell>F1-score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Hyperparamters of fine-tuning for ERNIE-GENBASE and ERNIE-GENLARGE.</figDesc><table><row><cell>Model</cell><cell>Data Params RG-1 / RG-2 / RG-L</cell></row><row><cell>BERTSHARE [Rothe et al., 2019]</cell><cell>16G 110M 39.25 / 18.09 / 36.45</cell></row><row><cell>BERTSUMABS [Liu and Lapata, 2019]</cell><cell>16G 110M 41.72 / 19.39 / 38.76</cell></row><row><cell>MASS [Song et al., 2019]</cell><cell>18G 160M 42.12 / 19.50 / 39.01</cell></row><row><cell>UNILM LARGE [Dong et al., 2019]</cell><cell>16G 340M 43.33 / 20.21 / 40.51</cell></row><row><cell>T5 LARGE [Raffel et al., 2019]</cell><cell>750G 340M 42.50 / 20.68 / 39.75</cell></row><row><cell>T5 XLARGE [Raffel et al., 2019]</cell><cell>750G 11B 43.52 / 21.55 / 40.69</cell></row><row><cell>BART LARGE [Lewis et al., 2019]</cell><cell>430G 400M 44.16 / 21.28 / 40.90</cell></row><row><cell>PEGASUS(C4) [Zhang et al., 2019]</cell><cell>750G 568M 43.90 / 21.20 / 40.76</cell></row><row><cell cols="2">PEGASUS(HugeNews) [Zhang et al., 2019] 3.8T 568M 44.17 / 21.47 / 41.11</cell></row><row><cell>ERNIE-GEN BASE</cell><cell>16G 110M 42.30 / 19.92 / 39.68</cell></row><row><cell>ERNIE-GEN LARGE</cell><cell>16G 340M 44.02 / 21.17 / 41.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results on CNN/DailyMail. C4 and HugeNews are two massive datasets of 750G and 3.8T respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Question generation results on SQuAD. Models in the upper block and the lower block use different test ↔ dev split method.</figDesc><table><row><cell>Model</cell><cell>BLEU-1/2</cell><cell>Distinct-1/2</cell></row><row><cell>LIC [Bao et al., 2020]</cell><cell cols="2">40.5 / 32.0 0.019 / 0.113</cell></row><row><cell cols="3">PLATO w/o latent [Bao et al., 2020] 45.8 / 35.7 0.012 / 0.064</cell></row><row><cell>PLATO [Bao et al., 2020]</cell><cell cols="2">40.6 / 31.5 0.021 / 0.121</cell></row><row><cell>ERNIE-GEN LARGE</cell><cell cols="2">46.8 / 36.4 0.023 / 0.168</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art results on Persona-Chat.</figDesc><table><row><cell>training set and testing set with the original development set</cell></row><row><cell>unchanged. We also conduct experiment with the reversed</cell></row><row><cell>dev↔test split as [Zhao et al., 2018] indicates. In Table 4,</cell></row><row><cell>we present the results of ERNIE-GEN and several previous</cell></row><row><cell>works. Again, ERNIE-GEN outperforms UNILM LARGE and</cell></row><row><cell>achieves a new state-of-the-art result on question generation</cell></row><row><cell>by giving +1.82 BLEU-4 scores.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Generative question answering results on the development set of CoQA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>-span 33.43 / 15.04 / 31.14 39.75 / 17.62 / 37.21 23.37 / 25.56 / 51.32 32.97 / 14.92 / 30.94 39.54 / 17.57 / 36.95 23.10 / 25.14 / 51.42   </figDesc><table><row><cell>1 ERNIE-GEN</cell><cell>33.75 / 15.23 / 31.35 39.92 / 17.46 / 37.40 23.52 / 25.61 / 51.45 33.30 / 15.04 / 31.22 39.54 / 17.61 / 37.00 22.99 / 25.14 / 51.31</cell></row><row><cell>2 -noise-aware</cell><cell>33.57 / 15.15 / 31.28 39.78 / 17.63 / 37.23 23.40 / 25.50 / 51.36 33.01 / 14.94 / 31.00 39.53 / 17.61 / 36.97 23.09 / 25.15 / 51.41</cell></row><row><cell>3 -span-by4 -2 and 3</cell><cell>33.23 / 14.77 / 31.00 39.71 / 17.55 / 37.18 23.34 / 25.54 / 51.30 32.57 / 14.68 / 30.60 39.49 / 17.66 / 36.96 22.89 / 25.08 / 51.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Ablation study for ERNIE-GENBASE and its variants. Particularly, We set ρp = 0.05 in pre-training (row 1), while removing the span-by-span generation task (row 3), we set ρp = 0.2 because the pre-training becomes easier.RG-L)  32.98 / 14.67 / 30.51 32.93 /14.46 / 30.53 2 CNN/DM 10k (RG-1 / RG-2 / RG-L) 39.25 / 16.70 / 36.65 39.56 / 16.93 / 36.94 3 SQuAD QG (Bleu-4 / MTR / RG-L) 21.95 / 24.53 / 50.34 22.13 / 24.66 / 50.51 Fine-tuning with noise-aware generation 4 Gigaword 10k (RG-1 / RG-2 / RG-L) 32.99 / 14.83 / 30.84 33.23 / 14.77 / 31.00 5 CNN/DM 10k (RG-1 / RG-2 / RG-L) 39.34 / 17.30 / 36.75 39.71 / 17.55 / 37.18 6 SQuAD QG (Bleu-4 / MTR / RG-L) 23.23 / 25.47 / 51.25 23.34 / 25.54 / 51.30</figDesc><table><row><cell># Task (Metrics)</cell><cell>Typical generation Infilling generation</cell></row><row><cell cols="2">Fine-tuning without noise-aware generation</cell></row><row><cell>1 Gigaword 10k (RG-1 / RG-2 /</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Results of models pre-trained with typical generation and infilling generation. Tasks in the upper block are fine-tuned without noising, while the others are fine-tuned with noise-aware generation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Question generation results on SQuAD. ERNIE-GEN with the ‡ mark is pre-trained with the 430GB text corpora.The fine-tuning hyperparameters of ERNIE-GEN ‡ LARGE are presented inTable 11.</figDesc><table><row><cell></cell><cell cols="3">CNN/DailyMail Gigaword SQuAD QG</cell></row><row><cell>Learning rate</cell><cell>4e-5</cell><cell>3e-5</cell><cell>1.25e-5</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>128</cell><cell>32</cell></row><row><cell>Epochs</cell><cell>17</cell><cell>5</cell><cell>10</cell></row><row><cell>Dropout rate</cell><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Warmup ratio</cell><cell>0.02</cell><cell>0.1</cell><cell>0.25</cell></row><row><cell>Noising rate ρ f</cell><cell>0.7</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>Label smooth</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Beam size</cell><cell>5</cell><cell>6</cell><cell>5</cell></row><row><cell>Length penalty</cell><cell>1.2</cell><cell>0.7</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameters used for fine-tuning on CNN/DailyMail, Gigaword, and SQuAD QG.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We donate the number of layers as L, the hidden size as H and the number of self-attention heads as A.2 https://github.com/PaddlePaddle/Paddle</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key Research and Development Project of China (No. 2018AAA0101900).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<idno>arXiv:1910.13461</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<editor>Lan et al., 2020] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radue Soricut</editor>
		<meeting><address><addrLine>Omer Levy, Ves; Bart</addrLine></address></meeting>
		<imprint>
			<publisher>De</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR. Lewis et al., 2019] Mike Lewis. noising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A reinforced topicaware convolutional sequence-to-sequence model for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapata ; Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<idno>arXiv:1904.09223</idno>
	</analytic>
	<monogr>
		<title level="m">Enhanced representation through knowledge integration</title>
		<editor>Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen, Changyou Chen, and Lawrence Carin</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An end-to-end generative architecture for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3123" to="3133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Addressing semantic drift in question generation for semisupervised question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<editor>Zhang and Mohit Bansal</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2495" to="2509" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question generation with maxout pointer and gated self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08777</idno>
	</analytic>
	<monogr>
		<title level="m">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<imprint>
			<publisher>Yao Zhao</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3901" to="3910" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Appendix A.1 Pre-training on Large-scale Text Corpora Recent works for pre-training verify that larger scaled pretraining corpora can improve the performances on downstream tasks. We pre-train ERNIE-GEN LARGE model on the 430GB text corpora with 1 epoch and 1M training steps. Our 430GB text corpora is extracted from the corpus used by RoBERTa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>We fine-tune ERNIE-GEN LARGE on two abstractive summarization datasets including Gigaword and CNN/Daily Mail, the evaluation results are reported in Table 9. Notice that the performance increase significantly as ERNIE-GEN LARGE pre-trains on larger scaled text corpora. Model Data Params RG-1 / RG-2 / RG-L</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Gigaword</surname></persName>
		</author>
		<idno>10k ERNIE-GEN LARGE 16G 340M 35.05 / 16.10 / 32.50</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernie-Gen ‡</forename><surname>Large</surname></persName>
		</author>
		<idno>430G 340M 35.51 / 16.79 / 33.23</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Gigaword</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegasus ;</forename><surname>Zhang</surname></persName>
		</author>
		<idno>750G 568M 38.75 / 19.96 / 36.14</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Pegasus(hugenews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>568M 39.12 / 19.86 / 36.24 ERNIE-GEN LARGE 16G 340M 39.25 / 20.25 / 36.53</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernie-Gen ‡</forename><surname>Large</surname></persName>
		</author>
		<idno>430G 340M 39.46 / 20.34 / 36.74</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Cnn/Daily Mail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T5</forename><surname>Large [raffel</surname></persName>
		</author>
		<idno>750G 340M 42.50 / 20.68 / 39.75</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T5</forename><surname>Xlarge [raffel</surname></persName>
		</author>
		<idno>11B 43.52 / 21.55 / 40.69</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Large [lewis</surname></persName>
		</author>
		<idno>160G 400M 44.16 / 21.28 / 40.90</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>568M 43.90 / 21.20 / 40.76</idno>
	</analytic>
	<monogr>
		<title level="j">PEGASUS</title>
		<imprint>
			<biblScope unit="issue">C4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Pegasus(hugenews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>568M 44.17 / 21.47 / 41.11 ERNIE-GEN LARGE 16G 340M 44.02 / 21.17 / 41.26</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernie-Gen ‡</forename><surname>Large</surname></persName>
		</author>
		<idno>430G 340M 44.31 / 21.35 / 41.60</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
