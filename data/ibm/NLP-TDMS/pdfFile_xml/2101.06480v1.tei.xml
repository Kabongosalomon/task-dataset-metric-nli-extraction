<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoungjip</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung SDS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Choo</surname></persName>
							<email>jinho12.choo@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung SDS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeong-Dae</forename><surname>Kwon</surname></persName>
							<email>y.d.kwon@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung SDS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongho</forename><surname>Joe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung SDS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjai</forename><surname>Min</surname></persName>
							<email>seungjai.min@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung SDS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjune</forename><surname>Gwon</surname></persName>
							<email>gyj.gwon@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung SDS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces SelfMatch, a semi-supervised learning method that combines the power of contrastive self-supervised learning and consistency regularization. SelfMatch consists of two stages: (1) self-supervised pre-training based on contrastive learning and (2) semi-supervised fine-tuning based on augmentation consistency regularization. We empirically demonstrate that SelfMatch achieves the state-of-the-art results on standard benchmark datasets such as CIFAR-10 and SVHN. For example, for CIFAR-10 with 40 labeled examples, SelfMatch achieves 93.19% accuracy that outperforms the strong previous methods such as MixMatch (52.46%), UDA (70.95%), ReMixMatch (80.9%), and FixMatch (86.19%). We note that SelfMatch can close the gap between supervised learning (95.87%) and semi-supervised learning (93.19%) by using only a few labels for each class.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have shown that they can achieve human-level performance in many tasks <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b13">[14]</ref>. However, such high performance is usually achieved in a supervised learning setting that exploits a very large number of labeled training examples. Since labeling a large number of examples requires considerable time and cost, label efficient learning algorithms are in high demand.</p><p>Semi-supervised learning <ref type="bibr" target="#b3">[4]</ref> is one of the attractive approaches that addresses the label inefficiency problem. Semi-supervised learning enables a deep neural network to learn with small labeled data by leveraging large unlabeled data <ref type="bibr" target="#b19">[20]</ref>[31] <ref type="bibr" target="#b18">[19]</ref> <ref type="bibr" target="#b33">[34]</ref>[2] <ref type="bibr" target="#b34">[35]</ref>[3] <ref type="bibr" target="#b28">[29]</ref>.</p><p>Recently, self-supervised learning <ref type="bibr" target="#b16">[17]</ref> is attracting high attention as a means of unsupervised representation learning. It learns representations by performing a pretext task in which labels can be automatically generated <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b36">[37]</ref>[25] <ref type="bibr" target="#b10">[11]</ref>. Also, contrastive self-supervised learning <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b25">[26]</ref>[1] <ref type="bibr" target="#b4">[5]</ref>[32] <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b32">[33]</ref> learns representations by contrasting different views of examples in a task-agnostic way. It is shown that self-supervised learning provides representations good for downstream tasks.</p><p>In this paper, we introduce SelfMatch, a semi-supervised learning method that combines the power of contrastive self-supervised learning and consistency regularization. SelfMatch consists of two 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. stages: (1) self-supervised pre-training based on contrastive learning and (2) semi-supervised finetuning based on augmentation consistency regularization. We adopt SimCLR <ref type="bibr" target="#b4">[5]</ref> for self-supervised pre-training, and FixMatch <ref type="bibr" target="#b28">[29]</ref> for semi-supervised fine-tuning. SelfMatch achieves new state-ofthe-art results on standard benchmarks such as CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> and SVHN <ref type="bibr" target="#b23">[24]</ref> (see <ref type="figure" target="#fig_0">Figure 1</ref>). We conjecture that this is because the two stages are complementary in using unlabeled data. Contrastive self-supervised pre-training learns representations by maximizing the mutual information between different views of the data. Meanwhile, consistency regularization further enhances the learned representations by minimizing the cross entropy between class predictions of augmented data (see Appendix E). By combining two closely related but complementary stages, SelfMatch improves from other methods such as S4L <ref type="bibr" target="#b35">[36]</ref> that also uses self-supervised pre-training (see Appendix A).</p><p>Our contributions can be summarized as follows.</p><p>• We introduce SelfMatch, a semi-supervised learning method that consists of two stages:</p><p>(1) self-supervised pre-training based on contrastive learning and (2) semi-supervised finetuning based on augmentation consistency regularization. (see <ref type="figure" target="#fig_1">Figure 2</ref>). • We empirically demonstrate that SelfMatch achieves the state-of-the-art results on standard benchmarks such as CIFAR-10 and SVHN (see <ref type="table" target="#tab_0">Table 1</ref>). Especially, we show that SelfMatch can close the gap between supervised learning and semi-supervised learning by using only a few labels (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-supervised representation learning</head><p>Self-supervised learning <ref type="bibr" target="#b16">[17]</ref> is a class of methods to unsupervised representation learning. Specifically, it aims to learn representations that can be used for downstream tasks via pretext tasks in which the training labels are automatically generated. Such pretext tasks include context prediction <ref type="bibr" target="#b9">[10]</ref>, image colorization <ref type="bibr" target="#b36">[37]</ref>, Jigsaw puzzle solving <ref type="bibr" target="#b24">[25]</ref>, and rotation degree classification (RotNet) <ref type="bibr" target="#b10">[11]</ref>.</p><p>Contrastive self-supervised learning is a task-agnostic approach to unsupervised representation learning. It aims to learn representations by contrasting two views from the same or different samples. Its objective functions are based on InfoMax principle <ref type="bibr" target="#b32">[33]</ref>. This approach includes Deep InfoMax (DIM) <ref type="bibr" target="#b15">[16]</ref>, CPC <ref type="bibr" target="#b25">[26]</ref>, Augmented Multiscale DIM <ref type="bibr" target="#b0">[1]</ref>, SimCLR <ref type="bibr" target="#b4">[5]</ref>, CMC <ref type="bibr" target="#b31">[32]</ref>, MoCo <ref type="bibr" target="#b14">[15]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-supervised learning</head><p>Semi-supervised learning enables a deep neural network to learn with small labeled data by leveraging large unlabeled data. To leverage unlabeled data, diverse methods like consistency regularization <ref type="bibr" target="#b26">[27]</ref>[31] <ref type="bibr" target="#b18">[19]</ref>, entropy minimization <ref type="bibr" target="#b11">[12]</ref>, and pseudo-labeling <ref type="bibr" target="#b19">[20]</ref> have been proposed.</p><p>In pseudo-labeling <ref type="bibr" target="#b19">[20]</ref>, a model is first trained with a small number of labeled examples, and then it assigns pseudo-labels with high confidence on unlabeled examples. And then, the pseudolabeled examples are used to further train the model in a supervised manner. However, the vanilla pseudo-labeling method suffers from the over-fitting and noisy labels.</p><p>Consistency regularization has been introduced by Π-model <ref type="bibr" target="#b26">[27]</ref>, and is further developed by many following works such as Mean Teacher <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b18">[19]</ref>. In the basic consistency regularization, an input image is transformed by a stochastic transformation function and the training objective is to minimize the distance between the model predictions of randomly transformed images by using L 2 distance.</p><p>Very recently, advanced consistency regularization methods have been introduced. These methods enhance basic consistency regularization methods with pseudo-labeling and improved data augmentation, and provide very high accuracy that is comparable to supervised learning with full labels. They include ICT <ref type="bibr" target="#b33">[34]</ref>, MixMatch <ref type="bibr" target="#b1">[2]</ref>, UDA <ref type="bibr" target="#b34">[35]</ref>, ReMixMatch <ref type="bibr" target="#b2">[3]</ref>, and FixMatch <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>An overview of SelfMatch is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. SelfMatch consists of two stages: (1) self-supervised pre-training based on contrastive learning and (2) semi-supervised fine-tuning based on augmentation consistency regularization. The model p model (y|x) consists of an encoder f (·) and a head c(·). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-supervised pre-training based on contrastive learning</head><p>To achieve the high accuracy with only a few labels, SelfMatch pre-trains encoder f (·) by leveraging unlabeled data. For this unsupervised pre-training, SelfMatch adopts SimCLR <ref type="bibr" target="#b4">[5]</ref>, one of the most promising self-supervised learning methods. SimCLR learns representations by encouraging two viewsx i andx j from the same image x to be similar, and two viewsx i andx k (k = i) from different images to be dissimilar.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, SimCLR consists of four components: data augmentation T (·), base encoder f (·), projection head g(·), and contrastive loss L c . For data augmentation, SelfMatch uses random crop and color distortion. For the base encoder, we use ResNet-34 <ref type="bibr" target="#b12">[13]</ref>. For the projection head, we use a MLP consisting of two layers with Dropout and ReLU activation. Finally, the contrastive loss is formulated as follows:</p><formula xml:id="formula_0">L c = − log exp(sim(zi,zj )/τ ) 2B k=1 1(k =i)exp(sim(zi,z k )/τ ) ,</formula><p>where sim(·) is a similarity measure function, τ is a temperature parameter scaling the similarity, 1(k = i) is an indicator function evaluating to 1 iff k = i, and B is a batch size. We set hyperparameters to τ = 0.5 and B = 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semi-supervised fine-tuning based on augmentation consistency regularization</head><p>Unlike the related methods such as S4L <ref type="bibr" target="#b35">[36]</ref> (using semi-supervised fine-tuning based on VAT <ref type="bibr" target="#b22">[23]</ref>) and SimCLRv2 <ref type="bibr" target="#b5">[6]</ref> (using supervised fine-tuning with few labels), SelfMatch exploits semi-supervised fine-tuning based on augmentation consistency. For augmentation consistency, we adopt FixMatch <ref type="bibr" target="#b28">[29]</ref>. It encourages the consistent prediction between weakly and strongly augmented examples (x w i andx s i ). More concretely, FixMatch uses a model output q w i of a weakly augmented input as the pseudo-label for a strongly augmented inputx s i (see <ref type="figure" target="#fig_1">Figure 2</ref>). For weak augmentation, we use random crop and random horizontal flip. For strong augmentation, SelfMatch adopts RandAugment (RA) <ref type="bibr" target="#b7">[8]</ref>, an effective automated augmentation method (see Appendix B). For the classification head c(·), SelfMatch uses a MLP consisting of two layers with Dropout <ref type="bibr" target="#b29">[30]</ref> and ReLU activation.</p><p>The loss function of FixMatch consists of supervised and unsupervised loss:</p><formula xml:id="formula_1">L semi = L s + λ u L u .</formula><p>The supervised loss L s is formulated as follows:</p><formula xml:id="formula_2">L s = 1 B B i=1 H(p i , q w i ), where B is a batch size, p i is a one-hot encoded label of sample x i , and q w i is a model output p model (y|x w i ) of a weakly augmented inputx w i .</formula><p>For the experiments, we set B to 64. The unsupervised loss L u is formulated as follow:</p><formula xml:id="formula_3">L u = 1 µB µB i=1 1(max(q w i ) ≥ c)H(q w i , q s i ), where 1(max(q w i ) ≥ c)</formula><p>is an indicator function, c is a confidence threshold, B is a batch size, and µ is the ratio of labeled and unlabeled samples in a batch. Here,q w i is arg max(q w i ) and q s i is a model output of a strongly augmented inputx s i . We set hyperparameters to c = 0.95, µ = 7, and λ u = 1. For training, SelfMatch uses a standard SGD optimizer with momentum β = 0.9 and exploits cosine learning rate decay <ref type="bibr" target="#b21">[22]</ref> with the initial learning rate η = 0.03. Also, it utilizes Exponential Moving Average (EMA) <ref type="bibr" target="#b30">[31]</ref> with weight decay 0.999 for stable training and inference.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the performance of SelfMatch on standard classification benchmark datasets such as CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> and SVHN <ref type="bibr" target="#b23">[24]</ref>. To see the label efficiency, we perform evaluation with varying amount of labeled data. For CIFAR-10, we use 50,000 unlabeled training images and 10,000 testing images. For SVHN, we use 73,357 unlabeled training images and 26,032 test images. We present SelfMatch results using the average value from three evaluation runs with different random seeds. It is remarkable that the gap between the supervised learning (95.87%) and the semi-supervised learning (93.19%) can be narrowed down to a few percent with only 4 labels per class (see also <ref type="figure" target="#fig_0">Figure 1</ref>). To further demonstrate the effectiveness of SelfMatch, a comparison of learning curves of different methods is presented in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>SVHN <ref type="table" target="#tab_0">Table 1</ref> shows the results for SVHN. SelfMatch achieves very high accuracy that are within the margin of error of other state-of-the-art results.</p><p>Ablation study <ref type="table" target="#tab_1">Table 2</ref> shows the effect of self-supervised pre-training for two representative semi-supervised learning methods, MixMatch <ref type="bibr" target="#b1">[2]</ref> and FixMatch <ref type="bibr" target="#b28">[29]</ref>. At 40 labeled examples, selfsupervised pre-training improves MixMatch by 10.96%, and FixMatch by 7.00%. Further ablation study can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce SelfMatch, a semi-supervised learning method that consists of two stages:</p><p>(1) self-supervised pre-training based on contrastive learning and (2) semi-supervised fine-tuning based on augmentation consistency regularization. We empirically demonstrate that SelfMatch can close the gap between supervised learning and semi-supervised learning using only a few labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Comparison with related method</head><p>A comparison with a closely related method is shown in <ref type="table" target="#tab_3">Table 3</ref>. Very recently, Zhai et al. presented S4L <ref type="bibr" target="#b35">[36]</ref>, a semi-supervised learning method that consists of three stages: (1) self-supervised pre-training based on rotation prediction (RotNet <ref type="bibr" target="#b10">[11]</ref>), (2) semi-supervised fine-tuning based on adversarial consistency (VAT <ref type="bibr" target="#b22">[23]</ref>), and (3) supervised fine-tuning with a few labeled data. SelfMatch is different from S4L. First, SelfMatch proposes to use contrastive self-supervised pre-training (e.g., SimCLR <ref type="bibr" target="#b4">[5]</ref>, MoCo <ref type="bibr" target="#b14">[15]</ref>, etc). Since contrastive self-supervised learning is task-agnostic, it provides better representations for downstream tasks. Second, SelfMatch proposes to use semi-supervised fine-tuning based on augmentation consistency (e.g., FixMatch <ref type="bibr" target="#b28">[29]</ref>, ReMixMatch <ref type="bibr" target="#b2">[3]</ref>, etc.). Since contastive self-supervised learning and augmentation consistency regularization are commonly based on data augmentation, SelfMatch is architecturally simpler and has more rooms to be optimized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Transformations in RandAugment</head><p>SelfMatch uses RandAugment <ref type="bibr" target="#b7">[8]</ref> for strong augmentation in the semi-supervised fine-tuning stage. <ref type="table" target="#tab_4">Table 4</ref> shows the list of transformations used in RandAugment. At each strong data augmentation process, two transformations with random magnitude are randomly selected and serially performed. C Further ablation study <ref type="table" target="#tab_5">Table 5</ref> shows further ablation study. More specifically, it shows baseline accuracy results of selfsupervised pre-training for CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> with 4000 labels. Usually, in unsupervised representation learning, the performance of learned representation is measured by using a linear classification head and full labeled data <ref type="bibr" target="#b4">[5]</ref>. As shown in <ref type="table" target="#tab_5">Table 5</ref>, when following such an evaluation protocol for CIFAR-10, SimCLR <ref type="bibr" target="#b4">[5]</ref> provides about 92.27% accuracy. When using a MLP head consisting of two layers with Dropout <ref type="bibr" target="#b29">[30]</ref> and ReLU activation, SimCLR provides about 92.69% accuracy (+0.42%).</p><p>In semi-supervised learning setting with 4000 labels, it decreases to 91.36% (-1.33%). This can be considered as a baseline method that exploits self-supervised pre-training. More specifically, the baseline method consists of (1) self-supervised pre-training and (2) supervised fine-turning with 4000 labels. Unlike this, using semi-supervised fine-truing based on augmentation consistency (FixMatch <ref type="bibr" target="#b28">[29]</ref>) in the second stage, SelfMatch improves the baseline method up to 95.94% in a large margin (+4.58%).  To demonstrate the effectiveness of SelfMatch in more detail, we present a comparison of learning curves for CIFAR-10 in <ref type="figure" target="#fig_2">Figure 3</ref>. Each figure shows a comparison of learning curves of three most advanced methods including SelfMatch (ours), FixMatch <ref type="bibr" target="#b28">[29]</ref> and MixMatch <ref type="bibr" target="#b1">[2]</ref> for varying number of labels (i.e., 40, 250, 4000 labels). Note that the learning curves of SelfMatch only present the second stage of SelfMatch, that is the semi-supervised fine-tuning stage after the self-supervised pre-training stage.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, compared to other methods, SelfMatch not only achieves higher accuracy, but also reaches such high accuracy more rapidly. The accuracy gain is more prominent in the case where only a few labels are available (see <ref type="figure" target="#fig_2">Figure 3a)</ref>. These results are also summarized in <ref type="table" target="#tab_0">Table 1</ref>. More importantly, SelfMatch reaches such high accuracy more rapidly. For 250 and 4000 labels, SelfMatch reaches an accuracy of greater than 90% through just one epoch. In the case of 40 labels, SelfMatch achieves an accuracy of 90% through around 10 epochs, while FixMatch and MixMatch do not reach such high accuracy even though iterating through a very large number of training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Discussion</head><p>Relation between contrastive self-supervision and consistency regularization In this paper, we introduce SelfMatch, a semi-supervised learning method that combines the power of contrastive self-supervised learning and consistency regularization. To further motivate the reason why we combine these two approaches, we summarize the relation between the two approaches in <ref type="table">Table 6</ref>. Note that the relation is based on SimCLR <ref type="bibr" target="#b4">[5]</ref> and FixMatch <ref type="bibr" target="#b28">[29]</ref> adopted by SelfMatch. If other methods are considered for each approach, the comparison would be slightly different.</p><p>SelfMatch consists of two stages: (1) self-supervised pre-training based on contrastive learning and (2) semi-supervised fine-tuning based on consistency regularization. We empirically found that SelfMatch not only achieves higher accuracy, but also reaches such high accuracy more rapidly. As mentioned in Section 1, we conjecture that this is because the two stages are complementary in using unlabeled data. Contrastive self-supervised learning methods such as SimCLR <ref type="bibr" target="#b4">[5]</ref> use loss functions based on the InfoMax principle <ref type="bibr" target="#b20">[21]</ref>, and the mutual information is usually estimated by InfoNCE <ref type="bibr" target="#b25">[26]</ref>. Also, Michael et al. <ref type="bibr" target="#b32">[33]</ref> have shown that InfoNCE losses are related with triplet losses <ref type="bibr" target="#b27">[28]</ref>. We have a similar perspective. In the pre-training stage of SelfMatch, contrastive self-supervised learning learns representations by maximizing a lower bound (e.g., InfoNCE) of the mutual information between different views of the data. This InfoNCE loss can be interpreted as encouraging two objectives: (1) maximizing the similarity between different views from the same sample and (2) minimizing the similarity between different views from different samples. The former objective can be considered as using intra-sample statistics, and the later objective can be understood as leveraging inter-sample statistics. Meanwhile, in the fine-tuning stage of SelfMatch, consistency regularization <ref type="bibr" target="#b28">[29]</ref> further enhances the learned representations by minimizing the cross entropy (or distance) between class predictions of different views from the same sample. This objective can be considered as exploiting intra-sample statistics.</p><p>Furthermore, two approaches are slightly different in exploiting intra-sample statistics. The target entities to optimize are normalized representations in contrastive self-supervised learning, while they are class probability distributions in consistency regularization. Also, consistency regularization of FixMatch exploits asymmetric augmentation (i.e., weak and strong augmentation of the same sample). <ref type="table">Table 6</ref>: Relation between contrastive self-supervision and consistency regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive self-supervision Consistency regularization Method</head><p>SimCLR <ref type="bibr" target="#b4">[5]</ref> FixMatch <ref type="bibr" target="#b28">[29]</ref> Loss function − log exp(sim(zi,zj )/τ ) Large-scale experiment In this paper, we imperially demonstrate the effectiveness of SelfMatch by using standard benchmark datasets such as CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> and SVHN <ref type="bibr" target="#b23">[24]</ref>. However, these datasets are rather small. Therefore, we are doing experiments by using larger datasets such as STL-10 <ref type="bibr" target="#b6">[7]</ref> and ImageNet <ref type="bibr" target="#b8">[9]</ref>, and will add the results in an extended version of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Accuracy comparison of semisupervised learning methods. SelfMatch can close the gap between supervised and semi-supervised learning by using only a few labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>SelfMatch overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A comparison of learning curves for CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2B k=1 1 (</head><label>1</label><figDesc>k =i)exp(sim(zi,z k )/τ ) 1 µB µB i=1 1(max(q w i ) ≥ c)H(q w i , q s i ) Maximizing the similarity between different views form the same sample (z i and positive sample z j )Minimizing the cross entropy (or distance) between predictions of different views of the same sample (q w i and q s i ) Minimizing the similarity between different views from different samples (z i and negative sample z k )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of accuracy for CIFAR-10 and SVHN. 46±11.50 88.95±0.86 93.58±0.10 57.45±14.53 96.02±0.23 96.5±0.28 UDA 70.95±5.93 91.18±1.08 95.12±0.18 47.37±20.51 94.31±2.76 97.54±0.24 ReMixMatch 80.90±9.64 94.56±0.05 95.28±0.13 96.66±0.20 97.08±0.48 97.35±0.08 FixMatch(RA) 86.19±3.37 94.93±0.65 95.74±0.05 96.04±2.17 97.52±0.38 97.72±0.11 SelfMatch 93.19±1.08 95.13±0.26 95.94±0.08 96.58±1.02 97.37±0.43 97.49±0.07</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell>SVHN</cell></row><row><cell>Method</cell><cell>40 labels</cell><cell>250 labels 4000 labels 40 labels</cell><cell>250 labels 1000 labels</cell></row><row><cell>Supervised</cell><cell></cell><cell>95.87</cell><cell>97.41</cell></row><row><cell>Pseudo-Label</cell><cell>-</cell><cell>50.22±0.43 83.91±0.28 -</cell><cell>79.79±1.09 90.06±0.61</cell></row><row><cell>Π-Model</cell><cell>-</cell><cell>45.74±3.87 85.99±0.38 -</cell><cell>81.04±1.92 92.46±0.36</cell></row><row><cell cols="2">Mean Teacher -</cell><cell>67.68±2.30 90.81±0.19 -</cell><cell>96.43±0.11 96.58±0.07</cell></row><row><cell>MixMatch</cell><cell>52.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>40</cell><cell>250</cell><cell>4000</cell><cell>Method</cell><cell>40</cell><cell>250</cell><cell>4000</cell></row><row><cell cols="5">MixMatch 52.46 88.95 93.58 FixMatch</cell><cell cols="3">86.19 94.93 95.74</cell></row><row><cell>SimCLR +</cell><cell>63.42</cell><cell>92.58</cell><cell>94.96</cell><cell>SimCLR +</cell><cell>93.19</cell><cell>95.13</cell><cell>95.94</cell></row><row><cell>MixMatch</cell><cell>(+10.96)</cell><cell>(+3.63)</cell><cell>(+1.38)</cell><cell>FixMatch</cell><cell>(+7.00)</cell><cell>(+0.2)</cell><cell>(+0.2)</cell></row></table><note>Effect of self-supervised pre-training on semi-supervised learning (CIFAR-10).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>shows the results for CIFAR-10. SelfMatch achieves the state-of-the-art for all number of labeled examples. More important, at 40 labeled examples (4 labels for each class), SelfMatch achieves 93.19% accuracy that outperforms the strong previous methods including MixMatch [2] (52.46%), UDA [35] (70.95%), ReMixMatch [3] (80.9%), and FixMatch [29] (86.19%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>A comparison with a related method.</figDesc><table><row><cell>Phase</cell><cell>S4L [36]</cell><cell>SelfMatch (Ours)</cell></row><row><cell>1. Pre-training</cell><cell>Task-specific self-supervised (RotNet [11])</cell><cell>Task-agnostic self-supervised (SimCLR [5])</cell></row><row><cell>2. Fine-tuning A</cell><cell>Semi-supervised (adversarial consistency, VAT [23])</cell><cell>Semi-supervised (augmentation consistency, FixMatch [29])</cell></row><row><cell>3. Fine-tuning B</cell><cell>Supervised with few labels</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>List of transformations used in RandAugment<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell cols="2">Transformation Description</cell></row><row><cell>Autocontrast</cell><cell>Maximizing the image contrast.</cell></row><row><cell>Brightness</cell><cell>Adjusting the brightness of the image. (0: black image, 1: original image)</cell></row><row><cell>Color</cell><cell>Adjusting the color balance of the image. (0: black &amp; while image, 1: original image)</cell></row><row><cell>Contrast</cell><cell>Adjusting the contrast of the image. (0: a gray image, 1: original image)</cell></row><row><cell>Equalize</cell><cell>Equalizing the image histogram.</cell></row><row><cell>Identity</cell><cell>The original image.</cell></row><row><cell>Posterize</cell><cell>Reducing each pixel to [4, 8] bits.</cell></row><row><cell>Rotate</cell><cell>Rotating the image by [-30, 30] degrees.</cell></row><row><cell>Sharpness</cell><cell>Adjusting the sharpness of the image. (0: blurred image, 1: original image)</cell></row><row><cell>Shear-x</cell><cell>Shearing the image along the horizontal axis with rate [-0.3, 0.3].</cell></row><row><cell>Shear-y</cell><cell>Shearing the image along the vertical axis with rate [-0.3, 0.3].</cell></row><row><cell>Solarize</cell><cell>Inverting all pixels above a threshold value of [0, 1].</cell></row><row><cell>Translate-x</cell><cell>Translating the image horizontally by ([-0.3, 0.3] x image width) pixels.</cell></row><row><cell>Translate-y</cell><cell>Translating the image vertically by ([-0.3, 0.3] x image height) pixels.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Baseline accuracy results of self-supervised pre-training (CIFAR-10).</figDesc><table><row><cell>Method</cell><cell cols="2">Encoder (ResNet-34)</cell><cell>Head</cell><cell cols="2">Labels Accuracy (%)</cell></row><row><cell cols="2">Supervised random init.</cell><cell></cell><cell cols="2">Linear (512 × 10) 50000</cell><cell>95.87</cell></row><row><cell cols="2">Supervised random init.</cell><cell></cell><cell cols="2">Linear (512 × 10) 4000</cell><cell>79.74</cell></row><row><cell>SimCLR</cell><cell>frozen</cell><cell></cell><cell cols="2">Linear (512 × 10) 50000</cell><cell>92.27</cell></row><row><cell>SimCLR</cell><cell>frozen</cell><cell></cell><cell>MLP (2 layers)</cell><cell>50000</cell><cell>92.69</cell></row><row><cell>SimCLR</cell><cell>frozen</cell><cell></cell><cell cols="2">Linear (512 × 10) 4000</cell><cell>90.99</cell></row><row><cell>SimCLR</cell><cell>frozen</cell><cell></cell><cell>MLP (2 layers)</cell><cell>4000</cell><cell>91.36</cell></row><row><cell>SelfMatch</cell><cell cols="3">fine-tuned with FixMatch MLP (2 layers)</cell><cell>4000</cell><cell>95.94 (+4.58)</cell></row><row><cell cols="3">D Comparison of learning curves</cell><cell></cell><cell></cell></row><row><cell cols="2">(a) 40 labels</cell><cell cols="2">(b) 250 labels</cell><cell cols="2">(c) 4000 labels</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big selfsupervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
