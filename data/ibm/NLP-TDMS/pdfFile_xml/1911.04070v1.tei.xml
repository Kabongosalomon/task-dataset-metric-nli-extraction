<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BP-Transformer: Modelling Long-Range Context via Binary Partitioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University § New York University Shanghai</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University § New York University Shanghai</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University § New York University Shanghai</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University § New York University Shanghai</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University § New York University Shanghai</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Shanghai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University § New York University Shanghai</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University § New York University Shanghai</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BP-Transformer: Modelling Long-Range Context via Binary Partitioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of selfattention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields O(k · n log(n/k)) connections where k is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer, a self-attention based model, has achieved many impressive results on Natural Language Processing (NLP) tasks, notably machine translation <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>, language modeling <ref type="bibr" target="#b23">(Radford et al., 2018)</ref>, and text classification <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref>. However, its self-attention mechanism imposes a quadratic cost with respect to sequence length, limiting its wider application, especially for long text.</p><p>To address this problem, some previous works have explored different directions. (1) Hierarchical Transformers <ref type="bibr" target="#b20">(Miculicich et al., 2018;</ref><ref type="bibr" target="#b16">Liu and Lapata, 2019)</ref> uses two Transformers in a hierarchical architecture: one Transformer models the sentence representation with word-level context, and another the document representation with the sentence-level context. (2) Lightweight Transformers <ref type="bibr" target="#b6">(Child et al., 2019;</ref><ref type="bibr" target="#b27">Sukhbaatar et al., *</ref> Work done during internship at AWS Shanghai AI Lab. 1 https://github.com/yzh119/BPT Transformer Transformer-XL BPT <ref type="bibr">[5,</ref><ref type="bibr">6]</ref> [10,11] 3 4 2 1 10 11 <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>  2019; <ref type="bibr" target="#b11">Guo et al., 2019;</ref><ref type="bibr" target="#b8">Dai et al., 2019)</ref> reduce the complexity by reconstructing the connections between tokens. Besides the computational cost, the fullyconnected nature of Transformer does not incorporate the commonsensible inductive bias of language, such as sequential or syntax structure. The dependency relations between tokens are totally learned from scratch. Therefore, Transformer usually performs better on huge datasets and is easy to overfit on small datasets <ref type="bibr" target="#b11">(Guo et al., 2019)</ref>.</p><p>The above observation motivates us to explore better structure for self-attention models to balance the capability and computation complexity. In this paper, we propose a new architecture called BP-Transformer (BPT for short), which partitions the input sequence into different multi-scale spans via binary partitioning (BP). BPT incorporates an inductive bias of attending the context information from fine-grain to coarse-grain as the relative distance increases. The farther the context information is, the coarser its representation is. BPT can be regard as graph neural network, whose nodes are the multi-scale spans. A token node can attend the smaller-scale span for the closer context and the larger-scale span for the longer-distance context. The representations of nodes are updated with Graph Self-Attention <ref type="bibr" target="#b30">(Velickovic et al., 2018)</ref>.</p><p>Moreover, to better represent the position information of the span nodes and token nodes, we generalize the notion of relative position <ref type="bibr" target="#b24">(Shaw et al., 2018)</ref> from sequences to trees and show that it better captures position bias.</p><p>Thus, BPT incorporates the advantages of both hierarchical and lightweight Transformerss: (1) it models the long-range context in an hierarchical fashion, (2) reduces computation cost with fewer edges, and finally, (3) introduces coarse-to-fine connections to approximate the reasonable inductive bias of language, with a net effect of making BPT easier to train.</p><p>We evaluate BPT on a variety of Sentence-Level and Document-Level NLP tasks: language modeling, machine translation and text classification. The experiment results show that BPT consistently outperforms previous self-attention based models. We also show that the inductive bias of BPT works nicely on short text and can scale to large datasets. Finally, we show BPT is faster and more memory efficient than vanilla Transformer when dealing with long sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recap: Transformer</head><p>Given a sentence with n input tokens, the Transformer model iteratively computes at layer t the d-dimensional representations of each input token H t ∈ R n×d , where H 0 represents the initial token embeddings. The core of a Transformer step is Multi-head Self-Attention (MSA), which can be formulated as follows: </p><formula xml:id="formula_0">MSA(H) = [head 1 , · · · , head h ]W O , head i = softmax Q i K T i √ d V i , Q i = HW Q i , K i = HW K i , V i = HW V i ,<label>(1)</label></formula><formula xml:id="formula_1">Z t = norm(H t + MSA(H t )),<label>(2)</label></formula><formula xml:id="formula_2">H t+1 = norm(Z t + FFN(Z t )),<label>(3)</label></formula><p>where norm represents the layer normalization <ref type="bibr">(Ba et al., 2016)</ref> and FFN stands for the Position-wise Feed-Forward Network in <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>. Note that each step t has its own parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Attention</head><p>Some previous work has explored the direction of applying self-attention on hierarchical features: HAN <ref type="bibr" target="#b34">(Yang et al., 2016)</ref> exploits a twolevel attention mechanism that first applies selfattention on word features to get a sentence representation, then uses self-attention on sentence level features to get a document level features. <ref type="bibr" target="#b25">Shen et al. (2018)</ref> proposed a network structured called "bi-directional block self-attention network(Bi-BloSAN)" that divides a sequence into blocks, and sequentially applies intra-block attention and inter-block attention inside a layer. <ref type="bibr" target="#b20">Miculicich et al. (2018)</ref> uses a HAN structure to get sentence-level feature in Transformers for Document-Level Machine Translation. Different from them, our model updates hierarchical features synchronously inside a layer, and update them iteratively by stacking layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lightweight Self-Attention</head><p>Recently there has also been several works focusing on reducing the computational cost of Self-Attention in Transformers: T-DMCA  reduced the memory usage by first dividing the sequence tokens into blocks with similar length and performing attention inside each block independently. Sparse Transformer <ref type="bibr" target="#b6">(Child et al., 2019)</ref> decomposes attention into two categories: for a sequence with length n, we divide it into √ n equal-sized blocks. Each token attends to its previous tokens inside a √ n block it lies in, and to √ n previous blocks. Compared to our model, the Sparse Transformer does not maintain the representations of hierarchical features, and the computational cost of Sparse Transformer is O(n √ n) while ours is O(n log n). Transformer-XL <ref type="bibr" target="#b8">(Dai et al., 2019)</ref> introduces the notion of recurrence into Transformer. It divides the input sequence into multiple segments and recurrently attends to the hidden states of the previous segments. They achieved state-of-the-art on several language modeling benchmarks. Compared to our model, Transformer-XL could only model sequences in one direction, making it hard to deal with tasks where bi-directional information is required. <ref type="bibr" target="#b27">Sukhbaatar et al. (2019)</ref> proposed a adaptive mechanism to learn optimal context length in transformers for each head per layer, thus reducing the total computational and memory cost of transformers. <ref type="bibr" target="#b11">Guo et al. (2019)</ref> suggest that the fullyconnected nature of self-attention in Transformer is not a good inductive bias, they proposed Star-Transformer which links adjacency words coupled with a central relay node to capture both local and global dependencies, with such reduction, Star-Transformer achieved significant improvements against standard Transformer on moderate sized datasets. However, Star-Transformer is not suitable for auto-regressive models in which each word should only be conditioned on its previous words, while the relay node in Star-Transformer summarizes the whole sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>In this paper, we balance the model capability and computation complexity by incorporating the inductive bias. The key insight is that not every token needs to be attended to for context representation. Instead, for an given input token, we can group its context into different-scale nonoverlapping spans, and the scale of a span increases with its relative distance. That is, instead attending to every token, the input token attends to different spans away from it in a fine-to-coarse fashion.</p><p>We now describe our model as graph neural network and detail it in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer as Graph Neural Networks</head><p>A valid perspective is to view information fusing with self-attention in Transformer as message passing on a fully-connected graph, with input tokens as nodes and attentions between nodes as edges <ref type="bibr" target="#b2">(Battaglia et al., 2018)</ref>. In particular, such a process is very similar to Graph Attention Network <ref type="bibr" target="#b30">(Velickovic et al., 2018)</ref>. Thus, different graph structure encodes different inductive bias of attention and results in different time/space complexity.</p><p>To describe Transformer in GNN framework, we first construct a fully-connected graph G, in which each node is a token of the input sequence. All nodes in G are interconnected and each node has a self-loop edge.</p><p>We extend the self-attention mechanism of Transformer to graph, called Graph Self-Attention (GSA). For a given node u, we update its representation according to its neighbour nodes, formulated as h u ← GSA(G, h u ).</p><p>Let A(u) denote the set of the neighbour nodes of u in G, GSA(G, h u ) is detailed as follows:</p><formula xml:id="formula_3">A u = concat({h v | v ∈ A(u)}),<label>(4)</label></formula><formula xml:id="formula_4">Q u i = H k W Q i ,K u i = A u W K i ,V u i = A u W V i ,<label>(5)</label></formula><formula xml:id="formula_5">head u i = softmax Q u i K u i T √ d V u i ,<label>(6)</label></formula><formula xml:id="formula_6">GSA(G, h u ) = [head u 1 , · · · , head u h ]W O ,<label>(7)</label></formula><p>where d is the dimension of h, and</p><formula xml:id="formula_7">W Q i , W K i , W V i</formula><p>are trainable parameters of the i-th attention head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Node Construction</head><p>To achieve the effect of fine-to-coarse attention, we partition a sequence into multi-granular spans via binary partitioning (BP).</p><p>Binary partitioning is a generic process of recursively dividing a sequence into two until the partitioning satisfies one or more requirements. In this paper, we use a simple rule to stop subdividing when a partition just contains a single token. For a sequence with length n, there are 2n − 1 partitions. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the process of binary partitioning over a sequence. Each partition can be regarded as a node in GNN and its representation is computed according to its contained tokens.</p><p>I was busy writing my next paper .</p><p>I was busy writing my next paper .</p><p>I was busy writing my next paper .</p><p>I was busy writing my next paper . The binary partitioning of a sequence constructs a perfect binary tree in which all internal nodes have two children and all leaf nodes have the same depth. Each leaf node corresponds to an input token in the sequence.</p><p>We simply divide the nodes into two types, token and span, both of which are used as nodes in our GNN construction:</p><p>Token nodes the leaf nodes in the binary partition tree. Span nodes the internal node of the tree, each has at least two child nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Edge Construction</head><p>The binary partitioning generating a binary tree. For a sequence with n tokens, we have n token nodes and n − 1 span nodes. Formally, let u l,m denote the m-th node at level l. The level increases in bottom-up fashion. The level of token nodes is set to 0. A span node u l,m represents a partition consisting of token nodes u 0,2 l * m+1 , · · · , u 0,2 l * (m+1) .</p><p>To reduce the distance of information transmission, we do not directly use the tree structure to construct the edges in graph since the path is long for two long-distance tokens in the tree structure.</p><p>We construct two kinds of edges:</p><p>Affiliated Edges Given a span node u l,m , we add a directed edge from each of its contained token nodes u 0,2 l * m+1 , · · · , u 0,2 l * (m+1) . There are 2 l edges u 0,2 l * m+i → u l,m (1 ≤ i ≤ 2 l ).</p><p>The role of affiliated edges is to shorten the path between a span node and its corresponding token nodes. With the affiliated edges, the representation of a span node is computed by directly aggregating the information from its contained token nodes.</p><p>Although we do not adopt the tree structure, the affiliated edges still can incorporate the inductive bias of the hierarchical linguistic structure within the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Edges</head><p>The power of Transformer comes from relating every pair of tokens. To reduce the computation complexity while retaining the ability to capture long-range context, we model the context with a fine-to-coarse strategy.</p><p>For a leaf node, to model its local context, we connect it to neighbor token nodes or lower-level span nodes. Similarly, we connect it to higher-level span nodes for long-range context.</p><p>In detail, for a leaf node u 0,i , we add the incoming edges from the different granularity. For simplicity, we describe the process of constructing edges from its right context of node u 0,i . The edges from the left context is conducted similarly.</p><p>We use a hyper-parameter k to determine the connection density of the graph. We add k edges per level to capture the information from the right context.</p><p>For node u 0,i , its contextual nodes are u 0,p 0 , · · · , u 0,p 0 +k−1 , (8) u 1,p 1 , · · · , u 1,p 1 +k−1 , (9) · · · (10) u l,p l , · · · , u 1,p l +k−1 ,</p><formula xml:id="formula_8">· · · ,<label>(11)</label></formula><p>where p l is the start index at level l and can be computed recursively: p l = parent(p l−1 + k) and p 0 = i + 1.</p><p>For the sake of computation efficiency, when the index p l +k−1 is odd, we also add its next node in the same layer as the contextual nodes. Thus, the start index at next level is p l+1 = parent(p l + k + 1).</p><p>In practice, it is easy to find the contextual nodes in a recursive fashion. Given a leaf node u, the whole procedure is described in Algorithm 1.</p><p>After collect all the contextual nodes, we add a directed edge from each contextual node to node u 0,i .</p><p>Algorithm 1 Finding contextual nodes</p><formula xml:id="formula_10">function NEIGHBORS(u, k) N ← {u}, l ← left(u), r ← right(u) repeat for i ← 1 to k do N ← N ∪ {l, r} l ← left(l) r ← right(r) end for l ← parent(l) r ← parent(r)</formula><p>until l and r reach the boundary return N end function</p><p>Finally, for a sequence with length n, we can construct a directed graph G. The number of nodes is O(2n), the number of edges is O(kn log n/k).</p><p>We can see that the distances between any two token nodes are no greater than 2 in graph G. This property enables our model to learn long-term dependencies easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Update</head><p>After graph G being constructed, we update representations of all nodes via Graph Self-Attention (GSA) described in Section 3.1.</p><p>Since G is a directed graph, for a given node u, its neighbours A(u) is set to all its predecessor nodes in G. If we set A(u) to all the token nodes, we recover the model to the vanilla Transformer.</p><p>Recall that the predecessors of a token node is the multi-scale spans it attending to, while the predecessors of a span node are all its contained token nodes, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. Therefore, BPT connected each two tokens via at most two edges.</p><p>In our experiments, we update all nodes synchronously within the graph layer. The representations of span nodes are initialized with all zeroes, while the representations of token nodes are initialized with the corresponding word embeddings.</p><p>We can stack multiple graph layers as in vanilla Transformer, where each layer gets its own W · · and W O . Algorithm 2 demonstrates the overall update algorithm.</p><p>Depending on the downstream tasks, we either take as output of representation of the root node in the final layer (e.g. in text classification and natural language inference), or the representations of all the token nodes in the final layer (e.g. in language modeling and machine translation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 The update of graph</head><p>Require: G = (V, E) the underlying graph, N the number of layers, H 0 initial hidden states 1: for i := 1 to N do: 2:</p><formula xml:id="formula_11">Z i ← norm H i−1 + GSA (i) G, H i−1 3: H i ← norm Z i + FFN (i) Z i 4: end for 5: return H N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relative Positional Encoding on Tree</head><p>As in <ref type="bibr" target="#b24">(Shaw et al., 2018)</ref>, introducing the relative distances between words in computing the selfattention helps encode the relative order among tokens. Here we draw a similar analogy on the tree. For each node v in A(u), we consider the relative positional difference on the tree between u and v, and assign a latent representation r v,u of such difference:</p><formula xml:id="formula_12">• r v,u = r self if v = u.</formula><p>• r v,u = r left j,i or r right j,i , if v is the i-th left/right node to join the neighborhood set of u at the j-th level in Algorithm 1 of finding top-down context nodes.</p><p>• r v,u = r anc j , if u is the ancestor of v in the tree at level j.</p><p>All the r self , r left j,i , r right j,i and r anc j are trainable parameters.</p><p>Then, we modify Eq. (6) to include positional representations:</p><formula xml:id="formula_13">R u = concat({rv,u | v ∈ A(u)}), head u i = softmax Q u i (K u i + R u ) T √ d V u i .<label>(13)</label></formula><p>Note that the relative positional representations are shared across attention heads, which is the same as in <ref type="bibr" target="#b24">(Shaw et al., 2018)</ref>, and each layer gets its own set of positional representations. When k is set to be larger then the sentence length, our model degenerates to Vanilla Transformer with positional encodings. In the following section we will show that a small k (e.g. 4) is enough for achieving good performance in word level NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We measure the performance of BPT on variety of tasks at both sentence level and document level.</p><p>On document level tasks, we achieved state-ofthe-art performance on language modeling, machine translation and text classification. For sentence level tasks, BPT performs consistently better then vanilla Transformer and Star Transformer, suggesting the inductive bias encoded by BPT is reasonable and effective for natural language. The experimental results show the superior ability of BPT in modeling the long-range context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Classification</head><p>We use SST-5 dataset <ref type="bibr" target="#b26">(Socher et al., 2013) and</ref><ref type="bibr">IMDB dataset (Maas et al., 2011)</ref> to measure the performance of our model on classification for short and long text. The former has fine-grained labels with 215,154 phrases in 11,855 sentences with average length of 19, and the latter has positive/negative labels on 50,000 multi-sentence reviews with average length 294. We use pre-trained GloVe embedding <ref type="bibr" target="#b21">(Pennington et al., 2014)</ref>   <ref type="bibr" target="#b14">(Li et al., 2015)</ref> 49.8 -Tree-LSTM <ref type="bibr" target="#b26">(Socher et al., 2013)</ref> 51.0 -QRNN  -91.4</p><p>BCN+Char+CoVe <ref type="bibr" target="#b19">(McCann et al., 2017)</ref> 53.7 91.8 <ref type="table">Table 1</ref>: Test accuracy on SST-5 and IMDB. In BPT, k = 2 and k = 4 for SST and IMDB respectively. The last model used word embeddings pretrained with translation and additional character-level embeddings.</p><p>We report the average test accuracy of BPT of 10 runs in <ref type="table">Table 1</ref>, the value inside brackets indicates standard derivation. On SST-5, our model outperforms Transformer and LSTM based models. On IMDB, our proposed model outperforms a bidirectional LSTM initialized with pre-trained character embedding and CoVe embedding <ref type="bibr">(Mc-Cann et al., 2017)</ref>.</p><p>On IMDB, our model outperforms Vanilla Transformer and Star Transformer by a large margin: 1.62 and 2.88 respectively. To study the effect of k on final accuracy, we tried different k ∈ {1, 2, 4, 8, 16, 32, 64}. <ref type="figure" target="#fig_4">Figure 4</ref> shows a large k does not bring benefits, though it increases the graph density and time/memory cost of BPT. The best performance was obtained at k = 2 and k = 4 for SST and IMDB respectively, which is a small value.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Sensitivity to Sequence Shift</head><p>Since BPT divide sequence in binary fashion, a concern is whether a shift in sequence affects its performance. To measure if the output of BPT is sensitive to shift, we take the model trained on SST with best validation loss and evaluate it in a setting different from training: we append n placeholder symbols in the front of each sentence, and initialize their embedding with all zeros. We varies n from 0 to 7 and found out the test accuracy changes very little as shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Modeling</head><p>To see how BPT exploits with long-term dependencies, we evaluate our model on Character Level Language Modeling datasets of moderate size: Enwiki8 (LLC., 2009) and Text8 (LLC., 2009). We use bits-per-character(bpc for short, the lower the better) to measure the performance of our model. Character level tasks require more fine-grained interactions between characters, we select a much larger k = 64 for such tasks. The baseline models we select are multi-scale RNN based models <ref type="bibr" target="#b7">(Chung et al., 2017;</ref><ref type="bibr" target="#b35">Zilly et al., 2017;</ref><ref type="bibr" target="#b13">Krause et al., 2016)</ref> and Transformer-based models <ref type="bibr" target="#b0">(Al-Rfou et al., 2018;</ref><ref type="bibr" target="#b8">Dai et al., 2019;</ref><ref type="bibr" target="#b27">Sukhbaatar et al., 2019)</ref>. All Transformers use the same base setting (12 layers, d = 512, d f f = 2048) for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Enwiki8 <ref type="formula">Text8</ref>   In <ref type="table" target="#tab_6">Table 3</ref>, we show that BPT can achieve stateof-the-art performance on both datasets with a small number of parameters.</p><p>To compare different sparse attention patterns, we fix the context length: l = 512 and see how the performance of different models varies as we change the attention degree (the number of incoming edges of each token in the context of viewing Transformer as Graph Neural Networks). For BPT, we select different k ∈ <ref type="bibr">{1, 2, 4, 8, 16, 32, 64, 128}, for Sparse Transformer (Child et al., 2019)</ref>, we use the default setting described in the paper (c = 8, stride = 128); for Restricted Transformer <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> (restrict self-attention to a neighborhood window of size w), we select w ∈ {32, 64, 128, 256, 512}. <ref type="figure">Figure 5</ref> suggests that BPT's fine-to-coarse sparse attention is more effective than Restricted Transformer and Sparse Transformer: with the same attention degree, BPT always gets better performance.</p><p>To see how BPT exploits long-term dependency, we fixed k to 64 and varies context length in {512, 1024, 2048, 4096, 8192}. We do not try context length longer than 8192 because its exceeds the average article length in Enwik8 and Text8. As shown in <ref type="table" target="#tab_8">Table 4</ref>, the performance increases with the context length.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Machine Translation</head><p>BPT can also be applied to Encoder-Decoder frameworks by replacing backbone network in <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref> from Transformer to BPT. In this section we evaluate two settings: Document-Level and Sentence-Level Machine Translation. In Document-Level Machine Translation tasks, the self-attention in both encoder and decoder are applied at document level, while the attention between encoder and decoder are applied between aligned sentences. For a mini-batch of sentence pairs with source sentences of lengths {n i } and target sentences of lengths {m i }, the number of connections are i kn i log( i n i /k) for encoder, i km i log( i m i /k) for decoder, and i n i · m i for attention between encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Document Level Machine Translation</head><p>We conduct experiments with TED Talks Chineseto-English(Zh-En) dataset from IWSLT 2014 and 2015 <ref type="bibr" target="#b4">(Cettolo et al., 2012</ref><ref type="bibr" target="#b5">(Cettolo et al., , 2016</ref>, the average document length is 120 (in sentences). For each sentence, we take its preceding context of fixed length, and their corresponding translations as a single sample.</p><p>The baseline models are <ref type="bibr">HAN-NMT (Miculi-cich et al., 2018)</ref> and Transformer+cache <ref type="bibr" target="#b28">(Tu et al., 2018)</ref>. We follow the setting of <ref type="bibr" target="#b20">Miculicich et al. (2018)</ref> with a vocabulary size of 30k for both Chinese and English, and use dev2010 for development and tst2010-2013 for testing. Unlike previous models, our model is trained from scratch and do not require pre-training on sentence-level translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model BLEU</head><p>Transformer <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> 16.87 Transformer+cache <ref type="bibr" target="#b28">(Tu et al., 2018)</ref> 17.32 HAN-NMT <ref type="bibr" target="#b20">(Miculicich et al., 2018)</ref>   In <ref type="table" target="#tab_10">Table 5</ref> we show that with careful selection of hyper-parameters, Transformer trained at sentence-level could beat reported results of previous Document-Level models. BPT with k = 4 and context length of 32 could further improve the baseline result by 0.93 in terms of BLEU score, which is a significant margin.</p><p>We also examine the effect of context length and k on final BLEU scores, the results are shown in <ref type="table" target="#tab_12">Table 6</ref>. Similar to <ref type="bibr" target="#b28">Tu et al. (2018)</ref> and <ref type="bibr" target="#b20">Miculicich et al. (2018)</ref>, we found a small context length is enough for achieving good performance on IWSLT for Document-Level Translation. However, as we increases context size, the performance of BPT does not get worse as these models and Transformers, suggesting the inductive bias encoded by BPT makes the model less likely to overfit.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Sentence Level Machine Translation</head><p>IWSLT is a relatively small dataset with 0.21M sentence pairs, to see if BPT scales to large dataset, we train a BPT on WMT14 English-to-German dataset with 4.5M sentence pairs. We follow the same setting as <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>, but to replace the Transformer encoder/de-coder with a BPT encoder/decoder. The number of parameters remains the same. The baseline model we select is Transformer(base). We trained the network for 40 epochs and take the average of last 10 checkpoint for decoding, the beam size is set to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model BLEU</head><p>ByteNet <ref type="bibr" target="#b12">(Kalchbrenner et al., 2016)</ref> 23.75 GNMT+RL <ref type="bibr" target="#b33">(Wu et al., 2016)</ref> 24.6 ConvS2S <ref type="bibr" target="#b10">(Gehring et al., 2017)</ref> 25.16 Transformer <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> 27.3</p><p>Transformer <ref type="formula">(</ref>   <ref type="table" target="#tab_14">Table 7</ref> report the de-tokenized SacreBLEU score 2 <ref type="bibr" target="#b22">(Post, 2018)</ref> of BPT and Vanilla Transformer on test set: newstest 2014. In the setting of k = 2 and k = 4, BPT outperforms Vanilla Transformer with the same number of parameters and a sparse attention pattern.</p><p>The best setting of BPT on WMT14 is k = 4, the same as the best setting of BPT on Document-Level Machine Translation(IWSLT) and Text Classification(IMDB), suggesting k = 4 a general setting for word-level NLP tasks, on both small and large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Throughput and GPU Memory Footprint</head><p>BPT improves the time/space complexity of Transformer models from O(d · n 2 ) to O(d · k · n log n/k) in theory, such speedup cannot be achieved by tensor-based attention operators. To address this problem, we designed a set of CUDA kernels for sparse attentions 3 .</p><p>We compare the GPU memory footprint and throughput of BPT and vanilla Transformer during inference under the same setting 4 for language modeling. The k is set to 1, 4, 16, 64 respectively, covering best settings for word-based tasks(k = 4) and character-based tasks(k = 64). We fix the number of tokens to 8192 each batch and varies the sequence length. <ref type="figure">Figure 6</ref> and 7 depicts how the GPU memory and speed varies as we increases sequence length. We show that BPT consistently utilizes less GPU memory compared to Transformer, making it possible to be applied on tasks that require long sequence modeling such as time-series prediction.</p><p>As for speed, BPT increases the number of nodes from n to 2n which brings additional overhead linear to sequence length, rendering BPT not as fast as Transformer when dealing with short text. However, as the sequence length grows, the speed of BPT is steady while Transformer become too slow for practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper introduces a hierarchical fine-to-coarse self-attention based model that is versatile and flexible for a variety of natural language processing tasks. By imposing structural inductive bias this way we are able to strike a balance between the power of the model and training/computational efficiency.</p><p>This work can be extended in a number of interesting ways. The representations have not yet naturally captured syntactic and semantic meanings. Instead of only using the root and the token representations, other intermediate representations can be more directly exposed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Different attention pattern in Transformerlike models. Solid line refers to direct attention, while the dashed line denotes dependency. Unrelated connections and self loops are omitted for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Binary partitioning of a sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>fathers Four score and seven brought forth on this continent a new nation Four score and seven years ago our fathers brought forth on this continent a new nation 4 The figure illustrates how to build the graph: nodes at different levels are colored differently, dashed lines are edges connects token nodes to span nodes; solid lines are edges connect to token nodes. The r * * are relative positions assigned to edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Effects of hyperparameter k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Throughput vs sequence length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>as input features and fixed them during training. The hidden size of all our models are set to 300. For IMDB, we apply the same training/validation set split ratio (0.9) as in<ref type="bibr" target="#b19">McCann et al. (2017)</ref>.</figDesc><table><row><cell>Model</cell><cell>SST-5</cell><cell>IMDB</cell></row><row><cell>BPT</cell><cell cols="2">52.71(0.32) 92.12(0.11)</cell></row><row><cell>Star Transformer</cell><cell>52.9</cell><cell>90.50</cell></row><row><cell>Transformer</cell><cell>50.4</cell><cell>89.24</cell></row><row><cell>Bi-LSTM</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, suggesting</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Accuracy with different sequence shift on</cell></row><row><cell>SST-5.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Test BPC on Enwiki8/Text8. Note that Transformer-XL can be only used for language modeling. l denotes the context length.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Test BPC on Enwiki8/Text8 with different</cell></row><row><cell>context lengths.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>BLEU score on IWSLT 2015 Zh-En</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>BLEU score vs context length on different models</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>BLEU score on newstest 2014</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Setting: BLEU+c.mixed+l.en-de+#.1+s.exp+t .wmt14+tok.intl+v.1.4.1 3 the speed of BPT could be further improved with better optimized kernels 4 N = 6, d = 512, d f f = 2048, h = 8</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Implementation Details</p><p>We use Deep Graph Library <ref type="bibr">(Wang et al., 2019)</ref> for building Binary Partition graphs.</p><p>The following table summarizes the hyperparameters used in BPT.    For full details please refer to the configurations in our source code: https://github.com/ yzh119/BPT/tree/master/configs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04444</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wit3: Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of European Association for Machine Translation</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The iwslt 2016 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niehues</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stüker</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/sparse-transformers" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09113</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Startransformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiplicative lstm for sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07959</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eudard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00185</idno>
		<title level="m">When are tree structures necessary for deep learning of representations? arXiv preprint</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Vancouver</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13164</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llc</forename><surname>Multimedia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6294" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Document-level neural machine translation with hierarchical attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1325</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2947" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunder-standingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bi-directional block selfattention for fast and memory-efficient sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to remember translation history with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="407" to="420" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1909.01315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="4189" to="4198" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
