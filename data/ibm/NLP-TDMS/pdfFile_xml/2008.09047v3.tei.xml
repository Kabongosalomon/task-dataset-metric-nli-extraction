<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the recent deep learning-based 3D human pose and mesh estimation methods regress the pose and shape parameters of human mesh models, such as SMPL and MANO, from an input image. The first weakness of these methods is an appearance domain gap problem, due to different image appearance between train data from controlled environments, such as a laboratory, and test data from in-the-wild environments. The second weakness is that the estimation of the pose parameters is quite challenging owing to the representation issues of 3D rotations. To overcome the above weaknesses, we propose Pose2Mesh, a novel graph convolutional neural network (GraphCNN)-based system that estimates the 3D coordinates of human mesh vertices directly from the 2D human pose. The 2D human pose as input provides essential human body articulation information, while having a relatively homogeneous geometric property between the two domains. Also, the proposed system avoids the representation issues, while fully exploiting the mesh topology using a GraphCNN in a coarse-to-fine manner. We show that our Pose2Mesh outperforms the previous 3D human pose and mesh estimation methods on various benchmark datasets. The codes are publicly available 1 . * equal contribution 1 https://github.com/hongsukchoi/Pose2Mesh_RELEASE</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human pose and mesh estimation aims to recover 3D human joint and mesh vertex locations simultaneously. It is a challenging task due to the depth and scale ambiguity, and the complex human body and hand articulation. There have been diverse approaches to address this problem, and recently, deep learningbased methods have shown noticeable performance improvement.</p><p>Most of the deep learning-based methods rely on human mesh models, such as SMPL <ref type="bibr" target="#b36">[37]</ref> and MANO <ref type="bibr" target="#b53">[54]</ref>. They can be generally categorized into a modelbased approach and a model-free approach. The model-based approach trains a network to predict the model parameters and generates a human mesh by decoding them <ref type="bibr">[5-7, 27, 31, 33, 47, 48, 52]</ref>. On the contrary, the model-free approach regresses the coordinates of a 3D human mesh directly <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>. Both approaches compute the 3D human pose by multiplying the output mesh with a joint regression matrix, which is defined in the human mesh models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Although the recent deep learning-based methods have shown significant improvement, they have two major drawbacks. First, when tested on in-the-wild data, the methods inherently suffer from the appearance domain gap between controlled and in-the-wild environment data. The data captured from the controlled environments <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref> is valuable train data in 3D human pose and estimation, because it has accurate 3D annotations. However, due to the significant difference of image appearance between the two domains, such as backgrounds and clothes, an image-based approach cannot fully benefit from the data. The second drawback is that the pose parameters of the human mesh models might not be an appropriate regression target, as addressed in Kolotouros et al. <ref type="bibr" target="#b31">[32]</ref>. The SMPL pose parameters, for example, represent 3D rotations in an axisangle, which can suffer from the non-unique problem (i.e., periodicity). While many works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref> tried to avoid the periodicity by using a rotation matrix as the prediction target, it still has a non-minimal representation issue.</p><p>To resolve the above issues, we propose Pose2Mesh, a graph convolutional system that recovers 3D human pose and mesh from the 2D human pose, in a model-free fashion. It has two advantages over existing methods. First, the proposed system benefits from a relatively homogeneous geometric property of the input 2D poses from controlled and in-the-wild environments. They not only alleviates the appearance domain gap issue, but also provide essential geometric information on the human articulation. Also, the 2D poses can be estimated accurately from in-the-wild images, since many well-performing methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b65">66]</ref> are trained on large-scale in-the-wild 2D human pose datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>. The second advantage is that Pose2Mesh avoids the representation issues of the pose parameters, while exploiting the human mesh topology (i.e., face and edge information). It directly regresses the 3D coordinates of mesh vertices using a graph convolutional neural network (GraphCNN) with graphs constructed from the mesh topology.</p><p>We designed Pose2Mesh in a cascaded architecture, which consists of PoseNet and MeshNet. PoseNet lifts the 2D human pose to the 3D human pose. MeshNet takes both 2D and 3D human poses to estimate the 3D human mesh in a coarseto-fine manner. During the forward propagation, the mesh features are initially processed in a coarse resolution and gradually upsampled to a fine resolution. <ref type="figure" target="#fig_0">Figure 1</ref> depicts the overall pipeline of the system.</p><p>The experimental results show that the proposed Pose2Mesh outperforms the previous state-of-the-art 3D human pose and mesh estimation methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> on various publicly available 3D human body and hand datasets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b69">70]</ref>. Particularly, our Pose2Mesh provides the state-of-the-art result on in-the-wild dataset <ref type="bibr" target="#b38">[39]</ref>, even when it is trained only on the controlled setting dataset <ref type="bibr" target="#b21">[22]</ref>.</p><p>We summarize our contributions as follows.</p><p>• We propose a novel system, Pose2Mesh, that recovers 3D human pose and mesh from the 2D human pose. The input 2D human pose lets Pose2Mesh robust to the appearance domain gap between controlled and in-the-wild environment data. • Our Pose2Mesh directly regresses 3D coordinates of a human mesh using GraphCNN. It avoids representation issues of the model parameters and leverages the pre-defined mesh topology. • We show that Pose2Mesh outperforms previous 3D human pose and mesh estimation methods on various publicly available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>3D human body pose estimation. Current 3D human body pose estimation methods can be categorized into two approaches according to the input type: an image-based approach and a 2D pose-based approach. The image-based approach takes an RGB image as an input for 3D body pose estimation. Sun et al. <ref type="bibr" target="#b58">[59]</ref> proposed to use compositional loss, which exploits the joint connection structure. Sun et al. <ref type="bibr" target="#b59">[60]</ref> employed soft-argmax operation to regress the 3D coordinates of body joints in a differentiable way. Sharma et al. <ref type="bibr" target="#b55">[56]</ref> incorporated a generative model and depth ordering of joints to predict the most reliable 3D pose that corresponds to the estimated 2D pose. The 2D pose-based approach lifts the 2D human pose to the 3D space. Martinez et al. <ref type="bibr" target="#b39">[40]</ref> introduced a simple network that consists of consecutive fullyconnected layers, which lifts the 2D human pose to the 3D space. Zhao et al. <ref type="bibr" target="#b67">[68]</ref> developed a semantic GraphCNN to use spatial relationships between joint coordinates. Our work follows the 2D pose-based approach, to make the Pose2Mesh more robust to the domain difference between the controlled environment of the training set and in-the-wild environment of the testing set. 3D human body and hand pose and mesh estimation. A model-based approach trains a neural network to estimate the human mesh model parameters <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b53">54]</ref>. It has been widely used for the 3D human mesh estimation, since it does not necessarily require 3D annotation for mesh supervision. Pavlakos et al. <ref type="bibr" target="#b51">[52]</ref> proposed a system that could be only supervised by 2D joint coordinates and silhouette. Omran et al. <ref type="bibr" target="#b46">[47]</ref> trained a network with 2D joint coordinates, which takes human part segmentation as input. Kanazawa et al. <ref type="bibr" target="#b26">[27]</ref> utilized adversarial loss to regress plausible SMPL parameters. Baek et al. <ref type="bibr" target="#b4">[5]</ref> trained a CNN to estimate parameters of the MANO model using neural renderer <ref type="bibr" target="#b28">[29]</ref>. Kolotouros et al. <ref type="bibr" target="#b30">[31]</ref> introduced a self-improving system that consists of SMPL parameter regressor and iterative fitting framework <ref type="bibr" target="#b5">[6]</ref>.</p><p>Recently, the advance of fitting frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b49">50]</ref> has motivated a modelfree approach, which estimates human mesh coordinates directly. It enabled researchers to obtain 3D mesh annotation, which is essential for the model-free methods, from in-the-wild data. Kolotouros et al. <ref type="bibr" target="#b31">[32]</ref> proposed a GraphCNN, which learns the deformation of the template body mesh to the target body mesh. Ge et al. <ref type="bibr" target="#b14">[15]</ref> adopted a GraphCNN to estimate vertices of hand mesh. Moon et al. <ref type="bibr" target="#b44">[45]</ref> proposed a new heatmap representation, called lixel, to recover 3D human meshes.</p><p>Our Pose2Mesh differs from the above methods, which are image-based, in that it uses the 2D human pose as an input. The proposed system can benefit from the data with 3D annotations, which are captured from controlled environments <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>, without the appearance domain gap issue. GraphCNN for mesh processing. Recently, many methods consider a mesh as a graph structure and process it using the GraphCNN, since it can fully exploit mesh topology compared with simple stacked fully-connected layers. Wang et al. <ref type="bibr" target="#b64">[65]</ref> adopted a GraphCNN to learn a deformation from an initial ellipsoid mesh to the target object mesh in a coarse-to-fine manner. Verma et al. <ref type="bibr" target="#b62">[63]</ref> proposed a novel graph convolution operator for the shape correspondence problem. Ranjan et al. <ref type="bibr" target="#b52">[53]</ref> also proposed a GraphCNN-based VAE, which learns a latent space of the human face meshes in a hierarchical manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PoseNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthesizing errors on the input 2D pose</head><p>PoseNet estimates the root joint-relative 3D pose P 3D ∈ R J×3 from the 2D pose, where J denotes the number of human joints. We define the root joint of the human body and hand as pelvis and wrist, respectively. The estimated 2D pose often contains errors <ref type="bibr" target="#b54">[55]</ref>, especially under severe occlusions or challenging poses. To make PoseNet robust to the errors, we synthesize 2D input poses by adding realistic errors on the ground truth 2D pose, following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b43">44]</ref>, during the training stage. We represent the estimated 2D pose or the synthesized 2D pose as P 2D ∈ R J×2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">2D input pose normalization</head><p>We apply standard normalization to P 2D , following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b63">64]</ref>. For this, we subtract the mean from P 2D and divide it by the standard deviation, which becomes P 2D . The mean and the standard deviation of P 2D represent the 2D location and scale of the subject, respectively. This normalization is necessary because P 3D is independent of scale and location of the 2D input pose P 2D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network architecture</head><p>The architecture of the PoseNet is based on that of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref>. The normalized 2D input poseP 2D is converted to a 4096-dimensional feature vector through a fully-connected layer. Then, it is fed to the two residual blocks <ref type="bibr" target="#b20">[21]</ref>. Finally, the output feature vector of the residual blocks is converted to (3J)-dimensional vector, which represents P 3D , by a full-connected layer.  The coarsening initially generates multiple coarse graphs from G M , and adds fake nodes without edges to each graph, following <ref type="bibr" target="#b12">[13]</ref>. The numbers of vertices range from 96 to 12288 for body meshes and from 68 to 1088 for hand meshes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss function</head><p>We train the PoseNet by minimizing L1 distance between the predicted 3D pose P 3D and groundtruth. The loss function L pose is defined as follows:</p><formula xml:id="formula_0">L pose = P 3D − P 3D * 1 ,<label>(1)</label></formula><p>where the asterisk indicates the groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MeshNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph convolution on pose</head><p>MeshNet concatenatesP 2D and P 3D into P ∈ R J×5 . Then, it estimates the root joint-relative 3D mesh M ∈ R V ×3 from P, where V denotes the number of human mesh vertices. To this end, MeshNet uses the spectral graph convolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b56">57]</ref>, which can be defined as the multiplication of a signal x ∈ R N with a filter g θ = diag(θ) in Fourier domain as follows:</p><formula xml:id="formula_1">g θ * x = U g θ U T x,<label>(2)</label></formula><p>where graph Fourier basis U is the matrix of the eigenvectors of the normalized graph Laplacian L <ref type="bibr" target="#b11">[12]</ref>, and U T x denotes the graph Fourier transform of x. Specifically, to reduce the computational complexity, we design MeshNet to be based on Chebysev spectral graph convolution <ref type="bibr" target="#b12">[13]</ref>.</p><p>Graph construction. We construct a graph of P,</p><formula xml:id="formula_2">G P = (V P , A P ), where V P = P = {p i } J i=1</formula><p>is a set of J human joints, and A P ∈ {0, 1} J×J is an adjacency matrix. A P defines the edge connections between the joints based on the human skeleton and symmetrical relationships <ref type="bibr" target="#b8">[9]</ref>, where (A P ) ij = 1 if joints i and j are the same or connected, and (A P ) ij = 0 otherwise. The normalized Laplaican is computed as</p><formula xml:id="formula_3">L P = I J − D −1/2 P A P D −1/2 P ,</formula><p>where I J is the identity matrix, and D P is the diagonal matrix which represents the degree of each joint in V P as</p><formula xml:id="formula_4">(D P ) ij = j (A P ) ij . The scaled Laplacian is computed asL P = 2L P /λ max − I J .</formula><p>Spectral convolution on graph. Then, MeshNet performs the spectral graph convolution on G P , which is defined as follows:</p><formula xml:id="formula_5">F out = K−1 k=0 T k L P F in Θ k ,<label>(3)</label></formula><p>where F in ∈ R J×fin and F out ∈ R J×fout are the input and output feature maps respectively,</p><formula xml:id="formula_6">T k x = 2xT k−1 x − T k−2</formula><p>x is the Chebysev polynomial <ref type="bibr" target="#b17">[18]</ref> of order k, and Θ k ∈ R fin×fout is the kth Chebysev coefficient matrix, whose elements are the trainable parameters of the graph convolutional layer. f in and f out are the input and output feature dimensions respectively. The initial input feature map F in is P in practice, where f in = 5. This graph convolution is K-localized, which means at most K-hop neighbor nodes from each node are affected <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref>, since it is a K-order polynomial in the Laplacian. Our MeshNet sets K = 3 for all graph convolutional layers following <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Coarse-to-fine mesh upsampling</head><p>We gradually upsample G P to the graph of M,</p><formula xml:id="formula_7">G M = (V M , A M ), where V M = M = {m i } V i=1</formula><p>is a set of V human mesh vertices, and A M ∈ {0, 1} V ×V is an adjacency matrix defining edges of the human mesh. To this end, we apply the graph coarsening <ref type="bibr" target="#b13">[14]</ref> technique to G M , which creates various resolutions of graphs,</p><formula xml:id="formula_8">{G c M = (V c M , A c M )} C c=0 ,</formula><p>where C denotes the number of coarsening steps, following Defferrard et al. <ref type="bibr" target="#b12">[13]</ref>. <ref type="figure" target="#fig_2">Figure 2</ref> shows the coarsening process and a balanced binary tree structure of mesh graphs, where the ith vertex in G c+1 M is a parent node of the 2i − 1th and 2ith vertices in G c M , and 2|V c+1</p><formula xml:id="formula_9">M | = |V c M |. i starts from 1. The final output of MeshNet is V M , which is converted from V 0</formula><p>M by a pre-defined indices mapping. During the forward propagation, MeshNet first upsamples the G P to the coarsest mesh graph G C M by reshaping and a fully-connected layer. Then, it performs the spectral graph convolution on each resolution of mesh graphs as follows:</p><formula xml:id="formula_10">F out = K−1 k=0 T k Lc M F in Θ k ,<label>(4)</label></formula><p>whereL c M denotes the scaled Laplacian of G c M , and the other notations are defined in the same manner as Equation 3. Following <ref type="bibr" target="#b14">[15]</ref>, MeshNet performs mesh upsampling by copying features of each parent vertex in G c+1 M to the corresponding children vertices in G c M . The upsampling process is defined as follows:</p><formula xml:id="formula_11">F c = ψ(F T c+1 ) T ,<label>(5)</label></formula><p>where To facilitate the learning process, we additionally incorporate a residual connection between each resolution. <ref type="figure" target="#fig_4">Figure 3</ref> shows the overall architecture of MeshNet.</p><formula xml:id="formula_12">F c ∈ R V c M ×fc is the first feature map of G c M , F c+1 ∈ R V c+1 M ×fc+1 is the last feature map of G c+1 M , ψ : R fc+1×V c+1 M → R fc+1×V c M denotes a nearest-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss functions</head><p>To train our MeshNet, we use four loss functions. Vertex coordinate loss. We minimize L1 distance between the predicted 3D mesh coordinates M and groundtruth, which is defined as follows:</p><formula xml:id="formula_13">L vertex = M − M * 1 ,<label>(6)</label></formula><p>where the asterisk indicates the groundtruth. Joint coordinate loss. We use a L1 loss function between the groundtruth root-relative 3d pose and the 3D pose regressed from M, to train our MeshNet to estimate mesh vertices aligned with joint locations. The 3D pose is calculated as J M, where J ∈ R J×V is a joint regression matrix defined in SMPL or MANO model. The loss function is defined as follows:</p><formula xml:id="formula_14">L joint = J M − P 3D * 1 ,<label>(7)</label></formula><p>where the asterisk indicates the groundtruth. Surface normal loss. We supervise normal vectors of an output mesh surface to be consistent with groundtruth. This consistency loss improves surface smoothness and local details <ref type="bibr" target="#b64">[65]</ref>. The loss function L normal is defined as follows:</p><formula xml:id="formula_15">L normal = f {i,j}⊂f m i − m j m i − m j 2 , n * f ,<label>(8)</label></formula><p>where f and n * f denote a triangle face in the human mesh and a groundtruth unit normal vector of f , respectively. m i and m j denote the ith and jth vertices in f . Surface edge loss. We define edge length consistency loss between predicted and groundtruth edges, following <ref type="bibr" target="#b64">[65]</ref>. The edge loss is effective in recovering smoothness of hands, feet, and a mouth, which have dense vertices. The loss function L edge is defined as follows:</p><formula xml:id="formula_16">L edge = f {i,j}⊂f | m i − m j 2 − m * i − m * j 2 |,<label>(9)</label></formula><p>where f and the asterisk denote a triangle face in the human mesh and the groundtruth, respectively. m i and m j denote ith and jth vertex in f . We define the total loss of our MeshNet, L mesh , as a weighted sum of all four loss functions:</p><formula xml:id="formula_17">L mesh = λ v L vertex + λ j L joint + λ n L normal + λ e L edge ,<label>(10)</label></formula><p>where λ v = 1, λ j = 1, λ n = 0.1, and λ e = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation Details</head><p>PyTorch <ref type="bibr" target="#b48">[49]</ref> is used for implementation. We first pre-train our PoseNet, and then train the whole network, Pose2Mesh, in an end-to-end manner. Empirically, our two-step training strategy gives better performance than the one-step training. The weights are updated by the Rmsprop optimization <ref type="bibr" target="#b60">[61]</ref> with a mini-batch size of 64. We pre-train PoseNet 60 epochs with a learning rate 10 −3 . The learning rate is reduced by a factor of 10 after the 30th epoch. After integrating the pretrained PoseNet to Pose2Mesh, we train the whole network 15 epochs with a learning rate 10 −3 . The learning rate is reduced by a factor of 10 after the 12th epoch. In addition, we set λ e to 0 until 7 epoch on the second training stage, since it tends to cause local optima at the early training phase. We used four NVIDIA RTX 2080 Ti GPUs for Pose2Mesh training, which took at least a half day and at most two and a half days, depending on the training datasets. In inference time, we use 2D pose outputs from Sun et al. <ref type="bibr" target="#b57">[58]</ref> and Xiao et al. <ref type="bibr" target="#b65">[66]</ref>. They run at 5 fps and 67 fps respectively, and our Pose2Mesh runs at 37 fps. Thus, the proposed system can process from 4 fps to 22 fps in practice, which shows the applicability to real-time applications.</p><p>6 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and evaluation metrics</head><p>Human3.6M. Human3.6M <ref type="bibr" target="#b21">[22]</ref> is a large-scale indoor 3D body pose benchmark, which consists of 3.6M video frames. The groundtruth 3D poses are obtained using a motion capture system, but there are no groundtruth 3D meshes. As a result, for 3D mesh supervision, most of the previous 3D pose and mesh estimation works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> used pseudo-groundtruth obtained from Mosh <ref type="bibr" target="#b35">[36]</ref>. However, due to the license issue, the pseudo-groundtruth from Mosh is not currently publicly accessible. Thus, we generate new pseudo-groundtruth 3D meshes by fitting SMPL parameters to the 3D groundtruth poses using SMPLify-X <ref type="bibr" target="#b49">[50]</ref>. For the fair comparison, we trained and tested previous state-of-the-art methods on the obtained groundtruth using their officially released code. Following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b50">51]</ref>, all methods are trained on 5 subjects (S1, S5, S6, S7, S8) and tested on 2 subjects (S9, S11). We report our performance for the 3D pose using two evaluation metrics. One is mean per joint position error (MPJPE) <ref type="bibr" target="#b21">[22]</ref>, which measures the Euclidean distance in millimeters between the estimated and groundtruth joint coordinates,   after aligning the root joint. The other one is PA-MPJPE, which calculates MPJPE after further alignment (i.e., Procrustes analysis (PA) <ref type="bibr" target="#b16">[17]</ref>). J M is used for the estimated joint coordinates. We only evaluate 14 joints out of 17 estimated joints following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref>. 3DPW. 3DPW <ref type="bibr" target="#b38">[39]</ref> is captured from in-the-wild and contains 3D body pose and mesh annotations. It consists of 51K video frames, and IMU sensors are leveraged to acquire the groundtruth 3D pose and mesh. We only use the test set of 3DPW for evaluation following <ref type="bibr" target="#b30">[31]</ref>. MPJPE and mean per vertex position error (MPVPE) are used for evaluation. 14 joints from J M, whose joint set follows that of Human3.6M, are evaluated for MPJPE as above. MPVPE measures the Euclidean distance in millimeters between the estimated and groundtruth vertex coordinates, after aligning the root joint. COCO. COCO <ref type="bibr" target="#b34">[35]</ref> is an in-the-wild dataset with various 2D annotations such as detection and human joints. To exploit this dataset on 3D mesh learning, Kolotouros et al. <ref type="bibr" target="#b30">[31]</ref> fitted SMPL parameters to 2D joints using SMPLify <ref type="bibr" target="#b5">[6]</ref>. Following them, we use the processed data for training. MuCo-3DHP. MuCo-3DHP <ref type="bibr" target="#b41">[42]</ref> is synthesized from the existing MPI-INF-3DHP 3D single-person pose estimation dataset <ref type="bibr" target="#b40">[41]</ref>. It consists of 200K frames, and half of them have augmented backgrounds. For the background augmentation, we use images of COCO that do not include humans to follow Moon et al. <ref type="bibr" target="#b42">[43]</ref>. Following them, we use this dataset only for the training. FreiHAND. FreiHAND <ref type="bibr" target="#b69">[70]</ref> is a large-scale 3D hand pose and mesh dataset. It consists of a total of 134K frames for training and testing. Following Zimmermann et al. <ref type="bibr" target="#b69">[70]</ref>, we report PA-MPVPE, F-scores, and additionally PA-MPJPE of Pose2Mesh. J M is evaluated for the joint errors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation study</head><p>To analyze each component of the proposed system, we trained different networks on Human3.6M, and evaluated on Human3.6M and 3DPW. The test 2D input poses used in Human3.6M and 3DPW evaluation are outputs from Integral Regression <ref type="bibr" target="#b59">[60]</ref> and HRNet <ref type="bibr" target="#b57">[58]</ref> respectively, which are obtained using groundtruth bounding boxes.</p><p>Regression target and network design. To demonstrate the effectiveness of regressing the 3D mesh vertex coordinates using GraphCNN, we compare MPJPE and PA-MPJPE of four different combinations of the regression target and the network design in <ref type="table" target="#tab_0">Table 1</ref>. First, vertex-GraphCNN, our Pose2Mesh, substantially improves the joint errors compared to vertex-FC, which regresses vertex coordinates with a network of fully-connected layers. This proves the importance of exploiting the human mesh topology with GraphCNN, when estimating the 3D vertex coordinates. Second, vertex-GraphCNN provides better performance than both networks estimating SMPL parameters, while maintaining the considerably smaller number of network parameters. Taken together, the effectiveness of our mesh coordinate regression scheme using GraphCNN is clearly justified. In this comparison, the same PoseNet and cascaded architecture are employed for all networks. On top of the PoseNet, vertex-FC and param-FC used a series of fully-connected layers, whereas param-GraphCNN added fully-connected layers on top of Pose2Mesh. For the fair comparison, when training param-FC and param-GraphCNN, we also supervised the reconstructed mesh from the predicted SMPL parameters with L vertex and L joint . The networks estimating SMPL parameters incorporated Zhou et al.'s method <ref type="bibr" target="#b68">[69]</ref> for continuous rotations. Coarse-to-fine mesh upsampling. We compare a coarse-to-fine mesh upsampling scheme and a direct mesh upsampling scheme. The direct upsampling method performs graph convolution on the lowest resolution mesh until the middle layer of MeshNet, and then directly upsamples it to the highest one (e.g., 96 to 12288 for the human body mesh). While it has the same number of graph convolution layers and almost the same number of parameters, our coarse-tofine model consumes half as much GPU memory and runs 1.5 times faster than the direct upsampling method. It is because graph convolution on the highest resolution takes much more time and memory than graph convolution on lower resolutions. In addition, the coarse-to-fine upsampling method provides a slightly  lower joint error, as shown in <ref type="table" target="#tab_1">Table 2</ref>. These results confirm the effectiveness of our coarse-to-fine upsampling strategy.</p><p>Cascaded architecture analysis. We analyze the cascaded architecture of Pose2Mesh to demonstrate its validity in <ref type="table" target="#tab_2">Table 3</ref>. To be specific, we construct (a) a GraphCNN that directly takes a 2D pose, (b) a cascaded network that predicts mesh coordinates from a 3D pose from pretrained PoseNet, and (c) our Pose2Mesh. All methods are both trained by synthesized 2D poses. First, (a) outperforms (b), which implies a 3D pose output from PoseNet may lack geometry information in the 2D input pose. If we concatenate the 3D pose output with the 2D input pose as (c), it provides the lowest errors. This explains that depth information in 3D poses could positively affect 3D mesh estimation.</p><p>To further verify the superiority of the cascaded architecture, we explore the upper bounds of (a) and (d) a GraphCNN that takes a 3D pose in <ref type="table" target="#tab_0">Table 13</ref>. To this end, we fed the groundtruth 2D pose and 3D pose to (a) and (d) as test inputs, respectively. Apparently, since the input 3D pose contains additional depth information, the upper bound of (d) is considerably higher than that of (a). We also fed state-of-the-art 3D pose outputs from <ref type="bibr" target="#b42">[43]</ref> to (d), to validate the practical potential for performance improvement. Surprisingly, the performance is comparable to the upper bound of (a). Thus, our Pose2Mesh will substantially outperform (a) a graph convolution network that directly takes a 2D pose, if we can improve the performance of PoseNet. In summary, the above results prove the validity of our cascaded architecture of Pose2Mesh. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with state-of-the-art methods</head><p>Human3.6M. We compare our Pose2Mesh with the previous state-of-the-art 3D body pose and mesh estimation methods on Human3.6M in <ref type="table" target="#tab_4">Table 5</ref>. First, when we train all methods only on Human3.6M, our Pose2Mesh significantly outperforms other methods. However, when we train the methods additionally on COCO, the performance of the previous baselines increases, but that of Pose2Mesh slightly decreases. The performance gain of other methods is a well-known phenomenon <ref type="bibr" target="#b59">[60]</ref> among image-based methods, which tend to generalize better when trained with diverse images from in-the-wild. Whereas, our Pose2Mesh does not benefit from more images in the same manner, since it only takes the 2D pose. We analyze the reason for the performance drop is that the test set and train set of Human3.6M have similar poses, which are from the same action categories. Thus, overfitting the network to the poses of Human3.6M can lead to better accuracy. Nevertheless, in both cases, our Pose2Mesh outperforms the previous methods in both MPJPE and PA-MPJPE. The test 2D input poses for Pose2Mesh are estimated by the method of Sun et al. <ref type="bibr" target="#b59">[60]</ref> trained on MPII <ref type="bibr" target="#b2">[3]</ref>, using groundtruth bounding boxes. 3DPW. We compare MPJPE, PA-MPJPE, and MPVPE of our Pose2Mesh with the previous state-of-the-art 3D body pose and mesh estimation works on 3DPW, which is an in-the-wild dataset, in <ref type="table" target="#tab_5">Table 6</ref>. First, when the image-based methods are trained only on Human3.6M, they give extremely high errors. This verifies that the image-based methods suffer from the appearance domain gap between train and test data from controlled and in-the-wild environments respectively. In fact, since Human3.6M is an indoor dataset from the controlled environment, the image appearance from it are very different from in-the-wild image appearance. On the contrary, the 2D pose-based approach of Pose2Mesh can benefit from accurate 3D annotations of the lab-recorded 3D datasets <ref type="bibr" target="#b21">[22]</ref> without the appearance domain gap issue, utilizing the homogeneous geometric property of 2D poses from different domains. Indeed, Pose2Mesh gives far lower errors on in-the-wild images from 3DPW, even when it is only trained on Human3.6M while other methods are additionally trained on COCO. The experimental results suggest that a 3D pose and mesh estimation approach may not necessarily require 3D data captured from in-the-wild environments, which is extremely challenging to acquire, to give accurate predictions. The test 2D input poses for Pose2Mesh are estimated by HRNet <ref type="bibr" target="#b57">[58]</ref> and Simple <ref type="bibr" target="#b65">[66]</ref> trained on FreiHAND. We present the comparison between our Pose2Mesh and other state-of-the-art 3D hand pose and mesh estimation works in <ref type="table" target="#tab_6">Table 7</ref>. The proposed system outperforms other methods in various metrics, including PA-MPVPE and F-scores. The test 2D input poses for Pose2Mesh are estimated by HR-Net <ref type="bibr" target="#b57">[58]</ref> trained on FreiHAND <ref type="bibr" target="#b69">[70]</ref>, using bounding boxes from Mask R-CNN <ref type="bibr" target="#b19">[20]</ref> with ResNet-50 backbone <ref type="bibr" target="#b20">[21]</ref>.</p><p>Comparison with different train sets. We report MPJPE and PA-MPJPE of Pose2Mesh trained on Human3.6M, COCO, and MuCo-3DHP, and other methods trained on different train sets in <ref type="table" target="#tab_7">Table 8</ref>. The train sets include Hu-man3.6M, COCO, MPII <ref type="bibr" target="#b2">[3]</ref> , LSP <ref type="bibr" target="#b23">[24]</ref>, LSP-Extended <ref type="bibr" target="#b24">[25]</ref>, UP <ref type="bibr" target="#b33">[34]</ref>, and MPI-INF-3DHP <ref type="bibr" target="#b40">[41]</ref>. Each method is trained on a different subset of them. In the table, the errors of <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> decrease by a large margin compared to the errors in <ref type="table" target="#tab_4">Table 5</ref> and 6. Although it shows that the image-based methods can improve the generalizability with weak-supervision on in-the-wild 2D pose datasets, Pose2Mesh still provides the lowest errors in 3DPW, which is the in-the-wild benchmark. This suggests that tackling the domain gap issue to fully benefit from the 3D data of controlled environments is an important task to recover accurate 3D pose and mesh from in-the-wild images. We measured the PA-MPJPE of Pose2Mesh on Human3.6M by testing only on the frontal camera set, following the previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. In addition, we used 2D human poses estimated from DarkPose <ref type="bibr" target="#b66">[67]</ref> as input for 3DPW evaluation, which improved HRNet <ref type="bibr" target="#b57">[58]</ref>. processing, such as model fitting <ref type="bibr" target="#b31">[32]</ref>. More qualitative results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Although the proposed system benefits from the homogeneous geometric property of input 2D poses from different domains, it could be challenging to recover various 3D shapes solely from the pose. While it may be true, we found that the 2D pose still carries necessary information to reason the corresponding 3D shape. In the literature, SMPLify <ref type="bibr" target="#b5">[6]</ref> has experimentally verified that under the canonical body pose, utilizing 2D pose significantly drops the body shape fitting error compared to using the mean body shape. We show that Pose2Mesh can recover various body shapes from the 2D pose in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We propose a novel and general system, Pose2Mesh, for 3D human mesh and pose estimation from a 2D human pose. The input 2D pose enables the system to benefit from the 3D data captured from the controlled settings without the appearance domain gap issue. The model-free approach using GraphCNN allows it to fully exploit mesh topology, while avoiding the representation issues of the 3D rotation parameters. We plan to enhance the shape recover capability of Pose2Mesh using denser keypoints or part segmentation, while maintaining the above advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of "Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose"</head><p>In this supplementary material, we present more experimental results that could not be included in the main manuscript due to the lack of space. 9 Qualitative results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Shape recovery</head><p>We trained and tested Pose2Mesh on SURREAL <ref type="bibr" target="#b61">[62]</ref>, which have various samples in terms of the body shape, to verify the capability of shape recovery. As shown in <ref type="figure">Figure 5</ref>, Pose2Mesh can recover a 3D body shape corresponding to an input image, though not perfectly. The shape features of individuals, such as the bone length ratio and fatness, are expressed in the outputs of Pose2Mesh. This implies that the information embedded in joint locations (e.g. the distance between hip joints) carries a certain amount of shape cue.  <ref type="figure">Fig. 5</ref>: The Pose2Mesh predictions compared with the groundtruth mesh, and the mesh decoded from groundtruth pose parameters and the mean shape parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Additional results</head><p>Here, we present more qualitative results on COCO <ref type="bibr" target="#b34">[35]</ref> validation set and Frei-HAND <ref type="bibr" target="#b69">[70]</ref> test set in <ref type="figure">Figure 6</ref>. The images at the fourth row show some of the failure cases. Although the people on the first and second images appear to be overweight, the predicted meshes seem to be closer to the average shape. The right arm pose of the mesh in the third column is bent, though it appears straight. <ref type="figure">Fig. 6</ref>: Additional qualitative results on COCO and FreiHAND.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Comparison with the state-of-the-art</head><p>We present the qualitative comparison between our Pose2Mesh and GraphCMR <ref type="bibr" target="#b31">[32]</ref> in <ref type="figure" target="#fig_7">Figure 7</ref>. We regard GraphCMR as a suitable comparison target, since it is also the model-free method and regresses coordinates of human mesh defined by SMPL <ref type="bibr" target="#b36">[37]</ref> using GraphCNN like ours. As the figure shows, our Pose2Mesh provides much more visually pleasant mesh results than GraphCMR. Based on the loss function analysis in Section 7 and the visual results of GraphCMR, we conjecture that the surface losses such as the normal loss and the edge loss are the reason for the difference.  <ref type="figure" target="#fig_8">Figure 8</ref> shows the detailed network architecture of PoseNet. First, the normalized input 2D pose vector is converted to a 4096-dimensional feature vector by a fully-connected layer. Then, it is fed to the two residual blocks, where each block consists of a fully connected layer, 1D batch normalization, ReLU activation, and the dropout. The dimension of the feature map in the residual block is 4096, and the dropout probability is set to 0.5. Finally, the output from the residual block is converted to (3J)-dimensional vector, the 3D pose vector, by a fully-connected layer. The 3D pose vector represents the root-relative 3D pose coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose2Mesh (Ours) GraphCMR input image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Accuracy of PoseNet</head><p>We present MPJPE and PA-MPJPE of PoseNet on the benchmarks in <ref type="table" target="#tab_9">Table 9</ref>. For the Human3.6M benchmark <ref type="bibr" target="#b21">[22]</ref>, 14 common joints out of 17 Human3.6M defined joints are evaluated following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref>. For the 3DPW benchmark <ref type="bibr" target="#b38">[39]</ref>, COCO defined 17 joints are evaluated and J M from the groundtruth SMPL meshes are used as groundtruth. The 2D pose outputs from <ref type="bibr" target="#b59">[60]</ref> and <ref type="bibr" target="#b57">[58]</ref> are taken as test inputs on Human3.6M and 3DPW respectively. For the FreiHAND benchmark, only FreiHAND train set is used during training, and 21 MANO <ref type="bibr" target="#b53">[54]</ref> hand joints are evaluated by the official evaluation website. The 2D pose outputs from <ref type="bibr" target="#b57">[58]</ref> are taken as test inputs.  11 Pre-defined joint sets and graph structures</p><p>We use different pre-defined joint sets and graph structures for Human3.6M, 3DPW, SURREAL, and FreiHAND benchmarks, as shown in <ref type="figure" target="#fig_9">Figure 9</ref>.  12 Pseudo-groundtruth SMPL parameters of Human3.6M</p><p>Mosh <ref type="bibr" target="#b35">[36]</ref> method can compute SMPL parameters from the marker data in Hu-man3.6M. Since Human3.6M does not provide 3D mesh annotations, most of the previous 3D pose and mesh estimation papers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref> used the SMPL parameters obtained by Mosh method as the groundtruth for the supervision. However, due to the license issue, the SMPL parameters are not currently available. Furthermore, the source code of Mosh is not publicly released.</p><p>For the 3D mesh supervision, we alternatively obtain groundtruth SMPL parameters by applying SMPLify-X [50] on the groundtruth 3D joint coordinates of Human3.6M. Although the obtained SMPL parameters are not perfectly aligned to the groundtruth 3D joint coordinates, we confirmed that the error of the SMPLify-X is much less than those of current state-of-the-art 3D human pose estimation methods, as shown in <ref type="table" target="#tab_0">Table 10</ref>. Thus, we believe using SMPL parameters obtained by SMPLify-X as groundtruth is reasonable. For the fair comparison, all the previous works and our system are trained on our SMPL parameters from SMPLify-X.</p><p>During the fitting process of SMPLify-X, we adopted a neutral gender SMPL body model. However, we empirically found that the fitting process produces gender-specific body shapes, which correspond to each subject. As a result, since most of the subjects in the training set of Human3.6M are female, our Pose2Mesh trained on Human3.6M tends to produce female body shape meshes. We tried to fix the identity code of the SMPL body model obtained from the T-pose; however, it produces higher errors. Thus, we did not fix the identity code for each subject. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Synthetic data from AMASS</head><p>We leverage additional synthetic data from AMASS <ref type="bibr" target="#b37">[38]</ref> to boost the performance of Pose2Mesh. AMASS is a new database that unifies 15 different optical marker-based mocap datasets within a common framework. It created SMPL parameters from mocap data by a method named Mosh++. We used CMU <ref type="bibr" target="#b0">[1]</ref> and BML-Movi <ref type="bibr" target="#b15">[16]</ref> from the database in training PoseNet and only CMU in training Pose2Mesh.</p><p>To be specific, we generated paired 2D pose-3D mesh data by projecting a 3D pose obtained from a mesh to the image plane, using camera parameters from Human3.6M. As shown in <ref type="table" target="#tab_0">Table 11</ref>, when AMASS is added, both the joint error and surface error decrease. Exploiting AMASS data in this fashion is not possible for <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b31">[32]</ref>, and <ref type="bibr" target="#b30">[31]</ref>, since they need pairs of image and 2D/3D annotations.  <ref type="bibr" target="#b57">[58]</ref> on the training set as the input poses in the training stage, since there are no verified synthetic errors for the hand joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.2">Effect of synthesizing the input 2D poses</head><p>To demonstrate the validity of the synthesizing process, we compare MPJPE and PA-MPJPE of Pose2Mesh trained with the groundtruth 2D poses, and the synthesized input 2D poses in <ref type="table" target="#tab_0">Table 12</ref>. For Human3.6M, only Human3.6M train set is used for the training, and for 3DPW benchmark, Human3.6M and COCO are used for the training. The test 2D input poses used in Human3.6M and 3DPW evaluation are outputs from Integral Regression <ref type="bibr" target="#b59">[60]</ref> and HRNet <ref type="bibr" target="#b57">[58]</ref> respectively, using groundtruth bounding boxes. Apparently, when our Pose2Mesh is trained with the synthesized input 2D poses, Pose2Mesh performs far better on both benchmarks. This proves that the synthesizing process makes Pose2Mesh more robust to the errors in the input 2D poses and increases the estimation accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15">Train/test with groundtruth input poses</head><p>We present the upper bounds of Pose2Mesh, PoseNet, and MeshNet on Hu-man3.6M and 3DPW benchmarks by training and testing with groundtruth input poses in <ref type="table" target="#tab_0">Table 13</ref>. Pose2Mesh and PoseNet take the groundtruth 2D pose as an input, while MeshNet takes the groundtruth 3D pose as an input. As the table shows, the upper bound of Pose2Mesh is similar to that of PoseNet, which implies that the 3D pose errors of Pose2Mesh follow those of PoseNet as analyzed in Section 7.2 of the main manuscript. In addition, the upper bound of MeshNet indicates that we can recover highly accurate 3D human meshes if we can estimate nearly perfect 3D poses. The MPJPE and PA-MPJPE of Pose2Mesh and MeshNet are measured on the 3D pose regressed from the mesh output, while the accuracy of PoseNet is measured on the lifted 3D pose. For the Human3.6M benchmark, only Hu-man3.6M train set is used to train the network. For the 3DPW benchmark, Human3.6M, COCO, AMASS train sets are used to train the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16">Effect of each loss function</head><p>We analyze the effect of joint coordinate loss L joint , surface normal loss L normal , and surface edge loss L edge on reconstructing a 3D human mesh in <ref type="table" target="#tab_0">Table 14</ref> and <ref type="figure" target="#fig_0">Figure 10</ref>. Human3.6M is used for the training and testing. As the table shows, training without L joint has a relatively distinctive effect on MPJPE and PA-MPJPE, while other settings show numerically negligible differences. On the other hand, as the figure shows, training without L normal or L edge clearly decreases the visual quality of the mesh output, while training without L joint has nearly no effect on the visual quality of the meshes. To be specific, training without L normal impairs the overall smoothness of the mesh and local details of mouth, hands, and feet. Similarly, training without L edge ruins the details of body parts that have dense vertices, especially mouth, hands, and feet, by making serious artifacts caused by flying vertices.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The overall pipeline of Pose2Mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The coarsening initially generates multiple coarse graphs from G M , and adds fake nodes without edges to each graph, following [13]. The numbers of vertices range from 96 to 12288 for body meshes and from 68 to 1088 for hand meshes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>The network architecture of MeshNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>neighbor upsampling function, and f c and f c+1 are the feature dimensions of vertices in F c and F c+1 respectively. The nearest upsampling function copies the feature of the ith vertex in G c+1 M to the 2i − 1th and 2ith vertices in G c M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 Fig. 4 :</head><label>44</label><figDesc>shows the qualitative results on COCO validation set and FreiHAND test set. Our Pose2Mesh outputs visually decent human meshes without post-Qualitative results of the proposed Pose2Mesh. First to third rows: COCO, fourth row: FreiHAND.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>The mesh quality comparison between our Pose2Mesh and GraphCMR [32]. 10 Details of PoseNet 10.1 Network architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>The detailed network architecture of PoseNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>The joint sets and graph structures of each dataset that are used in Pose2Mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Qualitative results for the ablation study on the effectiveness of each loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The performance comparison between four combinations of regression target and network design tested on Human3.6M. 'no. param.' denotes the number of parameters of a network, which estimates SMPL parameters or vertex coordinates from the output of PoseNet. MPJPE no. param. MPJPE PA-MPJPE no. param.</figDesc><table><row><cell cols="3">FC 55.5 MPJPE PA-SMPL param. target\network 72.8</cell><cell>17.3M</cell><cell>79.1</cell><cell>GraphCNN 59.1</cell><cell>13.5M</cell></row><row><cell>vertex coord.</cell><cell>119.6</cell><cell>95.1</cell><cell>37.5M</cell><cell>64.9</cell><cell>48.0</cell><cell>8.8M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The performance comparison on Human3.6M between two upsampling schems. GPU mem. and fps denote the required memory during training and fps in inference time respectively.</figDesc><table><row><cell>method</cell><cell cols="2">GPU mem. fps MPJPE</cell></row><row><cell>direct</cell><cell>10G</cell><cell>24 65.3</cell></row><row><cell>coarse-to-fine</cell><cell>6G</cell><cell>37 64.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The MPJPE comparison between four architectures tested on 3DPW.</figDesc><table><row><cell>architecture</cell><cell>MPJPE</cell></row><row><cell>2D→mesh</cell><cell>101.1</cell></row><row><cell>2D→3D→mesh</cell><cell>103.2</cell></row><row><cell>2D→3D+2D→mesh</cell><cell>100.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The upper bounds of the two different graph convolutional networks that take a 2D pose and a 3D pose. Tested on Human3.6M.</figDesc><table><row><cell>test input</cell><cell>architecture</cell><cell>MPJPE</cell><cell>PA-MPJPE</cell></row><row><cell>2D pose GT</cell><cell>2D→mesh</cell><cell>55.5</cell><cell>38.4</cell></row><row><cell>3D pose from [43]</cell><cell>3D→mesh</cell><cell>56.3</cell><cell>43.2</cell></row><row><cell>3D pose GT</cell><cell>3D→mesh</cell><cell>29.0</cell><cell>23.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The accuracy comparison between state-of-the-art methods and Pose2Mesh on Human3.6M. The dataset names on top are training sets.</figDesc><table><row><cell>method</cell><cell cols="4">Human3.6M MPJPE PA-MPJPE MPJPE PA-MPJPE Human3.6M + COCO</cell></row><row><cell>HMR [27]</cell><cell>184.7</cell><cell>88.4</cell><cell>153.2</cell><cell>85.5</cell></row><row><cell>GraphCMR [32]</cell><cell>148.0</cell><cell>104.6</cell><cell>78.3</cell><cell>59.5</cell></row><row><cell>SPIN [31]</cell><cell>85.6</cell><cell>55.6</cell><cell>72.9</cell><cell>51.9</cell></row><row><cell>Pose2Mesh (Ours)</cell><cell>64.9</cell><cell>48.0</cell><cell>67.9</cell><cell>49.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The accuracy comparison between state-of-the-art methods and Pose2Mesh on 3DPW. The dataset names on top are training sets.</figDesc><table><row><cell>method</cell><cell cols="6">Human3.6M MPJPE PA-MPJPE MPVPE MPJPE PA-MPJPE MPVPE Human3.6M + COCO</cell></row><row><cell>HMR [27]</cell><cell>377.3</cell><cell>165.7</cell><cell>481.0</cell><cell>300.4</cell><cell>137.2</cell><cell>406.8</cell></row><row><cell>GraphCMR [32]</cell><cell>332.5</cell><cell>177.4</cell><cell>380.8</cell><cell>126.5</cell><cell>80.1</cell><cell>144.8</cell></row><row><cell>SPIN [31]</cell><cell>313.8</cell><cell>156.0</cell><cell>344.3</cell><cell>113.1</cell><cell>71.7</cell><cell>122.8</cell></row><row><cell cols="2">Pose2Mesh (Simple [66]) 101.8</cell><cell>64.2</cell><cell>119.1</cell><cell>92.3</cell><cell>61.0</cell><cell>110.5</cell></row><row><cell cols="2">Pose2Mesh (HR [58]) 100.5</cell><cell>63.0</cell><cell>117.5</cell><cell>91.4</cell><cell>60.1</cell><cell>109.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>The accuracy comparison between state-of-the-art methods and Pose2Mesh on FreiHAND.</figDesc><table><row><cell>method</cell><cell cols="3">PA-MPVPE PA-MPJPE F@5 mm</cell><cell>F@15 mm</cell></row><row><cell>Hasson et al. [19]</cell><cell>13.2</cell><cell>-</cell><cell>0.436</cell><cell>0.908</cell></row><row><cell>Boukhayma et al. [7]</cell><cell>13.0</cell><cell>-</cell><cell>0.435</cell><cell>0.898</cell></row><row><cell>FreiHAND [70]</cell><cell>10.7</cell><cell>-</cell><cell>0.529</cell><cell>0.935</cell></row><row><cell>Pose2Mesh (Ours)</cell><cell>7.6</cell><cell>7.4</cell><cell>0.683</cell><cell>0.973</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>The accuracy comparison between state-of-the-art methods and Pose2Mesh on Human3.6M and 3DPW. Different train sets are used. * means synthetic data from AMASS<ref type="bibr" target="#b37">[38]</ref> is used. Refer to Section 13 for more details.</figDesc><table><row><cell>method</cell><cell cols="5">Human3.6M MPJPE PA-MPJPE MPJPE PA-MPJPE MPVPE 3DPW</cell></row><row><cell>SMPLify [6]</cell><cell>-</cell><cell>82.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Lassner et al. [34]</cell><cell>-</cell><cell>93.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HMR [27]</cell><cell>88.0</cell><cell>56.8</cell><cell>-</cell><cell>81.3</cell><cell>-</cell></row><row><cell>NBF [47]</cell><cell>-</cell><cell>59.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pavlakos et al. [52]</cell><cell>-</cell><cell>75.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Kanazawa et al. [28]</cell><cell>-</cell><cell>56.9</cell><cell>-</cell><cell>72.6</cell><cell>-</cell></row><row><cell>GraphCMR [32]</cell><cell>-</cell><cell>50.1</cell><cell>-</cell><cell>70.2</cell><cell>-</cell></row><row><cell>Arnab et al. [4]</cell><cell>77.8</cell><cell>54.3</cell><cell>-</cell><cell>72.2</cell><cell>-</cell></row><row><cell>SPIN [31]</cell><cell>-</cell><cell>41.1</cell><cell>-</cell><cell>59.2</cell><cell>116.4</cell></row><row><cell>Pose2Mesh (Ours)</cell><cell>64.9</cell><cell>46.3</cell><cell>88.9</cell><cell>58.3</cell><cell>106.3</cell></row><row><cell>Pose2Mesh (Ours) *</cell><cell>-</cell><cell>-</cell><cell>89.5</cell><cell>56.3</cell><cell>105.3</cell></row></table><note>COCO, using groundtruth bounding boxes. The average precision (AP) of [58] and [66] are 85.1 and 82.8 on 3DPW test set, 72.1 and 70.4 on COCO validation set, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>The MPJPE and PA-MPJPE of PoseNet on each benchmark.</figDesc><table><row><cell>train set</cell><cell cols="2">Human3.6M</cell><cell cols="2">Human3.6M + COCO</cell></row><row><cell>benchmark</cell><cell cols="4">MPJPE PA-MPJPE MPJPE PA-MPJPE</cell></row><row><cell>Human3.6M</cell><cell>65.1</cell><cell>48.4</cell><cell>66.7</cell><cell>48.9</cell></row><row><cell>3DPW</cell><cell>105.0</cell><cell>62.9</cell><cell>99.2</cell><cell>61.0</cell></row><row><cell></cell><cell cols="2">benchmark</cell><cell>PA-MPJPE</cell><cell></cell></row><row><cell></cell><cell cols="2">FreiHAND</cell><cell>8.56</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>To be specific, we employ Human3.6M body joints, COCO body joints, SMPL body joints, MANO hand joints for Human3.6M, 3DPW, SURREAL, FreiHAND benchmarks, respectively, in both training and testing stages. For the COCO joint set, we additionally define pelvis and neck joints that connect the upper body and lower body. The pelvis and neck coordinates are calculated as the middle point of right-left hips and right-left shoulders, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>left ear</cell><cell>right ear</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>left wrist</cell><cell>left elbow left knee left shoulder left hip</cell><cell>head nose neck right shoulder right elbow right wrist torso pelvis right hip right knee</cell><cell>left wrist</cell><cell>left ear left shoulder left knee elbow left left hip</cell><cell>right eye neck right shoulder right elbow right wrist right knee pelvis right hip nose</cell><cell>hand left</cell><cell>left left shoulder left knee left hip thorax torso wrist left left elbow</cell><cell>head pelvis spine right thorax right knee right right hip chest shoulder right neck elbow right wrist</cell><cell>hand right</cell><cell cols="3">thumb thumb 4 index index index index 1 2 3 middle 1 middle 2 4 middle 3 4 middle</cell><cell>1 ring 2 ring pinky pinky 2 ring pinky 3 4 3 pinky 4 ring</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>thumb</cell><cell>1</cell></row><row><cell></cell><cell>left ankle</cell><cell>right ankle</cell><cell></cell><cell>left ankle</cell><cell>right ankle</cell><cell></cell><cell>left ankle</cell><cell>right ankle</cell><cell></cell><cell></cell><cell>2</cell><cell>thumb 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>left toe</cell><cell>right toe</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Human3.6M body joints</cell><cell></cell><cell cols="2">COCO body joints</cell><cell></cell><cell cols="2">SMPL body joints</cell><cell></cell><cell></cell><cell></cell><cell>MANO hand joints</cell></row><row><cell></cell><cell cols="2">( Human3.6M dataset )</cell><cell></cell><cell cols="2">( 3DPW dataset )</cell><cell></cell><cell cols="2">( SURREAL dataset )</cell><cell></cell><cell></cell><cell cols="2">( FreiHAND dataset )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>The MPJPE comparison between SMPLify-X fitting results and stateof-the-art 3D human pose estimation methods. "*" takes multi-view RGB images as inputs.</figDesc><table><row><cell>methods</cell><cell>MPJPE</cell></row><row><cell>Moon et al. [43]</cell><cell>53.3</cell></row><row><cell>Sun et al. [60]</cell><cell>49.6</cell></row><row><cell>Iskakov et al. [23]*</cell><cell>20.8</cell></row><row><cell>SMPLify-X from GT 3D pose</cell><cell>13.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>The MPJPE and MPVPE of our Pose2Mesh on 3DPW with accumulative training datasets. We used psuedo-groundtruth SMPL parameters of COCO obtained by NeuralAnnot<ref type="bibr" target="#b45">[46]</ref>. The 2D pose outputs from<ref type="bibr" target="#b57">[58]</ref> are used for input to Pose2Mesh.14 Synthesizing the input 2D poses in the training stage<ref type="bibr" target="#b13">14</ref>.1 Detailed description of the synthesis As described in Section 4.1 of the main manuscript, we synthesize the input 2D poses by adding randomly generated errors on the groundtruth 2D poses in the training stage. For this, we generate errors following Chang et al.<ref type="bibr" target="#b9">[10]</ref> and Moon et al.<ref type="bibr" target="#b43">[44]</ref> for Human3.6M and COCO body joint sets, respectively. On the other hand, for FreiHAND benchmark, we used detection outputs from</figDesc><table><row><cell>train sets</cell><cell cols="3">MPJPE PA-MPJPE MPVPE</cell></row><row><cell>Human3.6M+COCO</cell><cell>92.6</cell><cell>57.9</cell><cell>109.4</cell></row><row><cell>Human3.6M+COCO+AMASS</cell><cell>89.5</cell><cell>56.3</cell><cell>105.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>The MPJPE and PA-MPJPE comparison according to input type in the training stage.</figDesc><table><row><cell>input pose when training</cell><cell cols="2">Human3.6M MPJPE PA-MPJPE</cell><cell cols="2">3DPW MPJPE PA-MPJPE</cell></row><row><cell>2D pose GT</cell><cell>70.4</cell><cell>50.6</cell><cell>153.7</cell><cell>94.4</cell></row><row><cell>2D pose synthesized (Ours)</cell><cell>64.9</cell><cell>48.7</cell><cell>91.4</cell><cell>60.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>The upper bounds of Pose2Mesh, PoseNet, and MeshNet on Hu-man3.6m and 3DPW benchmarks.</figDesc><table><row><cell>networks</cell><cell cols="2">Human3.6M MPJPE PA-MPJPE</cell><cell cols="2">3DPW MPJPE PA-MPJPE</cell></row><row><cell>Pose2Mesh with 2D pose GT</cell><cell>51.1</cell><cell>35.3</cell><cell>65.1</cell><cell>34.6</cell></row><row><cell>PoseNet with 2D pose GT</cell><cell>50.6</cell><cell>41.3</cell><cell>66.1</cell><cell>43.8</cell></row><row><cell>MeshNet with 3D pose GT</cell><cell>13.9</cell><cell>9.9</cell><cell>10.8</cell><cell>8.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>The MPJPE and PA MPJPE comparison between the networks trained from various combinations of loss functions.</figDesc><table><row><cell></cell><cell cols="2">settings</cell><cell>MPJPE</cell><cell>PA-MPJPE</cell><cell></cell></row><row><cell></cell><cell cols="2">full supervision (Ours)</cell><cell>64.9</cell><cell>48.7</cell><cell></cell></row><row><cell></cell><cell cols="2">without Ljoint</cell><cell>66.9</cell><cell>50.1</cell><cell></cell></row><row><cell></cell><cell cols="2">without L normal</cell><cell>64.6</cell><cell>48.5</cell><cell></cell></row><row><cell></cell><cell cols="2">without L edge</cell><cell>64.8</cell><cell>48.7</cell><cell></cell></row><row><cell>input image</cell><cell>groundtruth</cell><cell>full supervision</cell><cell>without</cell><cell>without</cell><cell>without</cell></row><row><cell></cell><cell></cell><cell>(Ours)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Pose2Mesh</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Carnegie Mellon University (CMU) Graphics Laboratory Motion Capture Database</title>
		<ptr target="http://mocap.cs.cmu.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pushing the envelope for rgb-based dense 3d hand pose estimation via neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d hand shape and pose from images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12029</idno>
		<title level="m">Absposelifter: Absolute 3d human pose lifting network from a single noisy 2d human pose</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">Spectral Graph Theory</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d hand shape and pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mahdaviani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kording</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01888</idno>
		<title level="m">Movi: A large multipurpose motion and video dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized procrustes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning joint reconstruction of hands and manipulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kalevatykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<publisher>TPAMI</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on GRAPHics and Interactive Techniques (SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on GRAPHics and Interactive Techniques (SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Neuralannot: Neural annotator for in-the-wild expressive 3d human pose and mesh training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11232</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Using a single rgb frame for real time 3d hand pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panteleris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generating 3d faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on GRAPHics and Interactive Techniques (SIGGRAPH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Benchmarking and error diagnosis in multiinstance pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Feastnet: Feature-steered graph convolutions for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Freihand: A dataset for markerless capture of hand pose and shape from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
