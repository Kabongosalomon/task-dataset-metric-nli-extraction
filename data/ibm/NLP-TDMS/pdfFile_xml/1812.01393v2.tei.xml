<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TextField: Learning A Deep Direction Field for Irregular Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Xiang</forename><forename type="middle">Bai</forename></persName>
						</author>
						<title level="a" type="main">TextField: Learning A Deep Direction Field for Irregular Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Scene text detection</term>
					<term>multi-oriented text</term>
					<term>curved text</term>
					<term>deep neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text detection is an important step of scene text reading system. The main challenges lie on significantly varied sizes and aspect ratios, arbitrary orientations and shapes. Driven by recent progress in deep learning, impressive performances have been achieved for multi-oriented text detection. Yet, the performance drops dramatically in detecting curved texts due to the limited text representation (e.g., horizontal bounding boxes, rotated rectangles, or quadrilaterals). It is of great interest to detect curved texts, which are actually very common in natural scenes. In this paper, we present a novel text detector named TextField for detecting irregular scene texts. Specifically, we learn a direction field pointing away from the nearest text boundary to each text point. This direction field is represented by an image of two-dimensional vectors and learned via a fully convolutional neural network. It encodes both binary text mask and direction information used to separate adjacent text instances, which is challenging for classical segmentationbased approaches. Based on the learned direction field, we apply a simple yet effective morphological-based post-processing to achieve the final detection. Experimental results show that the proposed TextField outperforms the state-of-the-art methods by a large margin (28% and 8%) on two curved text datasets: Total-Text and SCUT-CTW1500, respectively, and also achieves very competitive performance on multi-oriented datasets: IC-DAR 2015 and MSRA-TD500. Furthermore, TextField is robust in generalizing to unseen datasets. The code is available at https://github.com/YukangWang/TextField.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Scene text frequently appears on many scenes and carries important information for many applications, such as product search <ref type="bibr" target="#b0">[1]</ref>, scene understanding <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and autonomous driving <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Scene text reading is thus of great importance. As compared to general object detection, scene text detection, the prerequisite step of scene text recognition, faces particular challenges <ref type="bibr" target="#b5">[6]</ref> due to significantly varied aspect ratios and sizes (usually small), uncontrollable lighting conditions, arbitrary orientations and shapes. To cope with these challenges, traditional methods <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b14">[15]</ref> tend to involve complete pipelines and resort to specifically engineered features. The traditional pipeline usually consists of candidate character/word generation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, candidate filtering <ref type="bibr" target="#b17">[18]</ref> and grouping <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Each module requires careful parameter tuning and specifical heuristic rules designing to make it work properly. It is thus <ref type="bibr">Yongchao</ref>    <ref type="figure">Fig. 1</ref>: Different text representations. Classical relatively simple text representations in (a-c) fail to accurately delimit irregular texts. The text instances in (e) stick together using binary text mask representation in (d), requiring heavy postprocessing to extract text instances. The proposed direction field in (f) is able to precisely describe irregular text instances.</p><p>difficult to optimize the whole pipeline, and also results in low detection speed. Thanks to recent development of object detection <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> and segmentation <ref type="bibr" target="#b21">[22]</ref> with deep learning, scene text detection has witnessed a great progress <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b44">[45]</ref>. They can be roughly divided into three categories: 1) Regression-based methods. Scene text is a specific type of object. Many recent methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b31">[32]</ref> adapt the general object detection framework to detect texts by directly regressing horizontal/oriented rectangles or quadrilaterals, which enclose texts. Some other methods attempt to regress text parts <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b41">[42]</ref> or corners <ref type="bibr" target="#b34">[35]</ref> followed by a linking or combination process. 2) Segmentation-based methods. Scene text detection can also be regarded as text instance segmentation. Several methods <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b40">[41]</ref> rely on fully convolutional network to segment text areas. A heavy post-processing is usually involved to extract text instances from the segmented text areas. 3) Hybrid methods. Some other methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref> predict text score maps via segmentation and then obtain bounding boxes via regression.</p><p>The popular regression-based methods and existing hybrid methods <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b35">[36]</ref> achieve excellent performances on standard benchmarks. Yet, they have a strong bottleneck which assumes a text instance has a linear shape, thus adopting relatively simple text representation in terms of horizontal/oriented rectangles or quadrilaterals. Their performances drop significantly arXiv:1812.01393v2 [cs.CV] 29 Jul 2019 for detecting text of irregular shapes, e.g., curved text. Besides, as depicted in <ref type="figure">Fig. 1(a-c)</ref>, the traditional simple text representations do not achieve precise text delimitation providing texts' geometrical properties, which are useful for the subsequent recognition <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Segmentation-based methods <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b40">[41]</ref> may not suffer from this problem. Yet, as depicted in <ref type="figure">Fig. 1(e)</ref>, though the predicted text region is a good estimation of text areas, it is rather difficult to separate close text instances. Indeed, many efforts of segmentation-based methods focus on how to separate segmented text regions into text instances.</p><p>In real-world scenarios, curved texts appear frequently <ref type="bibr" target="#b47">[48]</ref> and can be easily found in real life scenes such as bottles, spherical objects, clothes, logos, signboards. In two recently released datasets (Total-Text <ref type="bibr" target="#b40">[41]</ref> and SCUT-CTW1500 <ref type="bibr" target="#b48">[49]</ref>) for scene text detection, around 40% text instances are curved texts.</p><p>In this paper, we propose a novel text detector deemed TextField for detecting texts of arbitrary shapes and orientations. Inspired by component tree representation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b49">[50]</ref>- <ref type="bibr" target="#b51">[52]</ref> that links neighboring pixels following their intensity order to form candidate characters, we propose to learn a deep direction field, which is similar to the notion of flux image <ref type="bibr" target="#b52">[53]</ref>, to link neighboring pixels and form candidate text parts. The learned direction information is further used to group text parts into text instances. For that, the text areas are translated into text direction field first, pointing away from the nearest text boundary to each text point. Specifically, this direction field is encoded by an image of two-dimensional vectors for network training. For text areas, the field is defined as a unit vector encoding the direction, and for non-text areas, the direction field is set to (0, 0). Thus, the magnitude information provides the text mask, while the direction information facilitates the post-processing of separating predicted text areas into text instances. An example of such direction field is given in <ref type="figure">Fig. 1(f)</ref>. We adopt a fully convolutional network to directly regress the direction field. The candidate text pixels are then obtained by thresholding the magnitude. The direction information is used to extract text instances from candidate text pixels via some morphological operators. This results in detections with precise delimitation of irregular scene texts. Several examples are given in <ref type="figure" target="#fig_1">Fig. 2</ref>. The proposed TextField significantly outperforms other methods by 28% and 8% in F-measure on Total-Text <ref type="bibr" target="#b40">[41]</ref> and SCUT-CTW1500 <ref type="bibr" target="#b48">[49]</ref>, respectively, while achieving very competitive performances on two widely adopted multi-oriented text datasets.</p><p>The main contributions of this paper are three folds: 1) We propose a novel direction field which can represent scene texts of arbitrary shapes. This direction field encodes both binary text mask and direction information facilitating the subsequent text grouping process. 2) Based on the direction field, we present a text detector named TextField, which efficiently detects irregular scene texts. 3) The proposed TextField significantly outperforms state-of-the-art methods on two curved text datasets and achieves competitive performances on two widely adopted multi-oriented text datasets.</p><p>The rest of this paper is organized as follows. We shortly review some related works on scene text detection in Section II. The proposed method is then detailed in Section III, followed by extensive experimental results in Section IV. Finally, we conclude and give some perspectives in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Scene text detection has been extensively exploited recently. We first review some representative methods in Section II-A. A comprehensive review of recent scene text detectors can be found in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b53">[54]</ref>. The comparison of the proposed TextField with some related works is depicted in Section II-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scene text detection</head><p>Scene text detection methods can be roughly classified into specifically engineered and deep learning-based methods. Before the era of deep learning, scene text detector pipelines usually consist of text component extraction and filtering, component grouping, and candidate filtering. The key step is extracting text components based on some engineered features. Maximally Stable Extremal Regions (MSER) <ref type="bibr" target="#b15">[16]</ref> and Stroke Width Transform (SWT) <ref type="bibr" target="#b16">[17]</ref> are two representative works for text component extraction. Many traditional methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> are based on these two algorithms. Other examples of this type are <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Most recent methods shift to deep neural networks to extract scene texts. In general, they can be roughly summarized into regression-based, segmentationbased, and hybrid methods. For the regression-based ones, they can be further divided into two categories based on the target to regress: proposal-based and part-based methods.</p><p>Proposal-based methods: Proposal-based methods are mainly inspired by recent object detection pipelines. TextBoxes <ref type="bibr" target="#b23">[24]</ref> directly adapts SSD <ref type="bibr" target="#b19">[20]</ref> for scene text detection by using long default boxes and convlutioinal filters to cope with the significantly varied aspect ratios. TextBoxes++ <ref type="bibr" target="#b29">[30]</ref> extends TextBoxes by regressing quadrilaterals instead of horizontal bounding boxes. Ma et al. <ref type="bibr" target="#b28">[29]</ref> attempt to solve the multi-oriented text detection by adopting Rotated Regional Proposal Network (RRPN) in the pipeline of faster r-cnn. Quadrilateral sliding windows are adopted in <ref type="bibr" target="#b22">[23]</ref> to detect multi-oriented texts. Wordsup <ref type="bibr" target="#b27">[28]</ref> explores word annotations for character-based text detection. SSTD <ref type="bibr" target="#b25">[26]</ref> introduces the attention mechanism by FCN to suppress background interference, improving accurate detection of small texts. In <ref type="bibr" target="#b30">[31]</ref>, Liao et al. propose to apply rotation-invariant and sensitive features for text/non-text box classification and regression, respectively, boosting long multi-oriented text detection. Wang et al. <ref type="bibr" target="#b31">[32]</ref> propose instance transformation network by considering geometry-aware information for scene text detection.</p><p>Part-based methods: Some other regression-based methods tend to regress text parts while predicting the linking relationship between them. In <ref type="bibr" target="#b32">[33]</ref>, the authors propose a Connectionist Text Proposal Network (CTPN) by first predicting vertical text parts, then adopting a recurrent neural network to link text parts. Shi et al. present a network named SegLink <ref type="bibr" target="#b33">[34]</ref> to first detect text parts named text segments while predicting the linking relationship between neighboring text segments. A novel framework named Markov Clustering Network (MCN) is proposed in <ref type="bibr" target="#b35">[36]</ref>. In this work, the authors propose to regard an image as a stochastic flow graph, where the flows are strong between text nodes (i.e., text pixels) but weak for the others. Then a Markov clustering process is applied to form text instances from the predicted flow graph. In <ref type="bibr" target="#b34">[35]</ref>, Lyu et al. propose to first regress four corners of text boxes, followed by a combination of corners and Non-Maximum Suppression (NMS) process to achieve accurate multi-oriented text localization.</p><p>Segmentation-based methods: Segmentation-based approaches regard text detection as a text area segmentation problem, which is usually achieved via Fully Convolutional Neural Network (FCN). They mainly differ in how to postprocess the predicted text regions into words or text lines. In <ref type="bibr" target="#b36">[37]</ref>, Zhang et al. adopted an FCN to estimate text blocks, on which candidate characters are extracted using MSER. Then they use traditional grouping and filtering strategies to achieve multi-oriented text detection. In addition to text block (word or line) prediction, Yao et al. <ref type="bibr" target="#b37">[38]</ref> also propose to predict both individual characters and the orientation of text boxes via an FCN in a holistic fashion. Then a grouping process based on the three estimated properties of text yields the text detection. Ch'ng et al. <ref type="bibr" target="#b40">[41]</ref> fine-tune DeconvNet <ref type="bibr" target="#b56">[57]</ref> to achieve curved text detection. In <ref type="bibr" target="#b39">[40]</ref>, the authors consider the text detection as an instance segmentation problem using multi-scale image inputs. They adopt an FCN to predict text blocks, followed by two CNN branches predicting text lines and instance-aware segmentations from the estimated text blocks. Wu et al. <ref type="bibr" target="#b38">[39]</ref> introduce text border in addition to text/non-text segmentation, which results in a three-class semantic segmentation, facilitating the separation of adjacent text instances. Xue et al. further improve <ref type="bibr" target="#b38">[39]</ref> by exploiting bootstrapping techniques and designing semantics-aware text border detection technique for accurate text localization.</p><p>Hybrid methods: It is also worth to mention that some other methods leverage segmentation to classify text/non-text pixels and then localize texts via bounding box regression. For example, East <ref type="bibr" target="#b24">[25]</ref> and Deep regression <ref type="bibr" target="#b26">[27]</ref> both perform perpixel rotated rectangle or quadrilateral estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with related works</head><p>TextField Versus Traditional component-based methods: Traditional methods rely on engineered features to extract text components, and heuristic grouping rules to form text instances. Each module requires careful parameter tuning, resulting in sub-optimal performance and slow runtime of the whole pipeline. The proposed TextField leverages deeply learned direction field which encodes both text mask and direction information facilitating subsequent text grouping process. The whole pipeline is more effective in both performance and runtime.</p><p>TextField Versus Proposal-based and hybrid methods: The proposal-based and hybrid scene text detection methods are mainly inspired by recent object detection pipelines, which have relatively less flexible text representations. They usually regress text instances in form of horizontal/oriented rectangles or quadrilaterals, having limited ability in detecting irregular texts (e.g., curved texts). TextField does not suffer from this limitation. Benefiting from the proposed direction field, TextField is able to accurately detect texts of irregular shapes.</p><p>TextField Versus Part-based methods: Part-based methods decompose the text instances into text parts, then attempt to link the neighboring text parts. They enjoy a more flexible representation, and can somehow alleviate the problem of relatively simple text representation inherited in proposalbased methods. Yet, driven by the employed linking or combination strategy, these methods usually produce multi-oriented text detections. The proposed direction field is versatile in representing multi-oriented and curved texts, making TextField perform equally well in detecting any irregular texts.</p><p>TextField Versus Segmentation-based methods: Due to the significantly varied sizes and aspect ratios, most segmentation-based methods are built upon semantic segmentation, followed by a heavy post-processing step to separate the predicted text areas into text instances. In addition to text mask, some information such as text border, text line, text box orientation, or linking relationship between neighboring pixels is also predicted to ease the separation of adjacent texts. Yet, such additional information either limits the method to multioriented text detection or also faces similar problem with text semantic segmentation in separating adjacent texts. TextField directly regresses the direction field which encodes both text mask and direction information that points away from text boundary, thus allowing simple separation of adjacent texts. In this sense, TextField is more elegant and efficient in detecting irregular texts.</p><p>It is worth to mention that direction information has also been diversely exploited in some other applications <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b59">[60]</ref>, which involve different definitions or usages. Given a test image, the network predicts a novel direction field in terms of a twochannel map, which can be regarded as an image of two-dimensional vectors. To better show the predicted direction field, we calculate and visualize its magnitude and direction information. Text instances are then obtained based on these information via the proposed post-processing using some morphological tools.  <ref type="figure">Fig. 4</ref>: Illustration of the proposed direction field. Given a training image and its text annotation, a binary text mask can be easily generated. For each text pixel p, we find its nearest non-text pixel N p . Then, a two-dimensional unit vector that points away from N p to p is defined as the direction field on p. For non-text pixels, the direction field is set to (0, 0). On the right, we visualize the direction information of the text direction field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>The proposed method relies on a fully convolutional neural network to produce a dense per-pixel direction field for detecting irregular texts. The pipeline is depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>. In general, we regard the text detection problem as text instance segmentation. For that, we propose a novel direction field, aiming at segmenting texts and also separating adjacent text instances. More specifically, for a text pixel p, its direction field is represented by a two-dimensional unit vector that points away from its nearest text boundary pixel. This direction field is detailed in Section III-B. Benefiting from such novel representation, the proposed TextField can easily separate text instances that lie close to each other. Furthermore, such direction field is appropriate for describing text of arbitrary shapes. We adopt a VGG16-based network to learn the direction field. To preserve spatial resolution and take full advantage of multi-level information, we exploit a widely used multi-level feature fusion strategy. in Section III-C. Some specific adaptions for the network training are given in Section III-D, including online hard negative mining and a weighted loss function for our perpixel regression task. Both adaptions are dedicated to force our network to focus more on hard pixels and eliminate the effects caused by quantitative imbalance between foreground and background pixels. Finally, a novel post-processing based on mathematical tools (see Section III-E) is proposed to group pixels, forming detected text instances thanks to the predicted text direction field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Direction field</head><p>As pointed out in Sec. II-B, though proposal-based and partbased text detectors have achieved impressive performances on multi-oriented text detection, they do not perform well for curved texts. Segmentation-based approaches can somehow tackle this limitation via binary text mask (of arbitrary shapes) segmentation. Yet they can hardly separate adjacent text instances. To address these issues, we propose a novel direction field for detecting irregular texts.</p><p>Instead of binary text mask involved in the segmentationbased approaches, we propose the direction field that encodes both binary text mask and direction information that can be used to separate adjacent text instances. As illustrated in <ref type="figure">Fig. 4</ref>, for each pixel p inside a text instance T , let N p be the nearest pixel to p lying outside the text instance T , we then define a two-dimensional unit vector V gt (p) that points away from N p to the underlying text pixel p. This unit vector V gt (p) directly encodes approximately relative location of p inside T and highlights the boundary between adjacent text instances. For the non-text area, we represent those pixels with (0, 0). Formally, the proposed direction field is given by:</p><formula xml:id="formula_0">V gt (p) =      − − → N p p/ − − → N p p , p ∈ T (0, 0), p ∈ T<label>(1)</label></formula><p>where − − → N p p denotes length of the vector starting from pixel N p to p, and T stands for all the text instances in an image. In practice, for each text pixel p, it is simple to compute its nearest pixel N p outside the text instance containing p by distance transform algorithm. Consequently, it is rather straightforward to transform a traditional text annotation to the proposed direction field.</p><p>The proposed direction field given by Eq. <ref type="formula" target="#formula_0">(1)</ref> is appropriate for detecting irregular texts. In fact, the magnitude of direction field V is equivalent to binary text mask. Thus, we rely on magnitude of V to differentiate text and non-text pixels. The direction information encoded in V facilitates the separation of adjacent text instances (see Sec. III-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network architecture</head><p>The proposed network architecture to learn the direction field for detecting irregular texts is depicted in <ref type="figure">Fig. 5</ref>. We adopt a fully convolutional neural network which mainly consists of two parts: feature extraction and multi-level feature fusion. The backbone network to extract features is the VGG16 network <ref type="bibr" target="#b60">[61]</ref> pre-trained on ImageNet <ref type="bibr" target="#b61">[62]</ref>. We discard the last pooling layer and its following fully connected layers. Since text sizes may vary significantly, it is difficult to detect small text instances with only coarse features. Therefore, we merge features from different stages to capture multi-scale text instances. More specifically, we exploit the feature maps from stage3, stage4, and stage5 of the VGG16 backbone network. These multi-level features are upsampled to the same size as the feature map from stage3, and are then merged together by concatenation. This is followed by three convolution layers, resulting in a two-channel map that predicts the direction field given by Eq. (1). Finally, we upsample the predicted direction field to the original size. We adopt bilinear interpolation for all the upsampling operations.</p><p>It is worth to note that the proposed method is not severely bottlenecked by the limited receptive field. In fact, the proposed direction field in Eq. (1) only relies on local clues (i.e., location of the nearest text boundary). Thus, we only require a receptive field that covers the short side of text instance. Whereas, for the classical proposal-based methods, a receptive field larger than the long side of underlying text instance is usually needed. Consequently, the proposed method is more flexible in detecting irregular long texts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Training objective:</head><p>We leverage the network depicted in Section III-C to regress the proposed direction field. The network parameters are optimized with an instance-balanced Euclidean loss. More specifically, the loss function to minimize is a weighted sum of the mean squared error on each pixel of the image domain Ω. This is given by:</p><formula xml:id="formula_1">L = p∈Ω w(p) * V gt (p) − V pred (p) 2 ,<label>(2)</label></formula><p>where V pred is the predicted direction field, and w(p) denotes the weight coefficient of pixel p. Since text sizes may vary significantly in scene images, if all text pixels contribute equally to the loss function, large text instances will be dominative in the loss computation while small ones will be ignored. To tackle this problem, we adopt an instancebalanced strategy. More precisely, for an image containing N text instances, the weight w for a given pixel p is defined as follows:</p><formula xml:id="formula_2">w(p) =      T ∈T |T | N * |Tp| , p ∈ T 1, p ∈ T<label>(3)</label></formula><p>where |T | denotes the total number of pixels in a text instance T , and T p stands for the text instance containing pixel p. In this way, each text instance of any size is endowed with the same weight, contributing equally to the loss function in Eq. <ref type="formula" target="#formula_1">(2)</ref>. This is consistent with current text detection system such that each text instance is equally important.</p><p>2) Online hard negative mining: In scene images, text instances usually occupy a small area of the image. Thus, the number of text pixels and non-text pixels is rather imbalanced. To alleviate this problem and to make the network training focus more on pixels which are hard to distinguish, we adopt hard mining following the online hard negative mining strategy proposed in <ref type="bibr" target="#b62">[63]</ref>. More specifically, non-text pixels are sorted in a decreasing order of their per-pixel loss. Then only the front γ * ( T ∈T |T |) non-text pixels are reserved for backpropagation, where γ is a given hype-parameter that denotes the ratio of non-text pixels with respect to the total number of text pixels when computing the total loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Inference and post-processing</head><p>For a given image, the trained network predicts the direction field in terms of 2D vectors. We propose a novel postprocessing pipeline using some morphological tools to obtain the final text detection results from this prediction. Precisely, as described in Section III-B, the magnitude of the predicted direction field V pred highlights text/non-text areas. Thus, we first threshold the magnitude image with a thresholding value λ m to obtain candidate text pixels C. It is worth to note that pixels lying around text symmetrical axis usually have low magnitude due to the cancellation of opposite direction in learning and upsampling. The text detection problem then amounts to group candidate text pixels into text instances. For that, we first segment the candidate text areas into text superpixels (i.e., text parts depicted in different color in <ref type="figure" target="#fig_4">Fig. 6(b)</ref>), which are then grouped together to form candidate text instances. A last text instance filtering step is adopted to yield the final detected texts. This process is depicted in <ref type="figure" target="#fig_4">Fig. 6</ref> and Algorithm 1, and summarized in the following:</p><p>Text superpixel segmentation: The magnitude information of the predicted direction field V pred is used to generate candidate text pixels. Then we rely on the direction information carried by V pred to segment the candidate text areas into text superpixels. Precisely, for each candidate text pixel p, the direction information ∠V pred (p) is binned into one of the 8 directions, pointing to its nearest neighboring candidate text pixel denoted as P(p), standing for the parent of pixel p. Each candidate text pixel points to a unique neighboring pixel. Consequently, the parent image P forms a forest structure F, partitioning the candidate text areas into text superpixels, each of which is represented by a tree T ∈ F. This text superpixel segmentation can be efficiently achieved by blob labeling algorithm (see line 7-15 in Algorithm 1).</p><p>Text superpixel grouping: Based on the segmented text superpixels represented by trees, we propose a simple grouping method to form candidate text instances. Since the proposed direction field encodes the direction away from the nearest boundary, the root pixels of all trees locate near the symmetry axis of each text instance. We consider all these root pixels as the representatives of all the text superpixels. The representatives of a text instance usually are close to each other (See <ref type="figure" target="#fig_4">Fig. 6</ref>). We apply a simple dilation δ (with 3 × 3 structuring element) to group the representatives of the same text instance. This is followed by a connected component labeling that forms candidate text instances. The text superpixel grouping is depicted in line 17-21 of Algorithm 1.</p><p>Text instance filtering: After the extraction of candidate text instances, we apply some filtering strategies to get rid of some non-text instances following their shapes and sizes. As illustrated in <ref type="figure" target="#fig_4">Fig. 6</ref>, the representative pixels of a text instance should have a symmetrical distribution of directions. Therefore, all the representative pixels of a text instance should be approximately paired in the sense of having opposite directions. Based on this observation, we count the ratio of non-paired representatives, and filter out the candidate text instances having a ratio lower than a given value λ r (set to 0.6). For the remaining candidate text instances, we apply a morphological closing φ (with 11 × 11 structuring element) to fill the inside holes. Then we also discard some noisy candidate instances whose areas are smaller than λ a (set to 200). The remaining candidate text instances are the final detected texts. The text instance filtering is given in line 23-27 in Algorithm 1.</p><p>Specifically, the proposed post-processing is detailed in Algorithm 1. The core body of the algorithm is the blob labeling to construct text superpixels via the forest structure encoded by the parenthood image P. This blob labeling process can be efficiently implemented using a stack data structure S and an auxiliary image visited. The text superpixels are labeled by the image L. Then we identify the representative pixels R by root pixels of those trees. These representative pixels are also stored by an image M. We then apply a dilation δ with kernel k 1 × k 1 (k 1 = 3) to group representative pixels, followed by a connected labeling CC Labeling to form candidate text instances. We then filter out some candidate text instances by the ratio of non-paired representatives Filter Unbalanced Text. The label of each remaining candidate text instance is then propagated to all the pixels inside the same text superpixels. Finally, we apply a closing φ with kernel k 2 × k 2 (k 2 = 11) to fill the holes inside each candidate text instance, followed by a removal of small candidate text instances. This post-processing gives the final detected texts encoded by M.</p><p>Algorithm 1: Text inference with a morphological postprocessing on predicted direction field V pred . M is the final text instance segmentation map. See the corresponding texts in Section III-E for details. <ref type="bibr" target="#b0">1</ref> Text Inference(V pred , λ m , λ r , λ a ) 2 M, L ← 0, l ← 0, C, R, S ← ∅, visited ← False, P ← p 0 //initialization ; 3 //get candidate text pixels 4 foreach p ∈ Ω do <ref type="bibr" target="#b4">5</ref> if |V pred (p)| ≥ λ m then C ← C ∪ p ; 6 //blob lableing to construct trees encoded by P 7 foreach p ∈ C and not visited(p) do </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The proposed method is appropriate for detecting irregular texts. In the following, we evaluate the proposed method on four public benchmark datasets: SCUT-CTW1500 <ref type="bibr" target="#b48">[49]</ref> and Total-Text <ref type="bibr" target="#b40">[41]</ref> which contain curved texts, ICDAR2015 Incidental Scene Text (IC15) <ref type="bibr" target="#b63">[64]</ref> and MSRA-TD500 <ref type="bibr" target="#b64">[65]</ref> which mainly consist of multi-oriented texts in terms of oriented rectangles or general quadrilaterals. SynthText in the Wild <ref type="bibr" target="#b65">[66]</ref> is also adopted to pre-train the network. A short description of these datasets and adopted evaluation protocol is given in Section IV-A. Some implementation details are depicted in Section IV-B, followed by curved text detection results in Section IV-C. The experimental results on multi-oriented text detection is given in Section IV-D to demonstrate the versatility of the proposed TextField. To further demonstrate the generality of TextField, cross dataset experiments are also presented in Section IV-E. The runtime analysis and some failures cases are given in Section IV-F and Section IV-G, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and evaluation protocol</head><p>SynthText in the Wild <ref type="bibr" target="#b65">[66]</ref>: SynthText contains 800k synthetic images generated by blending natural images with artificial text. Annotations are given in character, word, and line level. This dataset with word level annotation is used to pre-train the proposed model. <ref type="bibr" target="#b48">[49]</ref>: Different from classical multi-oriented text datasets, this dataset is quite challenging due to many curved texts. It consists of 1000 training images and 500 testing images. This dataset has more than 10k text annotations and at least one curved text per image. Each text instance is labeled by a polygon with 14 points. The annotation is given in line or curve level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCUT-CTW1500</head><p>Total-Text <ref type="bibr" target="#b40">[41]</ref>: Total-Text dataset also aims at solving the arbitrary-shaped text detection problem. It contains 1555 scene images, divided into 1255 training images and 300 testing images. This dataset contains many curved and multi-oriented texts. Annotations are given in word level with polygon-shaped bounding boxes instead of conventional rectangular bounding boxes.</p><p>ICDAR2015 Incidental Scene Text (IC15) <ref type="bibr" target="#b63">[64]</ref>: This dataset is widely used to benchmark multi-oriented text detectors. It was released for the Challenge 4 of ICDAR2015 Robust Reading Competition. Different from previous datasets with text captured in relatively high resolution, scene images in this dataset are taken by Google Glasses in an incidental manner. Therefore, text in these images is of various scales, orientations, contrast, blurring, and viewpoint, making it challenging for detection. This dataset is composed of 1000 training images and 500 testing images. Annotations are provided with word-level bounding quadrilaterals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSRA-TD500 [65]</head><p>: This dataset is dedicated for detecting multi-lingual long texts of arbitrary orientations. It consists of 300 training images and 200 testing images, annotated at the level of text lines. Since the number of training images is rather small, similar with other methods, we also utilize the images from HUST-TR400 <ref type="bibr" target="#b66">[67]</ref> as extra training data.</p><p>Evaluation protocol: We follow the standard evaluation protocol relying on precision, recall, and f -measure. Precisely, they are defined as following:</p><formula xml:id="formula_3">precision = T P T P + F P , Recall = T P T P + F N , f -measure = 2 × precision × recall precision + recall ,<label>(4)</label></formula><p>where T P , F P , and F N stands for the number of correctly detected text instances, incorrect detections, and missing text instances, respectively. For a detected text instance T , if T intersects a ground truth text instance with an IOU larger than a given thresholding value (typically set to 0.5), then the text instance T is considered as a correct detection. Since there is a trade-off between recall and precision, f -measure is a common compromised measurement for performance assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Data augmentation strategy is adopted to increase the training data and avoid over-fitting. Specifically, images are first randomly cropped with area ratios ranging from 0.1 to 1 and aspect ratios ranging from 0.3 to 3. The cropped image is then randomly rotated with 0 and ±90 degrees. Note that the randomly cropped patch is selected only when the proportion of contained texts with respect to all ground truth text areas in the original images is larger than a threshold value, randomly set to 0.1, 0.3, 0.5, and 0.7. Finally, the augmented images are resized to 384 × 384 or 768 × 768 during different training stages detailed in the following.</p><p>The proposed network is pre-trained on SynthText for one epoch, and then finetuned on SCUT-CTW1500, Total-Text, ICDAR2015 Incidental Scene Text, and MSRA-TD500, respectively. The training process is divided into three stages. In the pre-training stage, the augmented images are resized to 384 × 384 for the sake of training speed. The learning rate and the hyper-parameter γ involved in online hard negative mining are set to 10 −4 and 3, respectively. Then we finetune our model on each dataset for about 100 epochs with the same settings as pre-training stage. We continue to train the network for another 100 epochs by resizing the augmented images to 768 × 768 aiming at better handling multi-scale texts. In this last training stage, the learning rate is decayed to 10 −5 and γ is set to 6. In the whole training process, we adopt Adam <ref type="bibr" target="#b67">[68]</ref> to optimize the network. All the experiments are conducted on Caffe <ref type="bibr" target="#b68">[69]</ref> using a workstation with a single Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Curved text detection</head><p>The proposed TextField is appropriate to detect irregular texts. We first conduct experiments on two curved text datasets: SCUT-CTW1500 and Total-Text.</p><p>SCUT-CTW1500: This dataset mainly contains curved and multi-oriented texts. For each image, the annotation is given in line or curve level. The size of testing image is rather small. In testing phase, the images are resized to 576 × 576. The threshold parameter λ m is set to 0.59 for post-processing. A visualization example of the learned direction field and  <ref type="bibr" target="#b48">[49]</ref>.</p><p>Methods recall precision f-measure SegLink * <ref type="bibr" target="#b33">[34]</ref> 0.400 0.423 0.408 CTPN * <ref type="bibr" target="#b32">[33]</ref> 0.538 0.604 0.569 EAST * <ref type="bibr" target="#b24">[25]</ref> 0.491 0.787 0.604 DMPNet * <ref type="bibr" target="#b22">[23]</ref> 0.560 0.699 0.622 CTD <ref type="bibr" target="#b48">[49]</ref> 0.652 0.743 0.695 CTD+TLOC <ref type="bibr" target="#b48">[49]</ref>   some involved post-processing steps is depicted in <ref type="figure" target="#fig_6">Fig. 7(a)</ref>. Some qualitative results are given in <ref type="figure" target="#fig_7">Fig. 8(a)</ref>. The proposed TextField correctly detects text of arbitrary shapes with very accurate text boundaries. The quantitative results are shown in Tab. I. Compared with other state-of-the-art methods, our proposed method outperforms them by a large margin in terms of recall, precision, and f-measure. The proposed TextField achieves 81.4% F-measure, improving the state-ofthe-art methods by 8.0%. Total-Text: We also evaluate the proposed TextField on Total-Text whose annotations are given in word level. This dataset mainly contains curved and multi-oriented texts. In testing, all images are resized to 768 × 768. The threshold parameter λ m is set to 0.50 for post-processing. A visualization example of the learned direction field and some involved post-processing steps is illustrated in <ref type="figure" target="#fig_6">Fig. 7(b)</ref>. Some qualitative results are depicted in <ref type="figure" target="#fig_7">Fig. 8(b)</ref>. From this figure, we can observe that TextField also precisely detects word level irregular texts. And TextField is able to accurately separate close text instances of arbitrary shapes. The quantitative results are given in Tab. II. The proposed TextField achieves 80.6% F-measure on this dataset, significantly outperforming other methods.</p><p>From the qualitative results depicted in <ref type="figure" target="#fig_7">Fig. 8(a-b)</ref> and quantitative results given in Tab. I and Tab. II, the proposed TextField is able to detect irregular texts in both line-level and word-level. TextField establishes new state-of-the-art results in detecting curved texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-oriented text detection</head><p>As shown in Section IV-C, the proposed TextField significantly outperforms other methods on curved text detection. To further demonstrate the ability of TextField in detecting texts of arbitrary shapes, we evaluate TextField on ICDAR2015 Incidental Scene Text and MSRA-TD500 dataset, showing that TextField also achieves very competitive results on widely adopted multi-oriented datasets. Note that for these two experiments, we fit each text instance achieved with TextField by a minimum oriented bounding rectangle.</p><p>ICDAR2015 Incidental Scene Text: Images in this dataset are of low resolution and contain many small text instances. Methods recall precision f-measure FPS Zhang et al. <ref type="bibr" target="#b36">[37]</ref> 0.430 0.708 0.536 0.48 CTPN <ref type="bibr" target="#b32">[33]</ref> 0.516 0.742 0.609 7.1 Yao et al. <ref type="bibr" target="#b37">[38]</ref> 0.587 0.723 0.648 1.61 DMPNet <ref type="bibr" target="#b22">[23]</ref> 0.682 0.732 0.706 -SegLink <ref type="bibr" target="#b33">[34]</ref> 0.768 0.731 0.750 -MCN <ref type="bibr" target="#b35">[36]</ref> 0.800 0.720 0.760 -EAST <ref type="bibr" target="#b24">[25]</ref> 0.728 0.805 0.764 6.52 SSTD <ref type="bibr" target="#b25">[26]</ref> 0.730 0.800 0.770 7.7 RRPN <ref type="bibr" target="#b28">[29]</ref> 0.730 0.820 0.770 -ITN <ref type="bibr" target="#b31">[32]</ref> 0.741 0.857 0.795 -EAST † <ref type="bibr" target="#b24">[25]</ref> 0 Therefore, images are not resized. The original resolution of 1280 × 720 is used in testing. The threshold parameter λ m is set to 0.69 for post-processing. A visualization example of the learned direction field and some involved post-processing steps is shown in <ref type="figure" target="#fig_6">Fig. 7(c)</ref>. Some detection results on this dataset are given in <ref type="figure" target="#fig_7">Fig. 8(c)</ref>, where challenging texts of variant contrast and scales are correctly detected. The quantitative evaluation compared with other methods are depicted in Tab. III. The proposed TextField achieves competitive results with other stateof-the-art methods on this dataset. Following <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>, we also report the results of TextField under multi-scale evluation using 384 × 384, 768 × 768, and 1024 × 1024 inputs on IC15. TextField is also very competitive with other methods under multi-scale evaluation. Note that for fair comparison, we mainly compare with other methods using the same backbone network (i.e., VGG16 network). MSRA-TD500: This dataset contains both English and Chinese texts whose annotations are given in terms of text lines. The text scale varies significantly. In testing, we resize the images into 768 × 768. The threshold parameter λ m is set to 0.64 for post-processing. Due to the large character spacing in this dataset, we also group the detected texts with small aspect ratios before evaluating the TextField using the IC15 evaluation code. A visualization example of the learned direction field and some involved post-processing steps is depicted in <ref type="figure" target="#fig_6">Fig. 7(d)</ref>. Some qualitative illustrations are shown in <ref type="figure" target="#fig_7">Fig. 8(d)</ref>. The proposed TextField successfully detects long text lines of arbitrary orientations and sizes. The quantitative comparison with other methods on this dataset is given in Tab. IV. TextField also achieves competitive performance with other methods in detecting long multi-oriented texts. Specifically, TextField performs slightly worse than the methods in <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b35">[36]</ref> on MSRA-TD500. Yet, the performance of TextField is much better than them on IC15 dataset.</p><p>From the qualitative results in <ref type="figure" target="#fig_7">Fig. 8(c-d)</ref> and quantitative  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Cross dataset text detection</head><p>To further demonstrate the generalization ability of the proposed TextField, we also evaluate the TextField trained on one dataset and test the trained model on a different dataset annotated in the same level (e.g., word or line). Specifically, we first benchmark several classical models (trained on IC15) on Total-Text dataset. As depicted in Tab. V, The proposed TextField generalizes better on cross-dataset text detection. We then test the TextField (trained on Total-Text) on IC15 dataset, which gives acceptable results. We have also performed cross-dataset evaluations on two line-level annotated datasets: SCUT-CTW1500 and MSRA-TD500. As shown in Tab. V, for the line-based text detection, TextField also achieves very competitive results (under cross-dataset setting) with some state-of-the-art methods trained on the target dataset. Specifically, TextField trained on MSRA-TD500 containing only multi-oriented texts performs comparably with other methods properly trained on SCUT-CTW1500, a curved text dataset. Furthermore, it is worth to note that TextField trained on SCUT-CTW1500 containing mainly curved English texts also performs rather well (with a small degradation) in detecting multi-oriented Chinese texts in MSRA-TD500. These cross-dataset experiments demonstrate that the proposed TextField is effective in detecting irregular texts, and is also robust in generalizing to unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Runtime</head><p>The proposed TextField first yields the predicted direction field through the proposed network, then followed by a morphological post-processing step to achieve final text detection results. The runtime of TextField is thus decomposed into two stages: network inference and post-processing. For the network inference, using the VGG16 backbone network as depicted in <ref type="figure">Fig. 5</ref>, it takes about 130ms for a 1280 × 720 IC15 image and 100ms for a 768 × 768 MSRA-TD500 image on a Titan Xp GPU. As described in Section III-E, the post-processing is mainly composed of three steps: text superpixel segmentation, text superpixel grouping, and text instance filtering. The text superpixel segmentation could be achieved by the blob labeling algorithm which is very fast. The grouping step only involves some classical morphological operations. The text instance filtering step is also very fast thanks to the criterion incrementally computed during the grouping step. The whole post-processing stage takes about 36ms for a 1280 × 720 IC15 image and 24ms for a 768 × 768 MSRA-TD500 image on a 3.4GHz/8MB cache Intel core i7-2600, 16GB RAM. As depicted in Tab. III, the proposed TextField runs at 6.0 FPS using VGG16 backbone, which is on par with most state-ofthe-art methods. Furthermore, TextField is able to accurately detect irregular texts and generalizes well to unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Weakness</head><p>As demonstrated in previous experiments, TextField performs well in most cases of detecting texts of arbitrary shapes. It still fails for some difficult images, such as object occlusion, large character spacing. Some failure examples are given in <ref type="figure" target="#fig_8">Fig. 9</ref>. TextField also has some false detections on some text-like areas. Note that all these difficulties are common challenges for the other state-of-the-art methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented TextField, which learns a deep direction field for irregular text detection. Specifically, we propose a novel text direction field that points away from nearest text boundary to each text point. Such two-dimensional text direction field encodes both binary text mask and direction information that facilitates the separation of adjacent text instances, which remains challenging for classical segmentationbased approaches. TextField directly regresses the direction field followed by a simple yet effective post-processing step inspired by some morphological tools. Experiments on two curved text datasets (Total-Text and SCUT-CTW1500) and two widely-used datasets (ICDAR 2015 and MSRA-TD500) demonstrate that the proposed method outperforms all stateof-the-art methods by a large margin in detecting curved texts, and achieves very competitive performances in detecting multi-oriented texts. Furthermore, based on the cross-dataset evaluations, TextField also generalizes well to unseen datasets. In the future, we would explore more robust text superpixel grouping strategy (e.g., via explicitly learning the text center line) to further boost TextField, and investigate the common challenges faced by all state-of-the-art text detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Horizontal box (b) Rotated rectangle (c) Quadrilateral (d) Text mask (e) Predicted mask (f) Direction field</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Some irregular scene text detection results (enclosed by green contours) on some challenging images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Pipeline of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Illustration of the proposed post-processing on a test image. (a): Directions on candidate text pixels; (b): Text superpixels (in different color) and their representatives (in white); (c): Dilated and grouped representatives of text superpixels; (d): Labels of filtered representatives; (e): Candidate text instances; (f) Final segmented text instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 S 9 while S = ∅ do 10 p 19 R</head><label>891019</label><figDesc>.push(p), l ← l + 1 ; ← S.pop(), visited(p ) ← True, L(p ) ← l ;11 P(p ) ← N ∠V pred (p ) (p ) ; 12 foreach q ∈ N (p ) do 13 if q ∈ C and not visited(q) then 14 if q = N ∠V pred (p ) (p ) or p = N ∠V pred (q) (q) then15 S.push(q) ; 16 //grouping text superpixels via their representatives 17 foreach p ∈ C do 18 if P(p) = p 0 then ← R ∪ p, M(p) ← 1 ; 20 M ← δ k1 (M) ; 21 M ← CC Labeling(M) ; 22 //text instance filtering by the shape and size 23 M ← Filter unblanced Text(M, R, λ r ) ; 24 foreach r ∈ R do 25 M ← Propagate Label(M, L, r) ; 26 M ← φ k2 (M) ; 27 M ← Filter Small Regions(M, λ a ) ; 28 return M ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization of learned direction field and some involved post-processing steps on test images from SCUT-CTW500 in (a), Total-Text in (b), IC15 in (c), and MSRA-TD500 in (d), respectively. From left to right: input images, directions on candidate text pixels, text superpixels (in different color) and their representatives (in white), labels (in different color) of filtered representatives, and final segmented instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Some qualitative detection results on SCUT-CTW500 in (a), Total-Text in (b), IC15 in (c), and MSRA-TD500 in (d). The arbitrary-shaped texts are correctly detected with accurate text instance boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Some failure examples. Green contours: correct detections; Red contours: missing ground truths; Blue contours: false detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Xu, Yukang Wang, Wei Zhou, and Xiang Bai are with the School of Electronic Information and Communications, Huazhong University of Science and Technology (HUST), Wuhan, 430074, China. Yongpan Wang and Zhibo Yang are with Alibaba Group. Email: {yongchaoxu, wangyk, weizhou, xbai}@hust.edu.cn, yongpan@taobao.com, zhibo.yzb@alibabainc.com. (Corresponding author: Xiang Bai.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The network architecture is presented</figDesc><table><row><cell>Input Image</cell><cell></cell><cell>Output Maps</cell></row><row><cell></cell><cell></cell><cell>×4</cell></row><row><cell>VGG16 stage1 64, /2</cell><cell></cell><cell>1×1, 2</cell></row><row><cell>VGG16 stage2</cell><cell></cell><cell>1×1, 512</cell></row><row><cell>128, /2</cell><cell></cell><cell>1×1, 512</cell></row><row><cell>VGG16 stage3 256, /2</cell><cell>5×5, 256 1×1, 256 1×1, 256</cell><cell>concat</cell></row><row><cell>VGG16 stage4 512, /2</cell><cell>5×5, 512 1×1, 512 1×1, 256</cell><cell>×2</cell></row><row><cell></cell><cell></cell><cell>×4</cell></row><row><cell>VGG16 stage5 512</cell><cell>5×5, 512 1×1, 512 1×1, 256</cell><cell></cell></row></table><note>Fig. 5: Network architecture. We adopt the pre-trained VGG16 [61] as the backbone network and multi-level feature fusion to capture multi-scale text instances. The network is trained to predict dense per-pixel direction field.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Quantitative results of different methods evaluated on SCUT-CTW1500. * indicates the result obtained from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Quantitative results of different methods evaluated on Total-Text.</figDesc><table><row><cell>Methods</cell><cell>recall</cell><cell cols="2">precision f-measure</cell></row><row><cell cols="2">Ch'ng et al. [41] 0.400</cell><cell>0.330</cell><cell>0.360</cell></row><row><cell>Liao et al. [24]</cell><cell>0.455</cell><cell>0.621</cell><cell>0.525</cell></row><row><cell>TextField (Ours)</cell><cell>0.799</cell><cell>0.812</cell><cell>0.806</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison of methods on ICDAR2015 Incidental Scene Text. † means that the base net of the model is not VGG16. * stands for multi-scale version.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison of methods on MSRA-TD500. † stands for the base net of the model is not VGG16.</figDesc><table><row><cell>Methods</cell><cell>recall</cell><cell>precision</cell><cell>f-measure</cell></row><row><cell>He et al. [70]</cell><cell>0.610</cell><cell>0.760</cell><cell>0.690</cell></row><row><cell>EAST [25]</cell><cell>0.616</cell><cell>0.817</cell><cell>0.702</cell></row><row><cell>ITN [32]</cell><cell>0.656</cell><cell>0.803</cell><cell>0.722</cell></row><row><cell cols="2">Zhang et al. [37] 0.670</cell><cell>0.830</cell><cell>0.740</cell></row><row><cell>RRPN [29]</cell><cell>0.680</cell><cell>0.820</cell><cell>0.740</cell></row><row><cell>He et al.  † [27]</cell><cell>0.700</cell><cell>0.770</cell><cell>0.740</cell></row><row><cell>Yao et al. [38]</cell><cell>0.753</cell><cell>0.765</cell><cell>0.759</cell></row><row><cell>EAST  † [25]</cell><cell>0.674</cell><cell>0.873</cell><cell>0.761</cell></row><row><cell>Wu et al. [39]</cell><cell>0.780</cell><cell>0.770</cell><cell>0.770</cell></row><row><cell>SegLink [34]</cell><cell>0.700</cell><cell>0.860</cell><cell>0.770</cell></row><row><cell>RRD [31]</cell><cell>0.730</cell><cell>0.870</cell><cell>0.790</cell></row><row><cell>Lyu et al. [35]</cell><cell>0.762</cell><cell>0.876</cell><cell>0.815</cell></row><row><cell>MCN [36]</cell><cell>0.790</cell><cell>0.880</cell><cell>0.830</cell></row><row><cell>TextField (Ours)</cell><cell>0.759</cell><cell>0.874</cell><cell>0.813</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Cross-dataset evaluations of different methods on corresponding word-level and line-level datasets. Tab. III and Tab. IV, the proposed TextField is also capable to accurately detect multi-oriented texts in both line-level and word-level. This demonstrates the versatility of the proposed TextField.</figDesc><table><row><cell>Methods</cell><cell>recall</cell><cell cols="2">Total-Text (train on IC15) precision f-measure</cell></row><row><cell>SegLink [34]</cell><cell>0.332</cell><cell>0.356</cell><cell>0.344</cell></row><row><cell>EAST [25]</cell><cell>0.431</cell><cell>0.490</cell><cell>0.459</cell></row><row><cell>TextField (Ours)</cell><cell>0.652</cell><cell>0.615</cell><cell>0.633</cell></row><row><cell>Methods</cell><cell>recall</cell><cell cols="2">IC15 (train on Total-Text) precision f-measure</cell></row><row><cell>TextField (Ours)</cell><cell>0.660</cell><cell>0.771</cell><cell>0.711</cell></row><row><cell>Methods</cell><cell cols="3">SCUT-CTW1500 (train on TD500) recall precision f-measure</cell></row><row><cell>TextField (Ours)</cell><cell>0.700</cell><cell>0.753</cell><cell>0.726</cell></row><row><cell>Methods</cell><cell cols="3">MSRA-TD500 (train on SCUT-CTW1500) recall precision f-measure</cell></row><row><cell>TextField (Ours)</cell><cell>0.758</cell><cell>0.853</cell><cell>0.803</cell></row><row><cell>evaluations in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text detection in stores using a repetition prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>of IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scene text recognition in mobile applications by character descriptor and structure configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2972" to="2982" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detection and recognition of text embedded in online images via neural context models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI Conf. on Artificial Intelligence</title>
		<meeting>of the AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4103" to="4110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing text-based traffic guide panels with cascaded localization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="109" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded segmentationdetection networks for text-based traffic sign detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A hybrid approach to detect and localize texts in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="800" to="813" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scene text localization using gradient local correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Document Analysis and Recognition</title>
		<meeting>of International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1380" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1241" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced mser trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-orientation scene text detection with adaptive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1930" to="1937" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scene text extraction based on edges and support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="135" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multioriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3454" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI Conf. on Artificial Intelligence</title>
		<meeting>of the AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3047" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="745" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4950" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geometry-aware scene text detection with instance transformation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1381" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3482" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7553" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning markov clustering networks for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6936" to="6944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-organized text detection with minimal post-processing via border learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5010" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-scale fcn with cascaded instance aware segmentation for arbitrary oriented word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Document Analysis and Recognition</title>
		<meeting>of International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wetext: Scene text detection under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accurate scene text detection through border semantics awareness and bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An end-to-end textspotter with explicit alignment and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5020" to="5029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Verisimilar image synthesis for accurate detection and recognition of texts in scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="249" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2848939</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Textcatcher: a method to detect curved and challenging text in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fabrizio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert-Seidowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calarasanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boissel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="117" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02170</idno>
		<title level="m">Detecting curve text in the wild: New dataset and new solution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Antiextensive connected operators for image and sequence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliveras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Building the component tree in quasi-linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Couprie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3531" to="3539" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A comparative review of component tree computation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Carlinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Géraud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3885" to="3895" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deepflux for skeletons in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12608</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: Recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-script text extraction from natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Document Analysis and Recognition</title>
		<meeting>of International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="467" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Characterness: An indicator of text in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1666" to="1677" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Directional edge boxes: Exploiting inner normal direction cues for effective object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="701" to="713" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2858" to="2866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Document Analysis and Recognition</title>
		<meeting>of International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM international conference on Multimedia</title>
		<meeting>ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Text-attentional convolutional neural network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2529" to="2541" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
