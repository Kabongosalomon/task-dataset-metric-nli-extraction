<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthesizing the Unseen for Zero-shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<email>munawar.hayat@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">MBZ University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">North South University</orgName>
								<address>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<email>salman.khan@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">MBZ University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed</forename><surname>Waqas Zamir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<email>fahad.khan@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">MBZ University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Synthesizing the Unseen for Zero-shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Zero-shot object detection</term>
					<term>generative adversarial learning</term>
					<term>visual-semantic relationships</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The existing zero-shot detection approaches project visual features to the semantic domain for seen objects, hoping to map unseen objects to their corresponding semantics during inference. However, since the unseen objects are never visualized during training, the detection model is skewed towards seen content, thereby labeling unseen as background or a seen class. In this work, we propose to synthesize visual features for unseen classes, so that the model learns both seen and unseen objects in the visual domain. Consequently, the major challenge becomes, how to accurately synthesize unseen objects merely using their class semantics? Towards this ambitious goal, we propose a novel generative model that uses class-semantics to not only generate the features but also to discriminatively separate them. Further, using a unified model, we ensure the synthesized features have high diversity that represents the intra-class differences and variable localization precision in the detected bounding boxes. We test our approach on three object detection benchmarks, PASCAL VOC, MSCOCO, and ILSVRC detection, under both conventional and generalized settings, showing impressive gains over the state-of-the-art methods. Our codes are available at https://github.com/nasir6/zero_shot_detection</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is a challenging problem that seeks to simultaneously localize and classify object instances in an image <ref type="bibr" target="#b0">[1]</ref>. Traditional object detection methods work in a supervised setting where a large amount of annotated data is used to train models. Annotating object bounding boxes for training such models is a labor-intensive and expensive process. Further, for many rare occurring objects, we might not have any training examples. Humans, on the other hand, can easily identify unseen objects solely based upon the objects' attributes or their natural language description. Zero Shot Detection (ZSD) is a recently introduced paradigm which enables simultaneous localization and classification of arXiv:2010.09425v1 [cs.CV] <ref type="bibr" target="#b18">19</ref> Oct 2020 previously unseen objects. It is arguably the most extreme case of learning with minimal supervision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>ZSD is commonly accomplished by learning to project visual representations of different objects to a pre-defined semantic embedding space, and then performing nearest neighbor search in the semantic space at inference <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Since the unseen examples are never visualized during training, the model gets significantly biased towards the seen objects <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, leading to problems such as confusion with background and mode collapse resulting in high scores for only some unseen classes. In this work, we are motivated by the idea that if an object detector can visualize the unseen data distribution, the above-mentioned problems can be alleviated. To this end, we propose a conditional feature generation module to synthesize visual features for unseen objects, that are in turn used to directly adapt the classifier head of Faster-RCNN <ref type="bibr" target="#b0">[1]</ref>. While such feature synthesis approaches have been previously explored in the context of zero-shot classification, they cannot be directly applied to ZSD due to the unique challenges in detection setting such as localizing multiple objects per image and modeling diverse backgrounds.</p><p>The core of our approach is a novel feature synthesis module, guided by semantic space representations, which is capable of generating diverse and discriminative visual features for unseen classes. We generate exemplars in the feature space and use them to modify the projection vectors corresponding to unseen classes in the Faster-RCNN classification head. The major contributions of the paper are: (i) it proposes a novel approach to visual feature synthesis conditioned upon class-semantics and regularized to enhance feature diversity, (ii) feature generation process is jointly driven by classification loss in the semantic space for both seen and unseen classes, to ensure that generated features are discriminant and compatible with the object-classifier, (iii) extensive experiments on Pascal VOC, MSCOCO and ILSVRC detection datasets to demonstrate the effectiveness of the proposed method. For instance, we achieve a relative mAP gain of 53% on MS-COCO dataset over existing state-of-the-art on ZSD task. Our approach is also demonstrated to work favorably well for Generalized ZSD (GZSD) task that aims to detect both seen and unseen objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Zero-shot Recognition: The goal of Zero shot learning (ZSL) is to classify images of unseen classes given their textual semantics in the form of wordvecs <ref type="bibr" target="#b7">[8]</ref>, text-descriptions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref> or human annotated attributes <ref type="bibr" target="#b9">[10]</ref>. This is commonly done by learning a joint embedding space where semantics and visual features can interact. The embeddings can be learnt to project from visual-to-semantic <ref type="bibr" target="#b10">[11]</ref>, or semantic-to-visual space <ref type="bibr" target="#b7">[8]</ref>. Some methods also project both visual and semantic features into a common space <ref type="bibr" target="#b11">[12]</ref>. The existing methods which learn a projection or embedding space have multiple inherent limitations such as the hubness problem <ref type="bibr" target="#b12">[13]</ref> caused by shrinked low dimensional semantic space with limited or no diversity to encompass variations in the visual image space. These methods are therefore prone to mis-classify unseen samples into seen due to nonexistence of training samples for the unseen. Recently, generative approaches deploying variational auto-encoders (VAEs) or generative adverserial networks (GANs) have shown promises for ZSL <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. These approaches model the underlying data distribution of visual feature space by training a generator and a discriminator network that compete in a minimax game, thereby synthesizing features for unseen classes conditioned on their semantic representations. Zero-shot Detection: The existing literature on zero shot learning is dominated by zero shot classification (ZSC). Zero Shot Detection (ZSD), first introduced in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, is significantly more challenging compared with ZSC, since it aims to simultaneously localize and classify an unseen object. <ref type="bibr" target="#b1">[2]</ref> maps visual features to a semantic space and enforces max-margin constraints along-with meta-class clustering to enhance inter-class discrimination. The authors in <ref type="bibr" target="#b2">[3]</ref> incorporate an improved semantic mapping for the background in an iterative manner by first projecting the seen class visual features to their corresponding semantics and then the background bounding boxes to a set of diverse unseen semantic vectors. <ref type="bibr" target="#b3">[4]</ref> learns an embedding space as a convex combination of training class wordvecs. <ref type="bibr" target="#b4">[5]</ref> uses a Recurrent Neural Network to model natural language description of objects in the image.</p><p>Unlike ZSC, synthetic feature generation for unseen classes is less investigated for ZSD and only <ref type="bibr" target="#b17">[18]</ref> augments features. Ours is a novel feature synthesis approach that has the following major differences from <ref type="bibr" target="#b17">[18]</ref> (i) For feature generation, we only train a single GAN model, in comparison to <ref type="bibr" target="#b17">[18]</ref> which trains three isolated models. Our unified GAN model is capable of generating diverse and distinct features for unseen classes. (ii) We propose to incorporate a semantics guided loss function, which improves feature generation capability of the generator module for unseen categories. (iii) To enhance diversification amongst the generated features, we incorporate a mode seeking regularization term. We further compare our method directly with <ref type="bibr" target="#b17">[18]</ref> and show that it outperforms <ref type="bibr" target="#b17">[18]</ref> by a significant margin, while using a single unified generation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Motivation: Most of the existing approaches for ZSD address this problem in the semantic embedding space. This means that the visual features are mapped to semantic domain where unseen semantics are related with potential unseen object features to predict decision scores. We identify three problems with this line of investigation. (i) Unseen background confusion: Due to the low objectness scores for unseen objects, they frequently get confused as background during inference. To counter this, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref> use external data in the form of object annotations or vocabulary that are neither seen nor unseen. (ii) Biasness problem: Since, unseen objects are never experienced during training, the model becomes heavily biased towards seen classes. For this, approaches usually design specialized loss functions to regularize learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. (iii) Hubness problem: Only a few unseen classes get the highest scores in most cases. Addressing the problem in semantic space intensifies the hubness issue <ref type="bibr" target="#b19">[20]</ref>. Very recently, GTNet <ref type="bibr" target="#b17">[18]</ref> attempted to address these issues in the visual domain instead of the semantic space. Similar to <ref type="bibr" target="#b14">[15]</ref>, they generate synthesized features to train unseen classes in a supervised manner. We identify two important drawbacks in this approach. (i) They train multiple GAN models to incorporate variance due to intra-class differences and varying overlaps with ground-truth (IoU). These generative models are trained in a sequential manner, without an end-to-end learning mechanism, making it difficult to fix errors in early stages. (ii) In addition to synthesized unseen object features, they need to generate synthesized background features. As the background semantic is not easy to define, synthesized background features become too noisy than that of object features, thereby significantly hindering the learning process. In this paper, we attempt to solve this problem by training one unified GAN model to generate synthesized unseen object features that can be used to train with real background features without the help of synthesized background features. Further, without requiring multiple sequential generative models to inject feature diversity <ref type="bibr" target="#b17">[18]</ref>, we propose a simple regularization term to promote diversity in the synthesized features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Problem Formulation: Consider the train set X s contains image of seen objects and the test set X u contains images of seen+unseen objects. Each image can have multiple objects. Let's denote Y s = {1, · · · S} and Y u = {S +1, · · · S +U } respectively as the label sets for seen and unseen classes. Note that S and U denote total number of seen and unseen classes respectively, and Y S ∩ Y u = ∅. At training, we are given annotations in terms of class labels y ∈ Y s and bounding-box coordinates b ∈ R 4 for all seen objects in X s . We are also given semantic embeddings W s ∈ R d×S and W u ∈ R d×U for seen and unseen classes respectively (e.g., Glove <ref type="bibr" target="#b20">[21]</ref> and fastText <ref type="bibr" target="#b21">[22]</ref>). At inference, we are required to correctly predict the class-labels and bounding-box coordinates for the objects in images of X u . For ZSD settings, only unseen predictions are required, while for generalized ZSD, both seen and unseen predictions must be made.</p><p>We outline different steps used for our generative ZSD pipeline in Alg. 1 and <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates our method. The proposed ZSD framework is designed to work with any two-stage object detector. For this paper, we implement Faster-RCNN model with ResNet-101 backbone. We first train the Faster-RCNN model φ faster-rcnn on the training images X s comprising of only seen objects and their corresponding ground-truth annotations. Given an input image x ∈ X s , it is first represented in terms of activations of a pre-trained ResNet-101. Note that the backbone ResNet-101 was trained on ImageNet data by excluding images belonging to the overlapping unseen classes of the evaluated ZSD datasets. The extracted features are feed-forwarded to the region proposal network (RPN) of Faster-RCNN, which generates a set of candidate object bounding box proposals at different sizes and aspect ratios. These feature maps and the proposals are then mapped through an RoI pooling layer, to achieve a fixed-size representation for each proposal. Let's denote the feature maps corresponding to K bounding Once φ faster-rcnn is trained on the seen data X s , we use it to extract features for seen object anchor boxes. All candidate proposals with an intersection-overunion (IoU) ≥ 0.7 are considered as foreground, whereas the ones with IoU ≤ 0.3 are considered backgrounds. For N tr training images in X s , we therefore get bounding-box features F s ∈ R 1024×K.Ntr and their class-labels Y s ∈ R K.Ntr . Next, we learn a unified generative model to learn the relationship between visual and semantic domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unified Generative Model</head><p>Given object features F s , their class-labels Y s , and semantic vectors W s for seen training data X s , our goal is to learn a conditional generator G : W × Z → F, which takes a class embedding w ∈ W and a random noise vector z ∼ N (0, 1) ∈ R d sampled from a Gaussian distribution and outputs the featuresf ∈ F. The generator G learns the underlying distribution of the visual features F s and their relationship with the semantics W s . Once trained, the generator G is used to generate unseen class visual features. Specifically, our feature generation module optimizes the the following objective function,</p><formula xml:id="formula_0">min G max D α 1 L WGAN + α 2 L Cs + α 3 L Cu + α 4 L div ,<label>(1)</label></formula><p>where L WGAN minimizes the Wasserstein distance, conditioned upon class semantics, L Cs ensures the seen class features generated by G are suitable and aligned with a pre-trained classifier φ cls , and L Cu ensures the synthesized features for unseen classes are aligned with their semantic representations W u . α 1 , α 2 , α 3 , α 4 are the weighting hyper-parameters optimized on a held-out validation set. The proposed approach is able to generate sufficiently discriminative visual features to train the softmax classifier. Each term in Eq. 1 is discussed next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conditional Wasserstein GAN</head><p>We build upon improved WGAN <ref type="bibr" target="#b22">[23]</ref> and extend it to conditional WGAN (cW-GAN), by integrating the class embedding vectors. The loss L WGAN is given by,</p><formula xml:id="formula_1">L WGAN = E[D(f , y)] − E[D(f , y)] + λE[(||∇f D(f , y)|| 2 − 1) 2 ],<label>(2)</label></formula><p>where f are the real visual features,f = G(w, z) denotes the synthesized visual features conditioned upon class semantic vector w ∈ W s ,f = αf + (1 − α)f , α ∼ N (0, 1) and λ is the penalty coefficient. The first two terms provide an approximation of the Wasserstein distance, while the third term enforces gradients to a unit norm along the line connecting pairs of real and generated features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semantically Guided Feature Generation</head><p>Our end goal is to augment visual features using the proposed generative module such that they enhance discrimination capabilities of the classifier φ cls . In order to encourage the synthesized featuresf = G(w, z) to be meaningful and discriminative, we optimize the logliklihood of predictions for synthesized seenclass features,</p><formula xml:id="formula_2">L Cs = −E[log p(y|G(w, z); φ cls )], s.t., w ∈ W s ,<label>(3)</label></formula><p>where, y ∈ Y s denotes the ground-truth seen class labels, and p(y|G) is the class prediction probability computed by the linear softmax classifier φ cls . Note that φ cls was originally trained on the seen data X s and is kept frozen for the purpose of computing L Cs . While the conditional Wasserstein GAN captures underlying data distribution of visual features, the L Cs term enforces additional constraint and acts as a regularizer to enforce the generated features to be discriminative. The L Cs term in Eq. 3 can act as a regularizer for seen classes only. This is because L Cs employs pre-trained φ cls which was learnt for seen data. In order to enhance the generalization capability of our generator G towards unseen classes, we propose to incorporate another loss term L Cu . For this purpose, we redefine the classifier head in terms of class semantics, as φ Ws-cls : f − → fc − → W s − → softmax − → y pr , where f ∈ R 1024 are the input features, fc is the learnable fully-connected layer with weight matrix W fc ∈ R 1024×d and bias b fc ∈ R d , W s ∈ R d×S are the fixed non-trainable seen class semantics. The outputs of fc layer are matrix multiplied with W s followed by softmax operation to compute class predictions y pr . The classifier φ Ws-cls is trained on the features F s and ground-truth labels Y s of seen class bounding boxes. We can then easily define an unseen classifier φ Wu-cls by replacing the semantics matrix W s in φ Ws-cls with W u . The semantics guided regularizer loss term L Cu for synthesized unseen samples is then given by,</p><formula xml:id="formula_3">L Cu = −E[log p(y|G(w, z); φ Wu-cls )], s.t., w ∈ W u .<label>(4)</label></formula><p>The L Cu term therefore incorporates the unseen class-semantics information into feature synthesis, by ensuring that unseen features, after being projected onto fc layer are aligned with their respective semantics vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Enhancing Synthesis Diversity</head><p>Variations in synthesized features are important for learning a robust classifier. Our cWGAN based approach maps a single class semantic vector to multiple visual features. We observed that the conditional generation approach can suffer from mode collapse <ref type="bibr" target="#b23">[24]</ref> and generate similar output features conditioned upon prior semantics only, where the noise vectors (responsible for variations in the generated features) get ignored. In order to enhance the diversity of synthesized features, we adapt the mode seeking regularization which maximizes the distance between generations with respect to their corresponding input noise vectors <ref type="bibr" target="#b24">[25]</ref>. For this purpose, we define the diversity regularization loss L div as,</p><formula xml:id="formula_4">L div = E[||G(w, z 1 ) − G(w, z 2 )|| 1 /||z 1 − z 2 || 1 ].<label>(5)</label></formula><p>L div encourages the G to diversify the synthesized feature space and enhance chances of generating features from minor modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Unseen Synthesis and Detection</head><p>Optimizing the loss defined in Eq. 1 results in conditional visual feature generator G. We can synthesize an arbitrarily large number of featuresf u = G(z, w) for each unseen class by using its corresponding class semantics vector w ∈ W u and a random noise vector z ∼ N (0, 1). Repeating the process for all unseen classes, we get synthesized featuresF u and their corresponding class-labels Y u , which can then be used to update softmax classifier φ cls of φ faster-rcnn for unseen classes. At inference, a simple forward pass through φ faster-rcnn predicts both class-wise confidence scores and offsets for the bounding-box coordinates. We consider a fixed number of proposals from the RPN (100 in our case) and apply non-maximal suppression (NMS) with a threshold of 0.5 to obtain final detections. The classification confidence for the proposals are directly given by φ cls , whereas the bounding-box offset coordinates of an unseen class are estimated by the predictions for the seen class with maximum classification response. We observe that this is a reasonable assumption since visual features for the unseen class and its associated confusing seen class are similar. For the case of Generalized zero-shot-detection (GZSD), we simply consider all detections from seen and unseen objects together, whereas for ZSD, detections corresponding to seen objects are only considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Datasets: We extensively evaluate our proposed ZSD method on three popular object detection datasets: MSCOCO 2014 <ref type="bibr" target="#b25">[26]</ref>, ILSVRC Detection 2017 <ref type="bibr" target="#b26">[27]</ref> and PASCAL VOC 2007/2012 <ref type="bibr" target="#b27">[28]</ref>. For MSCOCO, we use 65/15 seen/unseen split proposed in <ref type="bibr" target="#b18">[19]</ref>. As argued in <ref type="bibr" target="#b18">[19]</ref>, this split exhibits rarity and diverseness of the unseen classes in comparison to another 48/17 split proposed in <ref type="bibr" target="#b2">[3]</ref>. We use 62,300 images for training set and 10,098 images from the validation set for testing ZSD and GZSD. For ILSVRC Detection 2017, we follow the 177/23 seen/unseen split proposed in <ref type="bibr">[</ref> We ignore other bounding-boxes with an IoU between 0.3 and 0.7, since a more accurate bounding box helps GAN in learning discriminative features. We first train our Faster-RCNN model on seen data for 12 epochs using standard procedure as in <ref type="bibr" target="#b28">[29]</ref>. Our category classifier φ cls , and bounding-box regressor φ reg both have a single fully-connected layer. The trained model is then used to extract visual features corresponding to bounding-boxes of ground-truth seen objects. We then train our generative model to learn the underlying data distribution of the extracted seen visual features. The generator G and discriminator D of our GAN model are simple singlelayered neural networks with 4096 hidden units. Through out our experiments, the loss re-weighting hyper-parameters in Eq. 1 are set as, α 1 = 1.0, α 2 = 0.1, α 3 = 0.1, α 4 = 1.0, using a small held-out validation set. The noise vector z has the same dimensions as the class-semantics vector w ∈ R d and is drawn from a unit Gaussian distribution with zero mean. We use λ = 10 as in <ref type="bibr" target="#b22">[23]</ref>. For training of our cWGAN model, we use Adam optimizer with learning rate 10 −4 , β 1 = 0.5, β 2 = 0.999. The loss term L Cu is included after first 5 epochs, when the generator G has started to synthesize meaningful features. Once the generative module is trained, we synthesize 300 features for each unseen class, conditioned upon their class-semantics, and use them to train φ cls for 30 epochs using Adam optimizer. To encode class-labels, unless mentioned otherwise, we use the FastText <ref type="bibr" target="#b29">[30]</ref> embedding vectors learnt on large corpus of non-annotated text. The implementation of the proposed method in Pytorch is available at https://github.com/nasir6/zero_shot_detection Evaluation metrics: Following previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref>, we report recall@100 (RE) and mean average precision (mAP) with IoU=0.5. We also report perclass average prevision (AP) to study category-wise performance. For GZSD, we report Harmonic Mean (HM) of performances for seen and unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons with the State-of-the-Art</head><p>Comparison methods: We compare our method against a number of recently proposed state-of-the-art ZSD and GZSD methods. These include: (a) SB, LAB <ref type="bibr" target="#b2">[3]</ref>, which is a background-aware approach that considers external annotations from object instances belonging to neither seen or unseen. This extra information helps SB, LAB <ref type="bibr" target="#b2">[3]</ref> to address the confusion between unseen and background. (b) DSES <ref type="bibr" target="#b2">[3]</ref> is a version of above approach that does not use background-aware representations but employs external data sources for background. (c) HRE <ref type="bibr" target="#b3">[4]</ref>: A YOLO based end-to-end ZSD approach based on the convex combination of region embeddings. (d) SAN <ref type="bibr" target="#b1">[2]</ref>: A Faster-RCNN based ZSD approach that takes advantage of super-class information and a max-margin loss to understand unseen objects better. (e) PL-48, PL-65 <ref type="bibr" target="#b18">[19]</ref>: A RetinaNet based ZSD approach that uses polarity loss for better alignment of visual features and semantics. (f ) ZSDTD <ref type="bibr" target="#b4">[5]</ref>: This approach uses textual description instead of a single-word class-label to define semantic representation. The additional textual description enriches the semantic space and helps to better relate semantics with the visual features. (g) GTNet <ref type="bibr" target="#b17">[18]</ref>: uses multiple GAN models alongwith textual descriptions similar to <ref type="bibr" target="#b4">[5]</ref>, to generate unseen features to train a Faster-RCNN based ZSD model in a supervised manner. (h) Baseline: The baseline method trains a standard Faster-RCNN model for seen data X s . To extend it to unseen classes for ZSD, it first gets seen predictions p s , and then project them onto class semantics to get unseen predictions p u = W u W T s p s as in <ref type="bibr" target="#b18">[19]</ref>. (i) Ours: This is our proposed ZSD approach.</p><p>MSCOCO results: Our results and comparisons with different state-of-the-art methods for ZSD and GZSD on MSCOCO dataset are presented in <ref type="table">Table 1</ref>. (a) ZSD results: The results demonstrate that our proposed method achieves a significant gain on both metrics (mAP and RE) over the existing methods on ZSD setting. The gain is specifically pronounced for the mAP metric, which is more challenging and meaningful to evaluate object detection algorithms. This is because mAP penalizes false positives while the RE measure does not impose any penalty on such errors. Despite the challenging nature of mAP metric, <ref type="table">Table 1</ref>. ZSD and GZSD performance of different methods on MSCOCO in terms of mAP and recall (RE). Note that our proposed feature synthesis based approach achieves a significant gain over the existing state-of-the-art. For the mAP metric, compared with the second best method PL-65 <ref type="bibr" target="#b18">[19]</ref>, our method shows a relative gain of 53% on ZSD and 38% on harmonic mean of seen and unseen for GZSD.  our method achieves a relative mAP gain of 53% over the second-best method (PL <ref type="bibr" target="#b18">[19]</ref>). We attribute such remarkable improvement to the fact that our approach addresses the zero shot learning problem by augmenting the visual features. In contrast, previous approaches such as SB <ref type="bibr" target="#b2">[3]</ref>, DSES <ref type="bibr" target="#b2">[3]</ref>, PL <ref type="bibr" target="#b18">[19]</ref> map visual features to the semantic space that limits their flexibility to learn strong representations mainly due to the noise in semantic domain. In comparison, our approach helps in reducing the biases towards the seen classes during training, avoids unseen-background confusion, and minimizes the hubness problem. In <ref type="figure">Fig. 2</ref>, we further show comparisons for ZSD recall@100 rates by varying the IoU. Note that the compared methods in <ref type="figure">Fig. 2</ref> use additional information in the form of textual description of concepts instead of a single-word class name. Even though, our proposed method uses much simpler semantic information (only semantic vectors for class labels), the results in <ref type="figure">Fig. 2</ref>   <ref type="bibr" target="#b2">[3]</ref>, ZSDTD <ref type="bibr" target="#b4">[5]</ref>, GTNet <ref type="bibr" target="#b17">[18]</ref> in terms of Recall@100 rates for different IoU settings on MSCOCO dataset. The proposed method consistently shows improved performance over existing state-of-the-art methods.</p><p>method consistently outperforms several established methods by a large margin for a variety of IoU settings. This comparison includes a recent generative ZSD approach, GTNet <ref type="bibr" target="#b17">[18]</ref>, that employs an ensemble of GANs to synthesize features.</p><p>(b) GZSD results: Our GZSD results in <ref type="table">Table 1</ref> also achieve a significant boost in performance. The generated synthesized features allow training of the detection model in a supervised manner. In this way, unseen instances get equal emphases as seen class objects during training. We note that the GZSD setting is more challenging and realistic since both seen and unseen classes are present at inference. An absolute HM mAP gain of 6.9% for GZSD is therefore quite significant for our proposed method. Compared with the baseline, which projects visual features to semantic space, our results demonstrate the effectiveness of augmenting the visual space, and learning a discriminative classifier for more accurate classification. These baseline results further indicate the limitations of mapping multiple visual features to a single class-semantic vector. One interesting trend is that the baseline still performs reasonably well according to the RE measure (in some cases even above the previous best methods), however the considerably low mAP scores tell us that the inflated performance from the baseline is prone to many false positives, that are not counted in the RE measure. For this reason, we believe the mAP scores are a more faithful depiction of ZSD methods.</p><p>(c) Class-wise performances: Our class-wise AP results on MSCOCO in <ref type="table" target="#tab_1">Table 2</ref> show that the performance gain for the proposed method is more pronounced for 'train', 'bear ' and 'toilet' classes. Since our feature generation is conditioned upon class-semantics, we observe that the feature synthesis module generates more meaningful features for unseen classes which have similar semantics in the seen data. The method shows worst performance for classes 'parking-meter ', 'frisbee', 'hot dog' and 'toaster '. These classes do not have close counterparts among the seen classes, which makes their detection harder.</p><p>(d) Qualitative results: <ref type="figure" target="#fig_3">Fig. 3</ref> shows some examples of detections from our method both for ZSD (top 2 rows) and GZSD (bottom 2 rows) settings. The visual results demonstrate the effectiveness of the proposed method in localizing unseen objects, and its capability to detect multiple seen+unseen objects with challenging occlusions and background clutter in real-life images. <ref type="table" target="#tab_3">Table 3</ref>, we compare different methods on PAS-CAL VOC dataset based on the setting mentioned in <ref type="bibr" target="#b3">[4]</ref>. The results suggest that the proposed method achieves state-of-the-art ZSD performance. A few examples images on PASCAL VOC shown in <ref type="figure" target="#fig_6">Fig. 5</ref> demonstrate the capability of our method to detect multiple unseen objects in real-life scenarios. The results in <ref type="table" target="#tab_3">Table 3</ref> further indicate that in addition to the unseen detection case, our method performs very well in the traditional seen detection task. We outperform the current best model PL <ref type="bibr" target="#b18">[19]</ref> by a significant margin, i.e., 73.6% vs. 63.5% for seen detection and 64.9% vs. 62.1% for unseen detection. A t-SNE visualization of our synthesized features for unseen classes is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. We observe that our generator can effectively capture the underlying data distribution of visual    features. The similar classes occur in close proximity of each other. We further observe that the synthesized features form class-wise clusters that are distinctive, thus aiding in learning a discriminative classifier on unseen classes. Synthesized features for similar classes (bus and train) are however sometimes confused with each other due to high similarity in their semantic space representation. ILSVRC DET 2017 results: In <ref type="table" target="#tab_5">Table 4</ref>, we report ZSD results on ILSVRC Detection dataset based on the settings mentioned in <ref type="bibr" target="#b1">[2]</ref>. We can notice from the results that, in most of the object categories, we outperform our closed competitor SAN <ref type="bibr" target="#b1">[2]</ref> by a large margin. Note that for a fair comparison, we  do not compare our method with reported results in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>, since both these methods use additional information in the form of textual description of classlabels. It has been previously shown in <ref type="bibr" target="#b4">[5]</ref> that the additional textual description information boosts performance across the board. For example, in their paper, SAN <ref type="bibr" target="#b1">[2]</ref> reports an mAP of 16.4 using single-word description for class-labels, whereas, <ref type="bibr" target="#b4">[5]</ref> reports an mAP of 20.3 for SAN using multi-word textual description of class-labels. Our improvements over SAN again demonstrates the significance of the proposed generative approach for synthesizing unseen features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC results: In</head><p>Distinct Foreground Bounding Boxes: The seen visual features are extracted based upon the anchor bounding boxes generated by using the groundtruth bounding boxes for seen classes in X s . We perform experiments by changing the definition of background and foreground bounding-boxes. Specifically, we consider two settings: (a) Distinct bounding-boxes: foreground object has a high overlap (IoU ≥ 0.7), and the background has minimal overlap with the object (IoU ≤ 0.3), and (b) Overlapping bounding-boxes: foreground has a medium overlap with the object of interest (IoU &gt; 0.5), and some background boxes have medium overlap with the object (IoU &lt; 0.5). We achieve an mAP of 19.0 vs 11.7 for distinct and overlapping boundig boxes respectively on MSCOCO 65/15 split. This suggests that the generative module synthesizes the most discriminant features when the bounding-boxes corresponding to the real visual features have a high overlap with the respective object and minimal background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The paper proposed a feature synthesis approach for simultaneous localization and categorization of objects in the framework of ZSD and GZSD. The proposed method can effectively learn the underlying visual-feature data distribution, by training a generative adversarial network model conditioned upon classsemantics. The GAN training is driven by a semantic-space unseen classifier, a seen classifier and a diversity enhancing regularizer. The method can therefore synthesize high quality unseen features which are distinct and discriminant for the subsequent classification stage. The proposed framework generalizes well to both seen and unseen objects and achieves impressive performance gains on a number of evaluated benchmarks including MSCOCO, PASCAL VOC and ILSVRC detection datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>The proposed feature synthesis base ZSD method Input: X s , X u , y ∈ Ys, b, Ws, Wu 1: φfaster-rcnn ← Train Faster-RCNN using seen data X s and annotations 2: Fs, Ys ← Extract features for b-boxes of X s using RPN of φfaster-rcnn 3: φWs-cls ← Train φWs-cls using Fs, Ys 4: φWu-cls ← Define φWu-cls using φWs-cls by replacing Ws with Wu 5: G ← Train GAN by optimizing loss in Eq. 1 6:Fu, Yu ← Syntesize features for unseen classes using G and Wu 7: φ cls ← Train φcls usingFu, Yu 8: φfaster-rcnn← Update φfaster-rcnn with φ cls 9: Evaluate φfaster-rcnn on X u Output: Class labels and bbox-coordinates for X u box proposals of an image with f i ∈ R 1024 , i = 1, · · · K. The features f i are then passed through two modules: bounding-box-regressor, and object-classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of proposed generative ZSD approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>48 4.0 28.7 .29 18.0 0.0 13.1 11.3 24.3 13.8 9.6 2.0 1.1 .24 .73 0.0 PL [19] 12.40 20.0 48.2 .63 28.3 13.8 12.4 21.8 15.1 8.9 8.5 .87 5.7 .04 1.7 .03 Ours-Baseline 8.80 1.9 31.8 0.0 59.3 3.8 0.6 0.1 19.6 10.7 2.8 0.0 0.8 0.0 0.0 0.0 Ours 19.0 10.1 48.7 1.2 64.0 64.1 12.2 0.7 28.0 16.4 19.4 0.1 18.7 1.2 0.5 0.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative results on MSCOCO for ZSD (top 2 rows) and GZSD (bottom 2 rows). Seen classes are shown with green and unseen with red. (best seen when zoomed)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>HRE [ 4 ]</head><label>4</label><figDesc>57.9 54.5 68.0 72.0 74.0 48.0 41.0 61.0 48.0 25.0 48.0 73.0 75.0 71.0 73.0 33.0 59.0 57.0 55.0 82.0 55.0 26.0 PL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>A t-SNE visualization of synthesized features by our approach for unseen classes on PASCAL VOC dataset (left) and MSCOCO dataset (right). The generated features form well-separated and distinctive clusters for different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Example unseen detections on PASCAL VOC. (best seen when zoomed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>.3 11.5 14.3 3.0 15.4 2.7 11.4 41.9 16.4 79.6 67.6 14.5 69.5 31.8 30.7 0.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Class-wise AP comparison of different methods on unseen classes of MSCOCO for ZSD. The proposed method shows significant gains for a number of individual classes. Compared with the second best method PL<ref type="bibr" target="#b18">[19]</ref>, our method shows an absolute mAP gain of 6.6%.</figDesc><table><row><cell></cell><cell>Metric</cell><cell></cell><cell cols="2">Method</cell><cell cols="3">Seen/Unseen split</cell><cell>ZSD</cell><cell cols="5">GZSD seen unseen HM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>SB [3]</cell><cell></cell><cell></cell><cell>48/17</cell><cell></cell><cell>0.70</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">DSES [3]</cell><cell></cell><cell>48/17</cell><cell></cell><cell>0.54</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">PL-48 [19]</cell><cell></cell><cell>48/17</cell><cell></cell><cell cols="2">10.01 35.92</cell><cell></cell><cell>4.12</cell><cell cols="2">7.39</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>mAP</cell><cell cols="3">PL-65 [19]</cell><cell></cell><cell>65/15</cell><cell></cell><cell cols="6">12.40 34.07 12.40 18.18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell>65/15</cell><cell></cell><cell cols="2">8.80 36.60</cell><cell></cell><cell>8.80</cell><cell cols="2">14.19</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell>65/15</cell><cell></cell><cell cols="6">19.0 36.90 19.0 25.08</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>SB [3]</cell><cell></cell><cell></cell><cell>48/17</cell><cell></cell><cell>24.39</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">DSES [3]</cell><cell></cell><cell>48/17</cell><cell></cell><cell cols="6">27.19 15.02 15.32 15.17</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">PL-48 [19]</cell><cell></cell><cell>48/17</cell><cell></cell><cell cols="6">43.56 38.24 26.32 31.18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>RE</cell><cell cols="3">PL-65 [19]</cell><cell></cell><cell>65/15</cell><cell></cell><cell cols="6">37.72 36.38 37.16 36.76</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell>65/15</cell><cell></cell><cell cols="6">44.40 56.40 44.40 49.69</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell>65/15</cell><cell></cell><cell cols="6">54.0 57.70 53.90 55.74</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Overall</cell><cell>aeroplane</cell><cell>train</cell><cell>parking meter</cell><cell>cat</cell><cell>bear</cell><cell>suitcase</cell><cell>frisbee</cell><cell>snow-board</cell><cell>fork</cell><cell>sand-wich</cell><cell>hot dog</cell><cell>toilet</cell><cell>mouse</cell><cell>toaster</cell><cell>hair</cell><cell>drier</cell></row><row><cell cols="2">PL-Base [19] 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>mAP scores on PASCAL VOC'07. Italic classes are unseen.</figDesc><table><row><cell>Method</cell><cell>Seen</cell><cell>Unseen</cell><cell>aeroplane</cell><cell>bicycle</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>d.table</cell><cell>horse</cell><cell>motrobike</cell><cell>person</cell><cell>p.plant</cell><cell>sheep</cell><cell>tvmonitor</cell><cell>car</cell><cell>dog</cell><cell>sofa</cell><cell>train</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b18">[19]</ref> 63.5 62.1 74.4 71.2 67.0 50.1 50.8 67.6 84.7 44.8 68.6 39.6 74.9 76.0 79.5 39.6 61.6 66.1 63.7 87.2 53.2 44.1 Ours 73.6 64.9 83.0 82.8 75.1 68.9 63.8 69.5 88.7 65.1 71.9 56.0 82.6 84.5 82.9 53.3 74.2 75.1 59.6 92.7 62.3 45.2</figDesc><table><row><cell>Car Dog Sofa Train</cell><cell>Airplane Cat Hair Drier Toilet Train Snowboard Suitcase Toaster Parking Meter Frisbee Sandwich Mouse Bear Fork Hot Dog</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>ZSD class-wise AP for unseen classes of ILSVRC DET 2017 dataset.</figDesc><table><row><cell>mean</cell><cell>p.box</cell><cell>syringe</cell><cell>harmonica</cell><cell>maraca</cell><cell>burrito</cell><cell>pineapple</cell><cell>electric-fan</cell><cell>iPod</cell><cell>dishwasher</cell><cell>canopener</cell><cell>plate-rack</cell><cell>bench</cell><cell>bowtie</cell><cell>s.trunk</cell><cell>scorpion</cell><cell>snail</cell><cell>hamster</cell><cell>tiger</cell><cell>ray</cell><cell>train</cell><cell>unicycle</cell><cell>golfball</cell><cell>h.bar</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="547" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="384" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Zero-shot object detection by hybrid region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06157</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zero-shot object detection with textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kanhere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8690" to="8697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaussian affinity for maxmargin class imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Striking the right balance with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4247" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Preserving semantic relations for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7603" to="7612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attribute-based classification for zeroshot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="453" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6568</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition using semantics-preserving adversarial embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1043" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature generating networks for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1004" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial training of variational autoencoders for high fidelity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<title level="m">Gtnet: Generative transfer network for zero-shot object detection. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2001</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Polarity loss for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mode seeking generative adversarial networks for diverse image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1429" to="1437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Advances in pretraining distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
