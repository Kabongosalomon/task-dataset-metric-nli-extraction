<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Dynamic Memory Network for Dialogue Act Classification with Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Institute for Data Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Dynamic Memory Network for Dialogue Act Classification with Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-dialog act classification</term>
					<term>dynamic memory net- work</term>
					<term>adversarial training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dialogue Act (DA) classification is a challenging problem in dialogue interpretation, which aims to attach semantic labels to utterances and characterize the speaker's intention. Currently, many existing approaches formulate the DA classification problem ranging from multi-classification to structured prediction, which suffer from two limitations: a) these methods are either handcrafted feature-based or have limited memories. b) adversarial examples can't be correctly classified by traditional training methods. To address these issues, in this paper we first cast the problem into a question and answering problem and proposed an improved dynamic memory networks with hierarchical pyramidal utterance encoder. Moreover, we apply adversarial training to train our proposed model. We evaluate our model on two public datasets, i.e., Switchboard dialogue act corpus and the MapTask corpus. Extensive experiments show that our proposed model is not only robust, but also achieves better performance when compared with some state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Dialogue Act (DA) which represents the meaning of utterances has been widely adopted in computational linguists, especially in the dialogue system. The automatic recognition of DAs is an important step toward understanding spontaneous dialogue, which will facilitate many applications such as human-computer dialogue systems <ref type="bibr" target="#b0">[1]</ref>, language understanding applications <ref type="bibr" target="#b1">[2]</ref>, spoken language translation <ref type="bibr" target="#b2">[3]</ref>, or automatic speech recognition <ref type="bibr" target="#b3">[4]</ref>. <ref type="table">Table I</ref> shows a snippet of dialog with utterances and their corresponding labels. In dialogs, each utterance is assigned a unique DA label, drawn from a welldefined set. Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria. From <ref type="table">Table I</ref>, we can also find that knowing the past utterances of dialog can help easing the prediction of the current DA state, thus help to narrow the range of utterance generation topics for the current turn <ref type="bibr" target="#b4">[5]</ref>. For instance, the "Greeting" and "Farewell" acts are often followed with another same type utterances, the "Answer" act often responds to the former "Question" type utterance. Motivation. Currently, there have been many research works focusing on the problem of DA classification, ranging from multi-class classification to structured prediction <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr">*</ref>   <ref type="table">Table I</ref>: A snippet of a conversation sample. Each utterance has related dialogue act label <ref type="bibr" target="#b4">[5]</ref>.</p><p>many previous works, hand-crafted features are created and fed into a multi-class classifier such as SVM and Naive Bayes <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, making the model labor intensive and can not be scaled up well across different datasets. To better learn the representation of utterance, recent studies <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have applied deep learning based models for the DA recognition task, and have shown promising results. However these previous works mainly suffer the limited memory due to the dynamic characteristic of dialog. In other words, each response in dialog is correlated with the previous utterances. For example, it is evident that during a conversation, the speaker's intent is influenced by the former utterance such as the previous "Greeting" and "Farewell" examples. This limitation makes these models can't represent dialogues with large numbers of turns and long utterances. Another limitation of these previous works lies in the training process of deep neural networks. It has been shown that traditional neural models are often vulnerable to adversarial examples, which are examples created by making small perturbations to the input <ref type="bibr" target="#b8">[9]</ref>. Motivated by these above mentioned issues, in this paper, we propose a unified framework integrating dynamic memory network and adversarial learning for DA classification. To better represent the state of utterances, we draw some insights from Dynamic Memory Network (DMN) which have been successfully applied in question and answering <ref type="bibr" target="#b9">[10]</ref>. Unlike previous attention-based deep neural networks, DMN can computes dynamic sentence representations dependently and hence can be easily be used for representation of dialog. Specifically, we first formulate the DA classification task into a questioning and answering setting (Q: What is the label of this utterance?). Then we propose an improved DMN to learn and memorize the utterance, which contains a hierarchical pyramid utterance encoder. <ref type="table">Moreover, we train our model via adversarial  training which is a process of training a model to correctly  classify both unmodified examples and adversarial examples. It  improves not only robustness to adversarial examples, but also  generalization performance for original examples.</ref> The main contributions of this paper can be summarized as follows:</p><p>• Unlike previous studies, to the best of knowledge, it's the first time that we model the DA classification problem from the perspective of question and answering, and propose an improved dynamic memory network with hierarchical pyramid utterance encoder for better representation of utterances. • To increase the robustness and generalization of our model, we make small perturbations to the input data, and apply adversarial training to train our proposed model. • Extensive experiments and analysis on two real-world datasets verify the effectiveness of our proposed model when compared with some state-of-the art baselines. Organization. The rest of this paper is organized as follows. In Section II, we provide a brief review of the related work about dialogue act recognition problem. In Section III, we first formulate the problem of dialogue act classification from the viewpoint of question and answering, and introduce some background knowledge about dynamic memory network. We elaborate our proposed improved dynamic memory network for DA in Section IV. Extensive experimental results and analysis are presented in Section V. Finally, we provide some concluding remarks in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review some related works on dialogue act classification, dynamic memory network and adversarial learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DA Classification</head><p>Most of the existing work for the problem of DA classification can be categorized as following two classes: a) Regarding the DA classification as a multi-classification problem <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. b) Regarding the DA classification as a sequence labeling problem <ref type="bibr" target="#b12">[13]</ref>.Recently, approaches based on deep learning methods improve many state-of-the-art techniques in NLP including DA classification accuracy on open-domain conversations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Kalchbrenner et al. <ref type="bibr" target="#b13">[14]</ref> use a mixture of CNN and RNN to represent utterances where CNNs are used to extract local features from each utterance and RNNs are used to create a general view of the whole dialogue. Khanpour et al. <ref type="bibr" target="#b6">[7]</ref> design a deep neural network model that benefits from pre-trained word embeddings combined with a variation of the RNN structure for the DA classification task. Ji et al. <ref type="bibr" target="#b7">[8]</ref> also investigate the performance of using standard RNN and CNN on DA classification and get the cutting edge results on the MRDA corpus using CNN. Lee et al. <ref type="bibr" target="#b14">[15]</ref> propose a model based on CNNs and RNNs that incorporates preceding short texts as context to classify current DAs. Unlike previous models, we cast the DA classification task into a question and answering problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Memory Network</head><p>One line of research related to dynamic memory network is attention and memory mechanism <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which have been successfully applied in many tasks such as text generation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and question answering <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In these works, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to assign a weight to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks <ref type="bibr" target="#b21">[22]</ref>. Based on these works, <ref type="bibr" target="#b22">[23]</ref> develops dynamic memory network which simultaneously contains memory updating mechanism and attention mechanism. <ref type="bibr" target="#b9">[10]</ref> proposes an improved dynamic memory network with some modifications in memory and input module. The dynamic memory network has been successfully applied in many scenarios such as question answering and sentiment analysis. To the best of our knowledge, it's the first time that we apply dynamic memory network in DA classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adversarial Learning</head><p>Adversarial training <ref type="bibr" target="#b23">[24]</ref> introduces an end-to-end and deterministic way of data perturbation by utilizing the gradient information. It could design adversarial examples to attack <ref type="bibr" target="#b24">[25]</ref> or improve robustness <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> of neural network models. Adversarial training is originally used in the context of image classication tasks where the input data is continuous. Miyato et al. <ref type="bibr" target="#b8">[9]</ref> adopt adversarial training to text classication by adding perturbations on word embeddings and also extends it to a semi-supervised setting by minimizing the entropy of the predicted label distributions on unlabeled data. Wu et al. <ref type="bibr" target="#b27">[28]</ref> apply adversarial training in relation extraction within the multi-instance multi-label learning framework. There are also other works on regularizing classifiers by adding random noise to the data, such as dropout <ref type="bibr" target="#b28">[29]</ref> and its variant for NLP tasks such as word dropout <ref type="bibr" target="#b29">[30]</ref>. In <ref type="bibr" target="#b30">[31]</ref>, Xie et al. discuss various data noising techniques for language models. Sogaard and Li et al. <ref type="bibr" target="#b31">[32]</ref> focus on linguistic adversaries. Inspired by these works, in this paper we apply adversarial training to train our dynamic memory network for the task of DA classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>In this section, we first declare some notifications through this paper and formulate the DA classification problem mathematically. We also present some background knowledge on dynamic memory network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task Description</head><p>Assume that we have a set D of N conversations, i.e. of utterances, i.e. C i = {u 1 , u 2 , · · · , u d } and the corresponding labels of C i are Y i = {y 1 , y 2 , · · · , y d }, where d is the number of utterances. In other words, for each utterance u j in each conversation, we have an associated target label y j ∈ Y, where Y is the set of all possible DAs. Each utterance u j in turn is itself a sequence of |u j | words stringed together, i.e.,</p><formula xml:id="formula_0">D = {C 1 , C 2 , . . . , C N } with (Y 1 , Y 2 , . . . , Y N ) correspond- ing</formula><formula xml:id="formula_1">u j = (w 1 , w 2 , . . . w |uj | ).</formula><p>In this paper, we cast the DA classification problem into a question and answering setting. Specifically, we first fix the question as "what is the label of this utterance?". Then for a given utterance, our goal is to generate the answer/label for it. For a given conversation C i = [u 1 , . . . , u d ], u j = (w 1 , w 2 , . . . w |uj | ) represents an utterance and w n denotes the n-th word in u j . Let H = [u 1 , . . . , u k−1 ] denote a dialogue context, the k − 1 historical utterances, and u k be a response which means the next utterance. Our goal is to predict the DA label of u k , given its dialogue context H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Memory Networks</head><p>Dynamic Memory Network (DMN) is a new neural network architecture based on attention mechanism with the ability of memorizing and reasoning. It has been widely used in question answering tasks since it was first proposed in <ref type="bibr" target="#b22">[23]</ref>. In this subsection, we introduce some background knowledge on DMN as presented in <ref type="bibr" target="#b22">[23]</ref>. The DMN consists of four modules: input module, question module, episode memory module and answer module.</p><p>1) Input Module: In this module, the input data corresponding to the question being asked to are encoded into a sequence of distributed vectors representations. We name the vectors as facts (evidences), denoted as E = [e 1 , e 2 , · · · , e |E| ], where |E| is the total number of facts, usually is the number of sentence in a document. So e i is the vector representation of the i-th sentence. The order of vectors in E cannot be changed at random because memory updates need to be based on the order of the facts. Long Short-Term memory (LSTM) <ref type="bibr" target="#b32">[33]</ref> and Gated Recurrent neural Units (GRU) <ref type="bibr" target="#b33">[34]</ref> are typically used to encode the input data. In <ref type="bibr" target="#b22">[23]</ref>, GRU is selected as an encoder for the trade-off between computational efficiency and performance effectiveness. The detailed operation of GRU is defined as follows:</p><formula xml:id="formula_2">r t = σ(W r x t + W r g t−1 + b r ), z t = σ(W z x t + W z g t−1 + b z ), g t = tanh(W g x t + W g (r t • g t ) + b g ), g t = z t •ĝ t + (1 − z t ) • g t−1 ,<label>(1)</label></formula><p>where r t and z t are the reset gate vector and update gate vector respectively, g t is the output vector, Ws and bs are weights matrices and biases, σ is the sigmoid activation function, • is an element-wise multiplication.</p><p>2) Question Module: This module is similar to the input module, which also utilizes GRU as the encoder and encodes a sequence of question words into a distributed vector representation q. And then q is fed into episode memory module and answer module separately.</p><p>3) Episode Memory Module: Episode memory module is the main component of the DMN. It is comprised of an attention mechanism and a memory updating mechanism. This module iterates over the input evidences representations and extracts the information to answer the question q. When the questions are too complex to answer so that we need reasoning, the episode memory network may iterates over the input evidences representations multiple times. The episode memory may be updated after each iteration. We denote the memory after ith iteration over the input evidences representations as m i . We initialize the foremost m 0 as the question vector q. The attention mechanism is in charge of producing a contextual vector c i , a weighted sum of the input evidences representation, with relevance of the question q and the previous memory m i−1 . The weights of evidences that contribute more to the answer are larger. The episode module may focus on the important information via soft alignment. The memory updating mechanism is in charge of producing the m i based on the contextual vector c i and the previous memory m i−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Answer</head><p>Module: The answer module generates an appropriate answer given the question representation q and the final episode memory. After multiple updates, m T contains all information that is required to answer the question. This module takes different execution modes according to different types of tasks. For single-token-answer case, it is treated as a simple classification task. The token-generation layer is composed of a linear layer with a softmax activation classifier to compute the probability distribution of the answer over the entire vocabulary table. For task that requires generating a sequence of tokens, another GRU is used to decode the concatenation of q and m T to a sequence of tokens.</p><p>C. An Overview <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of the network architecture of our proposed model. The framework can be divided into four parts: (a) Hierarchical pyramid utterance encoder module (cf. Sec. IV-A). In this module, we first represent the input utterance via a pyramidal BiGRU. We first embed each word token into a continuous distributed embedding using Glove <ref type="bibr" target="#b34">[35]</ref>, and then we add a perturbation for each word embedding for adversarial training. Then we fed the perturbed embedding to a pyramidal BiGRU. For dialogue history embedding, we apply a pyramidal BiGRU to embed each utterance and apply another utterance level GRU to represent the whole context. We call this encoder as hierarchical pyramidal utterance encoder. (b) General question module (cf. Sec. IV-B). In this module, we fix the question as "What is the label of this utterance?". We adopt a vanilla BiGRU to embed the question. (c) Attentional episode memory updates module (cf. Sec. IV-C). This module is the core component of our proposed framework which is comprised of attention and memory mechanism. The attention mechanism is to associate a weight to each hidden states (also called facts in DMN). The generated context vector contains the information of facts, question and memory. Then the context vector is fed into the memory updates module for memory updating. (d) Act prediction module (cf. Sec. IV-D). This module is the output module of our proposed framework. In our scenario, we use the softmax layer to map the hidden states to the DA label space. We will elaborate each component of this framework in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DMN WITH ADVERSARIAL TRAINING</head><p>In this section, we describe our proposed Dynamic Memory Network with Adversarial Learning (ALDMN) for dialogue act classification. We first describe our dynamic memory network with hierarchical pyramidal utterance encoder and general question module. Then, we describe adversarial training to train our model in detail. </p><formula xml:id="formula_3">Glove L Glove L Glove L Glove L w 1 w 2 w T 1 w T r r r r … … … … BiGRU BiGRU</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hierarchical Pyramidal Utterance Encoder</head><p>In DA classification task, the input is an utterance of sequence of n words {w 1 , w 2 , . . . , w n } with its history utterances H = [u 1 , . . . , u k−1 ]. Therefore, the facts of input are composed of two parts (i.e. input utterance facts e u and history utterance facts e H ). We model these two parts one by one.</p><p>To encode each utterance in the dialog, there have been many options such as LSTM <ref type="bibr" target="#b35">[36]</ref>, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Pyramidal bidirectional GRU which is an alternative bidirectional GRU that reduces the time dimension after each layer has been successfully applied in character-level sentence encoding and decoding <ref type="bibr" target="#b36">[37]</ref>. In this paper we try some available encoders, including LSTM, GRU, pyramidal GRU and their bidirectional versions. Experimental results show that the pyramidal bidirectional GRU achieves the best performance, so we adopt it for utterance encoding.</p><p>To start with, for the following adversarial training, since the utterance tokens are discrete, we define the perturbation on continuous word embeddings instead of discrete word inputs. Thus, we first use Glove <ref type="bibr" target="#b34">[35]</ref> to transform an utterance of n words {w 1 , w 2 , . . . , w n } into distributed continuous embeddings, and then add perturbations to them. The word vector of w i can be represented as follows:</p><formula xml:id="formula_4">v(w i ) = Glove(w i ) + r adv .<label>(2)</label></formula><p>We then feed the word embeddings into a pyramidal bidirectional GRU. The hidden states in each bidirectional GRU are defined as follows:</p><formula xml:id="formula_5">− → f i t = GRU( − → f i t−1 , e i−1 t ),<label>(3)</label></formula><formula xml:id="formula_6">← − f i t = GRU( ← − f i t+1 , e i−1 t ),<label>(4)</label></formula><formula xml:id="formula_7">h i t = − → f i t + ← − f i t+1 ,<label>(5)</label></formula><p>where GRU denotes the gated recurrent unit function, which has shown to improve the performance of RNNs <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Input utterance facts. For pyramidal Bi-GRU, the input from the previous layer input e 0 t = v 0 t and e i u,t = tanh(W (i)</p><formula xml:id="formula_8">pyr [h i−1 2t ; h i−1 2t+1 ]) + b (i) pyr ,<label>(6)</label></formula><p>for i &gt; 0. The weight matrix W (i) pyr thus reduces the number of hidden states for each additional hidden layer by half, and hence the encoder has a pyramidal structure. At the final hidden layer we obtain the encoded representation e consisting of T /2 N −1 hidden states, where N denotes the number of hidden layers. We can find that after the input sequence passes through the pyramidal encoder, the number of encoder output becomes less, which reduces the computational load for the following attention mechanism. Historical utterance facts. After encoding each utterance via pyramidal Bi-GRU, we apply another GRU to represent the historical utterances:</p><formula xml:id="formula_9">e i H,t = GRU(e i H,t−1 , e i−1 u,t−1 ).<label>(7)</label></formula><p>Finally, we can denote the input facts by e = {e 1 u , . . . , e |u| u , . . . , e 1 H , . . . , e |H| H }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. General Question Module</head><p>Different from the question answering task, in DA classification task, none of the utterance is provided with a question. Actually, we can consider all of the utterances facing the same question, such as "What is the label of this utterance?". Thus, we compute a general vector q representing the same question, where q is jointly learned with other model parameters and used in the subsequent module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attentional Episode Memory Updates</head><p>Attention mechanism and memory updating mechanism constitute the main components of this module. The episodic memory module, as depicted in <ref type="figure" target="#fig_0">Figure 1 (c)</ref>, retrieves information from the input facts e = {e 1 u , . . . , e |u| u , . . . , e 1 H , . . . , e |H| H } provided to it by focusing attention on a subset of these facts. We implement this attention by associating a single scalar value, the attention gate α i t , with each fact e i during pass t. Here the implementation of attention mechanism is the same as <ref type="bibr" target="#b16">[17]</ref>. At the t-th iteration, we concatenate the output e of the pyramidal encoder with previous iteration episodic memory m t−1 and question vector q, and then employ the basic soft attention to obtain the t-th contextual vector as:</p><formula xml:id="formula_10">z i t = [e i • q; e i • m t−1 ],<label>(8)</label></formula><formula xml:id="formula_11">β i t = W (2) tanh W (1) z i t + b (1) + b (2) ,<label>(9)</label></formula><formula xml:id="formula_12">α i t = exp(β i t ) n j=1 exp(β j t ) ,<label>(10)</label></formula><formula xml:id="formula_13">c t = n i=1 α i t e i ,<label>(11)</label></formula><p>where • is the element-wise multiplication, [·; ·] is the concatenation operation, W and b are model parameters. In <ref type="bibr" target="#b9">[10]</ref>, z i t is set to z i t = [e i • q; e i • m t−1 ; |e i − q|; |e i − m t−1 |], where | · | is the element-wise absolute value. However in our scenario, z i t = [e i • q; e i • m t−1 ] achieves better performance from the experimental results.</p><p>Following the memory update component used in <ref type="bibr" target="#b39">[40]</ref>, we first concatenate the previous episodic memory m t−1 , the current contextual vector c t and question vector q, and use a ReLU layer for the memory update:</p><formula xml:id="formula_14">m t = ReLU(W (3) [m t−1 ; c t ; q] + b (3) ),<label>(12)</label></formula><p>where [·; ·] is the concatenation operation, W (3) and b <ref type="bibr" target="#b2">(3)</ref> are model parameters. We use ReLU activation and initialize the memory vector as the question vector: m 0 = q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Act Prediction</head><p>The final episodic memory m T and question vector q are concatenated to compute the probability of the correct answer. In DA classification, the answer is the label of the corresponding utterance, which is different from the question answering task predicting a sequence token representing the correct answer. The classifier takes the episodic memory m T and question vector q as input:</p><formula xml:id="formula_15">p(y|m T , q) = softmax(W (4) [q; m T ] + b (4) ),<label>(13)</label></formula><p>where y represents the index of multi-class candidate answer. W <ref type="bibr" target="#b3">(4)</ref> and b <ref type="bibr" target="#b3">(4)</ref> are learnable parameters to be optimized in the model. The loss function is the negative log-likelihood of the true class labels y for each utterance:</p><formula xml:id="formula_16">L(u, θ) = − M i=1 log p(y (i) |u; θ),<label>(14)</label></formula><p>where M is the number of utterances in training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Adversarial Training</head><p>For classification task, many existing models use dropout or 2 for model regularization and achieve a good performance. However, these models demonstrate poor performance on adversarial examples even sometimes the perturbations are so small that humans cannot detect it. Adversarial learning is another regularization method of the model, which aims to train the model to correctly classify adversarial examples and real examples. Following the method of adding perturbations in <ref type="bibr" target="#b8">[9]</ref>, we create continuous perturbations by adding noise to word embedding. Let u denote the input utterance and θ the parameters of model. When applied to a model, adversarial training adds the following term to the loss function:</p><formula xml:id="formula_17">L adv (θ) = − M i=1 log p(y i |u i + r adv,i ; θ),<label>(15)</label></formula><p>where M is the total number of utterances in training data, r adv is the worst case perturbations against the current model which can be defined as follows:  where r is a perturbation on the input utterance u andθ is a constant set to the current parameters of a classifier, indicating that the backpropagation algorithm will be used to propagate gradients through the adversarial example construction process.</p><formula xml:id="formula_18">r adv = argmin r,||r||&lt; log p(y|u + r;θ),<label>(16)</label></formula><p>We can see that the r adv can not be calculated exactly in general, since exact minimization with respect to r is intractable for many models such as neural networks. Inspired by <ref type="bibr" target="#b23">[24]</ref>, we can approximate this value by linearizing log(y|u; θ) around u. Therefore, with a linear approximation and a 2 norm constraint in Eq.16, the resulting adversarial perturbation is formulated as follows:</p><formula xml:id="formula_19">r adv = g ||g|| ,<label>(17)</label></formula><p>where g is the normalized gradient of log-likelihood with respect to u, which can be formulated as follows:</p><formula xml:id="formula_20">g = ∇ u log p(y|u;θ).<label>(18)</label></formula><p>We can compute the perturbation by backpropagation in neural networks. To optimize the objective, we employ the Adam <ref type="bibr" target="#b40">[41]</ref>, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND ANALYSIS</head><p>In this section, we conduct extensive experiments and analysis on two real-world datasets to show the effectiveness of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate the performance of our model on two benchmark datasets used in several prior studies for the DA classification task, i.e. Switchboard Dialogue Act Corpus (SwDA) <ref type="bibr" target="#b41">[42]</ref> and MapTask Dialogue Act Corpus (MapTask) <ref type="bibr" target="#b42">[43]</ref>.</p><p>SwDA: This dataset consists of 1,155 telephone conversations and each of the utterance in the dialogues is mapped into 42 distinguished utterance types via DAMSL taxonomy. We shuffle the data randomly and use 1,050 conversations for training and 105 conversations for testing.</p><p>MapTask: This dataset comprises of 128 dialogues and more than 27,000 utterances. Each of the utterance is labeled with one of the 13 tags. Unlike SwDA, the MapTask corpus emphasizes on directions and conductions.</p><p>Table II presents different statistics for both datasets. For SwDA, training and testing sets are provided but not the validation set, so we use the standard practice of taking a part of training data set as validation set <ref type="bibr" target="#b14">[15]</ref>. We also shows the top 10 utterance labels on different datasets in <ref type="figure" target="#fig_2">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>We employ the standard accuracy as the metric of evaluating our proposed ALDMN method. The accuracy is defined as:</p><formula xml:id="formula_21">Accuracy = 1 M M i=1 1[ŷ i = y i ],<label>(19)</label></formula><p>whereŷ i and y i are the predicted label and ground true label, respectively. 1[·] is the indicator function. When the predicted label is the same as the true, label 1[·] equals to 1 otherwise 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>To train our proposed model, we first randomize the training data and set the mini-batch to 128. For each batch, the utterance is padded with a special token &lt;pad&gt; to the maximum length. We regard the words whose occurrence is less than 2 as Out of Vocabulary (OOV) tokens and replace them with &lt;unk&gt;s. We remove all punctuation marks except interrogation and all characters are converted to lower-case. We set the memory updating iterations to 3. We adopt Adam <ref type="bibr" target="#b40">[41]</ref> optimizer with the learning rate initialized to 0.01. We run the training data set for 45 epochs with early stopping when validation loss did not decline for five consecutive epochs. The random uniform initialization with range [−0.1, 0.1] is used for all matrix variables, including word embedding and other weights. Both the embedding and hidden dimensions are set to d = 200. The dropout rate is set to 0.2. <ref type="figure" target="#fig_3">Figure 4</ref> is the training loss curve of our method. We can see that our model is converged after around 300 iterations.</p><p>All the experiments in this paper are implemented with Python 2.7 based on TensorFlow, and run on a computer with an 2.2 GHz Intel Core i7 CPU, 64 GB 1600 MHz DDR3 RAM, and a Titan X GPU with 12 GB memory, running Ubuntu 16.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results and Analysis</head><p>We compare our proposed method with several other stateof-the-art methods for the problem of dialogue act classification as follows:</p><p>• UCI <ref type="bibr" target="#b43">[44]</ref> method embeds contextual information of utterance via hierarchical CNN/RNN for DA classification. • PDI <ref type="bibr" target="#b44">[45]</ref> method predicts the next label based on the current label probability distribution to avoid label bias. • DRLM-Conditional <ref type="bibr" target="#b45">[46]</ref> method is a latent variable recurrent neural network architecture for jointly modeling utterances and DA labels. • BiLSTM-Softmax <ref type="bibr" target="#b6">[7]</ref> method uses a bidirectional LSTM to embed utterances and then feeds them to a softmax classifier. • RCNN <ref type="bibr" target="#b13">[14]</ref>. Different from BiLSTM-Softmax, this method uses hierarchical CNN to embed utterances. • DMN <ref type="bibr" target="#b22">[23]</ref> methods is a DMN based method for sentiment classification, where BiLSTM is used for sentences embedding. • ADC <ref type="bibr" target="#b46">[47]</ref> and PDI <ref type="bibr" target="#b44">[45]</ref> methds are acoustic and discourse classification based on HMM and SVM. • GAN <ref type="bibr" target="#b47">[48]</ref> method is a generative neural network which incorporates attention technique and a label-to-label connection. <ref type="table" target="#tab_2">Table III and Table IV</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Impact of Pyramidal Encoder</head><p>To analyze the impact of pyramidal layer, we vary the layers number of pyramidal encoder from 1 to 3. When the layer number is 1, the pyramidal encoder reduced into a BiGRU encoder. <ref type="figure" target="#fig_5">Figure 5</ref> shows the classification accuracy with varying pyramid layers on SwDA and MapTask dataset. From this figure, we can find that our model achieves the best performance when the pyramid layer is set to 2. This verifies the effectiveness of the pyramidal encoder. Moreover, the performance decrease when the pyramid layer is set to 2. This can be illustrated by the fact that our encoder may be over-fitting with the number of layers increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Impact of Adversarial Learning</head><p>To demonstrate the effectiveness of adversarial learning on word embeddings, we compare architectures with/without adversarial learning. For adversarial training, we set = 3. For model without adversarial training, the dropout is applied. Comparing performances are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. From this figure, we can find that our model with adversarial can really improve the accuracy performance in both two datasets. Take SwDA as an example, with only dropout applied, our model achieves accuracy of 79.1%, while the accuracy of model with adversarial learning can reach up to 81.5%. This can be explained by the fact that the model without adversarial learning is strongly affected by the syntax of the dataset. In fact, in DA classification task, as the conversations are sometimes recorded in a casual setting, the grammar of utterances are not so strict compared to a formal document. What's more, due to   the model pre-training step, some antonyms, although greatly different in semantics, are very close in the vector space. For example, "big" is opposite to "small" in semantics, but they are sometimes in the similar context. So they are assigned the similar word embeddings on the baseline model while the embeddings cannot convey the actual meaning of these two words. Adversarial learning ensures the representation of sentences not be changed under some perturbations so that those words in same context with different meanings can be separated in the vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Performance on Various Data Distribution</head><p>We vary the length of utterances since the utterance length may have an effect on the representation. <ref type="figure">Figure 7</ref> shows the performance of our proposed model w.r.t. varying utterance on SwDA dataset. From this figure, we can find that our model achieves the best performance on the utterances whose length is around 40. This can be illustrated by the fact that short utterances always contain limited information while long utterances may contain more noises.  utterances (89.7%) with true label non-opinion are predicted correctly. Similarly, 6,478 utterances (25.4%) with true label opinion are predicted incorrectly as non-opinion whereas 17,638 utterances (70.5%) with true label opinion are predicted correctly. On further analysis of the cause of this confusion between these two class pairs, we identify that some utterances are classified correctly by the model. However, they are marked incorrectly classified because of bias in the ground truth. For some of the utterances, classes are not distinguishable even by humans because of the subjectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Confusion Matrix Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a unified framework integrating dynamic memory network and adversarial training for the task of dialogue act classification. Specifically, we first cast the act classification task into a question and answering problem and propose an improved dynamic memory network to represent and memorize the utterance with its corresponding history utterances. In addition, we apply adversarial training to train a more robust model. We demonstrate the effectiveness of our proposed model using the well-known public datasets SwDA and MapTask. Extensive experiments demonstrate that our model can achieve better performance than several state-of-theart solutions to the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>target DAs. For each conversation C i , it consists of a series An overview of the network architecture of our proposed model. (a) Hierarchical pyramidal utterance encoder module. (b) General question module. (c) Attentional episode memory updates module. (d) Act prediction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Network structure of pyramidal utterance encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The distribution of utterance labels on two datasets: a) SwDA, b) MapTask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Training loss of each iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Classification accuracy curves with different pyramid layer of pyramid-layer-1, pyramid-layer-2, pyramid-layer-3 on the two different datasets: a) SwDA, b) MapTask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Classification accuracy curves of with (green) and without (red) adversarial learning on two different datasets: a) SwDA, b) MapTask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 Figure 7 :</head><label>87</label><figDesc>shows the confusion matrix of our proposed model for the SwDA dataset. Among them the most confused pairs are (sd, sv) and (aa, b) which represent (statement-nonopinion, statement-opinion) and (agree-accept, acknowledge) respectively. The total number of utterances with DA 'sd', 'sv', 'aa', and 'b' are 72,824, 25,197, 10,820 and 37,096, respectively. 3,423 utterances (4.7%) with true label nonopinion are predicted incorrectly as opinion, whereas, 64,847 The performance of our proposed model w.r.t. varying length of utterances on SwDA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Confusion matrix of our proposed model for the SwDA dataset, where the row denotes the true label and the column denotes the predicted label. The numbers in the bracket besides the DA label in the rst cell of each row is the count of the number of utterances of that DA label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>|C| is the number of Dialogue Act classes, |V | is the vocabulary size. MinL, MeanL and MaxL indicate the minimum, mean and maximum of utterance length, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>respectively show the experimental Accuracy results of the methods on the SwDA and MRDA datasets. The hyper-parameters and parameters which achieve the best performance on the validation set are chosen to conduct the testing evaluation. From these two tables, we can find that our proposed model ALDMN definitely outperforms other baselines in both datasets. On SwDA dataset, our model</figDesc><table><row><cell>Model</cell><cell>Accuracy(%)</cell></row><row><cell>RCNN(Bulnsom et al. 2013)</cell><cell>73.9</cell></row><row><cell>BiLSTM-Softmax(Khanpour et al. 2016)</cell><cell>75.8</cell></row><row><cell>DRLM-Conditional(Ji et al. 2016)</cell><cell>77.0</cell></row><row><cell>PDI(Tran et al. 2017)</cell><cell>75.6</cell></row><row><cell>UCI(Liu et al. 2017)</cell><cell>79.9</cell></row><row><cell>DMN(Kumar et al., 2015)</cell><cell>75.2</cell></row><row><cell>ALDMN (Our Model)</cell><cell>81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III :</head><label>III</label><figDesc>Classification accuracy on SwDA corpus, comparing our ALDMN model with other methods as described in literatures.</figDesc><table><row><cell>Model</cell><cell>Accuracy(%)</cell></row><row><cell>ADC(Julia et al. 2010)</cell><cell>55.4</cell></row><row><cell>GAN(Tran et al. 2017b)</cell><cell>62.9</cell></row><row><cell>PDI(Tran et al. 2017a)</cell><cell>65.9</cell></row><row><cell>DMN(Kumar et al., 2015)</cell><cell>64.7</cell></row><row><cell>ALDMN (Our Model)</cell><cell>68.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV</head><label>IV</label><figDesc></figDesc><table><row><cell>: Classification accuracy on MapTask corpus, compar-</cell></row><row><cell>ing our ALDMN model with other methods as described in</cell></row><row><cell>literatures.</cell></row><row><cell>improves the accuracy over the best performing traditional</cell></row><row><cell>method UCI by 1.6%. On MapTask dataset, our model improves</cell></row><row><cell>the accuracy over the best performing traditional method PDI</cell></row><row><cell>by 2.6%. This verifies the effectiveness of our proposed DMN</cell></row><row><cell>module and adversarial training approach for DA classification.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech acts for dialogue agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of rational agency</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="169" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding student language: An unsupervised dialogue act classification approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ezen-Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Data Mining (JEDM)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="78" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting dialogue acts for a speech-to-speech translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reithinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="654" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dialogue act recognition via crf-attentive structured network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dialogue act classification using a bayesian approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vilar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Conference Speech and Computer</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dialogue act classification in domain-independent conversations using a deep recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guntakandla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2012" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A latent variable recurrent neural network for discourse relation language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01913</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
	<note>international conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dialogue act classification using language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reithinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A multidimensional approach to utterance segmentation and dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geertzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Petukhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue</title>
		<meeting>the 8th SIGdial Workshop on Discourse and Dialogue<address><addrLine>Antwerp</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dialog act tagging with support vector machines and hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Surendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.3584</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sequential short-text classification with recurrent and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03827</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving automatic source code summarization via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering</title>
		<meeting>the 33rd ACM/IEEE International Conference on Automated Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video question answering via hierarchical spatio-temporal attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Memen: multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>international conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Not just privacy: Improving performance of private deep learning in mobile cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2407" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Layerwise perturbationbased adversarial training for hard drive health degree prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02573</idno>
		<title level="m">Data noising as smoothing in neural network language models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust training under linguistic adversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv: Neural and Evolutionary Computing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dialogue act sequence labeling using hierarchical encoder with crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Towards neural network-based reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05508</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
	<note type="report_type">ICASSP-92</note>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The hcrc map task corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kowtko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and speech</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="366" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using context information for dialog act classification in dnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2170" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Preserving distributional information in dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2151" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A latent variable recurrent neural network for discourse relation language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="332" to="342" />
		</imprint>
	</monogr>
	<note>north american chapter of the association for computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dialog act classification using acoustic and discourse information of maptask data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Julia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Iftekharuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">U</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="289" to="311" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A generative attentional neural network model for dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zukerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="529" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
