<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grafit: Learning fine-grained image representations with coarse labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Facebook</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Grafit: Learning fine-grained image representations with coarse labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image classification now achieves a performance that meets many application needs <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b55">54]</ref>. In practice however, the dataset and labels available at training time do not necessarily correspond to those needed in subsequent applications <ref type="bibr" target="#b16">[17]</ref>. The granularity of the training-time concepts may not suffice for fine-grained downstream tasks.</p><p>This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets <ref type="bibr" target="#b30">[29]</ref> have been developed for specific domains, for instance to distinguish different plants <ref type="bibr" target="#b12">[13]</ref> or bird species <ref type="bibr" target="#b60">[59]</ref>.</p><p>Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to find enough images of rare classes, and annotating them precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol <ref type="bibr" target="#b39">[38]</ref> that states that: "Manually labeling a large number of images with the presence or absence of 19,794 different classes is not feasible". For this reason they resorted to computerassisted annotation, at the risk of introducing biases due to the assisting algorithm. Being able to get strong classification and image retrieval performance on fine concepts using only coarse labels at training time can circumvents the issue, liberating the data collection process from the quirks of a rigid fine-grained taxon-omy.</p><p>In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:</p><p>Category-level Retrieval. Given a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their fine-grained semantic similarity to a new query image outside the collection, as illustrated by <ref type="figure">Figure 1</ref>.</p><p>On-the-fly classification. For this task the finegrained labels are available at test time only, and we use a non-parametric kNN classifier <ref type="bibr" target="#b62">[61]</ref> for on-the-fly classification, i.e. without training on the fine-grained labels.</p><p>Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build upon recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b63">62]</ref> that exploits two losses to address both image classification and instance recognition, leveraging the "free" annotations provided by multiple data augmentations of a same instance, in the spirit of self-supervised learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">25]</ref>.</p><p>The second intuition is that it is best to explicitly infer coarse labels even when classifying for a finer granularity. For this purpose, we propose a simple method that exploits both a coarse classifier and image embeddings to improve fine-grained category-level retrieval. This strategy outperforms existing works that exploit coarse labels at training time but do not explicitly rely on them when retrieving finer-grained concepts <ref type="bibr" target="#b62">[61]</ref>.</p><p>In summary, in this context of coarse-to-fine representation learning, our paper makes the following contributions:</p><p>• We propose a method that learns a representation at a finer granularity than the one offered by the annotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly classification on ImageNet. This improvement is still +9.5% w.r.t. our own stronger baseline, everything being equal otherwise.</p><p>• Our approach performs similarly or better at the coarse level. A byproduct of our study is a very strong kNN-classifier on Imagenet: Grafit with ResNet-50 trunk reaches 79.6% top-1 accuracy at resolution 224×224.</p><p>• Grafit improves transfer learning: our experiments show that our representation discriminates better at a finer granularity. Everything being equal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy.</p><p>• As a result we establish the new state of the art on five public benchmarks for transfer learning: Oxford Flowers-102 <ref type="bibr" target="#b42">[41]</ref>, Stanford Cars <ref type="bibr" target="#b36">[35]</ref>, Food101 <ref type="bibr" target="#b6">[7]</ref>, iNaturalist 2018 <ref type="bibr" target="#b31">[30]</ref> &amp; 2019 <ref type="bibr" target="#b32">[31]</ref>.</p><p>This paper is organized as follows. After reviewing related works in Section 2, we present our method in Section 3. Section 4 compares our approach against baselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper.</p><p>In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one learned by a vanilla cross-entropy loss. Appendix B complements our experimental section 4 with more detailed results. Appendix C provides visual results associated with different levels of training/testing granularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Label granularity in image classification. In computer vision, the concept of granularity underlies several tasks, such as fine-grained <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">29]</ref> or hierarchical image classification <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b61">60,</ref><ref type="bibr" target="#b66">65]</ref>. Some authors consider a formal definition of granularity, see for instance Cui et al. <ref type="bibr" target="#b14">[15]</ref>. In our paper, we only consider levels of granularity relative to each other, where each coarse class is partitioned into a set of finer-grained classes.</p><p>In some works on hierarchical image classification <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b50">49]</ref>, a coarse annotation is available for all training images, but only a subset of the training images are labelled at a fine granularity. In this paper we consider the case where no fine labels at all are available at training time.</p><p>Train-Test granularity discrepancy. A few works consider the case where the test-time labels are finer than those available at training time and where each fine label belongs to one coarse label. Approaches to this task are based on clustering <ref type="bibr" target="#b62">[61]</ref> or transfer learning <ref type="bibr" target="#b34">[33]</ref>. Huh et al. <ref type="bibr" target="#b34">[33]</ref> address the question: "is the feature embedding induced by the coarse classification task capable of separating finer labels (which it never saw at training)?" To evaluate this, they consider the 1000 ImageNet classes as fine, and group them into 127 coarse classes with the WordNet <ref type="bibr" target="#b20">[20]</ref> hierarchy. Wu et al. <ref type="bibr" target="#b62">[61]</ref> evaluate on the 20 coarse classes of CIFAR-100 <ref type="bibr" target="#b37">[36]</ref> and on the same subdivision of Im-ageNet into 127 classes. They evaluate their method, Scalable Neighborhood Component Analysis (SNCA), with a kNN classifier applied on features extracted from a network trained with coarse labels. Note that this work departs from the popular framework of object/category discovery <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b59">58]</ref>, which is completely unsupervised.</p><p>In our work we mainly compare to the few works that consider coarse labels at train time, therefore SNCA <ref type="bibr" target="#b62">[61]</ref> is one of our baseline. We adopt their coarse labels definition and evaluation procedure for on-the-fly classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified embeddings for classes and instances.</head><p>Similar to Wu et al. <ref type="bibr" target="#b62">[61]</ref>, several Distance Metric Learning (DML) approaches like the Magnet loss <ref type="bibr" target="#b45">[44]</ref> or ProxyNCA <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b52">51]</ref> jointly take into account intra-and inter-class variability. This improves transfer learning performance and favors in some cases the emergence of finer hierarchical concepts. Berman et al. proposed Multigrain <ref type="bibr" target="#b2">[3]</ref>, which simply adds to the classification objective a triplet loss that pulls together different data-augmentations of a same image. Recent works on semi-supervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b67">66,</ref><ref type="bibr" target="#b70">69</ref>] rely on both supervised and self-supervised losses to get information from unlabelled data. For instance the approach of Xie et al. <ref type="bibr" target="#b63">[62]</ref> is similar to Multigrain, except that the Kullback-Leibler divergence replaces the triplet loss. Matching embeddings of the same images under different data-augmentations is the main signal in current works on self-supervised learning, which we discuss now.</p><p>Unsupervised and Self-Supervised Learning. In unsupervised and self-supervised approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b57">56]</ref> the model is trained on unlabeled data. Each image instance is considered as a distinct class and the methods aim at making the embeddings of different data-augmentations of a same instance more similar than those of other images. To deal with finer semantic levels than those provided by the labels, we use an approach similar to BYOL <ref type="bibr" target="#b26">[25]</ref>. BYOL only requires pairs of positive elements (no negatives), more specifically different augmentations of the same image. A desirable consequence is that this limits contradictory signals on the classification objective.</p><p>Transfer Learning. Transfer learning datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b42">41]</ref> are often fine grained and rely on a feature extractor pre-trained on another set of classes. However, the fine labels are not a subset of the pre-training labels, kNN classifier instance recognition <ref type="figure">Figure 2</ref>: Illustration of our method at train time. The convnet trunk that receives gradient is f θ and is used to update the target network f ξ as a moving average. The database of neighbors is updated by averaging embedding in each mini-batch with corresponding embeddings in the database. so we consider transfer learning as a generalization of our coarse-to-fine task. It is preferable to pre-train on a domain similar to the target <ref type="bibr" target="#b15">[16]</ref>, e.g., pre-training on iNaturalist <ref type="bibr" target="#b30">[29]</ref> is preferable to pre-training on Im-ageNet if the final objective is to discriminate between species of birds. The impact of pre-training granularity is discussed in prior works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b68">67]</ref>. In Section 4.6 we investigate how Grafit pre-training performs on fine-grained transfer learning datasets. <ref type="figure">Figure 2</ref> depicts our approach at training time. In this section we discuss the different components and training losses. Then we detail how we produce the category-level ranking, and how we perform on-thefly classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Grafit: Fitting a finer Granularity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training procedure: Grafit and Grafit FC</head><p>We first introduce an instance loss inspired by BYOL <ref type="bibr" target="#b26">[25]</ref> that favors fine-grained recognition. The Grafit model includes a trunk network f θ , to which we add two multi-layers perceptrons (MLP): a "projector" P θ and a predictor q θ . In the Grafit FC variant, P θ is linear for a more direct fair comparison with Wu et al. <ref type="bibr" target="#b62">[61]</ref>'s projector. The learnable parameters are represented by the vector θ. As in BYOL we define a "target network" f ξ as an exponential moving average of the main network f θ : the parameters ξ are not learned, but computed as ξ ← τ ξ + (1 − τ )θ, with a target decay rate τ ∈ [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance loss.</head><p>Each image x is transformed by T data augmentations (t 1 , . . . , t T ). Denoting cos the cosine similarity and g θ (x) = P θ (f θ (x)), the instance loss is:</p><formula xml:id="formula_0">L inst (x) = − 1≤i =j≤T cos q θ • g θ (t i (x)).g ξ (t j (x)) T (T − 1) ,<label>(1)</label></formula><p>kNN loss. A parametric classifier with softmax yields a representation that does not generalize naturally to new classes <ref type="bibr" target="#b62">[61]</ref> and is not adapted for kNN classification. Therefore, inspired by the neighborhood component analysis <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b48">47]</ref>, Wu et al. <ref type="bibr" target="#b62">[61]</ref> propose a loss function optimized directly for kNN evaluation, that we adopt and denote by L knn . Let x i be a training image with coarse label y i and σ a temperature hyper-parameter. For each image x i we select x j (j = i) as its neighbor with probability p i,j , computed as</p><formula xml:id="formula_1">p i,j ∝ exp cos(g θ (x i ), g θ (x j ))/σ ,<label>(2)</label></formula><p>where the p i,j are normalized so that j =i p i,j = 1.</p><p>The loss is then defined as:</p><formula xml:id="formula_2">L knn (x i , y i ) = − log j,yj =yi,j =i p i,j .<label>(3)</label></formula><p>We 2 -normalize after the P θ projection. The L knn scores all classes with Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory of embeddings.</head><p>One of the limitations of the kNN approach is that it requires to use all the features of the training set. To avoid recomputing all the embeddings of the training set, we use a memory M = {m 1 , . . . , m i , . . . }. It is updated as follows: when the image x i in the training set is in the current mini-batch, we update its embedding m i as follows: m i ← 1 2 (m i + g θ (x i )). In order to limit the memory space needed, we apply the L knn loss on the space of the projected features, which allows us to store smaller embedding and hence requires less memory. For instance for ImageNet we have to store 1.2M training images. Without the projection with ResNet-50 architecture for f θ , the memory size is 2048 × 1.2M but with a projection on a space of size 256 the memory size is 256 × 1.2M what is ×8 smaller. </p><formula xml:id="formula_3">L tot (x) = L knn (g θ (x), y) + L inst (x).<label>(4)</label></formula><p>Appendix B empirically shows that weighting differently the losses does not bring much performance.</p><p>Adapting the architecture at test-time. The training parameters include the model weights (f θ , P θ ) and the parameters related to L inst (f ξ , P ξ and q θ ) as described previously. At test time we remove the L inst branch, keeping only f θ and P θ . In order to have consistent representations of all the training images with the final weights, we re-compute m i = g θ (x i ) for each training image x i and store it in M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Category-level retrieval</head><p>For a given test image x the task is to order by semantic relevance all images from the training collection. In our coarse-to-fine case, a search result is deemed correct if it has the same fine label as the query.</p><p>Cosine-based ranking. The standard strategy to order the images is to compute g θ (x ), and to order all images x i in the collection by they cosine similarity score cos(g θ (x i ), g θ (x )) to the query (the g θ (x i ) are pre-computed in M). The experiments in Section 4 show that the way Grafit embeddings are trained already improves the ranking with that method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking conditioned by coarse prediction.</head><p>Let x be a test image and x a training image with coarse class y. Let p c (x, y) be the probability that the image x has coarse label y according to our classifier. Our conditional score ψ cond is a compromise between the embedding similarity and the coarse classification, in spirit of the loss in Equation 4:</p><formula xml:id="formula_4">ψ cond (x , x) = cos (g θ (x ), g θ (x)) + log p c (x , y) 1 − p c (x , y)</formula><p>.</p><p>(5) Note that, in that case, we rely on the fact that the collection in which we search is the training set, so that the coarse labels associated with the collection are known. In Section 4 we show experimentally that ψ cond improves the category-level retrieval performance in the coarse-to-fine context.</p><p>Conditional ranking: Oracle. If we assume that the coarse label of the query test image is known (given by an oracle), then we can set p c (x , y) = 1 y=y with y the coarse class of the test image x . This boils down to systematically putting images with the same coarse class as the test image first in the ranking. Experimentally, this shows the impact of test label prediction on the score, and provides an upper bound on the performance of the conditional ranking strategy. It is also relevant in practice in a scenario where the user provides this coarse labeling, for instance by selecting it from an interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">On-the-fly classification</head><p>In on-the-fly classification, a kNN classifier "knows" about the fine classes of the training images only at test time <ref type="bibr" target="#b62">[61]</ref>. Such a non-parametric classification does not require any training or fine-tuning. As a side note, this flexible classifier can handle settings with evolving datasets, including dynamic additions of new classes, although such setups are outside the scope of this paper.</p><p>For a test image x we compute the embedding g θ (x) and compare it to the training image embeddings stored in M. We select the k embeddings maximizing the cosine similarity to the query, (x 1 , ..., x k ), with labels (y 1 , ..., y k ). For a direct comparison with Wu et al. <ref type="bibr" target="#b62">[61]</ref> and consistently with Equation 3, we apply an exponentially decreasing neighbour weighting that computes the probability that x belongs to class y as</p><formula xml:id="formula_5">p kNN (x, y) ∝ k j=1,yj =y exp (cos(g θ (x), g θ (x j ))/σ) . (6)</formula><p>We normalize the probabilities so that y p kNN (x, y) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We consider evaluation scenarios where it is beneficial to learn at a finer granularity than that provided by the training labels. The first two tasks are coarse-tofine tasks (category-level retrieval and on-the-fly classification), where we measure the capacity of the network to discriminate fine labels without having seen them at training time. The third protocol is vanilla transfer learning, where we transfer from Imagenet to a fine-grained dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation metrics</head><p>We carry out our evaluations on public benchmarks, which statistics are detailed in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100 [36]</head><p>has 100 classes grouped into 20 coarse concepts of 5 fine classes each. For instance the coarse class large carnivore includes fine classes bear, leopard, lion, tiger and wolf. In all experiments, we use the coarse concepts to train our models and evaluate the trained model using the fine-grained labels.</p><p>ImageNet <ref type="bibr" target="#b47">[46]</ref> classes follow the WordNet <ref type="bibr" target="#b20">[20]</ref> hierarchy. We use the 127 coarse labels defined in Huh et al. <ref type="bibr" target="#b34">[33]</ref> in order to allow for a direct comparison with their method. iNaturalist-2018 offers 7 granularity levels from the most general to the most specific, that follow the biological taxonomy: Kingdom (6 classes), Phylum (25 classes), Class (57 classes), Order (272 classes), Family (1,118 classes), Genus (4,401 classes) and Species (8,142 classes). We consider pairs of (coarse,fine) granularity levels in our experiments. iNaturalist-2019 is similar to iNaturalist-2018 with fewer classes and images, and yields similar conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flowers-102, Stanford Cars and Food101</head><p>are finegrained benchmarks with no provided coarse labelling. Therefore we can use them for the transfer learning task.</p><p>Evaluation metrics. For category-level retrieval we report the mean average precision (mAP), as commonly done for retrieval tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">42]</ref>. For on-the-fly classification we report the top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We use existing baselines and introduce stronger ones:</p><p>Wu's baselines <ref type="bibr" target="#b62">[61]</ref> use activations of a network learned with cross-entropy loss, but evaluated with a kNN classifier. Huh et al. <ref type="bibr" target="#b34">[33]</ref> evaluate how a network trained on the 127 ImageNet coarse classes transfers on the 1000 fine labels 1 .</p><p>Our main baseline: we learn a network with cross-entropy loss, and perform retrieval or kNNclassification with the 2 -normalized embedding produced by the model trunk. We point out that, thanks <ref type="bibr" target="#b0">1</ref> They fine-tune a linear classifier with fine labels. We do not consider this task in the body of the paper, but refer to Appendix B.2: our approach provides a significant improvement in this case as well. <ref type="table">Table 2</ref>: Coarse-to-fine: comparison with the state of the art for category-level retrieval (mAP, %) and kNN classification (top-1, %), with the ResNet50 architecture. We compare Grafit with the state of the art <ref type="bibr" target="#b62">[61]</ref> and our stronger baselines. We highlight methods that use more parameters (32.9M vs ∼23.5M), see <ref type="table">Table 5</ref>  to our strong optimization strategy borrowed from recent works <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b51">50]</ref>, this baseline by itself outperforms all published results in several settings, for instance our ResNet-50 baseline without extra training data outperforms on ImageNet a ResNet-50 pretrained on YFCC100M <ref type="bibr" target="#b67">[66]</ref> (see <ref type="table" target="#tab_0">Table 12</ref> in Appendix B for a comparison).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNCA.</head><p>Wu et al. <ref type="bibr" target="#b62">[61]</ref> proposed SNCA, a model optimized with a kNN loss. In our implementation, we add a linear operator P θ to the network trunk f θ when training the supervised loss L knn .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNCA+.</head><p>We improve SNCA with our stronger optimization procedure. The retrieval or kNN evaluation uses features from a MLP instead of a simple linear projector, which means that its number of parameters is on par with Grafit (and larger than Grafit FC).</p><p>ClusterFit+. Same as for SNCA, we improve Clus-terFit <ref type="bibr" target="#b68">[67]</ref> with our training procedure, and crossvalidate the number of clusters (15000 for Imagenet and 1500 for CIFAR-100). As a result we improve its performance and have a fair comparison, everything being equal otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental details</head><p>Architectures. Most experiments are carried out using the ResNet-50 architecture <ref type="bibr" target="#b28">[27]</ref> except for Section 4.6 where we also use RegNet <ref type="bibr" target="#b44">[43]</ref> and ResNeXt <ref type="bibr" target="#b65">[64]</ref>.</p><p>Training settings. Our training procedure borrows from the bag of tricks <ref type="bibr" target="#b29">[28]</ref>: we use SGD with Nesterov  <ref type="bibr" target="#b13">[14]</ref> and Erasing <ref type="bibr" target="#b71">[70]</ref>. We train for 600 epochs with batches of 1024 images at resolution 224 × 224 pixels (except for CIFAR-100: 32 × 32). We set the temperature σ to 0.05 in all our experiments following Wu et al. <ref type="bibr" target="#b62">[61]</ref>. Appendix B.1 gives more details.</p><p>For the on-the-fly classification task, the unique hyper-parameter k is cross-validated in k ∈ {10, 15, 20, 25, 30}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Coarse-to-fine experiments</head><p>CIFAR and ImageNet experiments. <ref type="table">Table 2</ref> compares Grafit results for coarse to fine tasks with the baselines from Section 4.2. On CIFAR-100, Grafit outperforms other methods by +5.5% top-1 accuracy. On ImageNet the gain over other methods is +13.7%.</p><p>Grafit also outperforms other methods on categorylevel retrieval, by 13.2% on CIFAR and 11.1% on Im-ageNet. <ref type="table">Table 2</ref> shows that Grafit not only provides a better on-the-fly classification (as evaluated by the kNN metric), but that the ranked list is more relevant to the query (results for mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-to-Fine with different taxonomic ranks.</head><p>We showcase Grafit on various levels of coarse granularity by training one model on each annotation level of iNaturalist-2018 and evaluating on all levels with kNN classification <ref type="table" target="#tab_2">(Table 3</ref>) and retrieval <ref type="table" target="#tab_3">(Table 4</ref>). <ref type="figure" target="#fig_1">Figure 3</ref> presents results with retrieval and kNN classification for two of the most interesting cases: when the train and test granularities are the same (left), and on the finest test level (Species) with varying granularities at training time (right). On the left,  the accuracy of all methods decreases as the granularity increases: this is expected as the task moves from coarse classification to fine, as it is more difficult to discriminate amongst a larger number of classes. We observe that the performance drop of Grafit for category-level retrieval is reduced in comparison with other methods. On the right figures, the accuracy of all methods increases as the level of annotation increases (keeping evaluation at Species). Grafit also stands out in this context, outperforming other methods.</p><p>We report comprehensive results with Grafit and the baselines from Section 4.2 on iNaturalist-2019 &amp; <ref type="table">Table 5</ref>: Ablation study on CIFAR-100 and ImageNet with ResNet50 architecture. We report results both for on-the-fly classification (kNN classifier, top-1 accuracy, %) and category-level retrieval (mAP, %). The rows corresponding to the main baselines and methods discussed through our paper are highlighted: our baseline and improved SNCA+ in grey and red, and our two variants Grafit-FC and Grafit in blue. The last row is the result that Grafit would obtain with a perfect coarse classification. Visualizations. <ref type="figure">Figure 1</ref> shows visual results for the category-level retrieval task with Grafit. All the results for the baseline and Grafit have the correct coarse label, but our method is better at a finer granularity. In Appendix C we show that the improvement is even more evident when the granularity level at training time is coarser. <ref type="figure">Figure 4</ref> presents t-SNE visualizations <ref type="bibr" target="#b56">[55]</ref> of the latent spaces associated with the baseline and Grafit for images associated with a sub-hierarchy of iNaturalist-2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation studies</head><p>Losses, architectural choice and conditioning. Table 5 presents a study on CIFAR-100 and ImageNet-1k, where we ablate several components of our method. A large improvement stems from the instance loss when it supplements the supervised kNN loss. It is key for discriminating at a finer grain. The categorylevel retrieval significantly benefits from our approach, rising from 22.7% to 44.4% in the best case. Coarse conditioning also has a consistent measurable impact on performance, yielding around 3 mAP points across the various settings.  Settings. We initialize the network trunk with Ima-geNet pre-trained weights and fine-tune only the classifier. For our method, the network trunk f θ remains fixed and the projector P θ is discarded. For all methods we fine-tune during 240 epochs with a cosine learning rate schedule starting at 0.01 and batches of 512 images (details in Appendix B.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sanity check: training with coarse vs fine labels.</head><p>Classifier. We experiment with two types of classifiers: a standard linear classifier (FC) and a multi-layer perceptron (MLP) composed of two linear layers separated by a batch-normalization and a ReLU activation. We introduce this MLP because, during training, both Grafit and SNCA+ employ an MLP projector, so their feature space is not learned to be linearly separable. In contrast, the baseline is trained with a cross-entropy loss associated with a linear classifier.</p><p>Tasks. We evaluate on five classical transfer learning datasets: Oxford Flowers-102 <ref type="bibr" target="#b42">[41]</ref>, Stanford Cars <ref type="bibr" target="#b36">[35]</ref>, Food101 <ref type="bibr" target="#b6">[7]</ref>, iNaturalist 2018 <ref type="bibr" target="#b31">[30]</ref> &amp; 2019 <ref type="bibr" target="#b32">[31]</ref>. Table 1 summarizes some statistics associated with each dataset.</p><p>Results. <ref type="table" target="#tab_7">Table 7</ref> compares a ResNet-50 pretrained on ImageNet with Grafit, SNCA+, ClusterFit <ref type="bibr" target="#b68">[67]</ref> and our baseline on five transfer learning benchmarks. Our method outperforms all methods. The table also shows the relatively strong performance of SNCA+. <ref type="table" target="#tab_8">Table 8</ref> compares Grafit with the RegNetY-8.0GF <ref type="bibr" target="#b44">[43]</ref> architecture against the state of the art, on the same benchmarks. Note that this architecture is significantly faster than the EfficientNet-B7 and ResNet-152 employed in other papers, and that we use a lower resolution in most settings.</p><p>In <ref type="table" target="#tab_8">Table 8</ref> we consider models pre-trained on Ima-geNet with a classifier fine-tuned on the fine-grained target dataset. In each case we report results with Grafit (with a MLP for the projector P θ ) and Grafit FC. See more detailed results in Appendix B <ref type="table" target="#tab_0">Table 16</ref>.</p><p>In summary, Grafit establishes the new state of the art. We point out that we have used a consistent training scheme across all datasets, and a single architecture that is more efficient than in competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper has introduced a procedure to learn a neural network that offers a finer granularity than the one provided by the annotation. It improves the performance for fine-grained category retrieval within a coarsely annotated collection. For on-the-fly kNN classification, Grafit significantly reduces the gap with a network trained with fine labels. It also translates into better transfer learning to fine-grained datasets, outperforming the current state of the art with a more efficient network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material for Paper ID 5395</head><p>"Grafit: Learning fine-grained image representations with coarse labels"</p><p>In this supplementary material we report additional analyses, results and examples that complement our paper. Appendix A presents two experiments on the impact of self-supervised losses on the granularity and distribution of embeddings. Appendix B contains a more accurate description of the experimental settings and more detailed account of the experiments conducted for this paper. Appendix C presents additional visualizations of the ranking obtained with Grafit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. About granularity</head><p>Is it possible to create representations that discriminate between classes finer than the available coarse labels? Considering that we have seen only coarse labels at training time, how can we exploit the coarse classifier for fine-grained classification, if useful at all? In this section we discuss these two questions and construct an experiment to analyze the role of the losses and of the coarse classifier. We then provide empirical observations. Practical setup. In the following two experiments, we consider the CIFAR-100 benchmark that has two granularity levels with 20 and 100 classes (see Section 4.1).</p><p>We denote by f the Resnet-18 trunk mapping from the image space to an embedding space. We train the neural network trunk f with three possible losses:</p><p>• Baseline: regular cross-entropy classification training L CE with coarse or fine classes;</p><p>• Triplet loss: training a triplet loss L Triplet to differentiate between image instances (does not use the labels);</p><p>• L CE + L Triplet : sum of the two losses. This is intended to be a simple proxy of Grafit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Experiment: separating arbitrary fine labels</head><p>This experiment is inspired both by the Rademacher complexity <ref type="bibr" target="#b7">[8]</ref> and by the self-supervised learning (SSL) literature <ref type="bibr" target="#b3">[4]</ref>. In SSL, the standard way to evaluate the quality of a feature extractor f is to measure the accuracy of the network after learning a linear classifier l for the target classes on top of f . The Rademacher complexity measures how a class of <ref type="table">Table 9</ref>: Separability experiment on CIFAR-100. The trunk is trained with coarse labels only. Images with the same coarse label are randomly grouped into two distinct fine-grain labels (40 distinct labels in total). Then we fine-tune a linear classifier on this synthetic labels and measure the top-1 accuracy on fine-labels. When conditioning, the estimator exploits the hierarchy: we first predict the coarse class and condition on it to make the final prediction. We report results with three training losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Top-1 (%) loss no cond. coarse cond.</p><formula xml:id="formula_6">L CE 53.7 ±0.3 54.5 ±0.3 L Triplet 26.4 ±0.3 - L CE + L Triplet 57.1 ±0.2 58.5 ±0.3</formula><p>Random network 8.7 ±0.3 functions (i.e. l • f , with f fixed and l learned) is able to classify a set of images with random binary labels. For this experiment we train the trunk f jointly with a (coarse class) classifier with L CE using coarse labels. We hope to improve the granularity of f , i.e. improve the network trunk such that a (finergrained) classifier l trained on top of f performs better at discriminating between instances that have the same coarse label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random labels.</head><p>We generate synthetic fine labels by the following process: for each coarse label, we randomly and evenly split the training images into two subcategories, yielding 40 classes in total. Inspired by the empirical Rademacher estimation, we sample 10 distinct splits of random labels. For each split, we learn a linear classifier l on top of f i . We then compute the mean accuracy Impact of the loss terms. We report the results in Table 9. We can see that, to distinguish between our synthetic fine labels, training with the triplet loss L Triplet in combination with the classification loss L CE is es- sential: the sum of losses outperforms each individual loss.</p><p>Conditioning. We also measure the impact of conditioning on coarse classes: we first predict the coarse label with the coarse classifier, and leverage its softmax output to classify the fine class. This clearly improves the accuracy, which motivates our fusion strategy inspired by this conditioning in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Experiment: varying the training granularity</head><p>In this section we make empirical observations related to the training granularity in the embedding space.</p><p>We train f with one of the three losses and either coarse or fine labels as supervision. In a second stage, we train a linear classifer l on the Resnet-18 trunk with fine class supervision, and evaluate its accuracy on the test set.</p><p>Accuracies. We first quantify the quality of the representation space. The accuracies are reported in Table <ref type="bibr" target="#b9">10</ref>. We observe that the coarse labels are almost as good as the fine labels as a pre-training. The unsupervised L Triplet loss performs significantly worse, which concurs with our previous separability experiment. Combining this loss with the L CE loss improves is, both with coarse and fine supervisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size of the representation space.</head><p>We quantify the information content of embedding vectors by computing their principal components analysis (PCA). This is a reasonable proxy for information content given that the features are separated by linear classifiers afterwards. We observe the cumulative energy of the PCA components ordered by decreasing energy. We assume that a more uniform energy distribution (and thus lower curves) means that the representation is richer, since a few vector components cannot summarize it.  <ref type="figure" target="#fig_3">Figure 5</ref> shows the results. When training with L CE loss, the most uniform distribution for the principal components is obtained for the fine supervision. This is expected since it is a finer-grain separation of entities and that can not be summarized with a subspace as small as the one associated with a relatively small number of categories. Notice that the training granularity (20 or 100 classes) can be read as an inflexion point on the PCA decomposition curves. The loss L Triplet is not very informative on its own but does improve the cross-entropy representation when combined with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>. This simple preliminary experiment shows that the label granularity has a strong impact on very basic statistics of the embedding distribution. It is the basic intuition behind Grafit: a rich representation can be obtained using just coarse labels, if we combine them with a self-supervised loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additions to the experiments</head><p>This section details the training procedure of Grafit and provides more extensive experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Training settings</head><p>As described in the main part, our training procedure is inspired by Tong et al. <ref type="bibr" target="#b29">[28]</ref>: we use SGD with Nesterov momentum and cosine learning rates decay. We follow Goyal et al.'s <ref type="bibr" target="#b25">[24]</ref> recommendation for the learning rate magnitude: lr = 0.1 256 ×batchsize. The augmentations include random resized crop, RandAugment <ref type="bibr" target="#b13">[14]</ref> and Erasing <ref type="bibr" target="#b71">[70]</ref>. We train for 600 epochs with batches of 1024 images of resolution 224 × 224 pixels (except for CIFAR-100 where the resolution is 32 × 32). For Grafit with L inst we use T = 4 different data-augmentations on ImageNet and T = 8 on <ref type="table" target="#tab_0">Table 11</ref>: Category-level (mAP, %) and one-thefly kNN classification (top-1, %) performance in a coarse-to-fine setting on CIFAR-100 with different loss weighting. Our total loss is L tot (x) = L knn (g θ (x), y) + λL inst (x) with λ being a real-valued coefficient.  CIFAR-100. For the supervised loss we use one dataaugmentation in order to have the same training procedure as our supervised baseline.</p><p>Weighting of the losses. We investigate the impact of weighting the losses L knn and L inst . For example, on CIFAR-100 classification, <ref type="table" target="#tab_0">Table 11</ref> shows that an equal weighting gives the best or near-best results. Therefore, to avoid adding a hyper-parameter and in order to simplify the method, we chose to not use weighting, i.e. we just sum up the two losses.</p><p>A strong Baseline. Our training procedure improves the ResNet-50 performance and thus is a strong baseline against which we can compare Grafit. Therefore, <ref type="table" target="#tab_0">Table 12</ref> compares our baseline on ImageNet with other ResNet-50 training procedures. We observe that our training procedure gives better results than many other approaches. This makes it possible to isolate the contribution of our improved training practices and that of the Grafit loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. {coarse,fine}-to-{coarse,fine}: evaluation</head><p>We compare our main baselines and Grafit's performance in the 4 following scenarios: coarse-to-coarse, coarse-to-fine, fine-to-fine and fine-to-coarse. The evaluations are performed with two classifiers: a kNN classifier (kNN) and a linear classifier fine-tuned (FT) with a cross-entropy loss on top of the embeddings. The results in <ref type="table" target="#tab_0">Table 13</ref> show that Grafit training improves the accuracy in almost all settings, including the fine-to-fine setting, which is just regular classification with the vanilla labels for Imagenet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Coarse-to-Fine with different taxonomic rank</head><p>Datasets. We carry out evaluations on iNaturalist-2018, and with iNaturalist-2019 <ref type="bibr" target="#b32">[31]</ref>, which is a subset of iNaturalist-2018 <ref type="bibr" target="#b31">[30]</ref> where classes with too few images have been removed. iNaturalist 2019 dataset is thus composed of 268,243 images divided into 1,010 classes at the finest level. From the coarse to the finest level, we have 3 classes for Kingdom, 4 classes for Phylum, 9 classes for Class, 34 classes for Order, 57 classes for Family, 72 classes for Genus and 1,010 classes for Species.</p><p>Results. We report exhaustive results with our two coarse-to-fine evaluation protocols with all our baselines on iNaturalist-2018 <ref type="bibr" target="#b31">[30]</ref> and iNaturalist-2019 <ref type="bibr" target="#b32">[31]</ref> in <ref type="table" target="#tab_0">Table 14</ref>.</p><p>We comment more specifically the kNN classification accuracy (left) because for retrieval, Grafit outperform all the baselines by a large margin. The table on the right is divided in 10 matrices each containing results for one combination of a method and a dataset (iNaturalist 2018 or 2019).</p><p>The diagonal values in the matrices correspond to a traditional setting where the training and the test granularity are the same. Even in this case, the Grafit descriptors outperforms the baseline methods most often. On iNaturalist 2018, for Species, the finest and most challenging level, the additional Grafit loss improves the top-1 accuracy by 7% absolute. The gain is more marginal for iNaturalist 2019 (+0.9%), which shows that Grafit is especially useful for unbalanced class distributions where some classes are in a lowshot training regime.</p><p>The lower triangle of each matrix reports the coarse-to-fine results, which is the setting in which we focus in our paper. Grafit obtains the best results for most combinations, with accuracy gains of around 10 points with respect to the baseline and by a few points for ClusterFit+. It is interesting to look at the ∅ column, which is the unsupervised case. In that case, the baseline training reduces to a random network, but Grafit is able to extract signal from the kNN loss.</p><p>The upper triangle is the fine-to-coarse setting, where finer labels are available for the training images than what is used at test time. This is obviously not the setting of the paper but it is worth discussing these results. A natural baseline for fine-to-coarse is to discard the fine labels and train only with the coarse labels induced by the fine annotation. This would yield the same accuracy as on the corresponding entry of the diagonal of the matrix. Irrespective of the method, the fine-to-coarse training does not necessarily outperform this simple strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Transfer Learning Tasks</head><p>This section details the experimental settings for the transfer learning and reports more results and comparisons.</p><p>Fine-tuning settings As described in Section 4. <ref type="bibr" target="#b5">6</ref> We initialize the network trunk with ImageNet pretrained weights and fine-tune only the classifier. For our method, the pre-trained network trunk f θ remains fixed. The projector P θ is discarded. For all methods we fine-tune during 240 epochs with a cosine learning rate schedule starting at 0.01, and batches of 512 images.</p><p>For fine-tuning results in <ref type="table" target="#tab_8">Table 8</ref> we additionally use Cutmix <ref type="bibr" target="#b69">[68]</ref> and FixRes <ref type="bibr" target="#b54">[53]</ref> during fine-tuning and we fine-tune with more epochs (1000 for Flowers <ref type="bibr" target="#b42">[41]</ref> and Cars <ref type="bibr" target="#b36">[35]</ref>, 300 for Food-101 <ref type="bibr" target="#b6">[7]</ref> and iNaturalist <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b32">31]</ref>). These choices improve the performance for all the methods. <ref type="table" target="#tab_0">Table 15</ref> compares the performance obtained with Grafit for different architectures. We report results with Grafit topped with either a multilayers perceptron (MLP) or a linear classifier (FC). The accuracy increases for larger models. This shows that, although ResNet-50 serves as a running example architecture for Grafit, the method applies without modifications to other architectures. <ref type="table" target="#tab_0">Table 16</ref> compares the performances= obtained with Grafit and baselines with MLP and FC classifier. For all settings, the flexibility of the MLP is useful to outperform the linear classifier (FC). The transfer learning results are better or as good for Grafit variants. The gap with the baseline methods is higher for the iNaturalist variants. This is because the datasets are more challenging, as evidenced by the relatively low accuracies reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization</head><p>CIFAR. <ref type="figure">Figure 6</ref> shows for a giving test image in CIFAR-100 the 10 nearest neighbours in the training according the cosine similarity in the embedding space. In <ref type="figure">Figure 6</ref> models are trained on the 20 CIFAR-100 coarse classes. The correct classes are indicated in green. For example, in the first row, Grafit correctly identifies a butterfly query in 9 out of 10 results, while the baseline method succeeds only 5 times. The second row is a relative failure case, because Grafit confuses a van with a pickup truck. However, it correctly matches the colors of the vehicles.</p><p>iNaturalist. <ref type="figure">Figure 7, 8</ref> and <ref type="figure">Figure 9</ref> present similar results for three examples for iNaturalist-2018, but with several levels of granularity for the training set, which allow one to vizualize the importance of the training granularity as well. Each granularity level is identified with a color. The frame color around the image indicates which at which granularity the match is correct: for example, light orange means it is correct at the order level and green means that the result is correct at the finest granularity (Species).</p><p>We can observe from the colors and the image content that the level at which Grafit is correct is almost systematically better than the baseline 2 . For example, the baseline model trained at the genus granularity in <ref type="figure">Figure 7</ref> matches the deer query with a moose (rank 3).</p><p>In <ref type="figure">Figure 8</ref>, the butterfly is matched relatively easily with other butterflies by both classifiers, even when they are trained with coarse granularity. This is because butterflies have quite distinctive textures. However, Grafit slightly outperforms the baseline for finer granularity levels. <ref type="figure">Figure 9</ref> shows an orca query, which is quite distinctive with its black-and-white skin. The baseline <ref type="table" target="#tab_0">Table 14</ref>: Evaluation on iNaturalist-2018/2019 with all combinations of training / testing semantic levels. Left: on-the-fly k-NN classification accuracy (top-1, %) Right: category-level retrieval (mAP, %). We highlight the best and second-best result across methods for each train-test granularity combination. The diagonals (test = train granularity) are in bold. Lower triangles are coarse-to-fine combinations, handled in the paper. Upper triangles (fine-to-coarse) are reported for reference but not formally addressed by our approach: better strategies would exploit the hierarchy of concepts more explicitly. method is unable to distinguish it from other marine mammals, even when trained at the finest granularity. Grafit is able to distinguish these textures more accurately, so it gets perfect retrieval results even when trained at the genus granularity.   <ref type="figure">Figure 6</ref>: CIFAR-100: For given test images (top), we present the ranked list of train images that are most similar with embeddings obtained with a baseline method (top) and our method (bottom) train with coarse labels. Images in green indicate that the image belongs to the correct fine class; orange indicates the correct coarse class but incorrect fine class. In this example, all results are correct w.r.t. coarse granularity.  <ref type="figure">Figure 7</ref>: We compare Grafit and Baseline for different training granularity. We rank the 10 closest images in the iNaturalist-2018 train set for a query image in the test set. The ranking is obtained with a cosine similarity on the features space of each of the two approaches. See <ref type="table" target="#tab_0">Table 17</ref> for authors and image copyrights.  <ref type="figure">Figure 8</ref>: We compare Grafit and Baseline for different training granularity. We rank the 10 closest images in the iNaturalist-2018 train set for a query image in the test set. The ranking is obtained with a cosine similarity on the features space of each of the two approaches. See <ref type="table" target="#tab_0">Table 18</ref> for authors and image copyrights.  <ref type="figure">Figure 9</ref>: We compare Grafit and Baseline for different training granularity. We rank the 10 closest images in the iNaturalist-2018 train set for a query image in the test set. The ranking is obtained with a cosine similarity on the features space of each of the two approaches. See <ref type="table" target="#tab_0">Table 19</ref> for authors and image copyrights.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Combined loss .</head><label>loss</label><figDesc>Our method is summarized in Figure 2. The total loss at training time for an image x with label y is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>ki n g d o m p h yl u m cl a ss o rd er fa m ily g en u s sp ec ie s d o m p h yl u m cl a ss o rd er fa m ily g en u s sp ec ie s d o m p h yl u m cl a ss o rd er fa m ily g en u s sp ec ie s d o m p h yl u m cl a ss o rd er fa m ily g en u s sp ec ie s Evaluation on iNaturalist-2018 [30] with and left: train=test granularity right: test at finest granularity (species). We compare our method Grafit, SNCA+, ClusterFit+ and Baseline. Top: on-the-fly kNN classification (top-1 accuracy); bottom: category-level retrieval (mAP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(top-1, %) of l • f i on the training examples for the three losses. By evaluating to what extent one can fit a linear classifier l on top of f , this experiment measures how well the data are spread in the representation spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Total Energy CE (fine) CE + Triplet (fine) CE (coarse) CE + Triplet (coarse) Triplet (unsupervised) Cumulative energy of the PCA decomposition of CIFAR-100 image embeddings, depending on the granularity of the training labels (20 or 100 classes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets used for our different tasks. The four top datasets offer two or more levels of granularity, we use them for all coarse-to-fine tasks. The bottom three are fine-grained datasets employed to evaluate transfer learning.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Train size Test size</cell><cell>#classes</cell></row><row><cell>CIFAR-100 [36]</cell><cell>50,000</cell><cell>10,000</cell><cell>20/100</cell></row><row><cell>ImageNet [46]</cell><cell>1,281,167</cell><cell>50,000</cell><cell>127/1000</cell></row><row><cell>iNaturalist 2018 [30]</cell><cell>437,513</cell><cell cols="2">24,426 6/. . . /8,142</cell></row><row><cell>iNaturalist 2019 [31]</cell><cell>265,240</cell><cell cols="2">3,003 6/. . . /1,010</cell></row><row><cell>Flowers-102 [41]</cell><cell>2,040</cell><cell>6,149</cell><cell>102</cell></row><row><cell>Stanford Cars [35]</cell><cell>8,144</cell><cell>8,041</cell><cell>196</cell></row><row><cell>Food101 [7]</cell><cell>75,750</cell><cell>25,250</cell><cell>101</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for details.</figDesc><table><row><cell>Method</cell><cell cols="2">CIFAR-100 kNN mAP</cell><cell cols="2">ImageNet-1k kNN mAP</cell></row><row><cell>Baseline, Wu et al. [61]</cell><cell>54.2</cell><cell></cell><cell>48.1</cell><cell></cell></row><row><cell>SNCA, Wu et al. [61]</cell><cell>62.3</cell><cell></cell><cell>52.8</cell><cell></cell></row><row><cell>Baseline (ours)</cell><cell>71.8</cell><cell>42.5</cell><cell>54.7</cell><cell>22.7</cell></row><row><cell>ClusterFit+</cell><cell>72.5</cell><cell>23.0</cell><cell>59.5</cell><cell>12.7</cell></row><row><cell>SNCA+</cell><cell>72.2</cell><cell>35.9</cell><cell>55.4</cell><cell>31.8</cell></row><row><cell>Grafit FC</cell><cell>75.6</cell><cell>55.0</cell><cell>69.1</cell><cell>44.4</cell></row><row><cell>Grafit</cell><cell>77.7</cell><cell>55.7</cell><cell>69.1</cell><cell>42.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>kNN evaluation on iNaturalist-2018 with different semantic levels. The symbol ∅ refers to the unsupervised case (a unique class). We compare with the best competing method according toTable 2.</figDesc><table><row><cell></cell><cell>Train →</cell><cell>∅</cell><cell cols="7">Kingdom Phylum class Order Family Genus Species</cell></row><row><cell cols="2">↓Test / #classes →</cell><cell>1</cell><cell>6</cell><cell>25</cell><cell>57</cell><cell>272</cell><cell>1,118</cell><cell>4,401</cell><cell>8,142</cell></row><row><cell></cell><cell cols="2">Kingdom 70.9</cell><cell>94.7</cell><cell>95.0</cell><cell>95.3</cell><cell>95.6</cell><cell>96.2</cell><cell>96.3</cell><cell>96.1</cell></row><row><cell>ClusterFit+</cell><cell cols="2">Phylum 48.8 Class 40.4 Order 17.1 Family 5.6 Genus 0.9</cell><cell>87.4 80.2 54.5 38.3 26.7</cell><cell>90.3 83.8 59.0 42.1 29.5</cell><cell>90.7 85.7 61.4 44.4 31.5</cell><cell>91.1 86.7 70.8 54.3 40.1</cell><cell>92.6 88.8 73.9 63.0 49.4</cell><cell>92.6 88.8 74.3 64.2 53.9</cell><cell>92.2 88.2 72.3 61.9 51.7</cell></row><row><cell></cell><cell>Species</cell><cell>0.3</cell><cell>21.8</cell><cell>23.7</cell><cell>25.2</cell><cell>32.7</cell><cell>40.3</cell><cell>44.7</cell><cell>43.4</cell></row><row><cell></cell><cell cols="2">Kingdom 95.5</cell><cell>98.1</cell><cell>98.2</cell><cell>98.2</cell><cell>98.2</cell><cell>98.2</cell><cell>98.4</cell><cell>98.3</cell></row><row><cell></cell><cell cols="2">Phylum 90.0</cell><cell>94.1</cell><cell>96.6</cell><cell>96.7</cell><cell>96.8</cell><cell>96.7</cell><cell>96.9</cell><cell>96.7</cell></row><row><cell>Grafit</cell><cell cols="2">Class 82.2 Order 54.0 Family 33.7</cell><cell>87.5 61.7 42.1</cell><cell>90.9 66.9 48.7</cell><cell>94.5 72.7 55.1</cell><cell>94.9 87.1 70.9</cell><cell>94.9 87.5 81.8</cell><cell>95.0 87.6 82.4</cell><cell>95.0 87.3 82.1</cell></row><row><cell></cell><cell cols="2">Genus 20.5</cell><cell>27.0</cell><cell>33.5</cell><cell>39.5</cell><cell>54.2</cell><cell>64.6</cell><cell>75.6</cell><cell>75.5</cell></row><row><cell></cell><cell cols="2">Species 15.9</cell><cell>20.4</cell><cell>25.5</cell><cell>30.8</cell><cell>42.7</cell><cell>51.2</cell><cell>61.9</cell><cell>67.7</cell></row><row><cell cols="9">momentum and cosine learning rates decay. We fol-</cell></row><row><cell cols="9">low Goyal et al.'s [24] recommendation for the learn-</cell></row><row><cell cols="9">ing rate magnitude: lr = 0.1 256 ×batchsize. The data aug-</cell></row><row><cell cols="9">mentation consists of random resized crop, RandAug-</cell></row><row><cell>ment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Category-retrieval evaluation (mAP, %) on iNaturalist-2018 with different semantics levels. We compare with the best baseline according toTable 2.Train → Kingdom Phylum class Order Family Genus Species ↓Test / #classes →</figDesc><table><row><cell></cell><cell></cell><cell>6</cell><cell>25</cell><cell>57</cell><cell>272</cell><cell>1,118</cell><cell>4,401</cell><cell>8,142</cell></row><row><cell></cell><cell>Kingdom</cell><cell>97.6</cell><cell>83.3</cell><cell>75.9</cell><cell>59.2</cell><cell>56.0</cell><cell>54.9</cell><cell>55.0</cell></row><row><cell></cell><cell>Phylum</cell><cell>59.8</cell><cell>91.7</cell><cell>79.4</cell><cell>49.1</cell><cell>35.0</cell><cell>32.3</cell><cell>32.2</cell></row><row><cell>SNCA+</cell><cell>Class Order Family</cell><cell>41.3 9.09 2.24</cell><cell>73.1 24.9 6.43</cell><cell>89.9 35.7 11.2</cell><cell>49.2 77.9 35.7</cell><cell>28.1 35.3 68.4</cell><cell>23.6 18.0 29.1</cell><cell>23.0 15.0 21.7</cell></row><row><cell></cell><cell>Genus</cell><cell>0.39</cell><cell>2.47</cell><cell>5.03</cell><cell>18.1</cell><cell>36.6</cell><cell>60.5</cell><cell>46.0</cell></row><row><cell></cell><cell>Species</cell><cell>0.19</cell><cell>1.86</cell><cell>3.80</cell><cell>12.8</cell><cell>26.4</cell><cell>46.0</cell><cell>54.9</cell></row><row><cell></cell><cell>Kingdom</cell><cell>98.6</cell><cell>88.3</cell><cell>79.7</cell><cell>60.8</cell><cell>58.0</cell><cell>55.9</cell><cell>55.5</cell></row><row><cell></cell><cell>Phylum</cell><cell>67.8</cell><cell>97.2</cell><cell>82.1</cell><cell>50.9</cell><cell>38.9</cell><cell>34.2</cell><cell>33.0</cell></row><row><cell>Grafit</cell><cell>Class Order Family</cell><cell>50.1 17.7 8.70</cell><cell>74.9 30.7 13.2</cell><cell>95.4 42.7 18.0</cell><cell>51.2 88.3 43.9</cell><cell>32.3 42.3 83.1</cell><cell>25.9 21.1 34.8</cell><cell>24.1 16.2 24.2</cell></row><row><cell></cell><cell>Genus</cell><cell>6.78</cell><cell>9.72</cell><cell>13.5</cell><cell>29.0</cell><cell>46.9</cell><cell>77.2</cell><cell>53.9</cell></row><row><cell></cell><cell>Species</cell><cell>6.45</cell><cell>9.02</cell><cell>12.1</cell><cell>23.6</cell><cell>35.6</cell><cell>55.4</cell><cell>70.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>This concurs with our prior observations in Section 4.4 on iNaturalist-2018.Overall, in this setting Grafit provides some slight yet systematic improvement over the baseline. With a ResNet-50 architecture at image resolution 224 × 224 pixels, Grafit reaches 79.6% top-1 accuracy with a kNN classifier on ImageNet, which is competitive with classical cross-entropy results published for this architecture. See Appendix B for a comparison (Table 12) and more results on Imagenet.</figDesc><table><row><cell>compares the performance gap of several</cell></row></table><note>methods when training with coarse labels vs fine la- bels. The performance improvement of Grafit over competing methods on Imagenet is quite sizable: with fine-tuning, Grafit with coarse labels is almost on par with the baseline on fine labels. For on-the-fly classifi- cation, Grafit with coarse labels reaches 69.1% perfor- mance on Imagenet, significantly decreasing the gap with fine-grained labels settings. The kNN classifi- cation performance is 79.3%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>We compare coarse-to-fine and fine-to-fine context with mAP (%), kNN (top-1, %) and finetuning (FT) of a linear classifier with fine labels (top-1, %) on ImageNet.</figDesc><table><row><cell>Method</cell><cell cols="3">Train Coarse</cell><cell></cell><cell>Train Fine</cell><cell></cell></row><row><cell cols="3">(with ResNet50) mAP kNN</cell><cell>FT</cell><cell cols="2">mAP kNN</cell><cell>FT</cell></row><row><cell>Baseline</cell><cell>22.7</cell><cell>54.7</cell><cell>78.1</cell><cell>51.5</cell><cell>78.0</cell><cell>79.3</cell></row><row><cell>SNCA+</cell><cell>31.8</cell><cell>55.4</cell><cell>77.9</cell><cell>72.0</cell><cell>79.1</cell><cell>77.4</cell></row><row><cell>Grafit FC</cell><cell>44.4</cell><cell>69.1</cell><cell>78.3</cell><cell>72.4</cell><cell>79.2</cell><cell>78.5</cell></row><row><cell>Grafit</cell><cell>42.9</cell><cell>69.1</cell><cell>77.9</cell><cell>71.2</cell><cell>79.6</cell><cell>78.0</cell></row><row><cell cols="6">4.6. Transfer Learning to fine-grained datasets</cell><cell></cell></row><row><cell cols="7">We now evaluate Grafit for transfer learning on</cell></row><row><cell cols="7">fine-grained datasets (See) Table 2, with ImageNet</cell></row><row><cell>pre-training.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of transfer learning performance for different pre-training methods. All methods use a ResNet-50 pre-trained on Imagenet. The training procedues are the same (except the result reported for ClusterFit<ref type="bibr" target="#b68">[67]</ref>). We report the top-1 accuracy (%) with a single center crop evaluation at resolution 224 × 224. SeeTable 15of Appendix B.4 for additional results with other architectures.</figDesc><table><row><cell>Dataset</cell><cell>Baseline</cell><cell>ClusterFit</cell><cell>[67]</cell><cell>ClusterFit+</cell><cell>SNCA+</cell><cell>Grafit</cell><cell>Grafit FC</cell></row><row><cell>Flowers-102</cell><cell>96.2</cell><cell></cell><cell></cell><cell cols="4">96.2 98.2 98.2 97.6</cell></row><row><cell>Stanford Cars</cell><cell>90.0</cell><cell></cell><cell></cell><cell cols="4">89.4 92.5 92.5 92.7</cell></row><row><cell>Food101</cell><cell>88.9</cell><cell></cell><cell></cell><cell cols="4">88.9 88.8 89.5 88.7</cell></row><row><cell cols="2">iNaturalist 2018 68.4</cell><cell cols="2">49.7</cell><cell cols="4">67.5 69.2 69.8 68.5</cell></row><row><cell cols="2">iNaturalist 2019 73.7</cell><cell></cell><cell></cell><cell cols="4">73.8 74.5 75.9 74.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Best reported results (%)</cell><cell></cell><cell></cell><cell>Grafit</cell></row><row><cell>Dataset</cell><cell>State of the art</cell><cell cols="4"># Params Res Top-1 Top-1</cell></row><row><cell>Flowers-102</cell><cell>EfficientNet-B7 [50]</cell><cell cols="2">64M 600</cell><cell>98.8</cell><cell>99.1</cell></row><row><cell>Stanford Cars</cell><cell>EfficientNet-B7 [50]</cell><cell cols="2">64M 600</cell><cell>94.7</cell><cell>94.7</cell></row><row><cell>Food101</cell><cell>EfficientNet-B7 [50]</cell><cell cols="2">64M 600</cell><cell>93.0</cell><cell>93.7</cell></row><row><cell cols="2">iNaturalist 2018 ResNet-152 [12]</cell><cell cols="2">60M 224</cell><cell>69.1</cell><cell>81.2</cell></row><row><cell cols="2">iNaturalist 2019 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.1</cell></row></table><note>State of the art for transfer learning with pre- trained ImageNet-1k models. We report top-1 accu- racy (%) with a single center crop. For Grafit we use a 39M-parameter RegNetY-8.0GF [43] with resolution 384×384 pixels that is 4× faster than EfficientNetB7 at inference. "Res" is the inference resolution in pixels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Top-1 accuracy of a ResNet-18 on CIFAR-100 for different training schemes. We report the results after finetuning of the linear classifier on the fine labels (see Section A). The Triplet training is unsupervised, so the two columns are the same.</figDesc><table><row><cell>Method</cell><cell cols="2">Train Coarse Train Fine</cell></row><row><cell>L CE</cell><cell>80.4 ±0.2</cell><cell>80.6 ±0.2</cell></row><row><cell>L Triplet</cell><cell>76.5 ±0.2</cell><cell>76.5 ±0.2</cell></row><row><cell>L CE + L Triplet</cell><cell>80.9 ±0.2</cell><cell>81.3 ±0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table><row><cell cols="3">Performance comparison (top-1 accuracy)</cell></row><row><cell cols="3">with our ResNet-50 baseline and state of the art</cell></row><row><cell cols="3">ResNet-50 on ImageNet. All results are with single</cell></row><row><cell cols="3">center crop evaluation with image resolution 224 ×</cell></row><row><cell>224.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Extra-data</cell><cell>Top-1 (%)</cell></row><row><cell>ResNet-50 [27] PyTorch</cell><cell></cell><cell>76.2</cell></row><row><cell>RandAugment [14]</cell><cell></cell><cell>77.6</cell></row><row><cell>CutMix [68]</cell><cell></cell><cell>78.6</cell></row><row><cell>Noisy-Student [63]</cell><cell>JFT-300M [63]</cell><cell>78.9</cell></row><row><cell>Billion Scale [66]</cell><cell cols="2">YFCC100M [52] 79.1</cell></row><row><cell>Our Baseline</cell><cell></cell><cell>79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Performance comparison (top-1 accuracy) when learning and testing at different granularities (ResNet50). For CIFAR-100, there are 100 fine and 20 coarse concepts. ImageNet covers 1000 fine and 127 coarse concepts. We report the results of both the kNN classifier and of a linear classifier fine-tuned with the target granularity (FT).</figDesc><table><row><cell></cell><cell>Method</cell><cell>↓ Test</cell><cell cols="2">Train Coarse kNN FT</cell><cell cols="2">Train Fine kNN FT</cell></row><row><cell>CIFAR-100</cell><cell>Baseline SNCA+ Grafit Baseline SNCA+</cell><cell>Coarse Fine</cell><cell cols="4">89.3 ±0.1 88.4 ±0.3 90.6 ±0.1 71.8 ±0.3 72.2 ±0.3 82.0 ±0.4 81.7 ±0.1 82.9 ±0.1 89.4 ±0.2 90.3 ±0.1 90.5 ±0.2 88.9 ±0.3 88.8 ±0.1 90.2 ±0.1 90.6 ±0.1 90.6 ±0.3 90.9 ±0.2 82.3 ±0.2 82.7 ±0.2 82.7 ±0.2</cell></row><row><cell></cell><cell>Grafit</cell><cell></cell><cell>77.7 ±0.2</cell><cell>83.7 ±0.2</cell><cell>83.2 ±0.3</cell><cell>83.7 ±0.2</cell></row><row><cell>ImageNet-1k</cell><cell>Baseline SNCA+ Grafit Baseline SNCA+</cell><cell>Coarse Fine</cell><cell>87.0 ±0.1 87.7 ±0.1 88.4 ±0.1 54.7 ±0.2 55.4 ±0.2</cell><cell>87.6 ±0.1 87.5 ±0.1 87.3 ±0.1 78.1 ±0.1 77.9 ±0.1</cell><cell>87.4 ±0.1 88.9 ±0.1 89.2 ±0.1 78.0 ±0.1 79.1 ±0.1</cell><cell>87.9 ±0.1 87.2 ±0.1 87.7 ±0.1 79.3 ±0.1 77.4 ±0.1</cell></row><row><cell></cell><cell>Grafit</cell><cell></cell><cell>69.1 ±0.2</cell><cell>77.9 ±0.1</cell><cell>79.6 ±0.1</cell><cell>78.0 ±0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Test \ Train King. Phyl. Class Order Fam. Gen. Spec.</figDesc><table><row><cell></cell><cell>Test \ Train</cell><cell>∅</cell><cell cols="7">King. Phyl. Class Order Fam. Gen. Spec.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">iNaturalist-2018</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">iNaturalist-2018</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell># classes:</cell><cell>1</cell><cell>6</cell><cell>25</cell><cell>57</cell><cell>272</cell><cell cols="2">1118 4401</cell><cell>8142</cell><cell></cell><cell># classes:</cell><cell>6</cell><cell>25</cell><cell>57</cell><cell>272</cell><cell cols="2">1118 4401</cell><cell>8142</cell></row><row><cell></cell><cell>Kingdom</cell><cell>70.9</cell><cell>97.6</cell><cell>98.0</cell><cell>98.1</cell><cell>98.2</cell><cell>98.2</cell><cell>97.9</cell><cell>97.5</cell><cell></cell><cell>Kingdom</cell><cell>97.8</cell><cell>86.3</cell><cell>81.0</cell><cell>76.4</cell><cell>65.9</cell><cell>62.1</cell><cell>61.5</cell></row><row><cell>Baseline</cell><cell>Phylum Class Order Family</cell><cell>48.8 40.4 17.1 5.6</cell><cell>88.0 77.1 43.6 23.0</cell><cell>96.3 86.7 55.0 32.8</cell><cell>96.4 94.1 61.0 36.7</cell><cell>96.6 94.7 85.6 62.0</cell><cell>96.7 94.8 86.6 80.7</cell><cell>96.2 94.1 85.5 79.7</cell><cell>95.2 92.9 82.6 76.1</cell><cell>Baseline</cell><cell>Phylum Class Order Family</cell><cell>64.2 46.0 12.2 3.69</cell><cell>96.6 72.1 24.1 7.02</cell><cell>82.1 93.8 34.3 10.1</cell><cell>63.1 60.1 74.5 32.6</cell><cell>45.9 39.2 35.4 51.3</cell><cell>39.8 31.2 20.1 20.9</cell><cell>38.1 28.5 15.6 14.5</cell></row><row><cell></cell><cell>Genus</cell><cell>0.9</cell><cell>10.0</cell><cell>17.3</cell><cell>20.1</cell><cell>41.7</cell><cell>63.0</cell><cell>72.5</cell><cell>68.3</cell><cell></cell><cell>Genus</cell><cell>1.30</cell><cell>3.06</cell><cell>4.47</cell><cell>16.6</cell><cell>30.4</cell><cell>33.3</cell><cell>24.0</cell></row><row><cell></cell><cell>Species</cell><cell>0.3</cell><cell>6.3</cell><cell>11.5</cell><cell>13.6</cell><cell>31.2</cell><cell>51.3</cell><cell>61.6</cell><cell>60.2</cell><cell></cell><cell>Species</cell><cell>1.18</cell><cell>2.63</cell><cell>3.63</cell><cell>12.8</cell><cell>25.7</cell><cell>31.4</cell><cell>27.9</cell></row><row><cell></cell><cell>Kingdom</cell><cell>71.2</cell><cell>97.7</cell><cell>97.9</cell><cell>98.1</cell><cell>97.9</cell><cell>98.0</cell><cell>98.2</cell><cell>98.3</cell><cell></cell><cell>Kingdom</cell><cell>97.6</cell><cell>83.3</cell><cell>75.9</cell><cell>59.2</cell><cell>56.0</cell><cell>54.9</cell><cell>55.0</cell></row><row><cell></cell><cell>Phylum</cell><cell>48.0</cell><cell>68.7</cell><cell>96.1</cell><cell>96.4</cell><cell>96.4</cell><cell>96.5</cell><cell>96.7</cell><cell>96.7</cell><cell></cell><cell>Phylum</cell><cell>59.8</cell><cell>91.7</cell><cell>79.4</cell><cell>49.1</cell><cell>35.0</cell><cell>32.3</cell><cell>32.2</cell></row><row><cell>SNCA+</cell><cell>Class Order Family</cell><cell>39.4 16.2 5.2</cell><cell>56.7 23.3 7.8</cell><cell>84.8 47.4 23.2</cell><cell>93.9 59.0 33.2</cell><cell>94.3 85.4 57.8</cell><cell>94.6 86.2 80.2</cell><cell>94.7 86.7 81.1</cell><cell>94.7 86.7 81.3</cell><cell>SNCA+</cell><cell>Class Order Family</cell><cell>41.3 9.09 2.24</cell><cell>73.1 24.9 6.43</cell><cell>89.9 35.7 11.2</cell><cell>49.2 77.9 35.7</cell><cell>28.1 35.3 68.4</cell><cell>23.6 18.0 29.1</cell><cell>23.0 15.0 21.7</cell></row><row><cell></cell><cell>Genus</cell><cell>0.9</cell><cell>1.3</cell><cell>10.4</cell><cell>17.6</cell><cell>36.9</cell><cell>56.8</cell><cell>74.2</cell><cell>74.1</cell><cell></cell><cell>Genus</cell><cell>0.39</cell><cell>2.47</cell><cell>5.03</cell><cell>18.1</cell><cell>36.6</cell><cell>60.5</cell><cell>46.0</cell></row><row><cell></cell><cell>Species</cell><cell>0.3</cell><cell>0.5</cell><cell>6.3</cell><cell>11.9</cell><cell>26.2</cell><cell>42.4</cell><cell>58.9</cell><cell>64.6</cell><cell></cell><cell>Species</cell><cell>0.19</cell><cell>1.86</cell><cell>3.80</cell><cell>12.8</cell><cell>26.4</cell><cell>46.0</cell><cell>54.9</cell></row><row><cell></cell><cell>Kingdom</cell><cell>70.9</cell><cell>94.7</cell><cell>95.0</cell><cell>95.3</cell><cell>95.6</cell><cell>96.2</cell><cell>96.3</cell><cell>96.1</cell><cell></cell><cell>Kingdom</cell><cell>55.5</cell><cell>55.5</cell><cell>55.7</cell><cell>56.4</cell><cell>57.0</cell><cell>57.6</cell><cell>57.7</cell></row><row><cell>ClusterFit+</cell><cell>Phylum Class Order Family Genus</cell><cell>48.8 40.4 17.1 5.6 0.9</cell><cell>87.4 80.2 54.5 38.3 26.7</cell><cell>90.3 83.8 59.0 42.1 29.5</cell><cell>90.7 85.7 61.4 44.4 31.5</cell><cell>91.1 86.7 70.8 54.3 40.1</cell><cell>92.6 88.8 73.9 63.0 49.4</cell><cell>92.6 88.8 74.3 64.2 53.9</cell><cell>92.2 88.2 72.3 61.9 51.7</cell><cell>ClusterFit+</cell><cell>Phylum Class Order Family Genus</cell><cell>31.6 21.0 6.8 2.9 3.6</cell><cell>32.1 21.6 7.4 3.5 4.3</cell><cell>32.1 22.0 7.8 3.9 4.8</cell><cell>32.4 22.2 9.4 5.5 7.1</cell><cell>33.1 23.0 9.9 7.8 10.8</cell><cell>33.9 23.7 10.3 7.9 13.3</cell><cell>34.0 23.8 10.1 7.3 12.0</cell></row><row><cell></cell><cell>Species</cell><cell>0.3</cell><cell>21.8</cell><cell>23.7</cell><cell>25.2</cell><cell>32.7</cell><cell>40.3</cell><cell>44.7</cell><cell>43.4</cell><cell></cell><cell>Species</cell><cell>4.7</cell><cell>5.4</cell><cell>5.9</cell><cell>8.6</cell><cell>12.5</cell><cell>15.3</cell><cell>14.5</cell></row><row><cell></cell><cell>Kingdom</cell><cell>91.1</cell><cell>97.8</cell><cell>98.1</cell><cell>98.4</cell><cell>98.3</cell><cell>98.4</cell><cell>98.5</cell><cell>98.4</cell><cell></cell><cell>Kingdom</cell><cell>98.5</cell><cell>88.3</cell><cell>80.6</cell><cell>61.6</cell><cell>57.7</cell><cell>56.0</cell><cell>56.0</cell></row><row><cell>Grafit FC</cell><cell>Phylum Class Order Family</cell><cell>81.7 71.9 41.8 22.4</cell><cell>93.0 86.0 58.5 38.8</cell><cell>96.4 90.7 66.8 48.4</cell><cell>96.9 94.8 72.2 54.4</cell><cell>97.0 95.0 86.8 70.4</cell><cell>96.9 95.1 87.1 81.1</cell><cell>97.1 95.3 87.3 81.6</cell><cell>96.8 95.0 87.2 81.7</cell><cell>Grafit FC</cell><cell>Phylum Class Order Family</cell><cell>69.6 52.4 18.6 7.68</cell><cell>97.2 75.8 31.4 12.9</cell><cell>83.1 95.7 41.6 17.7</cell><cell>50.5 51.3 88.0 44.7</cell><cell>37.9 31.3 41.6 82.4</cell><cell>33.9 25.5 20.4 33.4</cell><cell>33.3 24.5 16.4 23.6</cell></row><row><cell></cell><cell>Genus</cell><cell>11.4</cell><cell>24.6</cell><cell>33.1</cell><cell>38.6</cell><cell>53.0</cell><cell>63.9</cell><cell>73.8</cell><cell>74.2</cell><cell></cell><cell>Genus</cell><cell>4.97</cell><cell>8.82</cell><cell>11.9</cell><cell>27.3</cell><cell>45.0</cell><cell>75.5</cell><cell>52.2</cell></row><row><cell></cell><cell>Species</cell><cell>8.13</cell><cell>18.8</cell><cell>25.6</cell><cell>29.9</cell><cell>41.5</cell><cell>50.9</cell><cell>60.9</cell><cell>65.9</cell><cell></cell><cell>Species</cell><cell>4.95</cell><cell>8.25</cell><cell>10.7</cell><cell>21.4</cell><cell>33.6</cell><cell>53.8</cell><cell>68.1</cell></row><row><cell></cell><cell>Kingdom</cell><cell>95.5</cell><cell>98.1</cell><cell>98.2</cell><cell>98.2</cell><cell>98.2</cell><cell>98.2</cell><cell>98.4</cell><cell>98.3</cell><cell></cell><cell>Kingdom</cell><cell>98.6</cell><cell>88.3</cell><cell>79.7</cell><cell>60.8</cell><cell>58.0</cell><cell>55.9</cell><cell>55.5</cell></row><row><cell></cell><cell>Phylum</cell><cell>90.0</cell><cell>94.1</cell><cell>96.6</cell><cell>96.7</cell><cell>96.8</cell><cell>96.7</cell><cell>96.9</cell><cell>96.7</cell><cell></cell><cell>Phylum</cell><cell>67.8</cell><cell>97.2</cell><cell>82.1</cell><cell>50.9</cell><cell>38.9</cell><cell>34.2</cell><cell>33.0</cell></row><row><cell>Grafit</cell><cell>Class Order Family</cell><cell>82.2 54.0 33.7</cell><cell>87.5 61.7 42.1</cell><cell>90.9 66.9 48.7</cell><cell>94.5 72.7 55.1</cell><cell>94.9 87.1 70.9</cell><cell>94.9 87.5 81.8</cell><cell>95.0 87.6 82.4</cell><cell>95.0 87.3 82.1</cell><cell>Grafit</cell><cell>Class Order Family</cell><cell>50.1 17.7 8.70</cell><cell>74.9 30.7 13.2</cell><cell>95.4 42.7 18.0</cell><cell>51.2 88.3 43.9</cell><cell>32.3 42.3 83.1</cell><cell>25.9 21.1 34.8</cell><cell>24.1 16.2 24.2</cell></row><row><cell></cell><cell>Genus</cell><cell>20.5</cell><cell>27.0</cell><cell>33.5</cell><cell>39.5</cell><cell>54.2</cell><cell>64.6</cell><cell>75.6</cell><cell>75.5</cell><cell></cell><cell>Genus</cell><cell>6.78</cell><cell>9.72</cell><cell>13.5</cell><cell>29.0</cell><cell>46.9</cell><cell>77.2</cell><cell>53.9</cell></row><row><cell></cell><cell>Species</cell><cell>15.9</cell><cell>20.4</cell><cell>25.5</cell><cell>30.8</cell><cell>42.7</cell><cell>51.2</cell><cell>61.9</cell><cell>67.7</cell><cell></cell><cell>Species</cell><cell>6.45</cell><cell>9.02</cell><cell>12.1</cell><cell>23.6</cell><cell>35.6</cell><cell>55.4</cell><cell>70.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">iNaturalist-2019</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">iNaturalist-2019</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell># classes:</cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>9</cell><cell>34</cell><cell>57</cell><cell>72</cell><cell>1010</cell><cell></cell><cell># classes:</cell><cell>3</cell><cell>4</cell><cell>9</cell><cell>34</cell><cell>57</cell><cell>72</cell><cell>1010</cell></row><row><cell></cell><cell>Kingdom</cell><cell>77.0</cell><cell>98.9</cell><cell>98.9</cell><cell>99.0</cell><cell>99.3</cell><cell>99.4</cell><cell>99.3</cell><cell>98.9</cell><cell></cell><cell>Kingdom</cell><cell>99.0</cell><cell>98.2</cell><cell>88.9</cell><cell>73.6</cell><cell>65.8</cell><cell>67.4</cell><cell>58.6</cell></row><row><cell>Baseline</cell><cell>Phylum Class Order Family</cell><cell>73.8 63.3 17.9 12.4</cell><cell>97.1 87.6 49.6 42.1</cell><cell>98.7 90.3 56.4 50.4</cell><cell>98.9 98.0 70.8 65.0</cell><cell>99.2 98.5 95.6 89.4</cell><cell>99.2 98.6 95.5 94.8</cell><cell>99.2 98.6 96.0 95.1</cell><cell>98.7 98.0 95.2 94.4</cell><cell>Baseline</cell><cell>Phylum Class Order Family</cell><cell>87.1 67.2 15.1 9.72</cell><cell>98.9 77.6 21.1 13.8</cell><cell>90.8 98.2 33.7 24.2</cell><cell>71.7 68.8 94.8 70.7</cell><cell>59.8 55.1 68.6 94.2</cell><cell>61.6 56.3 57.6 80.6</cell><cell>51.7 42.8 26.2 31.5</cell></row><row><cell></cell><cell>Genus</cell><cell>9.6</cell><cell>39.2</cell><cell>46.5</cell><cell>62.1</cell><cell>86.1</cell><cell>91.5</cell><cell>94.8</cell><cell>93.9</cell><cell></cell><cell>Genus</cell><cell>7.77</cell><cell>11.0</cell><cell>21.3</cell><cell>59.6</cell><cell>81.4</cell><cell>93.9</cell><cell>34.8</cell></row><row><cell></cell><cell>Species</cell><cell>1.5</cell><cell>9.8</cell><cell>13.5</cell><cell>20.6</cell><cell>34.5</cell><cell>39.9</cell><cell>42.4</cell><cell>75.0</cell><cell></cell><cell>Species</cell><cell>1.09</cell><cell>1.55</cell><cell>3.60</cell><cell>10.8</cell><cell>14.8</cell><cell>16.6</cell><cell>57.0</cell></row><row><cell></cell><cell>Kingdom</cell><cell>76.9</cell><cell>98.6</cell><cell>98.9</cell><cell>99.2</cell><cell>99.2</cell><cell>99.3</cell><cell>99.1</cell><cell>99.0</cell><cell></cell><cell>Kingdom</cell><cell>98.4</cell><cell>90.1</cell><cell>82.0</cell><cell>63.5</cell><cell>60.9</cell><cell>60.3</cell><cell>55.0</cell></row><row><cell></cell><cell>Phylum</cell><cell>73.3</cell><cell>87.1</cell><cell>98.8</cell><cell>99.1</cell><cell>99.1</cell><cell>99.1</cell><cell>98.9</cell><cell>99.0</cell><cell></cell><cell>Phylum</cell><cell>84.1</cell><cell>97.7</cell><cell>87.7</cell><cell>62.6</cell><cell>55.9</cell><cell>55.3</cell><cell>49.3</cell></row><row><cell>SNCA+</cell><cell>Class Order Family</cell><cell>62.3 17.6 12.2</cell><cell>74.9 19.7 12.7</cell><cell>84.1 30.2 20.7</cell><cell>98.2 55.4 45.5</cell><cell>98.6 95.3 88.2</cell><cell>98.3 95.2 94.5</cell><cell>98.1 95.2 94.6</cell><cell>97.8 94.2 93.5</cell><cell>SNCA+</cell><cell>Class Order Family</cell><cell>63.2 11.5 6.53</cell><cell>75.6 17.2 10.0</cell><cell>95.5 32.4 20.1</cell><cell>59.0 83.0 75.2</cell><cell>50.0 64.3 90.9</cell><cell>49.1 54.4 78.8</cell><cell>38.5 15.7 19.5</cell></row><row><cell></cell><cell>Genus</cell><cell>9.3</cell><cell>9.2</cell><cell>17.1</cell><cell>41.6</cell><cell>85.0</cell><cell>91.2</cell><cell>94.0</cell><cell>93.1</cell><cell></cell><cell>Genus</cell><cell>5.08</cell><cell>7.61</cell><cell>18.1</cell><cell>71.5</cell><cell>84.6</cell><cell>92.8</cell><cell>22.0</cell></row><row><cell></cell><cell>Species</cell><cell>1.3</cell><cell>1.0</cell><cell>1.8</cell><cell>10.4</cell><cell>36.0</cell><cell>40.8</cell><cell>42.3</cell><cell>74.7</cell><cell></cell><cell>Species</cell><cell>0.40</cell><cell>0.65</cell><cell>2.11</cell><cell>15.4</cell><cell>17.1</cell><cell>18.6</cell><cell>72.3</cell></row><row><cell></cell><cell>Kingdom</cell><cell>77.0</cell><cell>96.4</cell><cell>96.1</cell><cell>95.8</cell><cell>95.7</cell><cell>95.7</cell><cell>95.4</cell><cell>97.0</cell><cell></cell><cell>Kingdom</cell><cell>55.1</cell><cell>55.0</cell><cell>54.7</cell><cell>54.4</cell><cell>54.5</cell><cell>54.6</cell><cell>55.5</cell></row><row><cell>ClusterFit+</cell><cell>Phylum Class Order Family Genus</cell><cell>73.8 63.3 17.9 12.4 9.6</cell><cell>94.2 88.7 65.5 59.5 56.9</cell><cell>95.0 90.1 67.9 62.0 59.3</cell><cell>94.6 91.3 70.9 65.4 62.7</cell><cell>94.3 90.1 76.8 71.7 68.7</cell><cell>94.4 90.9 79.0 75.6 72.6</cell><cell>93.8 90.6 78.1 75.3 73.9</cell><cell>95.5 93.5 83.2 80.4 78.6</cell><cell>ClusterFit+</cell><cell>Phylum Class Order Family Genus</cell><cell>49.0 36.8 7.5 5.6 4.9</cell><cell>49.1 36.9 7.7 5.9 5.2</cell><cell>48.8 37.1 8.2 6.4 5.8</cell><cell>48.2 36.3 8.4 6.8 6.1</cell><cell>48.3 36.4 8.5 7.4 6.8</cell><cell>48.4 36.5 8.4 7.3 6.9</cell><cell>49.3 37.6 9.7 8.9 8.6</cell></row><row><cell></cell><cell>Species</cell><cell>1.5</cell><cell>24.5</cell><cell>25.6</cell><cell>27.3</cell><cell>31.1</cell><cell>33.6</cell><cell>33.9</cell><cell>49.6</cell><cell></cell><cell>Species</cell><cell>2.4</cell><cell>2.5</cell><cell>2.9</cell><cell>3.5</cell><cell>4.1</cell><cell>4.2</cell><cell>10.1</cell></row><row><cell></cell><cell>Kingdom</cell><cell>93.1</cell><cell>98.9</cell><cell>99.0</cell><cell>99.2</cell><cell>99.2</cell><cell>99.4</cell><cell>99.4</cell><cell>99.2</cell><cell></cell><cell>Kingdom</cell><cell>99.2</cell><cell>93.2</cell><cell>86.5</cell><cell>63.8</cell><cell>62.3</cell><cell>62.2</cell><cell>56.1</cell></row><row><cell>Grafit FC</cell><cell>Phylum Class Order Family</cell><cell>90.9 82.6 52.5 45.3</cell><cell>98.2 94.9 80.0 74.6</cell><cell>98.9 96.4 83.5 78.9</cell><cell>99.1 98.2 89.5 86.0</cell><cell>99.1 98.3 95.8 93.4</cell><cell>99.3 98.7 96.0 95.2</cell><cell>99.2 98.7 95.9 95.4</cell><cell>99.0 98.3 95.3 94.7</cell><cell>Grafit FC</cell><cell>Phylum Class Order Family</cell><cell>88.6 70.4 25.1 20.8</cell><cell>99.2 80.9 32.6 26.7</cell><cell>90.7 98.5 45.4 38.2</cell><cell>63.0 61.6 96.3 84.5</cell><cell>58.9 53.9 70.3 95.8</cell><cell>57.9 52.5 58.6 82.2</cell><cell>50.5 39.9 18.4 23.0</cell></row><row><cell></cell><cell>Genus</cell><cell>41.7</cell><cell>71.7</cell><cell>76.3</cell><cell>84.0</cell><cell>91.7</cell><cell>93.4</cell><cell>95.0</cell><cell>94.3</cell><cell></cell><cell>Genus</cell><cell>18.9</cell><cell>24.7</cell><cell>34.0</cell><cell>78.3</cell><cell>88.4</cell><cell>95.7</cell><cell>25.7</cell></row><row><cell></cell><cell>Species</cell><cell>12.0</cell><cell>29.5</cell><cell>32.6</cell><cell>40.4</cell><cell>51.8</cell><cell>53.2</cell><cell>53.9</cell><cell>75.9</cell><cell></cell><cell>Species</cell><cell>6.83</cell><cell>9.63</cell><cell>14.7</cell><cell>28.4</cell><cell>29.7</cell><cell>31.5</cell><cell>78.4</cell></row><row><cell></cell><cell>Kingdom</cell><cell>96.9</cell><cell>99.2</cell><cell>99.1</cell><cell>99.2</cell><cell>99.2</cell><cell>99.0</cell><cell>99.0</cell><cell>99.0</cell><cell></cell><cell>Kingdom</cell><cell>99.4</cell><cell>93.1</cell><cell>85.9</cell><cell>62.7</cell><cell>61.5</cell><cell>60.9</cell><cell>56.6</cell></row><row><cell></cell><cell>Phylum</cell><cell>96.4</cell><cell>98.8</cell><cell>98.9</cell><cell>99.0</cell><cell>99.0</cell><cell>98.9</cell><cell>98.9</cell><cell>98.7</cell><cell></cell><cell>Phylum</cell><cell>88.8</cell><cell>99.2</cell><cell>90.2</cell><cell>62.6</cell><cell>58.4</cell><cell>57.2</cell><cell>51.0</cell></row><row><cell>Grafit</cell><cell>Class Order Family</cell><cell>93.0 81.3 76.5</cell><cell>97.0 89.0 85.2</cell><cell>97.1 89.3 85.2</cell><cell>98.2 91.2 87.8</cell><cell>98.4 95.9 93.1</cell><cell>98.3 95.3 94.5</cell><cell>98.1 95.3 94.5</cell><cell>97.8 94.5 93.8</cell><cell>Grafit</cell><cell>Class Order Family</cell><cell>71.8 30.7 28.2</cell><cell>81.9 36.6 30.8</cell><cell>98.6 48.1 41.8</cell><cell>61.3 96.4 82.5</cell><cell>53.2 69.3 95.1</cell><cell>52.0 58.3 81.5</cell><cell>40.4 19.1 23.7</cell></row><row><cell></cell><cell>Genus</cell><cell>73.8</cell><cell>82.7</cell><cell>83.1</cell><cell>85.8</cell><cell>91.3</cell><cell>92.6</cell><cell>94.2</cell><cell>93.4</cell><cell></cell><cell>Genus</cell><cell>28.0</cell><cell>29.8</cell><cell>40.5</cell><cell>76.2</cell><cell>87.5</cell><cell>94.8</cell><cell>26.3</cell></row><row><cell></cell><cell>Species</cell><cell>31.0</cell><cell>41.6</cell><cell>41.4</cell><cell>46.0</cell><cell>51.8</cell><cell>53.5</cell><cell>55.3</cell><cell>75.3</cell><cell></cell><cell>Species</cell><cell>18.7</cell><cell>18.9</cell><cell>21.8</cell><cell>32.5</cell><cell>33.3</cell><cell>34.7</cell><cell>77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>Transfer learning task with various architectures pretrained on ImageNet with Grafit. We report the Top-1 accuracy (%) for the evaluation with a single center crop at resolution 224 × 224. 90.2 90.3 90.9 91.1 91.2 91.3 92.1 92.4 iNaturalist 2018 [30] 67.7 71.2 68.9 72.4 71.4 74.4 73.8 74.2 76.4 76.8 iNaturalist 2019 [31] 75.3 76.3 75.8 77.6 77.8 78.7 78.1 77.9 79.8 80.0</figDesc><table><row><cell></cell><cell>ResNeXt50-</cell><cell>32x4</cell><cell>[64]</cell><cell>ResNeXt50D-</cell><cell>32x4</cell><cell>[28]</cell><cell>ResNeXt50D-</cell><cell>32x8</cell><cell>[28]</cell><cell>RegNety-</cell><cell>4GF</cell><cell>[43]</cell><cell>RegNety-</cell><cell>8GF</cell><cell>[43]</cell></row><row><cell># Params</cell><cell cols="3">25M</cell><cell cols="3">25M</cell><cell cols="3">48M</cell><cell cols="3">21M</cell><cell cols="3">39M</cell></row><row><cell>Dataset</cell><cell cols="15">FC MLP FC MLP FC MLP FC MLP FC MLP</cell></row><row><cell>Flowers-102 [41]</cell><cell cols="15">95.5 98.3 95.9 98.6 96.3 98.7 98.1 98.6 99.0 98.8</cell></row><row><cell>Stanford Cars [35]</cell><cell cols="15">91.6 92.9 88.7 93.3 90.9 93.8 93.3 92.7 94.0 93.4</cell></row><row><cell>Food101 [7]</cell><cell cols="3">89.6 89.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Transfer learning with ResNet-50 pretrained on ImageNet. Comparison between different pre-training methods and two different classifiers trained on the target domain (a linear FC or an MLP). We report the top-1 accuracy (%) with a single center crop evaluation at resolution 224 × 224. 95.7 94.3 98.2 96.2 96.1 97.6 98.2 94.3 97.6 Stanford Cars [35] 90.0 89.8 91.6 92.5 89.4 89.3 91.4 92.5 91.4 92.7 Food101 [7] 88.2 88.9 88.7 88.8 88.5 88.9 88.9 89.5 88.5 88.7 iNaturalist 2018 [30] 65.0 68.4 64.7 69.2 64.2 67.5 65.6 69.8 65.2 68.5 iNaturalist 2019 [31] 72.8 73.7 73.1 74.5 71.8 73.8 74.1 75.9 73.9 74.6</figDesc><table><row><cell>Dataset</cell><cell cols="7">Baseline FC MLP FC MLP FC MLP FC MLP FC MLP SNCA+ ClusterFit+ Grafit Grafit FC</cell></row><row><cell cols="2">Flowers-102 [41] 96.2 Query↓ Rank 1 Rank 2 Rank 3</cell><cell>Rank 4</cell><cell>Rank 5</cell><cell>Rank 6</cell><cell>Rank 7</cell><cell>Rank 8</cell><cell>Rank 9</cell><cell>Rank 10</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grafit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grafit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grafit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">These examples are representative of typical comparisons, as we have not cherry-picked to show cases where our method is better.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greg Lasley: CC BY-NC 4.0, Robin Agarwal: CC BY-NC 4.0, Stefano Tito: CC BY-NC 4.0, Marion Zöller: CC BY-NC 4.0, Stefano Tito: CC BY-NC 4.0, Ronald Werson: CC</title>
		<idno>Pomeroy: CC BY-NC 4.0</idno>
	</analytic>
	<monogr>
		<title level="m">Authors:copyright for Figure 1 images from inaturalist-2018, top to down, left to right, employed for illustration of research work</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
	<note>Giuseppe Cagnetta: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0, martinswarren: CC BY-NC. 4.0, Donna Pomeroy: CC BY-NC 4.0, Robin Agarwal: CC BY-NC 4.0, Fernando de Juana: CC BY-NC 4.0, Giuseppe Cagnetta: CC BY-NC 4.0, Ronald Werson: CC BY-NC-ND 4.0, Marion Zöller: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, Ronald Werson: CC BY-NC-ND 4.0, Donna Pomeroy: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Marion Zöller: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, note = Accessed</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1777</idno>
		<title level="m">Neural codes for image retrieval</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05310</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Theory of classification: A survey of some recent advances. ESAIM: probability and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03673</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Tan Kiat Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrose</forename><surname>Yulong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tulig</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belongie</forename><surname>Melissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05372</idno>
		<title level="m">The herbarium challenge 2019 dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical automated data augmentation with a reduced search space</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Kumar Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10154</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Measuring dataset granularity. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Alexander D&amp;apos;amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moldovan</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Underspecification presents challenges for credibility in modern machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Hormozdiari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Mincu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Mitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Nielson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rory</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sayres</surname></persName>
		</author>
		<editor>Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Coarse2fine: A twostage training method for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Amir Erfan Eshratifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massoud</forename><surname>Gormish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02680</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database. Bradford Books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From region similarity to category discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Neighbourhood components analysis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cnn-rnn: a large-scale hierarchical image classification framework. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2017 dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2018 dataset</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2019 dataset</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep image category discovery using a transferred similarity function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01253</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">What makes imagenet good for transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi-Young</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>CIFAR</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hinton. Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep supervised t-distributed embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zineng</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bonner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Girshick, Kaiming He, and Piotr Dollár. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<title level="m">Metric learning with adaptive density discrimination. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From categories to subcategories: Largescale image classification with partial class label refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Ristin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A weakly supervised fine label classifier enhanced by coarse supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariborz</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasser</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Eu Wern Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<title level="m">Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis. European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Yfcc100m: the new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237</idno>
		<title level="m">Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrepancy: Fixefficientnet</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised image matching and object discovery as optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8279" to="8288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Toward unsupervised, multi-object discovery in large-scale image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02662,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A hierarchical loss and its problems when classifying non-hierarchically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cinna</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tygert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<title level="m">Self-training with noisy student improves imagenet classification</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Hyperclass augmented and regularized deep learning for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Ismet Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Kumar Mahajan</surname></persName>
		</author>
		<title level="m">Clusterfit: Improving generalization of visual representations. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cutmix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04899</idno>
		<title level="m">Regularization strategy to train strong classifiers with localizable features</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiaohua Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">CC BY-NC 4.0, Damon Tighe: CC BY-NC 4.0, Cathy Bell: CC BY-NC-ND 4.0, ONG OeBenin: CC BY-NC 4.0, ONG OeBenin: CC BY-NC 4.0, fiddleman: CC BY-NC 4.0, ONG OeBenin: CC BY-NC 4.0, Brian Gratwicke: CC BY 4.0, ONG OeBenin: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, gyrrlfalcon: CC BY-NC 4.0, Simon Kingston: CC BY-NC 4.0, Mark Freeman: CC BY-NC 4.0, Mike: CC BY-NC 4.0, tiyumq: CC BY-NC 4.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ong Oebenin</surname></persName>
		</author>
		<idno>Bailey: CC BY-NC 4.0, sliverman: CC BY-NC 4.0, Joanne Siderius: CC BY-NC 4.0, janaohrner: CC BY-NC 4.0</idno>
	</analytic>
	<monogr>
		<title level="m">Bruno Durand: CC BY-NC 4.0, Kyle Jones: CC BY-NC 4.0, Jakub Pełka: CC BY-NC 4.0, ekoaraba: CC BY-NC 4.0, Shane: CC BY-NC 4.0, dfwuw: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, gyrrlfalcon: CC BY-NC 4.0, dengland81: CC BY-NC 4.0, mafzam: CC BY-NC 4.0</title>
		<editor>CC BY-NC 4.0, ONG OeBenin: CC BY-NC 4.0, Damon Tighe: CC BY-NC 4.0, colinmorita: CC BY-NC 4.0, Bob Hislop: CC BY-NC 4.0, Skyla Slemp: CC BY-NC 4.0, Sam Kieschnick: CC BY-NC 4.0, Derek Broman: CC BY-NC 4.0, Donna Pomeroy</editor>
		<meeting><address><addrLine>Juan Carlos Pérez Magaña</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>ONG OeBenin: CC BY-NC 4.0, mcodellwildlife: CC BY-NC 4.0, Johnny Wilson: CC BY-NC 4.0, sea-kangaroo: CC BY-NC-ND 4.0, Brian Gratwicke: CC BY 4.0, 112692329998402018828: CC BY-NC-. SA 4.0, ONG OeBenin: CC BY-NC 4.0, summersilence: CC BY-NC 4.0, efarias1: CC BY-NC 4.0, redhat: CC BY-NC 4.0, Dale Hameister: CC BY-NC 4.0, 116916927065934112165: CC BY-NC-SA 4.0, sequeirajluis: CC BY-NC 4.0, Zac Cota: CC BY-NC 4.0, J Brew: CC BY-SA 4.0, Cullen Hanks: CC BY-NC 4.0, Allan Finlayson: CC BY-NC 4.0, dfwuw: CC BY-NC 4.0, Mark Freeman: CC BY-NC 4.0, mmski303: CC BY-NC 4.0, driles5: CC BY-NC 4.0, driles5: CC BY-NC 4.0, Marcus Garvie: CC BY-NC 4.0, ryanubrown: CC BY-NC-ND 4.0, efarias1: CC BY-NC 4.0, amarena: CC BY-NC 4.0, ttempel: CC BY-NC 4.0, tnewman: CC BY-NC 4.0, Juan Cruzado Cortés: CC BY-NC-SA 4.0, lonnyholmes: CC BY-NC 4.0, Audrey Kremer: CC BY-NC 4.0, texasblonde: CC BY-NC 4.0, Dale Hameister: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, Allan Finlayson: CC BY-NC 4.0, pfaucher: CC BY-NC 4.0, Edward George: CC BY-NC 4.0, Cullen Hanks: CC BY-NC 4.0, CK Kelly: CC BY 4.0, mustardlypig: CC BY-NC 4.0, ONG OeBenin: CC BY-NC 4.0, ONG OeBenin: CC BY-NC 4.0, ONG OeBenin: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, CK Kelly: CC BY 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, CK Kelly: CC BY 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, nighthawk0083: CC BY-NC 4.0, pfaucher: CC BY-NC 4.0, Johnny Wilson: CC BY-NC 4.0, ncowey: CC BY-NC 4.0, Johnny Wilson: CC BY-NC 4.0, redhat: CC BY-NC 4.0, ONG OeBenin: CC BY-NC 4.0, owlentine: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, Jakub Pełka: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, sequeirajluis: CC BY-NC 4.0, Robert J. &quot;Bob&quot; Nuelle, Jr. AICEZS: CC BY-NC 4.0</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Author and Creative Commons Copyright notice for images in Figure 8</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Giuseppe Cagnetta: CC BY-NC 4.0, cwwood: CC BY-SA 4.0, Donna Pomeroy: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, Fernando de Juana: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0, Monica Krancevic: CC BY-NC 4.0, brentano: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, stefanovet: CC BY-NC 4.0, Mark Rosenstein: CC BY-NC-SA 4.0, Mark Rosenstein: CC BY-NC-SA 4.0, Mark Nenadov: CC BY-NC 4.0, Robin Agarwal: CC BY-NC 4.0, Bryan: CC BY-NC 4.0, Ronald Werson: CC BY-NC-ND 4.0, Chris van Swaay: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Mark Rosenstein: CC</title>
		<idno>de Juana: CC BY-NC 4.0</idno>
	</analytic>
	<monogr>
		<title level="m">Giuseppe Cagnetta: CC BY-NC 4.0, Chris Evers: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Ronald Werson: CC BY-NC-ND 4.0, Donna Pomeroy: CC BY-NC 4.0, Monica Krancevic: CC BY-NC 4.0, smwhite: CC BY-NC 4.0, dwest: CC BY-NC 4.0, stefanovet: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, greglasley: CC BY-NC 4.0</title>
		<editor>CC BY-NC 4.0, Kent McFarland: CC BY-NC 4.0, Bruno Durand: CC BY-NC 4.0, José Belem Hernández Díaz: CC BY-NC 4.0, Nolan Eggert: CC BY-NC 4.0, Lee Elliott: CC BY-NC-SA 4.0, Ronald Werson: CC BY-NC-ND 4.0</editor>
		<meeting><address><addrLine>Liam O&apos;Brien; Steven; Donna; Donna; Donna</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Judith Lopez Sikora: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0. Ronald Werson: CC BY-NC-ND 4.0, Donna Pomeroy: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0, beschwar: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Fernando de Juana: CC BY-NC 4.0, Marion Zöller: CC BY-NC 4.0, Chuck Sexton: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0. Ronald Werson: CC BY-NC-ND 4.0, martinswarren: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Marion Zöller: CC BY-NC 4.0, Ronald Werson: CC BY-NC-ND 4.0, Chris van Swaay: CC BY-NC 4.0, Giuseppe Cagnetta: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, Robin Agarwal: CC BY-NC 4.0, Giuseppe Cagnetta: CC BY-NC 4.0, Marion Zöller: CC BY-NC 4.0, Ronald Werson: CC BY-NC-ND 4.0, Donna Pomeroy: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, stefanovet: CC BY-NC 4.0, Ronald Werson: CC BY-NC-ND 4.0, Ronald Werson: CC BY-NC-ND 4.0, martinswarren: CC BY-NC 4.0, Giuseppe Cagnetta: CC BY-NC 4.0, Fernando de Juana: CC BY-NC 4.0, Giuseppe Cagnetta: CC BY-NC 4.0, rada: CC BY-NC 4.0, Philip Mark Osso: CC BY-NC 4.0, Fernando de Juana: CC BY-NC 4.0, Giuseppe Cagnetta: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, Giuseppe Cagnetta: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0, Chris van Swaay: CC BY-NC 4.0, Giuseppe Cagnetta: CC BY-NC 4.0, rada: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, martinswarren: CC BY-NC 4.0, Fernando de Juana: CC BY-NC 4.0. Table 19: Author and Creative Commons Copyright notice for images in Figure 9</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Robin Agarwal: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Robin Agarwal: CC BY-NC 4.0, Robin Agarwal: CC BY-NC 4.0, David R: CC BY-NC-ND 4.0, summermule: CC BY-NC 4.0, slsfirefight: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, petecorradino: CC BY-NC 4.0, Mike Leveille: CC BY-NC 4.0, greglasley: CC BY-NC 4.0, tegmort: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Lou Justine ; R</forename><forename type="middle">J</forename><surname>Adams</surname></persName>
		</author>
		<idno>slsfirefight: CC BY-NC 4.0, 104623964081378888743: CC BY-NC-ND 4.0, Ken-ichi Ueda: CC BY-NC-SA 4.0</idno>
	</analytic>
	<monogr>
		<title level="m">jaliya: CC BY-NC 4.0, Chris Evers: CC BY-NC 4.0, Victor W Fazio III: CC BY-NC-ND 4.0, Chris Evers: CC BY-NC 4.0, J. Maughn: CC BY-NC 4.0, Andrew Cannizzaro: CC BY 4.0, 116916927065934112165: CC BY-NC-ND 4.0, gyrrlfalcon: CC BY-NC 4.0, kolasafamily: CC BY-NC 4.0, summermule: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0</title>
		<editor>J. Maughn: CC BY-NC 4.0, Tim Hite: CC BY 4.0, Mikael Behrens: CC BY-NC 4.0, colinmorita: CC BY-NC 4.0, Mary Joyce: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, icosahedron: CC BY 4.0, Donna Pomeroy: CC BY-NC 4.0, tnewman: CC BY-NC 4.0, phylocode: CC BY-NC 4.0, Marisa or Robin Agarwal: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Tom Benson</editor>
		<meeting><address><addrLine>Tom Benson; David J Barton; Donna; Donna; Donna; David J Barton</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Judith Lopez Sikora: CC BY-NC 4.0, Amy: CC BY-NC 4.0, Marisa or Robin Agarwal: CC BY-NC. 4.0, summermule: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Jennifer Rycenga: CC BY-NC 4.0, David J Barton: CC BY-NC 4.0, thylacine: CC BY-NC 4.0, greglasley: CC BY-NC 4.0, J. Maughn: CC BY-NC 4.0, Javier Solís: CC BY-NC 4.0, redhat: CC BY-NC 4.0, timputtre: CC BY-NC 4.0, icosahedron: CC BY 4.0, rbbrummitt: CC BY-NC 4.0, icosahedron: CC BY 4.0. Donna Pomeroy: CC BY-NC 4.0, sakuraisomi: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, Donna Pomeroy: CC BY-NC 4.0, J. Maughn: CC BY-NC 4.0, kestrel: CC BY-NC 4.0, BJ Stacey: CC BY-NC 4.0, summermule: CC BY-NC 4.0, thylacine: CC BY-NC 4.0, icosahedron: CC BY 4.0, KK: CC BY-NC 4.0, James Maughn: CC BY-NC 4.0, Javier Solís: CC BY-NC 4.0, rbbrummitt: CC BY-NC 4.0, J. Maughn: CC BY-NC 4.0, greglasley: CC BY-NC 4.0, greglasley: CC BY-NC 4.0, timputtre: CC BY-NC 4.0</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
