<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drone-based Object Counting by Spatially Regularized Regional Proposal Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Ru</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
							<email>yenlianglintw@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">GE Global Research</orgName>
								<address>
									<settlement>Niskayuna</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
							<email>whsu@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Drone-based Object Counting by Spatially Regularized Regional Proposal Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing counting methods often adopt regression-based approaches and cannot precisely localize the target objects, which hinders the further analysis (e.g., high-level understanding and fine-grained classification). In addition, most of prior work mainly focus on counting objects in static environments with fixed cameras. Motivated by the advent of unmanned flying vehicles (i.e., drones), we are interested in detecting and counting objects in such dynamic environments. We propose Layout Proposal Networks (LPNs) and spatial kernels to simultaneously count and localize target objects (e.g., cars) in videos recorded by the drone. Different from the conventional region proposal methods, we leverage the spatial layout information (e.g., cars often park regularly) and introduce these spatially regularized constraints into our network to improve the localization accuracy. To evaluate our counting method, we present a new large-scale car parking lot dataset (CARPK) that contains nearly 90,000 cars captured from different parking lots. To the best of our knowledge, it is the first and the largest drone view dataset that supports object counting, and provides the bounding box annotations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advent of unmanned flying vehicles, new potential applications emerge for unconstrained images and videos analysis for aerial view cameras. In this work, we address the counting problem for evaluating the number of objects (e.g., cars) in drone-based videos. Prior methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref> for monitoring the parking lot often assume that the locations of the monitored objects of a scene are already known in advance and the cameras are fixed, and cast car counting as a classification problem, which makes conventional car counting methods not directly applicable in unconstrained drone videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layout Proposal</head><p>Networks Drone-View Car Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counting number : 95 cars</head><p>Object Counting and Localization Drone videos <ref type="figure">Figure 1</ref>. We propose a Layout Proposal Network (LPN) to localize and count objects in drone videos. We introduce the spatial constraints for learning our network to improve the localization accuracy. Detailed network structure is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Current object counting methods often learn a regression model that maps the high-dimensional image space into non-negative counting numbers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref>. However, these methods can not generate precise object positions, which limits the further investigation and applications (e.g., recognition).</p><p>We observe that there exists certain layout patterns for a group of object instances, which can be utilized to improve the object counting accuracy. For example, cars are often parked in a row and animals are gathered in a certain layout (e.g., fish torus and duck swirl). In this paper, we introduce a novel Layout Proposal Network (LPN) that counts and localizes objects in drone videos <ref type="figure">(Figure 1</ref>). Different from existing object proposal methods, we introduce a new spatially regularized loss for learning our Layout Proposal Network. Note that our method learns the general adjacent relationship between object proposals and is not specific to a certain scene.</p><p>Our spatially regularized loss is a weighting scheme that <ref type="table">Table 1</ref>. Comparison of aerial view car-related datasets. In contrast to the PUCPR dataset, our dataset supports a counting task with bounding box annotations for all cars in a single scene. Most important of all, compared to other car datasets, our CARPK is the only dataset in drone-based scenes and also has a large enough number in order to provide sufficient training samples for deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Sensor Multi Scenes Resolution Annotation Format Car Numbers Counting Support OIRDS <ref type="bibr" target="#b27">[28]</ref> satellite low bounding box 180 VEDAI <ref type="bibr" target="#b19">[20]</ref> satellite low bounding box 2,950 COWC <ref type="bibr" target="#b17">[18]</ref> aerial low car center point 32,716 PUCPR <ref type="bibr" target="#b9">[10]</ref> camera   <ref type="bibr" target="#b27">[28]</ref>, VEDAI <ref type="bibr" target="#b19">[20]</ref>, COWC <ref type="bibr" target="#b17">[18]</ref>, PUCPR <ref type="bibr" target="#b9">[10]</ref>, and CARPK (ours) dataset respectively (two images for each dataset). Comparing to (a), (b), and (c), the PUCPR dataset and the CARPK dataset have greater number of cars in a single scene which is more appropriate for evaluating the counting task.</p><p>re-weights the importance scores for different object proposals and encourages region proposals to be placed in correct locations. It can also generally be embedded in any object detector system for object counting and detection. By exploiting spatial layout information, we improve the average recall of state-of-the-art region proposal approaches on a public PUCPR dataset <ref type="bibr" target="#b9">[10]</ref> (from 59.9% to 62.5%).</p><p>For evaluating the effectiveness and reliability of our approach, we introduce a new large-scale counting dataset CARPK <ref type="table">(Table 1)</ref>. Our dataset contains 89,777 cars, and provides bounding box annotations for each car. Also, we consider the sub-dataset PUCPR of PKLot <ref type="bibr" target="#b9">[10]</ref> which is the one that the scenes are closed to the aerial view in the PKLot dataset. Instead of a fixed camera view from a high story building ( <ref type="figure" target="#fig_0">Figure 2</ref>) in the PUCPR dataset, our new CARPK dataset provide the first and the largest-scale drone view parking lot dataset in unconstrained scenes. Besides, the PUCPR dataset can be only used in conjunction with a classification task, which classifies the pre-cropped images (car or not car) with given locations. Moreover, the PUCPR dataset only annotates partial region-of-interest parking ar-eas, and is therefore unable to support a counting task. Since our task is to count objects in images, we also annotate all cars in single full-image for the partial PUCPR dataset. The contents of our CARPK dataset are unscripted and diverse in various scenes for 4 different parking lots. To the best of our knowledge, our dataset is the first and the largest drone-based dataset that can support a counting task with manually labelled annotations for numerous cars in full images. The main contributions of this paper are summarized as follows:</p><p>1. To our knowledge, this is the first work that leverages spatial layout information for object region proposal. We improve the average recall of the state-of-the-art region proposal methods (i.e., 59.9% <ref type="bibr" target="#b21">[22]</ref> to 62.5%) on a public PUCPR dataset.</p><p>2. We introduce a new large-scale car parking lot dataset (CARPK) that contains nearly 90,000 cars in dronebased high resolution images recorded from the diverse scenes of parking lots. Most important of all, compared to other parking lot datasets, our CARPK dataset is the first and the largest dataset of parking lots that can support counting 1 .</p><p>3. We provide in-depth analyses for different decision choices of our region proposal method, and demonstrate that utilizing layout information can considerably reduce the proposals and improve the counting results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Counting</head><p>Most contemporary counting methods can be broadly divided into two categories. One is counting by regression method, the other is counting by detection instance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref>. Regression counters are usually a mapping of the high-dimension image space into non-negative counting numbers. Several methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> try to predict counts by using global regressors trained with low-level features. However, global regression methods ignore some constraints, such as the fact that people usually walk on the pavement and the size of instances. There are also a number of density regression-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5]</ref> which can estimate object counts by the density of a countable object and then aggregate over that density.</p><p>Recently, a wealth of works introduce deep learning into the crowd counting task. Instead of counting objects for constrained scenes in the preivous works, Zhang et al. <ref type="bibr" target="#b30">[31]</ref> address the problem of cross-scene crowd counting task, which is the weakness of the density estimation method in the past. Sindagi et al. <ref type="bibr" target="#b25">[26]</ref> incorporate global and local contextual information for better estimating the crowd counts. Mundhenk et al. <ref type="bibr" target="#b17">[18]</ref> evaluate the number of cars in a subspace of aerial imagery by extracting representations of image patches to approximate the appearance of object groups. Zhang et al. <ref type="bibr" target="#b31">[32]</ref> leverage FCN and LSTM to jointly estimate the vehicle density and counts in low resolution videos from city cameras. However, the regressionbased methods can not generate precise object positions, which seriously limits the further investigation and application (e.g., high-level understanding and fine-grained classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Proposals</head><p>Recent years have seen deep networks for region proposals developing well. Because detecting objects at several positions and scales during inference time requires a computationally demanding classifier, the best way to solve this problem is to look at a tiny subset of possible positions. A number of recent works prove that deep networks-based region proposal methods have surpassed the previous works <ref type="bibr" target="#b0">1</ref> The images and annotations of our CARPK and PUCPR+ are available at https://lafi.github.io/LPN/ <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b8">9]</ref>, which are based on the low-level cues, by a large margin.</p><p>DeepMask <ref type="bibr" target="#b18">[19]</ref>, which is developed for learning segmentation proposals, has, compared to Selective Search <ref type="bibr" target="#b28">[29]</ref>, ten times fewer proposals (100 v.s. 1000) at the same performance. The state-of-the-art object proposal method, Region Proposal Networks (RPNs) <ref type="bibr" target="#b21">[22]</ref>, has also shown that they just need 300 proposals and can surpass the result of 2000 proposals generated by <ref type="bibr" target="#b28">[29]</ref>. Other works like Multibox <ref type="bibr" target="#b26">[27]</ref> and Deepbox <ref type="bibr" target="#b32">[33]</ref> also have higher proposal recall with fewer number of region proposals than the previous works which are based on low-level cues. However, none of these region proposal methods have considered the spatial layout or the relation between recurring objects. Hence, we propose a Layout Proposal Networks (LPNs) that leverages thus structure information to achieve higher recall while using a smaller number of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>Since there is a lack of large standardized public datasets that contain numerous collections of cars in drone-based images, it has been difficult to create an automated counting system for deep learning models. For instance, OIRDS <ref type="bibr" target="#b27">[28]</ref> has merely 180 unique cars. The recent car-related dataset VEDAI <ref type="bibr" target="#b19">[20]</ref> has 2,950 cars, but these are still too few to utilize for the deep learners. A newer dataset COWC <ref type="bibr" target="#b17">[18]</ref> has 32,716 cars, but the resolutions of images remain low. It has only 24 to 48 pixels per car. Besides, rather than labelling in the format of bounding box, the annotation format is the center pixel point of a car which can not support further investigation, such as car model retrieval, statistics of brands of car, and exploring which kind of car most people will drive in the local area. Moreover, all above datasets are low resolution images and cannot provide detail informations for learning a fine-grained deep model. The problems of existing dataset are : 1) low resolution images which might harm the performance of the model trained on them and 2) less car numbers in the dataset which has the potential to cause overfitting during training a deep model.</p><p>Because existing datasets have these aforementioned problems, we have created a large-scale car parking lot dataset from drone view images, which are more appropriate to deep learning algorithms. It supports object counting, object localizing, and further investigations by providing the annotations in terms of bounding boxes. The most similar public dataset to ours, which also has the high resolution of car images, is the sub-dataset PUCPR of PKLot <ref type="bibr" target="#b9">[10]</ref>, which provides a view from the 10th floor of a building and therefore similar to drone view images to a certain degree. However, the PUCPR dataset can be only used in conjunction with a classification task, which classifies the precropped images (car or not car) with given locations. Moreover, this dataset has only annotated a portion of cars (100</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low probability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High probability</head><p>Neighbor cues <ref type="bibr">Figure 3</ref>. The key idea of our spatial layout scores. A predicted position which has more nearby cars can get higher confidence scores and has higher probability to be the position where the car is. certain parking spaces) from total 331 parking spaces in a single image, making it unable to support both counting and localizing tasks. Hence, we complete the annotations for all cars in a single image from the partial PUCPR dataset, called PUCPR+ dataset, which now has nearly 17,000 cars in total. Besides the incomplete annotation problem of the PUCPR, it has a fatal issue that their camera sensors are fixed and set in the same place, making the image scene of dataset completely the same -causing the deep learning model to encounter a dataset bias problem.</p><p>For this reason, we introduce a brand new dataset CARPK that the contents of our dataset are unscripted and diverse in various scenes for 4 different parking lots. Our dataset also contains approximately 90,000 cars in total with the view of drone. It is different from the view of camera from high story building in the PUCPR dataset. This is a large-scale dataset for car counting in the scenes of diverse parking lots. The image set is annotated by providing a bounding box per car. All labeled bounding boxes have been well recorded with the top-left points and the bottomright points. Cars located on the edge of the image are included as long as the marked region can be recognized and it is sure that the instance is a car. To the best of our knowledge, our dataset is the first and the largest drone viewbased parking lot dataset that can support counting with manually labeled annotations for a great amount of cars in a full-image. The details of dataset are listed in <ref type="table">Table 1</ref> and some examples are shown in <ref type="figure" target="#fig_0">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Our object counting system employs a region proposal module which takes regularized layout structure into account. It is a deep fully convolutional network that takes an image of arbitrary size as the input, and outputs the object-agnostic proposals which likely contain the instance. The entire system is a single unified framework for object counting ( <ref type="figure">Figure 1)</ref>. By leveraging the spatial information of the object of recurring instances, LPNs module is not only concerning the possible positions but also suggesting the object detection module which direction it should look at in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Layout Proposal Network</head><p>We observed that there exists certain layout patterns for a group of object instances, which can be used to predict objects that might appear adjacently in the same direction or near the same instances. Hence, we design a novel region proposal module that can leverage the structure layout and gather the confidence scores from nearby objects in certain directions ( <ref type="figure">Figure 3</ref>).</p><p>We comprehensively describe the designed network structure of LPNs ( <ref type="figure" target="#fig_2">Figure 4</ref>) as follows. Similar to RPNs <ref type="bibr" target="#b21">[22]</ref>, the network generates region proposals by sliding a small network over the shared convolutional feature map. It takes as input an 3 × 3 windows on last convolutional layer for reducing the representation dimensions, and then feeds features into two sibling 1 × 1 convolutional layers, where one is for localization and the other is for classifying whether the box belongs to foreground or background. The difference is that our loss function introduces the spatially regularized weights for the predicted boxes at each location. With the weights from spatial information, we minimize the loss of multi-task object function in networks. The loss function we use on each image is defined as:</p><formula xml:id="formula_0">L({u i }, {q i }, {p i }) = 1 N f g i K(c i , N * i ; u * i ) · L f g (u i , u * i ) + γ 1 N bg i L bg (q i , q * i ) + λ 1 N loc i L loc (u * i , p i , g * i )</formula><p>(1) where N f g and N bg are the normalized terms of the number matching default boxes for foreground and background. N loc is the same as N f g in that it only considers the number of foreground classes. The default box is marked u * i = 1 if the default box has an Intersection-over-Union (IoU) overlap higher than 0.7 with the ground-truth box, or the default box which has the highest IoU overlap with a groundtruth box; otherwise, it is marked q * i = 0 if the IoU overlap is lower than 0</p><formula xml:id="formula_1">.3. The L f g (u i , u * i ) = −log[u i u * i ] and L bg (q i , q * i ) = −log[(1 − q i )(1 − q * i )]</formula><p>are the negative loglikelihood that we want to minimize for true classes. Here, the i is the index of predicted boxes. In front of the foreground loss, K represents that we apply the spatially regularized weights for re-weighting the objective score of each predicted box. The weight is obtained by a Gaussian spatial  kernel for the center position c i of predicted box. It will give a rearranged weight according to the m neighbor groundtruth box centers, which are near to the c i . The real neighbor centers for c i are denoted as N * i = {c * 1 , ..., c * m } ∈ S ci , which fall inside the spatial window pixels size S on the input image. We use S = 255 in this paper to obtain a larger spatial range.</p><p>The L loc is the localization loss, which is a robust loss function <ref type="bibr" target="#b10">[11]</ref>. This term is only active for foreground predicted boxes (u * i = 1), otherwise 0. Similar to <ref type="bibr" target="#b21">[22]</ref>, we calculate the loss of offsets between the foreground predicted box p i and the ground truth box g i with their center position (x, y), width (w), and height (h) based on the default box (d).</p><formula xml:id="formula_2">L loc (u * i , p i , g * i ) = i∈f g v∈{x,y,w,h} u * i smooth L1 (p v i , g v * i ) (2) , where g v * i (similar to p v i ) is defined as below: g x * i = (g x i − d x i )/d w i , g w * i = log(g w i /d w i ), g y * i = (g y i − d y i )/d h i g h * i = log(g h i /d h i )<label>(3)</label></formula><p>In our experiment, we set γ and λ to be 1. Besides, in order to handle the small objects, instead of conv5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatial Pattern Score</head><p>Most of the objects of an instance exhibit a certain pattern between each other. For instance, cars will align in one direction on a parking lot and ships will hug the shore regularly. Even in biology, we can also find collective animal behavior that makes them look into a certain layout (e.g., fish torus, duck swirl, and ant mill). Hence, we introduce a method for re-weighting the region proposals in the training phase in an end-to-end manner. The proposed method can reduce the number of proposals in the inference phase for abating the computational cost of the counting and detection processes. It is especially important on embedded devices, such as the drone, to lower power consumption as the battery power only can provide the drone with energy to fly a mere 20 minutes.</p><p>For designing the pattern of layout, we apply different direction 2D Gaussian spatial kernels K (see Eq. 1) on the space of input images, where the center of the Gaussian kernel is the predicted box position c i . We compute the confidence weights over all positive predicted boxes. By incorporating the prior knowledge of layout from ground-truth, we can learn the weight for each predicted box. In Eq. 4, it illustrates that the spatial pattern score for predicted position c i is a summation of weights by the ground truth positions which are inside the S ci . We compute the score over the input triples (c i , N * i , u * i ):</p><formula xml:id="formula_3">K(c i , N * i , u * i ) = θ∈D m j∈N * i G(j; θ) if u * i = 1 1 otherwise,<label>(4)</label></formula><p>in which</p><formula xml:id="formula_4">G(j; θ) = α · e −( x θ j 2σ 2 x + y θ j 2σ 2 y ) ,<label>(5)</label></formula><p>is the 2D Gaussian spatial kernel that takes different rotated radius D = {θ 1 , ...θ r }, where we use r = 4 ranged from 0 to π. The coordinate tuple (x j , y j ) is the center position of jth ground-truth box in Eq. 5, and the coefficient α is the amplitude of the Gaussian function. All experiments use α = 1.</p><p>We only give the weights for the foreground predicted box c i where it is marked u * i = 1. By the means of aggregating weights from ground-truth boxes N * i in different direction kernels (Eq. 4), we can compute a summation of scores for taking various layout structures into account. It will give a higher probability to the object position, which has larger weight. Namely, the more similar objects of instances surrounding it, the more possible the predicted boxes are the same category of instances. Therefore, the predicted box collects the confidence from the same objects which are nearby ( <ref type="figure">Figure 3</ref>). By leveraging spatially regularized weights, we can learn a model for generating the region proposals where the objects of instance will appear with their own layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>In this section, we evaluate our approach on two different datasets. The PUCPR+ dataset, made from the sub-dataset of the public PKLot dataset <ref type="bibr" target="#b9">[10]</ref>, and the CARPK dataset are both used to estimate the validation of our proposed LPNs. Then, we evaluate our object counting model, which leverages the structure information on the PUCPR+ dataset and our CARPK dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setup</head><p>We implement our model on Caffe <ref type="bibr" target="#b12">[13]</ref>. For fairness in analyzing the effectiveness between different baseline methods, we implemented all of them based on the VGG-16 networks <ref type="bibr" target="#b24">[25]</ref> which contains 13 convolutional layers and 3 fully-connected layers. All the layer parameters of baselines and our proposed model are using the weights pretrained on ImageNet 2012 <ref type="bibr" target="#b23">[24]</ref>, followed by fine-tuning the models on our CARPK dataset or the PUCPR+ dataset depending on the experiments. We run our experiments on the environment of Linux workstation with Intel Xeon E5-2650 v3 2.3 GHz CPU, 128 GB memory, and one NVIDIA Tesla K80 GPU. Our multi-task joint training scheme takes approximately one day to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation of Region Proposal Methods</head><p>For evaluating the performance of our method LPNs, we use five-fold cross-validation on the PUCPR+ dataset to ensure that the same image would not appear across both training set and testing set. In order to better evaluate the recall while estimating localization accuracy, rather than reporting recall at particular IoU thresholds, we report the Average Recall (AR). It is an average of recall with IoU threshold t between 0.5 to 1, where AR = 1 t t i Recall(IoU t ). As the metric of recall at IoU of 0.5 is not predictive of detection accuracy, proposals with high recall but at low overlap are not effective for detection <ref type="bibr" target="#b11">[12]</ref>. Therefore, adopting the IoU range of the AR metric can simultaneously measure both proposal recall and localization accuracy to better predict the result of counting and localizing performance. We compare our proposal method LPNs against the state-of-the-art object proposal generator RPNs <ref type="bibr" target="#b21">[22]</ref> on the PUCPR+ dataset with different number of the object proposals. Our results are shown in <ref type="table" target="#tab_3">Table 2</ref>. It reveals that our proposal method LPNs, which leverages the regularized layout information, can achieve higher recall and surpass RPNs in the same number of proposals. The state-ofthe-art object proposal RPNs suffer from poor performance in average recall. We refer that the factors, which affect the performance, are upon on the inappropriate anchor size and the resolution of convolutional layer features. Hence, in the same manner, we apply the smaller anchor box size on RPNs on the conv4-3 layer, which is in the same setting as our approach. <ref type="table" target="#tab_3">Table 2</ref> shows that the RPNs utilize the small anchor size and the higher resolution feature map bring about a better improvement. It implies that the CNN model is not as powerful in scale variance as we thought when using inappropriate layers or unsuitable default box size for prediction. This experiment also shows that the performance of our proposed model LPNs with spatial regularized constraints still outperforms the revised RPNs (e.g., <ref type="bibr" target="#b13">14</ref>.1% better in 300 proposals and 8.42% better in 500 proposals). Besides, we also found that our method with spatial regularizer significantly performs better in the dense case 2 . The result indicates that the prior layout knowledge could <ref type="bibr" target="#b1">2</ref> We additionally divide the PUCPR+ dataset into dense and less dense cases. Our method has 16.30% large relative improvement compared to RPN-small in dense case, which is better than 8.27% for less dense case. Moreover, our method localizes the bounding box more precisely, i.e., our method achieves 64.4% recall in IoU at 0.7 which is almost 10% better than RPN-small 54.7% for 300 proposals. potentially benefit the outcome by giving the correct confidence score to the position of instances in images. For looking into the details of the effectiveness of our approach in region proposal, we also conduct the experiment on our CARPK dataset. In order to ensure that the same or the similar image scenes would not appear across both training and testing set, which would affect the observation of validation, we take 3 different scenes of the parking lot as training set and the remaining one scene of the parking lot as testing set. <ref type="table" target="#tab_4">Table 3</ref> reports the average recall of our methods, the state-of-the-art region proposal method RPNs, and the revised RPNs on CARPK dataset. In the experiment results, it comes as no surprise that by incorporating the additional layout prior information, our LPNs model boots both recall and localization accuracy of proposal method. Again, this result shows that the proposals generated by our approach are more effective and reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of Car Counting Accuracy</head><p>Since the goal of our proposed approach is to count the number of cars from drone view scenes, we compare our car counting system with three state-of-the-art methods which can also achieve the car counting task. A one-look regression-based counting method <ref type="bibr" target="#b17">[18]</ref> which is the up-todate method for estimating the number of cars in density object counting measure and two prominent object detection systems, Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> and YOLO <ref type="bibr" target="#b20">[21]</ref>, which have remarkable success in object detection task in recent years.</p><p>For fair comparison, all the methods are built based on a VGG-16 network <ref type="bibr" target="#b24">[25]</ref>. The only difference is that <ref type="bibr" target="#b17">[18]</ref> uses a softmax with 64 outputs as they assumed that the maximum number of cars in a scene is sufficiently small. However, the maximum number of cars in the CARPK dataset is far more than 64. The maximum number of cars is 331 in a single scene of the PUCPR+ dataset and 188 in a single scene of the CARPK dataset. Hence, we follow the setting from <ref type="bibr" target="#b17">[18]</ref> and train the network with a different output number to make this regression-based method compatible with the two datasets. We set the softmax to 400 outputs for the PUCPR+ dataset and 200 outputs for the CARPK dataset for evaluation. Last, the setting of two datasets, PUCPR+ and CARPK, are the same as the experiment of region proposal phase.</p><p>We employ two metrics, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), for evaluating the performance of counting methods. These two metrics have the similar physical meaning that estimates the error between the ground-truth car numbers y i and the predicted car numbers f i . MAE is the average absolute difference between ground-truth quantity y i and predicted quantity f i over all testing scenes where M AE = 1 n n i |f i − y i |. Similar, RMSE is the square root of the average of squared differences between ground-truth quantity and predicted quantity over all testing scenes where RM SE = 1 n n i (f i − y i ) 2 . The difference of the two metrics is that the RMSE should be more useful when large errors are particularly undesirable. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. In the counting task, these metrics have good physical meaning for representing the average counting error of cars in the scene. <ref type="table">Table 4</ref>. Comparison with the object detection methods and the global regression method for car counting on the PUCPR+ dataset. Np is the number of candidate boxes used in the object detector, which parameterizes the region proposal method. The " * " in front of the baseline methods represents that the method has been finetuned on PUCPR+ dataset. The " †" represents that the method is revised to fit our dataset. We compare three methods on the PUCPR+ dataset, where the maximum number of cars is 311 in a single scene. Since the softmax output number of <ref type="bibr" target="#b17">[18]</ref> is designed to be 400 for the PUCPR+ dataset, we also impartially compare to this dense object counting method with the number of region proposals limited to 400, which is a strict condition to our object counter. For YOLO, we select the parameter of confidence threshold at 0.15, which gives the best performance in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The experimental results are shown in <ref type="table">Table 4</ref>. The asterisk " * " in front of the YOLO and Faster R-CNN methods represents that the models have been fine-tuned on the PUCPR+ dataset, otherwise they are fine-tuned on the benchmark datasets (PASCAL VOC dataset and MS COCO dataset respectively), where they also have the car categories. Our proposed method outperforms the best RMSE on large-scale car counting, even with a very tough setting in the number of proposals. Note that we have comparable  MAE performance to the state-of-the-art car counting regression method <ref type="bibr" target="#b17">[18]</ref>, but the better RMSE implies that our method has better capability in some extreme cases. The methods that are fine-tuned on PASCAL and MS COCO get worse results. It reveals that the inferior outcomes are caused by the different perspective view of the object even when training with car category samples. The experiment results show that by incorporating the spatially regularized information, our Car Counting CNN model boosts the performance of counting. A counting and localization example result is shown in <ref type="figure" target="#fig_4">Figure 5</ref> (left). We further compare the counting methods on our challenging large-scale CARPK dataset where the maximum number of cars is 188 in a single scene. However, different from the PUCPR+ dataset which only has one parking lot, our CARPK dataset provides various scenes of diverse parking lots for cross-scene evaluation. In the setting of <ref type="bibr" target="#b17">[18]</ref> method, we also deign a 200 softmax output network for the CARPK dataset. In order to fairly compare the counting methods, we again restrict the number of proposals of object counter which has utilized the region proposal method with a tough number 200 3 . The quantitative results of car counting on our dataset are reported in <ref type="table" target="#tab_6">Table 5</ref>. The experiment results show that our car counting approach is reliably effective and has the best MAE and RMSE even in the cross-scene estimation. An counting and localizing example result is shown in <ref type="figure" target="#fig_4">Figure 5</ref> (right). Still our method can generate the feasible proposals and obtain the reasonable counting result close to the real number of cars in the <ref type="bibr" target="#b2">3</ref> Our method gets better performance when using bigger number of proposals (e.g., <ref type="bibr" target="#b7">8</ref>.04 and 12.06 for 1000 proposals in MAE and RMSE respectively) in the PUCPR+ dataset. In the CARPK dataset, our method also has 13.72 and 21.77 for 1000 proposals in MAE and RMSE respectively. scenes of the parking lots. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have created the to-date largest drone view dataset, called CARPK. It is a challenging dataset for various scenes of parking lots in a large-scale car counting task. Also, in the paper, we introduced a new way for generating the feasible region proposals, which leverage the spatial layout information for an object counting task with regularized structures. The learned deep model can specifically count objects better with the prior knowledge of object layout patterns. Our future work will involve global information, such as context, road scene, and other objects which can help distinguish between false car-like instances and real cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work was supported in part by the Ministry of Science and Technology, Taiwan, under Grant MOST 104-2622-8-002-002 and MOST 105-2218-E-002-032, and in part by MediaTek Inc, and grants from NVIDIA and the NVIDIA DGX-1 AI Supercomputer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a), (b), (c), (d), and (e) are the example scenes of OIRDS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The structure of the Layout Proposal Networks. At the loss layer, the structure weights are integrated for re-weighting the candidates to have better structure proposals. See more details in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Selected examples of car counting and localizing results on the PUCPR+ dataset (left) and the CARPK dataset (right). The counting model uses our proposed LPN trained on a VGG-16 model and combined with an object detector (Fast R-CNN), where the parameters setting of confidence score is 0.5 and non maximum suppression (NMS) is 0.3 for 2000 proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Result on the PUCPR+<ref type="bibr" target="#b9">[10]</ref> dataset for average recall at 100, 300, 500, 700, and 1000 proposals with the different components of approaches. The method in the middle column represents the RPN training with the small default box size on conv4-3 layer.</figDesc><table><row><cell cols="4">#Proposals RPN [22] RPN+small LPN (ours)</cell></row><row><cell>100</cell><cell>3.2%</cell><cell>20.5%</cell><cell>23.1%</cell></row><row><cell>300</cell><cell>9.1%</cell><cell>43.2%</cell><cell>49.3%</cell></row><row><cell>500</cell><cell>13.9%</cell><cell>53.4%</cell><cell>57.9%</cell></row><row><cell>700</cell><cell>17.4%</cell><cell>57.3%</cell><cell>60.7%</cell></row><row><cell>1000</cell><cell>21.2%</cell><cell>59.9%</cell><cell>62.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results on the CARPK dataset with different components.</figDesc><table><row><cell cols="4">#Proposals RPN [22] RPN+small LPN (ours)</cell></row><row><cell>100</cell><cell>11.4%</cell><cell>31.1%</cell><cell>34.7%</cell></row><row><cell>300</cell><cell>27.9%</cell><cell>46.5%</cell><cell>51.2%</cell></row><row><cell>500</cell><cell>34.3%</cell><cell>50.0%</cell><cell>54.5%</cell></row><row><cell>700</cell><cell>37.4%</cell><cell>51.8%</cell><cell>56.1%</cell></row><row><cell>1000</cell><cell>39.2%</cell><cell>53.4%</cell><cell>57.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison results on the CARPK dataset. The notation definition is similar toTable 4.</figDesc><table><row><cell>Method</cell><cell>N p</cell><cell cols="2">MAE RMSE</cell></row><row><cell>YOLO [21]</cell><cell>-</cell><cell cols="2">102.89 110.02</cell></row><row><cell>Faster R-CNN [22]</cell><cell cols="3">200 103.48 110.64</cell></row><row><cell>*YOLO</cell><cell>-</cell><cell>48.89</cell><cell>57.55</cell></row><row><cell>*Faster R-CNN</cell><cell cols="2">200 47.45</cell><cell>57.39</cell></row><row><cell>*Faster R-CNN (RPN-small)</cell><cell cols="2">200 24.32</cell><cell>37.62</cell></row><row><cell>†One-Look Regression [18]</cell><cell>-</cell><cell>59.46</cell><cell>66.84</cell></row><row><cell cols="3">Our Car Counting CNN Model 200 23.80</cell><cell>36.79</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast classification of empty and occupied parking spaces using integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahrnbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Astrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Car parking occupancy detection using smart camera networks and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Carrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vairo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition using kernel ridge regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pklot-a robust dataset for parking lot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>De Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Britto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Koerich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals? TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aerial car detection and urban understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kamenetsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DICTA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A viewpoint invariant approach for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic car counting method for unmanned aerial vehicle images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moranduzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vehicle detection in aerial imagery: a small target detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Razakarivony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Density-aware person detection and tracking in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">high-quality object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overhead imagery research data setan annotated data library &amp; tools to aid in the development of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Colder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eppolito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Carlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oertel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sallee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
